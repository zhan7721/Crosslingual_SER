Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
------------------NEXT SCRIPT: RUNNER_DE, former setting----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911928/matplotlib-qgtlg44p because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on de =======================

Load dataset: 
Loading de train data: fold_0...
Preprocess de fold_0 data for de model
Loading cn eval data: fold_0...
Preprocess cn fold_0 data for de model
Loading cn test data: fold_0...
Preprocess cn fold_0 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1272.9351, Training accuracy: 0.6656
Macro F1-score: 0.6715
Model performance on Angry speech (in training): 
	Precision: 0.8960, Recall: 0.4525, F1_score: 0.6013
Model performance on Happy speech (in training): 
	Precision: 0.4367, Recall: 0.7325, F1_score: 0.5472
Model performance on Neutral speech (in training): 
	Precision: 0.6916, Recall: 0.5775, F1_score: 0.6294
Model performance on Sad speech (in training): 
	Precision: 0.9160, Recall: 0.9000, F1_score: 0.9079

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:53<23:50:50, 53.69s/it]Training:   7%|▋         | 106/1600 [01:03<11:13,  2.22it/s] Training:  14%|█▍        | 223/1600 [01:13<05:12,  4.40it/s]Training:  22%|██▏       | 351/1600 [01:23<03:12,  6.48it/s]Training:  31%|███       | 497/1600 [01:33<02:08,  8.60it/s]Training:  41%|████      | 653/1600 [01:43<01:30, 10.50it/s]Training:  51%|█████     | 816/1600 [01:53<01:04, 12.12it/s]Training:  62%|██████▏   | 997/1600 [02:03<00:43, 13.81it/s]Training:  74%|███████▍  | 1180/1600 [02:13<00:27, 15.10it/s]Training:  86%|████████▌ | 1373/1600 [02:23<00:13, 16.31it/s]Training:  98%|█████████▊| 1567/1600 [02:33<00:01, 17.20it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 431.2556, Validation accuracy: 0.3300
Macro F1-score: 0.2759
Model performance on Angry speech (in validation): 
	Precision: 0.4500, Recall: 0.9000, F1_score: 0.6000
Model performance on Happy speech (in validation): 
	Precision: 0.0976, Recall: 0.1600, F1_score: 0.1212
Model performance on Neutral speech (in validation): 
	Precision: 0.7222, Recall: 0.2600, F1_score: 0.3824
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
New best accuracy for layer 4 on epoch 1: 0.3300. Model saved.
Epoch 2/100

Training Phase:
Training loss: 398.2102, Training accuracy: 0.9337
Macro F1-score: 0.9340
Model performance on Angry speech (in training): 
	Precision: 0.9564, Recall: 0.9325, F1_score: 0.9443
Model performance on Happy speech (in training): 
	Precision: 0.8810, Recall: 0.9250, F1_score: 0.9024
Model performance on Neutral speech (in training): 
	Precision: 0.9262, Recall: 0.9100, F1_score: 0.9180
Model performance on Sad speech (in training): 
	Precision: 0.9748, Recall: 0.9675, F1_score: 0.9711

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 206/1600 [00:10<01:08, 20.50it/s]Training:  26%|██▌       | 411/1600 [00:20<00:58, 20.31it/s]Training:  26%|██▌       | 411/1600 [00:30<00:58, 20.31it/s]Training:  38%|███▊      | 611/1600 [00:30<00:49, 20.00it/s]Training:  50%|█████     | 808/1600 [00:40<00:39, 19.83it/s]Training:  63%|██████▎   | 1006/1600 [00:50<00:29, 19.82it/s]Training:  75%|███████▌  | 1204/1600 [01:00<00:20, 19.58it/s]Training:  88%|████████▊ | 1402/1600 [01:10<00:10, 19.63it/s]Training: 100%|██████████| 1600/1600 [01:20<00:00, 19.60it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 521.9607, Validation accuracy: 0.2950
Macro F1-score: 0.2238
Model performance on Angry speech (in validation): 
	Precision: 0.3833, Recall: 0.9200, F1_score: 0.5412
Model performance on Happy speech (in validation): 
	Precision: 0.0597, Recall: 0.0800, F1_score: 0.0684
Model performance on Neutral speech (in validation): 
	Precision: 0.6923, Recall: 0.1800, F1_score: 0.2857
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 3/100

Training Phase:
Training loss: 179.6773, Training accuracy: 0.9694
Macro F1-score: 0.9694
Model performance on Angry speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Happy speech (in training): 
	Precision: 0.9504, Recall: 0.9575, F1_score: 0.9539
Model performance on Neutral speech (in training): 
	Precision: 0.9697, Recall: 0.9600, F1_score: 0.9648
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 197/1600 [00:10<01:11, 19.61it/s]Training:  25%|██▍       | 394/1600 [00:20<01:01, 19.51it/s]Training:  37%|███▋      | 590/1600 [00:30<00:51, 19.52it/s]Training:  49%|████▉     | 786/1600 [00:40<00:41, 19.47it/s]Training:  61%|██████▏   | 981/1600 [00:50<00:31, 19.45it/s]Training:  74%|███████▍  | 1181/1600 [01:00<00:21, 19.64it/s]Training:  86%|████████▋ | 1382/1600 [01:10<00:11, 19.78it/s]Training:  99%|█████████▉| 1583/1600 [01:20<00:00, 19.75it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 678.3624, Validation accuracy: 0.2500
Macro F1-score: 0.1806
Model performance on Angry speech (in validation): 
	Precision: 0.4265, Recall: 0.5800, F1_score: 0.4915
Model performance on Happy speech (in validation): 
	Precision: 0.1591, Recall: 0.4200, F1_score: 0.2308
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 4/100

Training Phase:
Training loss: 119.0237, Training accuracy: 0.9788
Macro F1-score: 0.9788
Model performance on Angry speech (in training): 
	Precision: 0.9798, Recall: 0.9725, F1_score: 0.9762
Model performance on Happy speech (in training): 
	Precision: 0.9556, Recall: 0.9675, F1_score: 0.9615
Model performance on Neutral speech (in training): 
	Precision: 0.9849, Recall: 0.9775, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.48it/s]Training:  24%|██▍       | 390/1600 [00:20<01:02, 19.48it/s]Training:  37%|███▋      | 586/1600 [00:30<00:51, 19.51it/s]Training:  49%|████▉     | 782/1600 [00:40<00:41, 19.52it/s]Training:  61%|██████    | 978/1600 [00:50<00:31, 19.50it/s]Training:  74%|███████▎  | 1179/1600 [01:00<00:21, 19.68it/s]Training:  86%|████████▋ | 1383/1600 [01:10<00:10, 19.89it/s]Training:  99%|█████████▉| 1588/1600 [01:20<00:00, 20.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 619.6008, Validation accuracy: 0.2150
Macro F1-score: 0.1192
Model performance on Angry speech (in validation): 
	Precision: 0.2941, Recall: 0.1000, F1_score: 0.1493
Model performance on Happy speech (in validation): 
	Precision: 0.2088, Recall: 0.7600, F1_score: 0.3276
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 5/100

Training Phase:
Training loss: 121.0541, Training accuracy: 0.9769
Macro F1-score: 0.9769
Model performance on Angry speech (in training): 
	Precision: 0.9873, Recall: 0.9700, F1_score: 0.9786
Model performance on Happy speech (in training): 
	Precision: 0.9558, Recall: 0.9725, F1_score: 0.9641
Model performance on Neutral speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 201/1600 [00:10<01:09, 20.05it/s]Training:  25%|██▌       | 402/1600 [00:20<01:01, 19.53it/s]Training:  38%|███▊      | 601/1600 [00:30<00:50, 19.68it/s]Training:  50%|█████     | 801/1600 [00:40<00:40, 19.78it/s]Training:  50%|█████     | 801/1600 [00:50<00:40, 19.78it/s]Training:  62%|██████▏   | 999/1600 [00:50<00:30, 19.69it/s]Training:  75%|███████▌  | 1201/1600 [01:00<00:20, 19.85it/s]Training:  88%|████████▊ | 1403/1600 [01:10<00:09, 19.79it/s]Training: 100%|██████████| 1600/1600 [01:21<00:00, 19.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 889.6910, Validation accuracy: 0.2000
Macro F1-score: 0.1436
Model performance on Angry speech (in validation): 
	Precision: 0.3735, Recall: 0.6200, F1_score: 0.4662
Model performance on Happy speech (in validation): 
	Precision: 0.0776, Recall: 0.1800, F1_score: 0.1084
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 6/100

Training Phase:
Training loss: 97.1997, Training accuracy: 0.9806
Macro F1-score: 0.9807
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9775, F1_score: 0.9849
Model performance on Happy speech (in training): 
	Precision: 0.9606, Recall: 0.9750, F1_score: 0.9677
Model performance on Neutral speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.47it/s]Training:  25%|██▍       | 397/1600 [00:20<01:00, 19.85it/s]Training:  37%|███▋      | 599/1600 [00:30<00:50, 19.67it/s]Training:  50%|████▉     | 794/1600 [00:40<00:41, 19.45it/s]Training:  62%|██████▏   | 992/1600 [00:50<00:31, 19.55it/s]Training:  74%|███████▍  | 1191/1600 [01:00<00:20, 19.66it/s]Training:  87%|████████▋ | 1392/1600 [01:10<00:10, 19.78it/s]Training: 100%|█████████▉| 1594/1600 [01:20<00:00, 19.91it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 792.9871, Validation accuracy: 0.2050
Macro F1-score: 0.1400
Model performance on Angry speech (in validation): 
	Precision: 0.3579, Recall: 0.6800, F1_score: 0.4690
Model performance on Happy speech (in validation): 
	Precision: 0.0673, Recall: 0.1400, F1_score: 0.0909
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 7/100

Training Phase:
Training loss: 67.3532, Training accuracy: 0.9844
Macro F1-score: 0.9844
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9725, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.93it/s]Training:  25%|██▌       | 403/1600 [00:20<00:59, 20.15it/s]Training:  38%|███▊      | 606/1600 [00:30<00:49, 20.20it/s]Training:  51%|█████     | 809/1600 [00:40<00:39, 20.10it/s]Training:  63%|██████▎   | 1010/1600 [00:50<00:29, 20.09it/s]Training:  76%|███████▌  | 1211/1600 [01:00<00:19, 20.05it/s]Training:  88%|████████▊ | 1411/1600 [01:10<00:09, 20.02it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 1033.8356, Validation accuracy: 0.2250
Macro F1-score: 0.1494
Model performance on Angry speech (in validation): 
	Precision: 0.3871, Recall: 0.2400, F1_score: 0.2963
Model performance on Happy speech (in validation): 
	Precision: 0.1953, Recall: 0.6600, F1_score: 0.3014
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 8/100

Training Phase:
Training loss: 76.6924, Training accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in training): 
	Precision: 0.9898, Recall: 0.9750, F1_score: 0.9824
Model performance on Happy speech (in training): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 202/1600 [00:10<01:09, 20.17it/s]Training:  25%|██▌       | 404/1600 [00:20<00:59, 20.07it/s]Training:  38%|███▊      | 607/1600 [00:30<00:49, 20.17it/s]Training:  51%|█████     | 810/1600 [00:40<00:39, 19.91it/s]Training:  63%|██████▎   | 1006/1600 [00:50<00:30, 19.80it/s]Training:  75%|███████▌  | 1205/1600 [01:00<00:19, 19.81it/s]Training:  88%|████████▊ | 1404/1600 [01:10<00:09, 19.69it/s]Training: 100%|█████████▉| 1599/1600 [01:20<00:00, 19.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 977.6966, Validation accuracy: 0.2250
Macro F1-score: 0.1593
Model performance on Angry speech (in validation): 
	Precision: 0.3830, Recall: 0.3600, F1_score: 0.3711
Model performance on Happy speech (in validation): 
	Precision: 0.1765, Recall: 0.5400, F1_score: 0.2660
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 9/100

Training Phase:
Training loss: 61.3104, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.36it/s]Training:  24%|██▍       | 390/1600 [00:20<01:02, 19.45it/s]Training:  37%|███▋      | 590/1600 [00:30<00:51, 19.69it/s]Training:  50%|████▉     | 793/1600 [00:40<00:40, 19.92it/s]Training:  62%|██████▏   | 996/1600 [00:50<00:30, 19.98it/s]Training:  75%|███████▍  | 1197/1600 [01:00<00:20, 19.76it/s]Training:  87%|████████▋ | 1395/1600 [01:10<00:10, 19.76it/s]Training: 100%|█████████▉| 1597/1600 [01:20<00:00, 19.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 898.1256, Validation accuracy: 0.1650
Macro F1-score: 0.1176
Model performance on Angry speech (in validation): 
	Precision: 0.2200, Recall: 0.2200, F1_score: 0.2200
Model performance on Happy speech (in validation): 
	Precision: 0.1419, Recall: 0.4200, F1_score: 0.2121
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 10/100

Training Phase:
Training loss: 41.1999, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.34it/s]Training:  24%|██▍       | 389/1600 [00:20<01:02, 19.38it/s]Training:  37%|███▋      | 585/1600 [00:30<00:52, 19.47it/s]Training:  49%|████▉     | 783/1600 [00:40<00:41, 19.55it/s]Training:  61%|██████▏   | 982/1600 [00:50<00:31, 19.65it/s]Training:  74%|███████▍  | 1182/1600 [01:00<00:21, 19.74it/s]Training:  87%|████████▋ | 1388/1600 [01:10<00:10, 20.02it/s]Training: 100%|█████████▉| 1597/1600 [01:20<00:00, 20.27it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 1093.2247, Validation accuracy: 0.1950
Macro F1-score: 0.1088
Model performance on Angry speech (in validation): 
	Precision: 0.1923, Recall: 0.1000, F1_score: 0.1316
Model performance on Happy speech (in validation): 
	Precision: 0.1954, Recall: 0.6800, F1_score: 0.3036
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 11/100

Training Phase:
Training loss: 40.6717, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 201/1600 [00:10<01:09, 20.03it/s]Training:  25%|██▌       | 402/1600 [00:20<00:59, 19.98it/s]Training:  38%|███▊      | 602/1600 [00:30<00:50, 19.92it/s]Training:  50%|█████     | 801/1600 [00:40<00:40, 19.80it/s]Training:  62%|██████▎   | 1000/1600 [00:50<00:30, 19.80it/s]Training:  75%|███████▍  | 1198/1600 [01:00<00:20, 19.71it/s]Training:  88%|████████▊ | 1403/1600 [01:10<00:09, 19.94it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 1013.8379, Validation accuracy: 0.1900
Macro F1-score: 0.1229
Model performance on Angry speech (in validation): 
	Precision: 0.2632, Recall: 0.2000, F1_score: 0.2273
Model performance on Happy speech (in validation): 
	Precision: 0.1728, Recall: 0.5600, F1_score: 0.2642
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.3300

Test Phase: 
Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   2%|▏         | 3/200 [00:00<00:09, 21.31it/s]Testing:   3%|▎         | 6/200 [00:00<00:09, 20.55it/s]Testing:   4%|▍         | 9/200 [00:00<00:09, 19.67it/s]Testing:   6%|▌         | 11/200 [00:00<00:09, 19.61it/s]Testing:   6%|▋         | 13/200 [00:00<00:09, 19.02it/s]Testing:   8%|▊         | 15/200 [00:00<00:10, 18.24it/s]Testing:  10%|▉         | 19/200 [00:00<00:07, 24.08it/s]Testing:  12%|█▎        | 25/200 [00:01<00:05, 31.46it/s]Testing:  14%|█▍        | 29/200 [00:01<00:05, 33.52it/s]Testing:  16%|█▋        | 33/200 [00:01<00:04, 34.34it/s]Testing:  18%|█▊        | 37/200 [00:01<00:04, 35.73it/s]Testing:  20%|██        | 41/200 [00:01<00:04, 36.60it/s]Testing:  24%|██▍       | 48/200 [00:01<00:03, 44.44it/s]Testing:  28%|██▊       | 57/200 [00:01<00:02, 55.85it/s]Testing:  32%|███▏      | 63/200 [00:01<00:02, 55.49it/s]Testing:  36%|███▌      | 71/200 [00:01<00:02, 57.62it/s]Testing:  40%|███▉      | 79/200 [00:02<00:01, 63.52it/s]Testing:  43%|████▎     | 86/200 [00:02<00:01, 61.10it/s]Testing:  46%|████▋     | 93/200 [00:02<00:01, 63.32it/s]Testing:  50%|█████     | 100/200 [00:02<00:01, 62.93it/s]Testing:  54%|█████▍    | 108/200 [00:02<00:01, 65.48it/s]Testing:  57%|█████▊    | 115/200 [00:02<00:01, 65.68it/s]Testing:  62%|██████▏   | 123/200 [00:02<00:01, 69.62it/s]Testing:  66%|██████▌   | 131/200 [00:02<00:01, 64.55it/s]Testing:  70%|██████▉   | 139/200 [00:02<00:00, 68.61it/s]Testing:  74%|███████▍  | 148/200 [00:03<00:00, 70.31it/s]Testing:  78%|███████▊  | 156/200 [00:03<00:00, 70.08it/s]Testing:  82%|████████▏ | 164/200 [00:03<00:00, 69.14it/s]Testing:  86%|████████▋ | 173/200 [00:03<00:00, 73.55it/s]Testing:  91%|█████████ | 182/200 [00:03<00:00, 76.98it/s]Testing:  96%|█████████▌| 191/200 [00:03<00:00, 80.15it/s]                                                          /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 432.1993, Test accuracy: 0.3300
Macro F1-score: 0.2748
Model performance on Angry speech (in test): 
	Precision: 0.4369, Recall: 0.9000, F1_score: 0.5882
Model performance on Happy speech (in test): 
	Precision: 0.1000, Recall: 0.1600, F1_score: 0.1231
Model performance on Neutral speech (in test): 
	Precision: 0.7647, Recall: 0.2600, F1_score: 0.3881
Model performance on Sad speech (in test): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000

======================= This is fold_1 on de =======================

Load dataset: 
Loading de train data: fold_1...
Preprocess de fold_1 data for de model
Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 1/1600 [00:00<05:02,  5.29 examples/s]Map:   0%|          | 6/1600 [00:00<01:10, 22.67 examples/s]Map:   1%|          | 18/1600 [00:00<00:32, 48.91 examples/s]Map:   2%|▏         | 26/1600 [00:00<00:28, 56.19 examples/s]Map:   2%|▏         | 34/1600 [00:00<00:25, 62.12 examples/s]Map:   3%|▎         | 44/1600 [00:00<00:22, 70.40 examples/s]Map:   3%|▎         | 54/1600 [00:00<00:20, 76.25 examples/s]Map:   4%|▍         | 63/1600 [00:01<00:19, 78.25 examples/s]Map:   4%|▍         | 71/1600 [00:01<00:19, 76.88 examples/s]Map:   5%|▌         | 81/1600 [00:01<00:18, 80.88 examples/s]Map:   6%|▌         | 90/1600 [00:01<00:21, 71.10 examples/s]Map:   6%|▋         | 101/1600 [00:01<00:21, 68.91 examples/s]Map:   7%|▋         | 110/1600 [00:01<00:20, 72.49 examples/s]Map:   7%|▋         | 119/1600 [00:01<00:19, 74.39 examples/s]Map:   8%|▊         | 128/1600 [00:01<00:19, 75.04 examples/s]Map:   8%|▊         | 136/1600 [00:02<00:19, 74.72 examples/s]Map:   9%|▉         | 145/1600 [00:02<00:19, 75.35 examples/s]Map:  10%|▉         | 153/1600 [00:02<00:19, 74.01 examples/s]Map:  10%|█         | 161/1600 [00:02<00:19, 73.43 examples/s]Map:  11%|█         | 169/1600 [00:02<00:20, 70.19 examples/s]Map:  11%|█         | 178/1600 [00:02<00:19, 71.48 examples/s]Map:  12%|█▏        | 187/1600 [00:02<00:22, 63.47 examples/s]Map:  12%|█▏        | 195/1600 [00:02<00:21, 66.39 examples/s]Map:  13%|█▎        | 204/1600 [00:02<00:19, 70.35 examples/s]Map:  13%|█▎        | 212/1600 [00:03<00:19, 69.60 examples/s]Map:  14%|█▍        | 221/1600 [00:03<00:19, 72.11 examples/s]Map:  14%|█▍        | 231/1600 [00:03<00:17, 77.26 examples/s]Map:  15%|█▌        | 240/1600 [00:03<00:17, 79.15 examples/s]Map:  16%|█▌        | 248/1600 [00:03<00:17, 77.62 examples/s]Map:  16%|█▌        | 258/1600 [00:03<00:17, 77.98 examples/s]Map:  17%|█▋        | 267/1600 [00:03<00:17, 75.74 examples/s]Map:  17%|█▋        | 277/1600 [00:03<00:17, 76.91 examples/s]Map:  18%|█▊        | 288/1600 [00:04<00:19, 67.00 examples/s]Map:  18%|█▊        | 295/1600 [00:04<00:19, 67.03 examples/s]Map:  19%|█▉        | 304/1600 [00:04<00:18, 68.28 examples/s]Map:  20%|█▉        | 314/1600 [00:04<00:18, 69.92 examples/s]Map:  20%|██        | 322/1600 [00:04<00:18, 68.12 examples/s]Map:  21%|██        | 330/1600 [00:04<00:17, 70.85 examples/s]Map:  21%|██        | 338/1600 [00:04<00:17, 70.83 examples/s]Map:  22%|██▏       | 346/1600 [00:04<00:17, 70.63 examples/s]Map:  22%|██▏       | 356/1600 [00:05<00:16, 74.35 examples/s]Map:  23%|██▎       | 364/1600 [00:05<00:16, 73.87 examples/s]Map:  23%|██▎       | 372/1600 [00:05<00:16, 72.85 examples/s]Map:  24%|██▍       | 380/1600 [00:05<00:29, 41.17 examples/s]Map:  24%|██▍       | 388/1600 [00:05<00:25, 46.93 examples/s]Map:  25%|██▍       | 399/1600 [00:05<00:20, 57.32 examples/s]Map:  26%|██▌       | 408/1600 [00:06<00:18, 63.50 examples/s]Map:  26%|██▋       | 421/1600 [00:06<00:15, 76.33 examples/s]Map:  27%|██▋       | 432/1600 [00:06<00:13, 84.12 examples/s]Map:  28%|██▊       | 443/1600 [00:06<00:13, 86.95 examples/s]Map:  28%|██▊       | 454/1600 [00:06<00:12, 91.38 examples/s]Map:  29%|██▉       | 467/1600 [00:06<00:11, 98.21 examples/s]Map:  30%|██▉       | 479/1600 [00:06<00:12, 89.23 examples/s]Map:  31%|███       | 491/1600 [00:06<00:13, 81.46 examples/s]Map:  31%|███▏      | 503/1600 [00:07<00:14, 77.37 examples/s]Map:  32%|███▏      | 512/1600 [00:07<00:14, 76.12 examples/s]Map:  32%|███▎      | 520/1600 [00:07<00:14, 74.91 examples/s]Map:  33%|███▎      | 529/1600 [00:07<00:14, 75.88 examples/s]Map:  34%|███▍      | 541/1600 [00:07<00:14, 74.95 examples/s]Map:  34%|███▍      | 549/1600 [00:07<00:14, 72.95 examples/s]Map:  35%|███▍      | 557/1600 [00:07<00:14, 72.11 examples/s]Map:  35%|███▌      | 565/1600 [00:07<00:14, 70.68 examples/s]Map:  36%|███▌      | 573/1600 [00:08<00:17, 57.48 examples/s]Map:  36%|███▋      | 581/1600 [00:08<00:16, 60.78 examples/s]Map:  37%|███▋      | 589/1600 [00:08<00:15, 63.20 examples/s]Map:  37%|███▋      | 597/1600 [00:08<00:15, 64.52 examples/s]Map:  38%|███▊      | 605/1600 [00:08<00:15, 66.02 examples/s]Map:  38%|███▊      | 613/1600 [00:08<00:14, 66.05 examples/s]Map:  39%|███▉      | 621/1600 [00:08<00:14, 66.66 examples/s]Map:  39%|███▉      | 629/1600 [00:08<00:14, 68.44 examples/s]Map:  40%|███▉      | 637/1600 [00:09<00:14, 68.37 examples/s]Map:  40%|████      | 647/1600 [00:09<00:13, 71.42 examples/s]Map:  41%|████      | 658/1600 [00:09<00:13, 70.33 examples/s]Map:  42%|████▏     | 666/1600 [00:09<00:15, 60.54 examples/s]Map:  42%|████▏     | 674/1600 [00:09<00:14, 64.14 examples/s]Map:  43%|████▎     | 682/1600 [00:09<00:13, 66.47 examples/s]Map:  43%|████▎     | 691/1600 [00:09<00:13, 69.16 examples/s]Map:  44%|████▎     | 699/1600 [00:10<00:12, 70.42 examples/s]Map:  44%|████▍     | 710/1600 [00:10<00:13, 67.83 examples/s]Map:  45%|████▍     | 719/1600 [00:10<00:12, 69.80 examples/s]Map:  45%|████▌     | 727/1600 [00:10<00:12, 68.02 examples/s]Map:  46%|████▌     | 735/1600 [00:10<00:12, 69.07 examples/s]Map:  46%|████▋     | 743/1600 [00:10<00:12, 68.95 examples/s]Map:  47%|████▋     | 750/1600 [00:10<00:12, 66.57 examples/s]Map:  47%|████▋     | 758/1600 [00:10<00:12, 67.63 examples/s]Map:  48%|████▊     | 767/1600 [00:11<00:15, 55.13 examples/s]Map:  48%|████▊     | 775/1600 [00:11<00:14, 57.41 examples/s]Map:  49%|████▉     | 782/1600 [00:11<00:14, 57.75 examples/s]Map:  49%|████▉     | 790/1600 [00:11<00:13, 62.04 examples/s]Map:  50%|█████     | 801/1600 [00:11<00:12, 65.31 examples/s]Map:  51%|█████     | 813/1600 [00:11<00:14, 55.55 examples/s]Map:  51%|█████▏    | 823/1600 [00:12<00:12, 62.93 examples/s]Map:  52%|█████▏    | 831/1600 [00:12<00:11, 65.39 examples/s]Map:  52%|█████▎    | 840/1600 [00:12<00:10, 69.26 examples/s]Map:  53%|█████▎    | 849/1600 [00:12<00:10, 71.20 examples/s]Map:  54%|█████▎    | 858/1600 [00:12<00:10, 73.39 examples/s]Map:  54%|█████▍    | 867/1600 [00:12<00:11, 62.55 examples/s]Map:  55%|█████▍    | 877/1600 [00:12<00:10, 68.60 examples/s]Map:  56%|█████▌    | 888/1600 [00:12<00:10, 68.13 examples/s]Map:  56%|█████▌    | 896/1600 [00:13<00:10, 66.86 examples/s]Map:  56%|█████▋    | 903/1600 [00:13<00:10, 66.63 examples/s]Map:  57%|█████▋    | 911/1600 [00:13<00:10, 65.29 examples/s]Map:  57%|█████▋    | 919/1600 [00:13<00:10, 64.45 examples/s]Map:  58%|█████▊    | 930/1600 [00:13<00:10, 63.72 examples/s]Map:  59%|█████▊    | 938/1600 [00:13<00:10, 64.62 examples/s]Map:  59%|█████▉    | 947/1600 [00:13<00:11, 59.31 examples/s]Map:  60%|█████▉    | 957/1600 [00:14<00:12, 50.43 examples/s]Map:  60%|██████    | 965/1600 [00:14<00:11, 55.84 examples/s]Map:  61%|██████    | 975/1600 [00:14<00:11, 56.58 examples/s]Map:  61%|██████▏   | 983/1600 [00:14<00:10, 59.42 examples/s]Map:  62%|██████▏   | 991/1600 [00:14<00:09, 61.23 examples/s]Map:  62%|██████▏   | 998/1600 [00:14<00:09, 61.59 examples/s]Map:  62%|██████▏   | 998/1600 [00:26<00:09, 61.59 examples/s]Map:  62%|██████▎   | 1000/1600 [01:16<29:42,  2.97s/ examples]Map:  63%|██████▎   | 1007/1600 [01:16<19:55,  2.02s/ examples]Map:  63%|██████▎   | 1015/1600 [01:16<12:55,  1.32s/ examples]Map:  64%|██████▍   | 1023/1600 [01:16<08:34,  1.12 examples/s]Map:  64%|██████▍   | 1030/1600 [01:17<06:02,  1.57 examples/s]Map:  65%|██████▍   | 1037/1600 [01:17<04:15,  2.20 examples/s]Map:  65%|██████▌   | 1044/1600 [01:17<02:59,  3.09 examples/s]Map:  66%|██████▌   | 1051/1600 [01:17<02:07,  4.31 examples/s]Map:  66%|██████▌   | 1059/1600 [01:17<01:27,  6.19 examples/s]Map:  67%|██████▋   | 1066/1600 [01:17<01:03,  8.39 examples/s]Map:  67%|██████▋   | 1074/1600 [01:17<00:44, 11.73 examples/s]Map:  68%|██████▊   | 1084/1600 [01:17<00:31, 16.46 examples/s]Map:  68%|██████▊   | 1093/1600 [01:18<00:23, 21.22 examples/s]Map:  69%|██████▉   | 1101/1600 [01:18<00:18, 26.34 examples/s]Map:  69%|██████▉   | 1109/1600 [01:18<00:15, 32.16 examples/s]Map:  70%|██████▉   | 1116/1600 [01:18<00:13, 36.87 examples/s]Map:  70%|███████   | 1123/1600 [01:18<00:13, 35.40 examples/s]Map:  71%|███████   | 1130/1600 [01:18<00:11, 40.34 examples/s]Map:  71%|███████   | 1137/1600 [01:18<00:10, 44.03 examples/s]Map:  72%|███████▏  | 1145/1600 [01:19<00:09, 49.53 examples/s]Map:  72%|███████▏  | 1153/1600 [01:19<00:08, 53.55 examples/s]Map:  73%|███████▎  | 1161/1600 [01:19<00:07, 57.66 examples/s]Map:  73%|███████▎  | 1168/1600 [01:19<00:07, 59.56 examples/s]Map:  74%|███████▎  | 1176/1600 [01:19<00:06, 61.51 examples/s]Map:  74%|███████▍  | 1186/1600 [01:19<00:06, 60.90 examples/s]Map:  75%|███████▍  | 1194/1600 [01:19<00:06, 63.28 examples/s]Map:  75%|███████▌  | 1202/1600 [01:19<00:06, 65.86 examples/s]Map:  76%|███████▌  | 1210/1600 [01:20<00:05, 65.94 examples/s]Map:  76%|███████▋  | 1220/1600 [01:20<00:06, 60.25 examples/s]Map:  77%|███████▋  | 1228/1600 [01:20<00:05, 63.25 examples/s]Map:  77%|███████▋  | 1236/1600 [01:20<00:05, 64.96 examples/s]Map:  78%|███████▊  | 1243/1600 [01:20<00:07, 50.27 examples/s]Map:  78%|███████▊  | 1251/1600 [01:20<00:06, 54.55 examples/s]Map:  79%|███████▊  | 1257/1600 [01:20<00:06, 55.46 examples/s]Map:  79%|███████▉  | 1265/1600 [01:20<00:05, 60.41 examples/s]Map:  80%|███████▉  | 1273/1600 [01:21<00:05, 63.13 examples/s]Map:  80%|████████  | 1280/1600 [01:21<00:05, 62.73 examples/s]Map:  80%|████████  | 1287/1600 [01:21<00:05, 61.17 examples/s]Map:  81%|████████  | 1297/1600 [01:21<00:05, 60.05 examples/s]Map:  82%|████████▏ | 1304/1600 [01:21<00:04, 59.79 examples/s]Map:  82%|████████▏ | 1311/1600 [01:21<00:04, 60.80 examples/s]Map:  82%|████████▏ | 1319/1600 [01:21<00:04, 61.38 examples/s]Map:  83%|████████▎ | 1326/1600 [01:21<00:04, 62.28 examples/s]Map:  83%|████████▎ | 1333/1600 [01:22<00:04, 62.77 examples/s]Map:  84%|████████▍ | 1340/1600 [01:22<00:04, 62.32 examples/s]Map:  84%|████████▍ | 1348/1600 [01:22<00:05, 49.72 examples/s]Map:  85%|████████▍ | 1355/1600 [01:22<00:04, 53.31 examples/s]Map:  85%|████████▌ | 1363/1600 [01:22<00:04, 56.81 examples/s]Map:  86%|████████▌ | 1371/1600 [01:22<00:03, 57.26 examples/s]Map:  86%|████████▌ | 1378/1600 [01:22<00:03, 58.88 examples/s]Map:  87%|████████▋ | 1386/1600 [01:23<00:03, 63.07 examples/s]Map:  87%|████████▋ | 1393/1600 [01:23<00:03, 61.94 examples/s]Map:  88%|████████▊ | 1402/1600 [01:23<00:03, 59.17 examples/s]Map:  88%|████████▊ | 1409/1600 [01:23<00:03, 59.53 examples/s]Map:  89%|████████▊ | 1419/1600 [01:23<00:03, 56.49 examples/s]Map:  89%|████████▉ | 1426/1600 [01:23<00:03, 56.00 examples/s]Map:  90%|████████▉ | 1433/1600 [01:23<00:03, 55.65 examples/s]Map:  90%|█████████ | 1440/1600 [01:23<00:02, 57.87 examples/s]Map:  90%|█████████ | 1447/1600 [01:24<00:02, 59.79 examples/s]Map:  91%|█████████ | 1454/1600 [01:24<00:02, 60.14 examples/s]Map:  91%|█████████▏| 1461/1600 [01:24<00:02, 60.26 examples/s]Map:  92%|█████████▏| 1468/1600 [01:24<00:02, 44.59 examples/s]Map:  92%|█████████▏| 1475/1600 [01:24<00:02, 47.59 examples/s]Map:  93%|█████████▎| 1482/1600 [01:24<00:02, 50.37 examples/s]Map:  93%|█████████▎| 1490/1600 [01:24<00:02, 54.97 examples/s]Map:  94%|█████████▎| 1497/1600 [01:25<00:01, 55.42 examples/s]Map:  94%|█████████▍| 1505/1600 [01:25<00:01, 57.56 examples/s]Map:  94%|█████████▍| 1512/1600 [01:25<00:01, 58.64 examples/s]Map:  95%|█████████▍| 1519/1600 [01:25<00:01, 59.35 examples/s]Map:  95%|█████████▌| 1527/1600 [01:25<00:01, 60.56 examples/s]Map:  96%|█████████▌| 1534/1600 [01:25<00:01, 61.36 examples/s]Map:  96%|█████████▋| 1544/1600 [01:25<00:00, 60.29 examples/s]Map:  97%|█████████▋| 1551/1600 [01:25<00:00, 61.13 examples/s]Map:  97%|█████████▋| 1559/1600 [01:26<00:00, 62.60 examples/s]Map:  98%|█████████▊| 1569/1600 [01:26<00:00, 61.91 examples/s]Map:  98%|█████████▊| 1576/1600 [01:26<00:00, 50.62 examples/s]Map:  99%|█████████▉| 1583/1600 [01:26<00:00, 51.74 examples/s]Map:  99%|█████████▉| 1590/1600 [01:26<00:00, 54.26 examples/s]Map: 100%|█████████▉| 1597/1600 [01:26<00:00, 56.35 examples/s]Map: 100%|█████████▉| 1597/1600 [01:38<00:00, 56.35 examples/s]Map: 100%|██████████| 1600/1600 [02:12<00:00,  2.30s/ examples]Map: 100%|██████████| 1600/1600 [02:12<00:00, 12.04 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▏         | 3/200 [00:00<00:09, 19.99 examples/s]Map:   4%|▍         | 8/200 [00:00<00:06, 29.25 examples/s]Map:   8%|▊         | 17/200 [00:00<00:04, 44.54 examples/s]Map:  12%|█▎        | 25/200 [00:00<00:03, 55.31 examples/s]Map:  17%|█▋        | 34/200 [00:00<00:02, 61.86 examples/s]Map:  22%|██▏       | 44/200 [00:00<00:02, 68.97 examples/s]Map:  28%|██▊       | 55/200 [00:00<00:01, 76.72 examples/s]Map:  32%|███▎      | 65/200 [00:01<00:01, 79.37 examples/s]Map:  38%|███▊      | 77/200 [00:01<00:01, 73.52 examples/s]Map:  42%|████▎     | 85/200 [00:01<00:01, 70.06 examples/s]Map:  47%|████▋     | 94/200 [00:01<00:01, 62.06 examples/s]Map:  52%|█████▏    | 103/200 [00:01<00:01, 63.87 examples/s]Map:  56%|█████▌    | 112/200 [00:01<00:01, 65.50 examples/s]Map:  60%|█████▉    | 119/200 [00:01<00:01, 63.39 examples/s]Map:  64%|██████▍   | 128/200 [00:02<00:01, 67.26 examples/s]Map:  68%|██████▊   | 136/200 [00:02<00:00, 66.00 examples/s]Map:  72%|███████▏  | 143/200 [00:02<00:00, 65.48 examples/s]Map:  76%|███████▌  | 151/200 [00:02<00:00, 64.14 examples/s]Map:  80%|███████▉  | 159/200 [00:02<00:00, 65.11 examples/s]Map:  83%|████████▎ | 166/200 [00:02<00:00, 62.62 examples/s]Map:  87%|████████▋ | 174/200 [00:02<00:00, 64.70 examples/s]Map:  92%|█████████▏| 184/200 [00:02<00:00, 62.61 examples/s]Map:  96%|█████████▋| 193/200 [00:03<00:00, 61.01 examples/s]Map: 100%|██████████| 200/200 [00:03<00:00, 49.66 examples/s]Map: 100%|██████████| 200/200 [00:16<00:00, 49.66 examples/s]Map: 100%|██████████| 200/200 [00:16<00:00, 12.32 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   1%|          | 2/200 [00:00<00:15, 13.13 examples/s]Map:   5%|▌         | 10/200 [00:00<00:05, 36.90 examples/s]Map:   8%|▊         | 17/200 [00:00<00:04, 45.63 examples/s]Map:  13%|█▎        | 26/200 [00:00<00:02, 58.36 examples/s]Map:  16%|█▋        | 33/200 [00:00<00:02, 60.72 examples/s]Map:  21%|██        | 42/200 [00:00<00:02, 65.59 examples/s]Map:  26%|██▌       | 52/200 [00:00<00:02, 72.74 examples/s]Map:  31%|███       | 62/200 [00:00<00:01, 77.11 examples/s]Map:  36%|███▌      | 72/200 [00:01<00:01, 79.89 examples/s]Map:  42%|████▏     | 83/200 [00:01<00:01, 73.59 examples/s]Map:  46%|████▌     | 91/200 [00:01<00:01, 61.30 examples/s]Map:  50%|█████     | 100/200 [00:01<00:01, 65.44 examples/s]Map:  55%|█████▌    | 110/200 [00:01<00:01, 69.50 examples/s]Map:  60%|██████    | 121/200 [00:01<00:01, 68.59 examples/s]Map:  66%|██████▌   | 132/200 [00:02<00:01, 67.91 examples/s]Map:  70%|███████   | 140/200 [00:02<00:00, 67.88 examples/s]Map:  74%|███████▍  | 149/200 [00:02<00:00, 69.93 examples/s]Map:  78%|███████▊  | 157/200 [00:02<00:00, 67.88 examples/s]Map:  82%|████████▎ | 165/200 [00:02<00:00, 68.03 examples/s]Map:  86%|████████▋ | 173/200 [00:02<00:00, 67.93 examples/s]Map:  92%|█████████▏| 183/200 [00:02<00:00, 64.66 examples/s]Map:  96%|█████████▌| 191/200 [00:02<00:00, 64.83 examples/s]Map: 100%|█████████▉| 199/200 [00:03<00:00, 64.50 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 12.92 examples/s]
Loading cn eval data: fold_1...
Preprocess cn fold_1 data for de model
Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 6/1600 [00:00<00:34, 46.87 examples/s]Map:   1%|          | 15/1600 [00:00<00:26, 59.12 examples/s]Map:   2%|▏         | 36/1600 [00:00<00:13, 115.69 examples/s]Map:   3%|▎         | 52/1600 [00:00<00:12, 127.46 examples/s]Map:   4%|▍         | 72/1600 [00:00<00:12, 126.07 examples/s]Map:   5%|▌         | 87/1600 [00:00<00:11, 128.74 examples/s]Map:   6%|▋         | 101/1600 [00:00<00:11, 129.28 examples/s]Map:   7%|▋         | 118/1600 [00:00<00:10, 138.46 examples/s]Map:   8%|▊         | 134/1600 [00:01<00:10, 142.54 examples/s]Map:   9%|▉         | 151/1600 [00:01<00:09, 146.96 examples/s]Map:  11%|█         | 169/1600 [00:01<00:09, 154.09 examples/s]Map:  12%|█▏        | 188/1600 [00:01<00:10, 140.22 examples/s]Map:  13%|█▎        | 206/1600 [00:01<00:09, 148.67 examples/s]Map:  14%|█▍        | 227/1600 [00:01<00:09, 142.42 examples/s]Map:  15%|█▌        | 242/1600 [00:01<00:09, 141.75 examples/s]Map:  16%|█▋        | 260/1600 [00:01<00:10, 128.21 examples/s]Map:  17%|█▋        | 276/1600 [00:02<00:11, 116.92 examples/s]Map:  18%|█▊        | 289/1600 [00:02<00:15, 83.88 examples/s] Map:  19%|█▉        | 300/1600 [00:02<00:14, 86.76 examples/s]Map:  20%|█▉        | 314/1600 [00:02<00:13, 96.37 examples/s]Map:  20%|██        | 325/1600 [00:02<00:12, 98.31 examples/s]Map:  21%|██        | 338/1600 [00:02<00:12, 104.04 examples/s]Map:  22%|██▏       | 351/1600 [00:02<00:11, 107.73 examples/s]Map:  23%|██▎       | 363/1600 [00:03<00:11, 107.89 examples/s]Map:  24%|██▎       | 376/1600 [00:03<00:11, 110.22 examples/s]Map:  24%|██▍       | 388/1600 [00:03<00:10, 110.91 examples/s]Map:  25%|██▌       | 402/1600 [00:03<00:10, 117.37 examples/s]Map:  26%|██▋       | 421/1600 [00:03<00:10, 109.78 examples/s]Map:  27%|██▋       | 438/1600 [00:03<00:09, 122.46 examples/s]Map:  28%|██▊       | 453/1600 [00:03<00:08, 128.04 examples/s]Map:  29%|██▉       | 467/1600 [00:03<00:08, 128.27 examples/s]Map:  30%|███       | 486/1600 [00:04<00:08, 124.30 examples/s]Map:  31%|███       | 499/1600 [00:04<00:08, 122.62 examples/s]Map:  32%|███▏      | 516/1600 [00:04<00:08, 133.34 examples/s]Map:  33%|███▎      | 533/1600 [00:04<00:08, 118.96 examples/s]Map:  34%|███▍      | 549/1600 [00:04<00:08, 128.07 examples/s]Map:  36%|███▌      | 568/1600 [00:04<00:07, 140.95 examples/s]Map:  37%|███▋      | 586/1600 [00:04<00:06, 149.77 examples/s]Map:  38%|███▊      | 605/1600 [00:04<00:06, 158.86 examples/s]Map:  39%|███▉      | 623/1600 [00:05<00:06, 161.74 examples/s]Map:  40%|████      | 646/1600 [00:05<00:06, 147.92 examples/s]Map:  42%|████▏     | 668/1600 [00:05<00:06, 144.60 examples/s]Map:  43%|████▎     | 689/1600 [00:05<00:06, 140.45 examples/s]Map:  44%|████▍     | 711/1600 [00:05<00:06, 140.89 examples/s]Map:  45%|████▌     | 727/1600 [00:05<00:06, 143.33 examples/s]Map:  46%|████▋     | 742/1600 [00:05<00:06, 125.45 examples/s]Map:  48%|████▊     | 760/1600 [00:06<00:06, 135.33 examples/s]Map:  49%|████▉     | 780/1600 [00:06<00:05, 147.56 examples/s]Map:  50%|█████     | 800/1600 [00:06<00:05, 159.53 examples/s]Map:  51%|█████     | 818/1600 [00:06<00:04, 163.45 examples/s]Map:  52%|█████▏    | 837/1600 [00:06<00:04, 168.26 examples/s]Map:  54%|█████▍    | 860/1600 [00:06<00:04, 148.44 examples/s]Map:  55%|█████▌    | 882/1600 [00:06<00:04, 144.98 examples/s]Map:  57%|█████▋    | 905/1600 [00:06<00:04, 145.69 examples/s]Map:  58%|█████▊    | 924/1600 [00:07<00:04, 154.52 examples/s]Map:  59%|█████▉    | 942/1600 [00:07<00:04, 157.22 examples/s]Map:  60%|██████    | 961/1600 [00:07<00:03, 162.48 examples/s]Map:  62%|██████▏   | 985/1600 [00:07<00:04, 151.11 examples/s]Map:  62%|██████▏   | 995/1600 [00:24<00:04, 151.11 examples/s]Map:  62%|██████▎   | 1000/1600 [00:47<06:22,  1.57 examples/s]Map:  64%|██████▍   | 1020/1600 [00:47<04:16,  2.26 examples/s]Map:  65%|██████▍   | 1038/1600 [00:48<02:58,  3.14 examples/s]Map:  66%|██████▌   | 1055/1600 [00:48<02:06,  4.32 examples/s]Map:  67%|██████▋   | 1073/1600 [00:48<01:27,  6.03 examples/s]Map:  68%|██████▊   | 1092/1600 [00:48<00:59,  8.51 examples/s]Map:  69%|██████▉   | 1106/1600 [00:48<00:44, 11.04 examples/s]Map:  70%|███████   | 1121/1600 [00:48<00:32, 14.54 examples/s]Map:  71%|███████   | 1137/1600 [00:48<00:23, 19.82 examples/s]Map:  72%|███████▏  | 1153/1600 [00:48<00:16, 26.68 examples/s]Map:  73%|███████▎  | 1170/1600 [00:49<00:11, 36.06 examples/s]Map:  74%|███████▍  | 1188/1600 [00:49<00:08, 48.00 examples/s]Map:  75%|███████▌  | 1205/1600 [00:49<00:06, 60.67 examples/s]Map:  77%|███████▋  | 1225/1600 [00:49<00:05, 69.72 examples/s]Map:  78%|███████▊  | 1246/1600 [00:49<00:04, 82.40 examples/s]Map:  79%|███████▉  | 1264/1600 [00:49<00:03, 87.96 examples/s]Map:  80%|████████  | 1280/1600 [00:49<00:03, 90.00 examples/s]Map:  81%|████████  | 1292/1600 [00:50<00:03, 93.28 examples/s]Map:  82%|████████▏ | 1309/1600 [00:50<00:02, 97.32 examples/s]Map:  83%|████████▎ | 1321/1600 [00:50<00:02, 100.37 examples/s]Map:  83%|████████▎ | 1335/1600 [00:50<00:02, 106.38 examples/s]Map:  85%|████████▍ | 1353/1600 [00:50<00:02, 97.40 examples/s] Map:  86%|████████▌ | 1370/1600 [00:50<00:02, 110.84 examples/s]Map:  87%|████████▋ | 1387/1600 [00:50<00:01, 123.02 examples/s]Map:  88%|████████▊ | 1404/1600 [00:51<00:01, 133.03 examples/s]Map:  89%|████████▉ | 1420/1600 [00:51<00:01, 138.01 examples/s]Map:  90%|████████▉ | 1438/1600 [00:51<00:01, 144.81 examples/s]Map:  91%|█████████ | 1457/1600 [00:51<00:01, 131.78 examples/s]Map:  92%|█████████▏| 1472/1600 [00:51<00:00, 133.20 examples/s]Map:  93%|█████████▎| 1487/1600 [00:51<00:00, 134.65 examples/s]Map:  94%|█████████▍| 1501/1600 [00:51<00:00, 135.46 examples/s]Map:  95%|█████████▍| 1517/1600 [00:51<00:00, 140.13 examples/s]Map:  96%|█████████▌| 1533/1600 [00:51<00:00, 141.56 examples/s]Map:  97%|█████████▋| 1550/1600 [00:52<00:00, 146.72 examples/s]Map:  98%|█████████▊| 1571/1600 [00:52<00:00, 128.65 examples/s]Map:  99%|█████████▉| 1591/1600 [00:52<00:00, 141.98 examples/s]Map: 100%|██████████| 1600/1600 [01:04<00:00, 141.98 examples/s]Map: 100%|██████████| 1600/1600 [01:17<00:00,  2.02 examples/s] Map: 100%|██████████| 1600/1600 [01:17<00:00, 20.59 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   3%|▎         | 6/200 [00:00<00:04, 47.29 examples/s]Map:   6%|▋         | 13/200 [00:00<00:04, 45.62 examples/s]Map:  16%|█▌        | 31/200 [00:00<00:01, 94.21 examples/s]Map:  22%|██▏       | 43/200 [00:00<00:01, 100.06 examples/s]Map:  28%|██▊       | 57/200 [00:00<00:01, 110.48 examples/s]Map:  36%|███▋      | 73/200 [00:00<00:01, 122.35 examples/s]Map:  46%|████▌     | 92/200 [00:00<00:00, 141.42 examples/s]Map:  55%|█████▍    | 109/200 [00:00<00:00, 133.84 examples/s]Map:  64%|██████▎   | 127/200 [00:01<00:00, 144.25 examples/s]Map:  73%|███████▎  | 146/200 [00:01<00:00, 131.51 examples/s]Map:  80%|████████  | 160/200 [00:01<00:00, 129.98 examples/s]Map:  88%|████████▊ | 175/200 [00:01<00:00, 131.20 examples/s]Map:  98%|█████████▊| 196/200 [00:01<00:00, 148.86 examples/s]Map: 100%|██████████| 200/200 [00:09<00:00, 21.80 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   3%|▎         | 6/200 [00:00<00:04, 44.94 examples/s]Map:   6%|▋         | 13/200 [00:00<00:03, 51.55 examples/s]Map:  16%|█▌        | 31/200 [00:00<00:01, 99.51 examples/s]Map:  22%|██▏       | 43/200 [00:00<00:01, 105.38 examples/s]Map:  30%|██▉       | 59/200 [00:00<00:01, 119.19 examples/s]Map:  37%|███▋      | 74/200 [00:00<00:00, 127.35 examples/s]Map:  46%|████▋     | 93/200 [00:00<00:00, 144.24 examples/s]Map:  56%|█████▌    | 112/200 [00:00<00:00, 134.97 examples/s]Map:  64%|██████▍   | 129/200 [00:01<00:00, 142.75 examples/s]Map:  74%|███████▎  | 147/200 [00:01<00:00, 129.66 examples/s]Map:  84%|████████▍ | 168/200 [00:01<00:00, 129.62 examples/s]Map:  93%|█████████▎| 186/200 [00:01<00:00, 138.03 examples/s]Map: 100%|██████████| 200/200 [00:09<00:00, 21.59 examples/s] 
Loading cn test data: fold_1...
Preprocess cn fold_1 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 435.7296, Training accuracy: 0.9056
Macro F1-score: 0.9054
Model performance on Angry speech (in training): 
	Precision: 0.8783, Recall: 0.9025, F1_score: 0.8903
Model performance on Happy speech (in training): 
	Precision: 0.8590, Recall: 0.8375, F1_score: 0.8481
Model performance on Neutral speech (in training): 
	Precision: 0.9213, Recall: 0.9075, F1_score: 0.9144
Model performance on Sad speech (in training): 
	Precision: 0.9630, Recall: 0.9750, F1_score: 0.9689

Eval Phase: 
Validation loss: 185.4105, Validation accuracy: 0.6550
Macro F1-score: 0.6558
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6600, F1_score: 0.7952
Model performance on Happy speech (in validation): 
	Precision: 0.6170, Recall: 0.5800, F1_score: 0.5979
Model performance on Neutral speech (in validation): 
	Precision: 0.4800, Recall: 0.4800, F1_score: 0.4800
Model performance on Sad speech (in validation): 
	Precision: 0.6429, Recall: 0.9000, F1_score: 0.7500
New best accuracy for layer 4 on epoch 1: 0.6550. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.45it/s]Training:  24%|██▍       | 392/1600 [00:20<01:01, 19.55it/s]Training:  37%|███▋      | 589/1600 [00:30<00:51, 19.52it/s]Training:  49%|████▉     | 784/1600 [00:40<00:42, 19.40it/s]Training:  61%|██████    | 978/1600 [00:50<00:32, 19.37it/s]Training:  73%|███████▎  | 1174/1600 [01:00<00:21, 19.44it/s]Training:  86%|████████▌ | 1370/1600 [01:10<00:11, 19.39it/s]Training:  98%|█████████▊| 1571/1600 [01:20<00:01, 19.60it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 202/1600 [00:10<01:09, 20.16it/s]Training:  25%|██▌       | 404/1600 [00:20<00:59, 20.00it/s]Training:  38%|███▊      | 6Training loss: 161.0688, Training accuracy: 0.9644
Macro F1-score: 0.9644
Model performance on Angry speech (in training): 
	Precision: 0.9626, Recall: 0.9650, F1_score: 0.9638
Model performance on Happy speech (in training): 
	Precision: 0.9447, Recall: 0.9400, F1_score: 0.9424
Model performance on Neutral speech (in training): 
	Precision: 0.9650, Recall: 0.9650, F1_score: 0.9650
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863

Eval Phase: 
Validation loss: 196.9361, Validation accuracy: 0.6250
Macro F1-score: 0.6338
Model performance on Angry speech (in validation): 
	Precision: 0.9565, Recall: 0.4400, F1_score: 0.6027
Model performance on Happy speech (in validation): 
	Precision: 0.4625, Recall: 0.7400, F1_score: 0.5692
Model performance on Neutral speech (in validation): 
	Precision: 0.5333, Recall: 0.6400, F1_score: 0.5818
Model performance on Sad speech (in validation): 
	Precision: 0.9189, Recall: 0.6800, F1_score: 0.7816
Epoch 3/100

Training Phase:
03/1600 [00:30<00:50, 19.85it/s]Training:  50%|█████     | 800/1600 [00:40<00:40, 19.60it/s]Training:  62%|██████▏   | 993/1600 [00:50<00:31, 19.41it/s]Training:  74%|███████▍  | 1189/1600 [01:00<00:21, 19.47it/s]Training:  87%|████████▋ | 1385/1600 [01:10<00:11, 19.43it/s]Training:  99%|█████████▉| 1584/1600 [01:20<00:00, 19.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.56it/s]Training:  25%|██▍       | 395/1600 [00:20<01:01, 19.73it/s]Training:  37%|███▋      | 594/1600 [00:30<00:51, 19.70it/s]Training:  49%|████▉     | 791/1600 [00:41<00:42, 18.99it/s]Training:  62%|██████▏   | 988/1600 [00:51<00:31, 19.24it/s]Training:  74%|███████▍Training loss: 119.2858, Training accuracy: 0.9769
Macro F1-score: 0.9769
Model performance on Angry speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Happy speech (in training): 
	Precision: 0.9626, Recall: 0.9650, F1_score: 0.9638
Model performance on Neutral speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 137.4928, Validation accuracy: 0.7600
Macro F1-score: 0.7542
Model performance on Angry speech (in validation): 
	Precision: 0.8571, Recall: 0.9600, F1_score: 0.9057
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.6000, F1_score: 0.6667
Model performance on Neutral speech (in validation): 
	Precision: 0.5600, Recall: 0.5600, F1_score: 0.5600
Model performance on Sad speech (in validation): 
	Precision: 0.8519, Recall: 0.9200, F1_score: 0.8846
New best accuracy for layer 4 on epoch 3: 0.7600. Model saved.
Epoch 4/100

Training Phase:
  | 1185/1600 [01:01<00:21, 19.33it/s]Training:  86%|████████▋ | 1382/1600 [01:11<00:11, 19.44it/s]Training:  99%|█████████▉| 1580/1600 [01:21<00:01, 19.53it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 197/1600 [00:10<01:11, 19.58it/s]Training:  25%|██▍       | 393/1600 [00:20<01:02, 19.41it/s]Training:  37%|███▋      | 592/1600 [00:30<00:51, 19.62it/s]Training:  49%|████▉     | 791/1600 [00:40<00:41, 19.59it/s]Training:  62%|██████▏   | 987/1600 [00:50<00:31, 19.55it/s]Training:  74%|███████▍  | 1182/1600 [01:00<00:21, 19.44it/s]Training:  86%|████████▋ | 1381/1600 [01:10<00:11, 19.56it/s]Training:  99%|█████████▉| 1582/1600 [01:20<00:00, 19.73it/s]                 Training loss: 75.9966, Training accuracy: 0.9825
Macro F1-score: 0.9825
Model performance on Angry speech (in training): 
	Precision: 0.9849, Recall: 0.9775, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9677, Recall: 0.9725, F1_score: 0.9701
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 130.2229, Validation accuracy: 0.7900
Macro F1-score: 0.7896
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.7255, Recall: 0.7400, F1_score: 0.7327
Model performance on Neutral speech (in validation): 
	Precision: 0.6383, Recall: 0.6000, F1_score: 0.6186
Model performance on Sad speech (in validation): 
	Precision: 0.8070, Recall: 0.9200, F1_score: 0.8598
New best accuracy for layer 4 on epoch 4: 0.7900. Model saved.
Epoch 5/100

Training Phase:
Training loss: 73.4467, Training accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 184.9074, Validation accuracy: 0.7350
Macro F1-score: 0.7220
Model performance on Angry speech (in validation): 
	Precision: 0.8000, Recall: 0.9600, F1_score: 0.8727
Model performance on Happy speech (in validation): 
	Precision: 0.8919, Recall: 0.6600, F1_score: 0.7586
Model performance on Neutral speech (in validation): 
	Precision: 0.5128, Recall: 0.4000, F1_score: 0.4494
Model performance on Sad speech (in validation): 
	Precision: 0.7188, Recall: 0.9200, F1_score: 0.8070
Epoch 6/100

Training Phase:
                                            Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.99it/s]Training:  25%|██▌       | 400/1600 [00:20<01:00, 19.97it/s]Training:  38%|███▊      | 600/1600 [00:30<00:50, 19.94it/s]Training:  50%|████▉     | 799/1600 [00:40<00:40, 19.83it/s]Training:  63%|██████▎   | 1003/1600 [00:50<00:29, 20.03it/s]Training:  63%|██████▎   | 1003/1600 [01:00<00:29, 20.03it/s]Training:  75%|███████▌  | 1205/1600 [01:00<00:19, 20.01it/s]Training:  88%|████████▊ | 1406/1600 [01:10<00:09, 20.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏Training loss: 70.6491, Training accuracy: 0.9838
Macro F1-score: 0.9837
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Happy speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 155.5071, Validation accuracy: 0.7450
Macro F1-score: 0.7345
Model performance on Angry speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.6000, F1_score: 0.6667
Model performance on Neutral speech (in validation): 
	Precision: 0.5238, Recall: 0.4400, F1_score: 0.4783
Model performance on Sad speech (in validation): 
	Precision: 0.7101, Recall: 0.9800, F1_score: 0.8235
Epoch 7/100

Training Phase:
        | 190/1600 [00:10<01:14, 18.95it/s]Training:  24%|██▍       | 387/1600 [00:20<01:02, 19.35it/s]Training:  37%|███▋      | 586/1600 [00:30<00:51, 19.59it/s]Training:  49%|████▉     | 787/1600 [00:40<00:41, 19.77it/s]Training:  62%|██████▏   | 988/1600 [00:50<00:30, 19.80it/s]Training:  74%|███████▍  | 1187/1600 [01:00<00:20, 19.74it/s]Training:  87%|████████▋ | 1389/1600 [01:10<00:10, 19.87it/s]Training:  99%|█████████▉| 1591/1600 [01:20<00:00, 19.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 202/1600 [00:10<01:09, 20.12it/s]Training:  25%|██▌       | 404/1600 [00:20<01:00, 19.70it/s]Training:  38%|███▊      | 601/1600 [00:30<00:50, 19.70it/s]Training:  50%|████▉     | 7Training loss: 64.0139, Training accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9752, Recall: 0.9825, F1_score: 0.9788
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 93.8305, Validation accuracy: 0.8250
Macro F1-score: 0.8231
Model performance on Angry speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Happy speech (in validation): 
	Precision: 0.8200, Recall: 0.8200, F1_score: 0.8200
Model performance on Neutral speech (in validation): 
	Precision: 0.7111, Recall: 0.6400, F1_score: 0.6737
Model performance on Sad speech (in validation): 
	Precision: 0.8036, Recall: 0.9000, F1_score: 0.8491
New best accuracy for layer 4 on epoch 7: 0.8250. Model saved.
Epoch 8/100

Training Phase:
98/1600 [00:40<00:41, 19.52it/s]Training:  62%|██████▏   | 995/1600 [00:50<00:30, 19.57it/s]Training:  74%|███████▍  | 1192/1600 [01:00<00:20, 19.58it/s]Training:  87%|████████▋ | 1391/1600 [01:10<00:10, 19.68it/s]Training:  99%|█████████▉| 1591/1600 [01:20<00:00, 19.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.55it/s]Training:  25%|██▍       | 399/1600 [00:20<01:00, 19.99it/s]Training:  38%|███▊      | 602/1600 [00:30<00:49, 19.96it/s]Training:  50%|█████     | 802/1600 [00:40<00:40, 19.84it/s]Training:  50%|█████     | 802/1600 [00:50<00:40, 19.84it/s]Training:  62%|██████▏   | 997/1600 [00:50<00:30, 19.59it/s]Training:  74%|███████▍Training loss: 33.1808, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 161.3973, Validation accuracy: 0.7750
Macro F1-score: 0.7712
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.8788, Recall: 0.5800, F1_score: 0.6988
Model performance on Neutral speech (in validation): 
	Precision: 0.5577, Recall: 0.5800, F1_score: 0.5686
Model performance on Sad speech (in validation): 
	Precision: 0.7273, Recall: 0.9600, F1_score: 0.8276
Epoch 9/100

Training Phase:
  | 1191/1600 [01:00<00:20, 19.51it/s]Training:  87%|████████▋ | 1388/1600 [01:10<00:10, 19.56it/s]Training:  99%|█████████▉| 1585/1600 [01:20<00:00, 19.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 201/1600 [00:10<01:09, 20.04it/s]Training:  25%|██▌       | 402/1600 [00:20<01:00, 19.74it/s]Training:  37%|███▋      | 598/1600 [00:30<00:51, 19.50it/s]Training:  50%|████▉     | 799/1600 [00:40<00:40, 19.73it/s]Training:  62%|██████▎   | 1000/1600 [00:50<00:30, 19.80it/s]Training:  75%|███████▌  | 1200/1600 [01:00<00:20, 19.82it/s]Training:  87%|████████▋ | 1399/1600 [01:10<00:10, 19.75it/s]Training: 100%|█████████▉| 1599/1600 [01:20<00:00, 19.83it/s]                Training loss: 40.0913, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 113.6430, Validation accuracy: 0.7850
Macro F1-score: 0.7897
Model performance on Angry speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Happy speech (in validation): 
	Precision: 0.8250, Recall: 0.6600, F1_score: 0.7333
Model performance on Neutral speech (in validation): 
	Precision: 0.5735, Recall: 0.7800, F1_score: 0.6610
Model performance on Sad speech (in validation): 
	Precision: 0.8810, Recall: 0.7400, F1_score: 0.8043
Epoch 10/100

Training Phase:
Training loss: 37.1486, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 134.0746, Validation accuracy: 0.7850
Macro F1-score: 0.7871
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
Model performance on Happy speech (in validation): 
	Precision: 0.7073, Recall: 0.5800, F1_score: 0.6374
Model performance on Neutral speech (in validation): 
	Precision: 0.6094, Recall: 0.7800, F1_score: 0.6842
Model performance on Sad speech (in validation): 
	Precision: 0.8846, Recall: 0.9200, F1_score: 0.9020
Epoch 11/100

Training Phase:
                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.92it/s]Training:  25%|██▌       | 400/1600 [00:20<01:00, 19.82it/s]Training:  37%|███▋      | 598/1600 [00:30<00:50, 19.73it/s]Training:  50%|████▉     | 795/1600 [00:40<00:41, 19.51it/s]Training:  62%|██████▏   | 988/1600 [00:50<00:31, 19.40it/s]Training:  74%|███████▍  | 1185/1600 [01:00<00:21, 19.49it/s]Training:  86%|████████▋ | 1382/1600 [01:10<00:11, 19.48it/s]Training:  99%|█████████▊| 1579/1600 [01:20<00:01, 19.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|Training loss: 2.8756, Training accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 178.6204, Validation accuracy: 0.7900
Macro F1-score: 0.7925
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Happy speech (in validation): 
	Precision: 0.6977, Recall: 0.6000, F1_score: 0.6452
Model performance on Neutral speech (in validation): 
	Precision: 0.6308, Recall: 0.8200, F1_score: 0.7130
Model performance on Sad speech (in validation): 
	Precision: 0.9020, Recall: 0.9200, F1_score: 0.9109
Epoch 12/100

Training Phase:
█▎        | 203/1600 [00:10<01:08, 20.30it/s]Training:  25%|██▌       | 406/1600 [00:20<00:58, 20.26it/s]Training:  25%|██▌       | 406/1600 [00:30<00:58, 20.26it/s]Training:  38%|███▊      | 607/1600 [00:30<00:50, 19.85it/s]Training:  50%|█████     | 801/1600 [00:40<00:40, 19.59it/s]Training:  62%|██████▏   | 994/1600 [00:50<00:31, 19.49it/s]Training:  74%|███████▍  | 1189/1600 [01:00<00:21, 19.46it/s]Training:  86%|████████▋ | 1384/1600 [01:10<00:11, 19.43it/s]Training:  99%|█████████▉| 1583/1600 [01:20<00:00, 19.56it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 202/1600 [00:10<01:09, 20.15it/s]Training:  25%|██▌       | 404/1600 [00:20<00:59, 20.08it/s]Training:  38%|███▊      |Training loss: 39.5762, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 169.2179, Validation accuracy: 0.7350
Macro F1-score: 0.7379
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Happy speech (in validation): 
	Precision: 0.6250, Recall: 0.5000, F1_score: 0.5556
Model performance on Neutral speech (in validation): 
	Precision: 0.5714, Recall: 0.8800, F1_score: 0.6929
Model performance on Sad speech (in validation): 
	Precision: 0.8936, Recall: 0.8400, F1_score: 0.8660
Epoch 13/100

Training Phase:
 605/1600 [00:30<00:49, 19.96it/s]Training:  51%|█████     | 810/1600 [00:40<00:39, 20.16it/s]Training:  63%|██████▎   | 1015/1600 [00:50<00:29, 20.16it/s]Training:  76%|███████▌  | 1217/1600 [01:00<00:19, 20.08it/s]Training:  89%|████████▉ | 1421/1600 [01:10<00:08, 20.18it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 204/1600 [00:10<01:08, 20.32it/s]Training:  26%|██▌       | 411/1600 [00:20<00:57, 20.54it/s]Training:  39%|███▊      | 618/1600 [00:30<00:48, 20.40it/s]Training:  51%|█████▏    | 821/1600 [00:40<00:39, 19.89it/s]Training:  64%|██████▎   | 1018/1600 [00:50<00:29, 19.81it/s]Training:  76%|███████▌  | 1215/1600 [01:01<00:19, 19.69it/s]Training:  88%|███████Training loss: 29.7252, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 195.8521, Validation accuracy: 0.6950
Macro F1-score: 0.6942
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.5200, F1_score: 0.6842
Model performance on Happy speech (in validation): 
	Precision: 0.5147, Recall: 0.7000, F1_score: 0.5932
Model performance on Neutral speech (in validation): 
	Precision: 0.6383, Recall: 0.6000, F1_score: 0.6186
Model performance on Sad speech (in validation): 
	Precision: 0.8136, Recall: 0.9600, F1_score: 0.8807
Epoch 14/100

Training Phase:
Training loss: 11.4281, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 243.8352, Validation accuracy: 0.6700
Macro F1-score: 0.6675
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.5000, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.4815, Recall: 0.5200, F1_score: 0.5000
Model performance on Neutral speech (in validation): 
	Precision: 0.5738, Recall: 0.7000, F1_score: 0.6306
Model performance on Sad speech (in validation): 
	Precision: 0.8000, Recall: 0.9600, F1_score: 0.8727
Epoch 15/100

Training Phase:
▊ | 1410/1600 [01:11<00:09, 19.46it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.94it/s]Training:  25%|██▌       | 400/1600 [00:20<01:01, 19.61it/s]Training:  37%|███▋      | 597/1600 [00:30<00:51, 19.62it/s]Training:  50%|████▉     | 795/1600 [00:40<00:40, 19.69it/s]Training:  62%|██████▏   | 995/1600 [00:50<00:30, 19.75it/s]Training:  75%|███████▍  | 1196/1600 [01:00<00:20, 19.87it/s]Training:  88%|████████▊ | 1401/1600 [01:10<00:09, 20.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 198/160Training loss: 57.9115, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 251.8822, Validation accuracy: 0.7150
Macro F1-score: 0.7087
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.8065, Recall: 0.5000, F1_score: 0.6173
Model performance on Neutral speech (in validation): 
	Precision: 0.4800, Recall: 0.4800, F1_score: 0.4800
Model performance on Sad speech (in validation): 
	Precision: 0.6622, Recall: 0.9800, F1_score: 0.7903
Epoch 16/100

Training Phase:
0 [00:10<01:11, 19.71it/s]Training:  25%|██▍       | 396/1600 [00:20<01:01, 19.70it/s]Training:  37%|███▋      | 593/1600 [00:30<00:51, 19.66it/s]Training:  50%|████▉     | 794/1600 [00:40<00:40, 19.83it/s]Training:  62%|██████▏   | 995/1600 [00:50<00:30, 19.81it/s]Training:  75%|███████▍  | 1198/1600 [01:00<00:20, 19.95it/s]Training:  88%|████████▊ | 1401/1600 [01:10<00:10, 19.86it/s]Training: 100%|█████████▉| 1598/1600 [01:20<00:00, 19.65it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 191/1600 [00:10<01:13, 19.07it/s]Training:  24%|██▍       | 389/1600 [00:20<01:02, 19.48it/s]Training:  37%|███▋      | 587/1600 [00:30<00:51, 19.54it/s]Training:  49%|████▉     | 784/1600 [00:40<00Training loss: 35.5835, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 193.5275, Validation accuracy: 0.6750
Macro F1-score: 0.6749
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Happy speech (in validation): 
	Precision: 0.5962, Recall: 0.6200, F1_score: 0.6078
Model performance on Neutral speech (in validation): 
	Precision: 0.5400, Recall: 0.5400, F1_score: 0.5400
Model performance on Sad speech (in validation): 
	Precision: 0.6866, Recall: 0.9200, F1_score: 0.7863
Epoch 17/100

Training Phase:
:41, 19.57it/s]Training:  61%|██████▏   | 981/1600 [00:50<00:31, 19.59it/s]Training:  74%|███████▍  | 1180/1600 [01:00<00:21, 19.68it/s]Training:  86%|████████▌ | 1379/1600 [01:10<00:11, 19.63it/s]Training:  99%|█████████▉| 1580/1600 [01:20<00:01, 19.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 192/1600 [00:10<01:13, 19.19it/s]Training:  25%|██▍       | 395/1600 [00:20<01:00, 19.79it/s]Training:  37%|███▋      | 598/1600 [00:30<00:50, 19.79it/s]Training:  50%|████▉     | 796/1600 [00:40<00:41, 19.36it/s]Training:  50%|████▉     | 796/1600 [00:51<00:41, 19.36it/s]Training:  59%|█████▉    | 949/1600 [00:51<00:37, 17.28it/s]Training:  70%|███████   | 1122/1600 [01:01<Training loss: 24.9532, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 147.9106, Validation accuracy: 0.7150
Macro F1-score: 0.7150
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Model performance on Happy speech (in validation): 
	Precision: 0.5593, Recall: 0.6600, F1_score: 0.6055
Model performance on Neutral speech (in validation): 
	Precision: 0.6379, Recall: 0.7400, F1_score: 0.6852
Model performance on Sad speech (in validation): 
	Precision: 0.8214, Recall: 0.9200, F1_score: 0.8679
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.8250

Test Phase: 
00:27, 17.29it/s]Training:  81%|████████  | 1296/1600 [01:11<00:17, 17.30it/s]Training:  92%|█████████▏| 1476/1600 [01:21<00:07, 17.52it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|▌         | 10/200 [00:00<00:02, 91.22it/s]Testing:  10%|█         | 20/200 [00:00<00:01, 92.58it/s]Testing:  15%|█▌        | 30/200 [00:00<00:01, 93.15it/s]Testing:  20%|██        | 40/200 [00:00<00:01, 92.88it/s]Testing:  25%|██▌       | 50/200 [00:00<00:01, 93.27it/s]Testing:  30%|███       | 60/200 [00:00<00:01, 94.20it/s]Testing:  35%|███▌      | 70/200 [00:00<00:01, 94.10it/s]Testing:  40%|████      | 80/200 [00:00<00:01, 94.44it/s]Testing:  45%|████▌     | 90/200 [00:00<00:01, 94.72it/s]Testing:  50%|█████     | 100/200 [00:01<00:01,Test loss: 140.1273, Test accuracy: 0.7450
Macro F1-score: 0.7508
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Happy speech (in test): 
	Precision: 0.6981, Recall: 0.7400, F1_score: 0.7184
Model performance on Neutral speech (in test): 
	Precision: 0.5714, Recall: 0.6400, F1_score: 0.6038
Model performance on Sad speech (in test): 
	Precision: 0.7800, Recall: 0.7800, F1_score: 0.7800

======================= This is fold_2 on de =======================

Load dataset: 
Loading de train data: fold_2...
Preprocess de fold_2 data for de model
 94.38it/s]Testing:  55%|█████▌    | 110/200 [00:01<00:00, 95.00it/s]Testing:  60%|██████    | 120/200 [00:01<00:00, 95.27it/s]Testing:  65%|██████▌   | 130/200 [00:01<00:00, 94.64it/s]Testing:  70%|███████   | 140/200 [00:01<00:00, 94.75it/s]Testing:  75%|███████▌  | 150/200 [00:01<00:00, 95.19it/s]Testing:  80%|████████  | 160/200 [00:01<00:00, 95.69it/s]Testing:  85%|████████▌ | 170/200 [00:01<00:00, 95.72it/s]Testing:  90%|█████████ | 180/200 [00:01<00:00, 94.94it/s]Testing:  95%|█████████▌| 190/200 [00:02<00:00, 94.64it/s]Testing: 100%|██████████| 200/200 [00:02<00:00, 95.32it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 3/1600 [00:00<01:16, 20.91 examples/s]Map:   1%|          | 9/1600 [00:00<00:42, 37.54 examples/s]Map:   1%|          | 19/1600 [00:00<00:28, 55.20 examples/s]Map:   2%|▏         | 28/1600 [00:00<00:23, 65.53 examples/s]Map:   2%|▎         | 40/1600 [00:00<00:19, 79.08 examples/s]Map:   3%|▎         | 50/1600 [00:00<00:18, 81.62 examples/s]Map:   4%|▎         | 59/1600 [00:00<00:18, 81.74 examples/s]Map:   4%|▍         | 72/1600 [00:00<00:18, 81.45 examples/s]Map:   5%|▌         | 81/1600 [00:01<00:18, 81.25 examples/s]Map:   6%|▌         | 92/1600 [00:01<00:21, 69.95 examples/s]Map:   6%|▋         | 100/1600 [00:01<00:20, 71.48 examples/s]Map:   7%|▋         | 109/1600 [00:01<00:20, 71.50 examples/s]Map:   7%|▋         | 117/1600 [00:01<00:20, 72.15 examples/s]Map:   8%|▊         | 126/1600 [00:01<00:19, 74.50 examples/s]Map:   8%|▊         | 134/1600 [00:01<00:19, 73.98 examples/s]Map:   9%|▉         | 146/1600 [00:02<00:20, 70.94 examples/s]Map:  10%|▉         | 155/1600 [00:02<00:19, 72.70 examples/s]Map:  10%|█         | 164/1600 [00:02<00:19, 72.71 examples/s]Map:  11%|█         | 173/1600 [00:02<00:19, 73.74 examples/s]Map:  11%|█▏        | 181/1600 [00:02<00:19, 72.84 examples/s]Map:  12%|█▏        | 189/1600 [00:02<00:19, 72.85 examples/s]Map:  12%|█▏        | 198/1600 [00:02<00:19, 73.49 examples/s]Map:  13%|█▎        | 210/1600 [00:02<00:22, 62.57 examples/s]Map:  14%|█▎        | 218/1600 [00:03<00:21, 65.38 examples/s]Map:  14%|█▍        | 227/1600 [00:03<00:20, 68.29 examples/s]Map:  15%|█▍        | 236/1600 [00:03<00:18, 71.86 examples/s]Map:  15%|█▌        | 245/1600 [00:03<00:18, 73.79 examples/s]Map:  16%|█▌        | 253/1600 [00:03<00:18, 73.65 examples/s]Map:  16%|█▋        | 262/1600 [00:03<00:17, 75.93 examples/s]Map:  17%|█▋        | 271/1600 [00:03<00:17, 76.30 examples/s]Map:  18%|█▊        | 280/1600 [00:03<00:17, 75.42 examples/s]Map:  18%|█▊        | 289/1600 [00:04<00:16, 78.93 examples/s]Map:  19%|█▊        | 297/1600 [00:04<00:16, 78.43 examples/s]Map:  19%|█▉        | 307/1600 [00:04<00:16, 79.02 examples/s]Map:  20%|█▉        | 316/1600 [00:04<00:19, 64.82 examples/s]Map:  20%|██        | 326/1600 [00:04<00:18, 69.00 examples/s]Map:  21%|██        | 334/1600 [00:04<00:17, 70.38 examples/s]Map:  21%|██▏       | 342/1600 [00:04<00:17, 70.10 examples/s]Map:  22%|██▏       | 350/1600 [00:04<00:17, 70.66 examples/s]Map:  22%|██▏       | 359/1600 [00:05<00:17, 71.75 examples/s]Map:  23%|██▎       | 368/1600 [00:05<00:16, 73.94 examples/s]Map:  24%|██▎       | 377/1600 [00:05<00:16, 75.85 examples/s]Map:  24%|██▍       | 385/1600 [00:05<00:16, 75.17 examples/s]Map:  25%|██▍       | 395/1600 [00:05<00:15, 78.85 examples/s]Map:  25%|██▌       | 406/1600 [00:05<00:14, 83.54 examples/s]Map:  26%|██▌       | 416/1600 [00:05<00:13, 85.85 examples/s]Map:  27%|██▋       | 429/1600 [00:05<00:15, 75.97 examples/s]Map:  28%|██▊       | 442/1600 [00:05<00:13, 86.57 examples/s]Map:  28%|██▊       | 454/1600 [00:06<00:12, 92.67 examples/s]Map:  29%|██▉       | 464/1600 [00:06<00:12, 93.46 examples/s]Map:  30%|██▉       | 476/1600 [00:06<00:11, 97.94 examples/s]Map:  30%|███       | 488/1600 [00:06<00:12, 88.36 examples/s]Map:  31%|███▏      | 500/1600 [00:06<00:13, 81.69 examples/s]Map:  32%|███▏      | 509/1600 [00:06<00:13, 79.65 examples/s]Map:  33%|███▎      | 522/1600 [00:06<00:14, 73.79 examples/s]Map:  33%|███▎      | 530/1600 [00:07<00:17, 62.40 examples/s]Map:  34%|███▎      | 538/1600 [00:07<00:16, 62.82 examples/s]Map:  34%|███▍      | 546/1600 [00:07<00:16, 64.39 examples/s]Map:  35%|███▍      | 555/1600 [00:07<00:15, 65.95 examples/s]Map:  35%|███▌      | 564/1600 [00:07<00:15, 68.44 examples/s]Map:  36%|███▌      | 571/1600 [00:07<00:15, 67.26 examples/s]Map:  36%|███▌      | 578/1600 [00:07<00:15, 65.88 examples/s]Map:  37%|███▋      | 585/1600 [00:07<00:15, 64.98 examples/s]Map:  37%|███▋      | 593/1600 [00:08<00:15, 65.80 examples/s]Map:  38%|███▊      | 600/1600 [00:08<00:15, 64.52 examples/s]Map:  38%|███▊      | 608/1600 [00:08<00:14, 66.60 examples/s]Map:  38%|███▊      | 616/1600 [00:08<00:14, 66.52 examples/s]Map:  39%|███▉      | 623/1600 [00:08<00:17, 54.64 examples/s]Map:  39%|███▉      | 631/1600 [00:08<00:16, 57.42 examples/s]Map:  40%|███▉      | 638/1600 [00:08<00:16, 58.09 examples/s]Map:  40%|████      | 646/1600 [00:08<00:15, 62.98 examples/s]Map:  41%|████      | 654/1600 [00:09<00:14, 66.11 examples/s]Map:  41%|████▏     | 662/1600 [00:09<00:13, 67.62 examples/s]Map:  42%|████▏     | 670/1600 [00:09<00:13, 70.23 examples/s]Map:  42%|████▏     | 678/1600 [00:09<00:13, 69.40 examples/s]Map:  43%|████▎     | 688/1600 [00:09<00:13, 67.80 examples/s]Map:  43%|████▎     | 695/1600 [00:09<00:13, 67.98 examples/s]Map:  44%|████▍     | 704/1600 [00:09<00:12, 69.95 examples/s]Map:  45%|████▍     | 715/1600 [00:09<00:12, 68.29 examples/s]Map:  45%|████▌     | 724/1600 [00:10<00:15, 57.76 examples/s]Map:  46%|████▌     | 732/1600 [00:10<00:14, 61.21 examples/s]Map:  46%|████▋     | 740/1600 [00:10<00:13, 63.36 examples/s]Map:  47%|████▋     | 748/1600 [00:10<00:12, 65.87 examples/s]Map:  47%|████▋     | 756/1600 [00:10<00:12, 68.84 examples/s]Map:  48%|████▊     | 765/1600 [00:10<00:11, 73.21 examples/s]Map:  48%|████▊     | 773/1600 [00:10<00:11, 73.24 examples/s]Map:  49%|████▉     | 781/1600 [00:10<00:11, 73.40 examples/s]Map:  49%|████▉     | 789/1600 [00:11<00:11, 72.86 examples/s]Map:  50%|█████     | 801/1600 [00:11<00:10, 74.47 examples/s]Map:  51%|█████     | 809/1600 [00:11<00:14, 54.27 examples/s]Map:  51%|█████     | 816/1600 [00:11<00:14, 52.66 examples/s]Map:  52%|█████▏    | 825/1600 [00:12<00:30, 25.61 examples/s]Map:  52%|█████▏    | 834/1600 [00:12<00:23, 32.90 examples/s]Map:  53%|█████▎    | 843/1600 [00:12<00:18, 39.85 examples/s]Map:  53%|█████▎    | 852/1600 [00:12<00:16, 46.72 examples/s]Map:  54%|█████▍    | 862/1600 [00:12<00:13, 55.06 examples/s]Map:  54%|█████▍    | 871/1600 [00:12<00:12, 60.39 examples/s]Map:  55%|█████▌    | 880/1600 [00:13<00:11, 64.64 examples/s]Map:  56%|█████▌    | 888/1600 [00:13<00:11, 63.41 examples/s]Map:  56%|█████▌    | 896/1600 [00:13<00:10, 64.68 examples/s]Map:  57%|█████▋    | 906/1600 [00:13<00:11, 62.43 examples/s]Map:  57%|█████▋    | 916/1600 [00:13<00:12, 52.80 examples/s]Map:  58%|█████▊    | 922/1600 [00:13<00:12, 53.70 examples/s]Map:  58%|█████▊    | 928/1600 [00:13<00:12, 52.26 examples/s]Map:  58%|█████▊    | 934/1600 [00:14<00:12, 53.18 examples/s]Map:  59%|█████▉    | 942/1600 [00:14<00:11, 56.01 examples/s]Map:  59%|█████▉    | 949/1600 [00:14<00:11, 55.90 examples/s]Map:  60%|█████▉    | 956/1600 [00:14<00:10, 58.65 examples/s]Map:  60%|██████    | 962/1600 [00:14<00:11, 57.95 examples/s]Map:  61%|██████    | 970/1600 [00:14<00:10, 59.87 examples/s]Map:  61%|██████    | 978/1600 [00:14<00:09, 62.49 examples/s]Map:  62%|██████▏   | 985/1600 [00:14<00:10, 60.48 examples/s]Map:  62%|██████▏   | 993/1600 [00:15<00:09, 62.01 examples/s]Map:  62%|██████▏   | 997/1600 [00:28<00:09, 62.01 examples/s]Map:  62%|██████▎   | 1000/1600 [01:15<24:46,  2.48s/ examples]Map:  63%|██████▎   | 1008/1600 [01:15<16:43,  1.69s/ examples]Map:  64%|██████▎   | 1016/1600 [01:15<11:23,  1.17s/ examples]Map:  64%|██████▍   | 1023/1600 [01:15<08:08,  1.18 examples/s]Map:  64%|██████▍   | 1030/1600 [01:15<05:46,  1.65 examples/s]Map:  65%|██████▍   | 1038/1600 [01:16<03:55,  2.39 examples/s]Map:  65%|██████▌   | 1046/1600 [01:16<02:42,  3.42 examples/s]Map:  66%|██████▌   | 1056/1600 [01:16<01:45,  5.16 examples/s]Map:  66%|██████▋   | 1063/1600 [01:16<01:18,  6.81 examples/s]Map:  67%|██████▋   | 1072/1600 [01:16<00:55,  9.53 examples/s]Map:  67%|██████▋   | 1079/1600 [01:16<00:42, 12.31 examples/s]Map:  68%|██████▊   | 1086/1600 [01:16<00:33, 15.55 examples/s]Map:  68%|██████▊   | 1093/1600 [01:16<00:26, 19.50 examples/s]Map:  69%|██████▉   | 1100/1600 [01:17<00:20, 24.39 examples/s]Map:  69%|██████▉   | 1108/1600 [01:17<00:15, 30.82 examples/s]Map:  70%|██████▉   | 1116/1600 [01:17<00:13, 36.95 examples/s]Map:  70%|███████   | 1123/1600 [01:17<00:13, 34.82 examples/s]Map:  71%|███████   | 1131/1600 [01:17<00:11, 40.99 examples/s]Map:  71%|███████   | 1138/1600 [01:17<00:10, 45.36 examples/s]Map:  72%|███████▏  | 1147/1600 [01:17<00:08, 52.28 examples/s]Map:  72%|███████▏  | 1154/1600 [01:18<00:08, 54.71 examples/s]Map:  73%|███████▎  | 1162/1600 [01:18<00:07, 58.35 examples/s]Map:  73%|███████▎  | 1169/1600 [01:18<00:07, 59.93 examples/s]Map:  74%|███████▎  | 1178/1600 [01:18<00:07, 58.89 examples/s]Map:  74%|███████▍  | 1185/1600 [01:18<00:07, 59.27 examples/s]Map:  75%|███████▍  | 1194/1600 [01:18<00:06, 62.41 examples/s]Map:  75%|███████▌  | 1201/1600 [01:18<00:06, 63.52 examples/s]Map:  76%|███████▌  | 1209/1600 [01:18<00:06, 64.10 examples/s]Map:  76%|███████▌  | 1217/1600 [01:18<00:05, 65.76 examples/s]Map:  77%|███████▋  | 1228/1600 [01:19<00:06, 61.88 examples/s]Map:  77%|███████▋  | 1236/1600 [01:19<00:05, 62.67 examples/s]Map:  78%|███████▊  | 1243/1600 [01:19<00:07, 49.33 examples/s]Map:  78%|███████▊  | 1251/1600 [01:19<00:06, 54.31 examples/s]Map:  79%|███████▊  | 1258/1600 [01:19<00:06, 55.11 examples/s]Map:  79%|███████▉  | 1265/1600 [01:19<00:05, 57.85 examples/s]Map:  80%|███████▉  | 1273/1600 [01:19<00:05, 63.03 examples/s]Map:  80%|████████  | 1281/1600 [01:20<00:04, 64.40 examples/s]Map:  81%|████████  | 1291/1600 [01:20<00:04, 63.42 examples/s]Map:  81%|████████  | 1298/1600 [01:20<00:04, 63.49 examples/s]Map:  82%|████████▏ | 1305/1600 [01:20<00:04, 61.68 examples/s]Map:  82%|████████▏ | 1312/1600 [01:20<00:04, 61.92 examples/s]Map:  82%|████████▎ | 1320/1600 [01:20<00:04, 64.09 examples/s]Map:  83%|████████▎ | 1328/1600 [01:20<00:04, 63.25 examples/s]Map:  84%|████████▎ | 1337/1600 [01:21<00:04, 59.27 examples/s]Map:  84%|████████▍ | 1345/1600 [01:21<00:04, 60.57 examples/s]Map:  84%|████████▍ | 1352/1600 [01:21<00:04, 49.67 examples/s]Map:  85%|████████▍ | 1359/1600 [01:21<00:04, 51.74 examples/s]Map:  85%|████████▌ | 1367/1600 [01:21<00:04, 55.98 examples/s]Map:  86%|████████▌ | 1375/1600 [01:21<00:03, 58.49 examples/s]Map:  86%|████████▋ | 1382/1600 [01:21<00:03, 58.24 examples/s]Map:  87%|████████▋ | 1390/1600 [01:21<00:03, 58.04 examples/s]Map:  87%|████████▋ | 1398/1600 [01:22<00:03, 58.96 examples/s]Map:  88%|████████▊ | 1404/1600 [01:22<00:03, 57.81 examples/s]Map:  88%|████████▊ | 1411/1600 [01:22<00:03, 59.11 examples/s]Map:  89%|████████▊ | 1419/1600 [01:22<00:02, 61.10 examples/s]Map:  89%|████████▉ | 1426/1600 [01:22<00:02, 60.67 examples/s]Map:  90%|████████▉ | 1433/1600 [01:22<00:02, 60.29 examples/s]Map:  90%|█████████ | 1441/1600 [01:22<00:02, 64.27 examples/s]Map:  90%|█████████ | 1448/1600 [01:22<00:02, 63.81 examples/s]Map:  91%|█████████ | 1458/1600 [01:23<00:02, 62.76 examples/s]Map:  92%|█████████▏| 1466/1600 [01:23<00:02, 49.41 examples/s]Map:  92%|█████████▏| 1473/1600 [01:23<00:02, 52.69 examples/s]Map:  92%|█████████▎| 1480/1600 [01:23<00:02, 55.28 examples/s]Map:  93%|█████████▎| 1488/1600 [01:23<00:01, 59.21 examples/s]Map:  93%|█████████▎| 1495/1600 [01:23<00:01, 59.87 examples/s]Map:  94%|█████████▍| 1502/1600 [01:23<00:01, 59.68 examples/s]Map:  94%|█████████▍| 1509/1600 [01:24<00:01, 57.76 examples/s]Map:  95%|█████████▍| 1517/1600 [01:24<00:01, 59.15 examples/s]Map:  95%|█████████▌| 1524/1600 [01:24<00:01, 61.28 examples/s]Map:  96%|█████████▌| 1532/1600 [01:24<00:01, 62.20 examples/s]Map:  96%|█████████▋| 1540/1600 [01:24<00:00, 63.11 examples/s]Map:  97%|█████████▋| 1547/1600 [01:24<00:00, 61.47 examples/s]Map:  97%|█████████▋| 1554/1600 [01:24<00:00, 61.36 examples/s]Map:  98%|█████████▊| 1561/1600 [01:24<00:00, 59.64 examples/s]Map:  98%|█████████▊| 1567/1600 [01:24<00:00, 59.38 examples/s]Map:  98%|█████████▊| 1575/1600 [01:25<00:00, 59.92 examples/s]Map:  99%|█████████▉| 1584/1600 [01:25<00:00, 50.07 examples/s]Map: 100%|█████████▉| 1592/1600 [01:25<00:00, 54.75 examples/s]Map: 100%|█████████▉| 1598/1600 [01:25<00:00, 55.71 examples/s]Map: 100%|█████████▉| 1598/1600 [01:38<00:00, 55.71 examples/s]Map: 100%|██████████| 1600/1600 [02:10<00:00,  2.37s/ examples]Map: 100%|██████████| 1600/1600 [02:10<00:00, 12.29 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   1%|          | 2/200 [00:00<00:13, 14.96 examples/s]Map:   2%|▎         | 5/200 [00:00<00:10, 19.49 examples/s]Map:   6%|▌         | 12/200 [00:00<00:05, 37.56 examples/s]Map:  10%|█         | 21/200 [00:00<00:03, 52.31 examples/s]Map:  16%|█▌        | 31/200 [00:00<00:02, 63.72 examples/s]Map:  20%|██        | 40/200 [00:00<00:02, 70.82 examples/s]Map:  24%|██▍       | 48/200 [00:00<00:02, 70.65 examples/s]Map:  30%|███       | 61/200 [00:00<00:01, 80.04 examples/s]Map:  34%|███▍      | 69/200 [00:01<00:01, 76.36 examples/s]Map:  40%|████      | 81/200 [00:01<00:01, 74.72 examples/s]Map:  44%|████▍     | 89/200 [00:01<00:01, 75.19 examples/s]Map:  50%|█████     | 100/200 [00:01<00:01, 63.31 examples/s]Map:  54%|█████▍    | 108/200 [00:01<00:01, 65.14 examples/s]Map:  57%|█████▊    | 115/200 [00:01<00:01, 63.79 examples/s]Map:  62%|██████▏   | 123/200 [00:01<00:01, 65.58 examples/s]Map:  66%|██████▌   | 132/200 [00:02<00:01, 65.82 examples/s]Map:  70%|███████   | 140/200 [00:02<00:00, 64.13 examples/s]Map:  74%|███████▎  | 147/200 [00:02<00:00, 62.93 examples/s]Map:  78%|███████▊  | 155/200 [00:02<00:00, 60.93 examples/s]Map:  81%|████████  | 162/200 [00:02<00:00, 60.20 examples/s]Map:  84%|████████▍ | 169/200 [00:02<00:00, 59.35 examples/s]Map:  88%|████████▊ | 177/200 [00:02<00:00, 62.84 examples/s]Map:  92%|█████████▏| 184/200 [00:02<00:00, 62.98 examples/s]Map:  96%|█████████▌| 191/200 [00:03<00:00, 61.80 examples/s]Map: 100%|██████████| 200/200 [00:16<00:00,  1.88 examples/s]Map: 100%|██████████| 200/200 [00:16<00:00, 12.04 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   1%|          | 2/200 [00:00<00:10, 18.28 examples/s]Map:   4%|▎         | 7/200 [00:00<00:05, 33.94 examples/s]Map:   7%|▋         | 14/200 [00:00<00:04, 46.09 examples/s]Map:  12%|█▏        | 23/200 [00:00<00:03, 58.18 examples/s]Map:  16%|█▌        | 32/200 [00:00<00:02, 65.25 examples/s]Map:  21%|██        | 42/200 [00:00<00:02, 71.45 examples/s]Map:  26%|██▌       | 52/200 [00:00<00:01, 77.54 examples/s]Map:  32%|███▏      | 63/200 [00:00<00:01, 82.71 examples/s]Map:  37%|███▋      | 74/200 [00:01<00:01, 75.85 examples/s]Map:  41%|████      | 82/200 [00:01<00:01, 74.33 examples/s]Map:  46%|████▌     | 92/200 [00:01<00:01, 59.30 examples/s]Map:  50%|█████     | 100/200 [00:01<00:01, 62.95 examples/s]Map:  55%|█████▌    | 110/200 [00:01<00:01, 69.43 examples/s]Map:  60%|██████    | 121/200 [00:01<00:01, 68.20 examples/s]Map:  64%|██████▍   | 129/200 [00:01<00:01, 67.75 examples/s]Map:  68%|██████▊   | 137/200 [00:02<00:00, 68.45 examples/s]Map:  72%|███████▎  | 145/200 [00:02<00:00, 68.71 examples/s]Map:  76%|███████▋  | 153/200 [00:02<00:00, 70.67 examples/s]Map:  80%|████████  | 161/200 [00:02<00:00, 67.62 examples/s]Map:  84%|████████▍ | 169/200 [00:02<00:00, 69.38 examples/s]Map:  90%|████████▉ | 179/200 [00:02<00:00, 64.10 examples/s]Map:  94%|█████████▎| 187/200 [00:02<00:00, 64.46 examples/s]Map:  98%|█████████▊| 195/200 [00:02<00:00, 67.38 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 13.15 examples/s]
Loading cn eval data: fold_2...
Preprocess cn fold_2 data for de model
Loading cn test data: fold_2...
Preprocess cn fold_2 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 371.9733, Training accuracy: 0.9200
Macro F1-score: 0.9201
Model performance on Angry speech (in training): 
	Precision: 0.9293, Recall: 0.9200, F1_score: 0.9246
Model performance on Happy speech (in training): 
	Precision: 0.8878, Recall: 0.8700, F1_score: 0.8788
Model performance on Neutral speech (in training): 
	Precision: 0.8884, Recall: 0.9350, F1_score: 0.9111
Model performance on Sad speech (in training): 
	Precision: 0.9770, Recall: 0.9550, F1_score: 0.9659

Eval Phase: 
Validation loss: 363.9687, Validation accuracy: 0.4600
Macro F1-score: 0.3792
Model performance on Angry speech (in validation): 
	Precision: 0.4699, Recall: 0.7800, F1_score: 0.5865
Model performance on Happy speech (in validation): 
	Precision: 0.3043, Recall: 0.1400, F1_score: 0.1918
Model performance on Neutral speech (in validation): 
	Precision: 0.5059, Recall: 0.8600, F1_score: 0.6370
Model performance on Sad speech (in validation): 
	Precision: 0.3333, Recall: 0.0600, F1_score: 0.1017
New best accuracy for layer 4 on epoch 1: 0.4600. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 197/1600 [00:10<01:11, 19.69it/s]Training:  25%|██▍       | 394/1600 [00:20<01:01, 19.46it/s]Training:  37%|███▋      | 588/1600 [00:30<00:52, 19.42it/s]Training:  49%|████▉     | 789/1600 [00:40<00:41, 19.66it/s]Training:  62%|██████▏   | 990/1600 [00:50<00:30, 19.78it/s]Training:  74%|███████▍  | 1190/1600 [01:00<00:20, 19.67it/s]Training:  87%|████████▋ | 1385/1600 [01:10<00:10, 19.60it/s]Training:  99%|█████████▉| 1582/1600 [01:20<00:00, 19.61it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.45it/s]Training:  24%|██▍       | 392/1600 [00:20<01:01, 19.55it/s]Training:  37%|███▋      |Training loss: 131.4789, Training accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in training): 
	Precision: 0.9723, Recall: 0.9650, F1_score: 0.9686
Model performance on Happy speech (in training): 
	Precision: 0.9523, Recall: 0.9475, F1_score: 0.9499
Model performance on Neutral speech (in training): 
	Precision: 0.9631, Recall: 0.9775, F1_score: 0.9702
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 566.6859, Validation accuracy: 0.4100
Macro F1-score: 0.3353
Model performance on Angry speech (in validation): 
	Precision: 0.4694, Recall: 0.9200, F1_score: 0.6216
Model performance on Happy speech (in validation): 
	Precision: 0.0833, Recall: 0.0200, F1_score: 0.0323
Model performance on Neutral speech (in validation): 
	Precision: 0.4355, Recall: 0.5400, F1_score: 0.4821
Model performance on Sad speech (in validation): 
	Precision: 0.2857, Recall: 0.1600, F1_score: 0.2051
Epoch 3/100

Training Phase:
 592/1600 [00:30<00:51, 19.74it/s]Training:  50%|████▉     | 792/1600 [00:40<00:41, 19.69it/s]Training:  62%|██████▏   | 989/1600 [00:50<00:31, 19.49it/s]Training:  74%|███████▍  | 1188/1600 [01:00<00:21, 19.61it/s]Training:  87%|████████▋ | 1387/1600 [01:10<00:10, 19.56it/s]Training:  99%|█████████▉| 1588/1600 [01:20<00:00, 19.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.99it/s]Training:  25%|██▌       | 400/1600 [00:20<01:00, 19.73it/s]Training:  37%|███▋      | 596/1600 [00:30<00:51, 19.63it/s]Training:  50%|████▉     | 796/1600 [00:40<00:40, 19.76it/s]Training:  62%|██████▏   | 996/1600 [00:50<00:30, 19.54it/s]Training:  75%|███████Training loss: 88.3128, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9848, Recall: 0.9750, F1_score: 0.9799
Model performance on Happy speech (in training): 
	Precision: 0.9678, Recall: 0.9775, F1_score: 0.9726
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Validation loss: 374.6943, Validation accuracy: 0.5150
Macro F1-score: 0.5105
Model performance on Angry speech (in validation): 
	Precision: 0.5333, Recall: 0.6400, F1_score: 0.5818
Model performance on Happy speech (in validation): 
	Precision: 0.4884, Recall: 0.4200, F1_score: 0.4516
Model performance on Neutral speech (in validation): 
	Precision: 0.5250, Recall: 0.4200, F1_score: 0.4667
Model performance on Sad speech (in validation): 
	Precision: 0.5088, Recall: 0.5800, F1_score: 0.5421
New best accuracy for layer 4 on epoch 3: 0.5150. Model saved.
Epoch 4/100

Training Phase:
  | 1194/1600 [01:00<00:20, 19.61it/s]Training:  87%|████████▋ | 1394/1600 [01:10<00:10, 19.70it/s]Training: 100%|█████████▉| 1594/1600 [01:20<00:00, 19.75it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 197/1600 [00:10<01:11, 19.62it/s]Training:  25%|██▍       | 394/1600 [00:20<01:01, 19.51it/s]Training:  37%|███▋      | 593/1600 [00:30<00:51, 19.67it/s]Training:  50%|████▉     | 792/1600 [00:40<00:40, 19.74it/s]Training:  62%|██████▏   | 992/1600 [00:50<00:30, 19.82it/s]Training:  74%|███████▍  | 1192/1600 [01:00<00:20, 19.72it/s]Training:  87%|████████▋ | 1388/1600 [01:10<00:10, 19.63it/s]Training:  99%|█████████▉| 1585/1600 [01:20<00:00, 19.63it/s]               Training loss: 68.5706, Training accuracy: 0.9838
Macro F1-score: 0.9838
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9750, F1_score: 0.9836
Model performance on Happy speech (in training): 
	Precision: 0.9631, Recall: 0.9775, F1_score: 0.9702
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 568.0959, Validation accuracy: 0.4250
Macro F1-score: 0.4080
Model performance on Angry speech (in validation): 
	Precision: 0.5532, Recall: 0.5200, F1_score: 0.5361
Model performance on Happy speech (in validation): 
	Precision: 0.3947, Recall: 0.6000, F1_score: 0.4762
Model performance on Neutral speech (in validation): 
	Precision: 0.4314, Recall: 0.4400, F1_score: 0.4356
Model performance on Sad speech (in validation): 
	Precision: 0.2692, Recall: 0.1400, F1_score: 0.1842
Epoch 5/100

Training Phase:
Training loss: 60.1498, Training accuracy: 0.9881
Macro F1-score: 0.9881
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9825, F1_score: 0.9874
Model performance on Happy speech (in training): 
	Precision: 0.9777, Recall: 0.9875, F1_score: 0.9826
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Validation loss: 390.7237, Validation accuracy: 0.5450
Macro F1-score: 0.5429
Model performance on Angry speech (in validation): 
	Precision: 0.6190, Recall: 0.5200, F1_score: 0.5652
Model performance on Happy speech (in validation): 
	Precision: 0.5238, Recall: 0.6600, F1_score: 0.5841
Model performance on Neutral speech (in validation): 
	Precision: 0.5385, Recall: 0.5600, F1_score: 0.5490
Model performance on Sad speech (in validation): 
	Precision: 0.5116, Recall: 0.4400, F1_score: 0.4731
New best accuracy for layer 4 on epoch 5: 0.5450. Model saved.
Epoch 6/100

Training Phase:
                                              Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 202/1600 [00:10<01:09, 20.12it/s]Training:  25%|██▌       | 404/1600 [00:20<00:59, 19.99it/s]Training:  38%|███▊      | 604/1600 [00:30<00:49, 19.92it/s]Training:  50%|█████     | 803/1600 [00:40<00:40, 19.75it/s]Training:  63%|██████▎   | 1004/1600 [00:50<00:30, 19.86it/s]Training:  75%|███████▌  | 1205/1600 [01:00<00:19, 19.81it/s]Training:  88%|████████▊ | 1403/1600 [01:10<00:09, 19.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 201/1600 [00:10<01:09, 20.09it/s]Training:  25%|██▌      Training loss: 40.5330, Training accuracy: 0.9888
Macro F1-score: 0.9888
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 501.8183, Validation accuracy: 0.4950
Macro F1-score: 0.4772
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.7200, F1_score: 0.5902
Model performance on Happy speech (in validation): 
	Precision: 0.4242, Recall: 0.2800, F1_score: 0.3373
Model performance on Neutral speech (in validation): 
	Precision: 0.5246, Recall: 0.6400, F1_score: 0.5766
Model performance on Sad speech (in validation): 
	Precision: 0.5000, Recall: 0.3400, F1_score: 0.4048
Epoch 7/100

Training Phase:
 | 402/1600 [00:20<01:00, 19.87it/s]Training:  38%|███▊      | 601/1600 [00:30<00:50, 19.88it/s]Training:  50%|█████     | 801/1600 [00:40<00:40, 19.93it/s]Training:  63%|██████▎   | 1002/1600 [00:50<00:29, 19.97it/s]Training:  75%|███████▌  | 1203/1600 [01:00<00:19, 19.96it/s]Training:  88%|████████▊ | 1403/1600 [01:10<00:09, 19.95it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 199/1600 [00:10<01:10, 19.89it/s]Training:  25%|██▍       | 398/1600 [00:20<01:00, 19.88it/s]Training:  37%|███▋      | 599/1600 [00:30<00:50, 19.94it/s]Training:  50%|█████     | 802/1600 [00:40<00:39, 20.06it/s]Training:  63%|██████▎   | 1005/1600 [00:50<00:29, 20.05it/s]Training:  75%|███████▌  | 120Training loss: 51.2026, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 492.4777, Validation accuracy: 0.5050
Macro F1-score: 0.4696
Model performance on Angry speech (in validation): 
	Precision: 0.4778, Recall: 0.8600, F1_score: 0.6143
Model performance on Happy speech (in validation): 
	Precision: 0.3158, Recall: 0.1200, F1_score: 0.1739
Model performance on Neutral speech (in validation): 
	Precision: 0.6250, Recall: 0.5000, F1_score: 0.5556
Model performance on Sad speech (in validation): 
	Precision: 0.5294, Recall: 0.5400, F1_score: 0.5347
Epoch 8/100

Training Phase:
Training loss: 41.5434, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
6/1600 [01:00<00:19, 19.81it/s]Training:  88%|████████▊ | 1406/1600 [01:10<00:09, 19.84it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.40it/s]Training:  24%|██▍       | 390/1600 [00:20<01:03, 19.18it/s]Training:  36%|███▋      | 584/1600 [00:30<00:52, 19.26it/s]Training:  49%|████▉     | 782/1600 [00:40<00:42, 19.45it/s]Training:  61%|██████▏   | 982/1600 [00:50<00:31, 19.63it/s]Training:  74%|███████▍  | 1182/1600 [01:00<00:21, 19.70it/s]Training:  86%|████████▋ | 1384/1600 [01:10<00:10, 19.86it/s]Training:  99%|█████████▉| 1591/1600 [01:20<00:00, 20.10it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?Validation loss: 590.7066, Validation accuracy: 0.4950
Macro F1-score: 0.4723
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.8000, F1_score: 0.6154
Model performance on Happy speech (in validation): 
	Precision: 0.3846, Recall: 0.2000, F1_score: 0.2632
Model performance on Neutral speech (in validation): 
	Precision: 0.5455, Recall: 0.4800, F1_score: 0.5106
Model performance on Sad speech (in validation): 
	Precision: 0.5000, Recall: 0.5000, F1_score: 0.5000
Epoch 9/100

Training Phase:
Training loss: 43.5107, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 448.8813, Validation accuracy: 0.5350
Macro F1-score: 0.5195
Model performance on Angry speech (in validation): 
	Precision: 0.5254, Recall: 0.6200, F1_score: 0.5688
Model performance on Happy speech (in validation): 
	Precision: 0.4783, Recall: 0.4400, F1_score: 0.4583
Model performance on Neutral speech (in validation): 
	Precision: 0.6818, Recall: 0.3000, F1_score: 0.4167
Model performance on Sad speech (in validation): 
	Precision: 0.5342, Recall: 0.7800, F1_score: 0.6341
Epoch 10/100

Training Phase:
, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 197/1600 [00:10<01:11, 19.62it/s]Training:  25%|██▍       | 394/1600 [00:20<01:02, 19.44it/s]Training:  37%|███▋      | 589/1600 [00:30<00:51, 19.45it/s]Training:  49%|████▉     | 784/1600 [00:40<00:42, 19.42it/s]Training:  61%|██████▏   | 983/1600 [00:50<00:31, 19.59it/s]Training:  74%|███████▍  | 1182/1600 [01:00<00:21, 19.59it/s]Training:  86%|████████▌ | 1379/1600 [01:10<00:11, 19.55it/s]Training:  98%|█████████▊| 1574/1600 [01:20<00:01, 19.39it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 190/1600 [00:10<01:14, 18.92it/s]Training:  24%|██▍       | 384/160Training loss: 13.6729, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9925, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9877, Recall: 1.0000, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 574.7898, Validation accuracy: 0.4900
Macro F1-score: 0.4863
Model performance on Angry speech (in validation): 
	Precision: 0.5082, Recall: 0.6200, F1_score: 0.5586
Model performance on Happy speech (in validation): 
	Precision: 0.4651, Recall: 0.4000, F1_score: 0.4301
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.4200, F1_score: 0.4565
Model performance on Sad speech (in validation): 
	Precision: 0.4815, Recall: 0.5200, F1_score: 0.5000
Epoch 11/100

Training Phase:
0 [00:20<01:03, 19.19it/s]Training:  36%|███▌      | 578/1600 [00:30<00:53, 19.23it/s]Training:  48%|████▊     | 775/1600 [00:40<00:42, 19.37it/s]Training:  61%|██████    | 971/1600 [00:50<00:32, 19.41it/s]Training:  73%|███████▎  | 1170/1600 [01:00<00:21, 19.56it/s]Training:  86%|████████▌ | 1369/1600 [01:10<00:11, 19.42it/s]Training:  98%|█████████▊| 1566/1600 [01:20<00:01, 19.48it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 203/1600 [00:10<01:08, 20.26it/s]Training:  25%|██▌       | 406/1600 [00:20<00:59, 19.91it/s]Training:  38%|███▊      | 605/1600 [00:30<00:50, 19.87it/s]Training:  50%|█████     | 804/1600 [00:40<00:40, 19.60it/s]Training:  63%|██████▎   | 1001/1600 [0Training loss: 25.8483, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 707.4042, Validation accuracy: 0.5000
Macro F1-score: 0.4572
Model performance on Angry speech (in validation): 
	Precision: 0.5490, Recall: 0.5600, F1_score: 0.5545
Model performance on Happy speech (in validation): 
	Precision: 0.4906, Recall: 0.5200, F1_score: 0.5049
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.1000, F1_score: 0.1667
Model performance on Sad speech (in validation): 
	Precision: 0.4767, Recall: 0.8200, F1_score: 0.6029
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.5450

Test Phase: 
0:50<00:30, 19.61it/s]Training:  75%|███████▍  | 1198/1600 [01:00<00:20, 19.52it/s]Training:  75%|███████▍  | 1198/1600 [01:10<00:20, 19.52it/s]Training:  87%|████████▋ | 1394/1600 [01:10<00:10, 19.53it/s]Training: 100%|█████████▉| 1592/1600 [01:21<00:00, 19.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|▌         | 10/200 [00:00<00:02, 93.10it/s]Testing:  10%|█         | 20/200 [00:00<00:01, 93.62it/s]Testing:  16%|█▌        | 31/200 [00:00<00:01, 96.39it/s]Testing:  20%|██        | 41/200 [00:00<00:01, 96.60it/s]Testing:  26%|██▌       | 51/200 [00:00<00:01, 95.52it/s]Testing:  30%|███       | 61/200 [00:00<00:01, 96.91it/s]Testing:  36%|███▌      | 71/200 [00:00<00:01, 96.08it/s]Testing:  40%|████  Test loss: 423.3249, Test accuracy: 0.5600
Macro F1-score: 0.5526
Model performance on Angry speech (in test): 
	Precision: 0.5789, Recall: 0.4400, F1_score: 0.5000
Model performance on Happy speech (in test): 
	Precision: 0.5224, Recall: 0.7000, F1_score: 0.5983
Model performance on Neutral speech (in test): 
	Precision: 0.5763, Recall: 0.6800, F1_score: 0.6239
Model performance on Sad speech (in test): 
	Precision: 0.5833, Recall: 0.4200, F1_score: 0.4884

======================= This is fold_3 on de =======================

Load dataset: 
Loading de train data: fold_3...
Preprocess de fold_3 data for de model
    | 81/200 [00:00<00:01, 95.32it/s]Testing:  46%|████▌     | 91/200 [00:00<00:01, 95.82it/s]Testing:  50%|█████     | 101/200 [00:01<00:01, 95.64it/s]Testing:  56%|█████▌    | 111/200 [00:01<00:00, 94.57it/s]Testing:  61%|██████    | 122/200 [00:01<00:00, 96.25it/s]Testing:  66%|██████▌   | 132/200 [00:01<00:00, 96.13it/s]Testing:  71%|███████   | 142/200 [00:01<00:00, 95.42it/s]Testing:  76%|███████▋  | 153/200 [00:01<00:00, 96.99it/s]Testing:  82%|████████▏ | 163/200 [00:01<00:00, 97.34it/s]Testing:  86%|████████▋ | 173/200 [00:01<00:00, 96.23it/s]Testing:  92%|█████████▏| 184/200 [00:01<00:00, 97.01it/s]Testing:  97%|█████████▋| 194/200 [00:02<00:00, 97.30it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 3/1600 [00:00<01:07, 23.56 examples/s]Map:   1%|          | 12/1600 [00:00<00:30, 52.73 examples/s]Map:   1%|▏         | 20/1600 [00:00<00:25, 61.14 examples/s]Map:   2%|▏         | 29/1600 [00:00<00:23, 67.71 examples/s]Map:   2%|▎         | 40/1600 [00:00<00:19, 80.37 examples/s]Map:   3%|▎         | 51/1600 [00:00<00:17, 86.56 examples/s]Map:   4%|▍         | 61/1600 [00:00<00:17, 87.73 examples/s]Map:   4%|▍         | 71/1600 [00:00<00:17, 88.99 examples/s]Map:   5%|▌         | 80/1600 [00:01<00:17, 87.58 examples/s]Map:   6%|▌         | 93/1600 [00:01<00:18, 81.46 examples/s]Map:   6%|▋         | 102/1600 [00:01<00:18, 80.25 examples/s]Map:   7%|▋         | 113/1600 [00:01<00:19, 77.02 examples/s]Map:   8%|▊         | 123/1600 [00:01<00:20, 70.33 examples/s]Map:   8%|▊         | 132/1600 [00:01<00:20, 70.75 examples/s]Map:   9%|▉         | 140/1600 [00:01<00:20, 70.56 examples/s]Map:   9%|▉         | 148/1600 [00:02<00:25, 56.28 examples/s]Map:  10%|▉         | 158/1600 [00:02<00:23, 62.45 examples/s]Map:  10%|█         | 166/1600 [00:02<00:21, 65.73 examples/s]Map:  11%|█         | 175/1600 [00:02<00:20, 69.34 examples/s]Map:  11%|█▏        | 183/1600 [00:02<00:19, 70.94 examples/s]Map:  12%|█▏        | 191/1600 [00:02<00:19, 72.19 examples/s]Map:  12%|█▎        | 200/1600 [00:02<00:18, 74.11 examples/s]Map:  13%|█▎        | 209/1600 [00:02<00:18, 74.03 examples/s]Map:  14%|█▎        | 219/1600 [00:03<00:17, 76.80 examples/s]Map:  14%|█▍        | 228/1600 [00:03<00:17, 77.53 examples/s]Map:  15%|█▍        | 236/1600 [00:03<00:18, 74.47 examples/s]Map:  15%|█▌        | 244/1600 [00:03<00:18, 73.73 examples/s]Map:  16%|█▌        | 252/1600 [00:03<00:18, 73.65 examples/s]Map:  16%|█▋        | 262/1600 [00:03<00:22, 58.97 examples/s]Map:  17%|█▋        | 272/1600 [00:03<00:19, 67.08 examples/s]Map:  18%|█▊        | 281/1600 [00:03<00:18, 69.55 examples/s]Map:  18%|█▊        | 290/1600 [00:04<00:18, 71.67 examples/s]Map:  19%|█▊        | 299/1600 [00:04<00:17, 73.46 examples/s]Map:  19%|█▉        | 307/1600 [00:04<00:17, 73.60 examples/s]Map:  20%|█▉        | 316/1600 [00:04<00:17, 73.72 examples/s]Map:  20%|██        | 324/1600 [00:04<00:17, 74.78 examples/s]Map:  21%|██        | 332/1600 [00:04<00:17, 74.24 examples/s]Map:  21%|██▏       | 341/1600 [00:04<00:17, 73.90 examples/s]Map:  22%|██▏       | 349/1600 [00:04<00:16, 74.05 examples/s]Map:  22%|██▏       | 358/1600 [00:04<00:16, 74.86 examples/s]Map:  23%|██▎       | 367/1600 [00:05<00:16, 75.63 examples/s]Map:  23%|██▎       | 375/1600 [00:05<00:19, 62.68 examples/s]Map:  24%|██▍       | 383/1600 [00:05<00:18, 66.29 examples/s]Map:  24%|██▍       | 392/1600 [00:05<00:17, 68.35 examples/s]Map:  25%|██▌       | 401/1600 [00:05<00:17, 69.80 examples/s]Map:  26%|██▌       | 414/1600 [00:05<00:14, 81.62 examples/s]Map:  27%|██▋       | 425/1600 [00:05<00:13, 87.91 examples/s]Map:  27%|██▋       | 436/1600 [00:05<00:12, 92.51 examples/s]Map:  28%|██▊       | 447/1600 [00:06<00:12, 96.07 examples/s]Map:  29%|██▊       | 459/1600 [00:06<00:11, 98.81 examples/s]Map:  29%|██▉       | 470/1600 [00:06<00:11, 100.47 examples/s]Map:  30%|███       | 481/1600 [00:06<00:13, 82.50 examples/s] Map:  31%|███       | 490/1600 [00:06<00:13, 79.45 examples/s]Map:  31%|███▏      | 501/1600 [00:06<00:15, 72.28 examples/s]Map:  32%|███▏      | 509/1600 [00:06<00:15, 69.47 examples/s]Map:  32%|███▏      | 518/1600 [00:06<00:15, 71.71 examples/s]Map:  33%|███▎      | 526/1600 [00:07<00:14, 71.86 examples/s]Map:  34%|███▎      | 537/1600 [00:07<00:15, 67.33 examples/s]Map:  34%|███▍      | 545/1600 [00:07<00:15, 69.25 examples/s]Map:  35%|███▍      | 553/1600 [00:07<00:15, 69.47 examples/s]Map:  35%|███▌      | 561/1600 [00:07<00:14, 71.34 examples/s]Map:  36%|███▌      | 572/1600 [00:07<00:14, 69.74 examples/s]Map:  36%|███▋      | 582/1600 [00:08<00:18, 55.88 examples/s]Map:  37%|███▋      | 590/1600 [00:08<00:16, 59.60 examples/s]Map:  37%|███▋      | 598/1600 [00:08<00:15, 63.62 examples/s]Map:  38%|███▊      | 605/1600 [00:08<00:15, 63.74 examples/s]Map:  38%|███▊      | 613/1600 [00:08<00:15, 64.52 examples/s]Map:  39%|███▉      | 622/1600 [00:08<00:14, 67.75 examples/s]Map:  39%|███▉      | 630/1600 [00:08<00:14, 69.24 examples/s]Map:  40%|███▉      | 638/1600 [00:08<00:13, 69.77 examples/s]Map:  40%|████      | 646/1600 [00:08<00:13, 70.32 examples/s]Map:  41%|████      | 654/1600 [00:09<00:13, 69.17 examples/s]Map:  41%|████▏     | 662/1600 [00:09<00:13, 69.60 examples/s]Map:  42%|████▏     | 673/1600 [00:09<00:13, 70.17 examples/s]Map:  43%|████▎     | 683/1600 [00:09<00:15, 58.76 examples/s]Map:  43%|████▎     | 691/1600 [00:09<00:14, 61.27 examples/s]Map:  44%|████▎     | 699/1600 [00:09<00:14, 61.06 examples/s]Map:  44%|████▍     | 707/1600 [00:09<00:14, 62.92 examples/s]Map:  45%|████▍     | 715/1600 [00:10<00:13, 63.89 examples/s]Map:  45%|████▌     | 722/1600 [00:10<00:13, 63.60 examples/s]Map:  46%|████▌     | 730/1600 [00:10<00:13, 66.52 examples/s]Map:  46%|████▌     | 739/1600 [00:10<00:12, 69.72 examples/s]Map:  47%|████▋     | 748/1600 [00:10<00:12, 69.19 examples/s]Map:  47%|████▋     | 756/1600 [00:10<00:12, 69.18 examples/s]Map:  48%|████▊     | 763/1600 [00:10<00:12, 67.45 examples/s]Map:  48%|████▊     | 770/1600 [00:10<00:14, 56.59 examples/s]Map:  49%|████▊     | 778/1600 [00:11<00:14, 58.56 examples/s]Map:  49%|████▉     | 786/1600 [00:11<00:13, 62.20 examples/s]Map:  50%|████▉     | 795/1600 [00:11<00:11, 68.35 examples/s]Map:  50%|█████     | 803/1600 [00:11<00:14, 55.94 examples/s]Map:  51%|█████     | 812/1600 [00:11<00:13, 56.38 examples/s]Map:  51%|█████▏    | 820/1600 [00:11<00:12, 60.11 examples/s]Map:  52%|█████▏    | 828/1600 [00:11<00:11, 64.69 examples/s]Map:  52%|█████▏    | 837/1600 [00:11<00:10, 70.33 examples/s]Map:  53%|█████▎    | 846/1600 [00:12<00:10, 74.25 examples/s]Map:  53%|█████▎    | 855/1600 [00:12<00:09, 76.37 examples/s]Map:  54%|█████▍    | 863/1600 [00:12<00:09, 76.79 examples/s]Map:  55%|█████▍    | 872/1600 [00:12<00:10, 66.94 examples/s]Map:  55%|█████▌    | 882/1600 [00:12<00:10, 70.97 examples/s]Map:  56%|█████▌    | 890/1600 [00:12<00:10, 70.35 examples/s]Map:  56%|█████▌    | 898/1600 [00:12<00:10, 67.72 examples/s]Map:  57%|█████▋    | 905/1600 [00:12<00:10, 66.22 examples/s]Map:  57%|█████▋    | 912/1600 [00:13<00:10, 64.38 examples/s]Map:  58%|█████▊    | 921/1600 [00:13<00:11, 59.82 examples/s]Map:  58%|█████▊    | 929/1600 [00:13<00:11, 60.20 examples/s]Map:  59%|█████▊    | 939/1600 [00:13<00:11, 57.61 examples/s]Map:  59%|█████▉    | 947/1600 [00:13<00:10, 60.20 examples/s]Map:  60%|█████▉    | 955/1600 [00:13<00:10, 60.85 examples/s]Map:  60%|██████    | 963/1600 [00:13<00:10, 61.77 examples/s]Map:  61%|██████    | 971/1600 [00:14<00:12, 51.27 examples/s]Map:  61%|██████▏   | 980/1600 [00:14<00:10, 56.60 examples/s]Map:  62%|██████▏   | 988/1600 [00:14<00:10, 58.71 examples/s]Map:  62%|██████▏   | 995/1600 [00:14<00:10, 58.68 examples/s]Map:  62%|██████▏   | 999/1600 [00:26<00:10, 58.68 examples/s]Map:  62%|██████▎   | 1000/1600 [01:16<26:40,  2.67s/ examples]Map:  63%|██████▎   | 1007/1600 [01:16<18:31,  1.87s/ examples]Map:  63%|██████▎   | 1015/1600 [01:16<12:19,  1.26s/ examples]Map:  64%|██████▍   | 1023/1600 [01:16<08:19,  1.16 examples/s]Map:  64%|██████▍   | 1030/1600 [01:17<05:55,  1.60 examples/s]Map:  65%|██████▍   | 1039/1600 [01:17<03:52,  2.42 examples/s]Map:  65%|██████▌   | 1046/1600 [01:17<02:48,  3.29 examples/s]Map:  66%|██████▌   | 1053/1600 [01:17<02:01,  4.50 examples/s]Map:  66%|██████▋   | 1063/1600 [01:17<01:19,  6.77 examples/s]Map:  67%|██████▋   | 1071/1600 [01:17<00:57,  9.19 examples/s]Map:  67%|██████▋   | 1078/1600 [01:17<00:43, 11.94 examples/s]Map:  68%|██████▊   | 1085/1600 [01:17<00:33, 15.48 examples/s]Map:  68%|██████▊   | 1093/1600 [01:18<00:24, 20.63 examples/s]Map:  69%|██████▉   | 1100/1600 [01:18<00:19, 25.10 examples/s]Map:  69%|██████▉   | 1107/1600 [01:18<00:16, 30.37 examples/s]Map:  70%|██████▉   | 1114/1600 [01:18<00:13, 35.74 examples/s]Map:  70%|███████   | 1122/1600 [01:18<00:11, 41.06 examples/s]Map:  71%|███████   | 1130/1600 [01:18<00:12, 37.97 examples/s]Map:  71%|███████   | 1138/1600 [01:18<00:10, 44.05 examples/s]Map:  72%|███████▏  | 1146/1600 [01:19<00:09, 49.91 examples/s]Map:  72%|███████▏  | 1153/1600 [01:19<00:08, 52.22 examples/s]Map:  72%|███████▎  | 1160/1600 [01:19<00:08, 54.19 examples/s]Map:  73%|███████▎  | 1167/1600 [01:19<00:07, 55.72 examples/s]Map:  73%|███████▎  | 1175/1600 [01:19<00:07, 59.82 examples/s]Map:  74%|███████▍  | 1185/1600 [01:19<00:07, 59.06 examples/s]Map:  75%|███████▍  | 1193/1600 [01:19<00:06, 61.61 examples/s]Map:  75%|███████▌  | 1202/1600 [01:19<00:05, 66.38 examples/s]Map:  76%|███████▌  | 1210/1600 [01:20<00:05, 67.41 examples/s]Map:  76%|███████▌  | 1217/1600 [01:20<00:05, 66.12 examples/s]Map:  77%|███████▋  | 1225/1600 [01:20<00:05, 67.72 examples/s]Map:  77%|███████▋  | 1235/1600 [01:20<00:05, 64.44 examples/s]Map:  78%|███████▊  | 1242/1600 [01:20<00:08, 40.13 examples/s]Map:  78%|███████▊  | 1251/1600 [01:20<00:08, 43.56 examples/s]Map:  79%|███████▊  | 1257/1600 [01:21<00:07, 45.09 examples/s]Map:  79%|███████▉  | 1264/1600 [01:21<00:06, 49.19 examples/s]Map:  80%|███████▉  | 1272/1600 [01:21<00:06, 53.68 examples/s]Map:  80%|████████  | 1280/1600 [01:21<00:05, 57.10 examples/s]Map:  81%|████████  | 1289/1600 [01:21<00:05, 55.98 examples/s]Map:  81%|████████  | 1296/1600 [01:21<00:05, 56.59 examples/s]Map:  82%|████████▏ | 1304/1600 [01:21<00:05, 58.63 examples/s]Map:  82%|████████▏ | 1312/1600 [01:21<00:04, 60.80 examples/s]Map:  82%|████████▎ | 1320/1600 [01:22<00:04, 61.35 examples/s]Map:  83%|████████▎ | 1328/1600 [01:22<00:04, 64.10 examples/s]Map:  84%|████████▎ | 1336/1600 [01:22<00:04, 65.18 examples/s]Map:  84%|████████▍ | 1344/1600 [01:22<00:03, 64.97 examples/s]Map:  84%|████████▍ | 1352/1600 [01:22<00:04, 50.79 examples/s]Map:  85%|████████▍ | 1359/1600 [01:22<00:04, 54.30 examples/s]Map:  85%|████████▌ | 1366/1600 [01:22<00:04, 55.94 examples/s]Map:  86%|████████▌ | 1374/1600 [01:22<00:03, 59.98 examples/s]Map:  86%|████████▋ | 1382/1600 [01:23<00:03, 60.72 examples/s]Map:  87%|████████▋ | 1390/1600 [01:23<00:03, 63.88 examples/s]Map:  87%|████████▋ | 1398/1600 [01:23<00:03, 64.34 examples/s]Map:  88%|████████▊ | 1405/1600 [01:23<00:03, 63.86 examples/s]Map:  88%|████████▊ | 1412/1600 [01:23<00:02, 63.52 examples/s]Map:  89%|████████▉ | 1420/1600 [01:23<00:02, 65.79 examples/s]Map:  89%|████████▉ | 1428/1600 [01:23<00:02, 66.63 examples/s]Map:  90%|████████▉ | 1435/1600 [01:23<00:02, 65.34 examples/s]Map:  90%|█████████ | 1442/1600 [01:24<00:02, 62.80 examples/s]Map:  91%|█████████ | 1451/1600 [01:24<00:02, 60.16 examples/s]Map:  91%|█████████ | 1458/1600 [01:24<00:02, 60.49 examples/s]Map:  92%|█████████▏| 1465/1600 [01:24<00:02, 47.58 examples/s]Map:  92%|█████████▏| 1472/1600 [01:24<00:02, 51.76 examples/s]Map:  92%|█████████▎| 1480/1600 [01:24<00:02, 54.53 examples/s]Map:  93%|█████████▎| 1487/1600 [01:24<00:02, 55.70 examples/s]Map:  93%|█████████▎| 1495/1600 [01:25<00:01, 59.95 examples/s]Map:  94%|█████████▍| 1502/1600 [01:25<00:01, 61.84 examples/s]Map:  94%|█████████▍| 1509/1600 [01:25<00:01, 61.08 examples/s]Map:  95%|█████████▍| 1516/1600 [01:25<00:01, 60.40 examples/s]Map:  95%|█████████▌| 1523/1600 [01:25<00:01, 59.45 examples/s]Map:  96%|█████████▌| 1531/1600 [01:25<00:01, 62.91 examples/s]Map:  96%|█████████▋| 1541/1600 [01:25<00:00, 61.58 examples/s]Map:  97%|█████████▋| 1548/1600 [01:25<00:00, 62.39 examples/s]Map:  97%|█████████▋| 1555/1600 [01:25<00:00, 61.72 examples/s]Map:  98%|█████████▊| 1563/1600 [01:26<00:00, 62.50 examples/s]Map:  98%|█████████▊| 1570/1600 [01:26<00:00, 60.01 examples/s]Map:  99%|█████████▊| 1579/1600 [01:26<00:00, 48.22 examples/s]Map:  99%|█████████▉| 1585/1600 [01:26<00:00, 49.26 examples/s]Map: 100%|█████████▉| 1593/1600 [01:26<00:00, 53.99 examples/s]Map: 100%|█████████▉| 1597/1600 [01:39<00:00, 53.99 examples/s]Map: 100%|██████████| 1600/1600 [02:12<00:00,  1.86s/ examples]Map: 100%|██████████| 1600/1600 [02:12<00:00, 12.10 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▎         | 5/200 [00:00<00:05, 32.55 examples/s]Map:   6%|▋         | 13/200 [00:00<00:03, 48.66 examples/s]Map:  11%|█         | 22/200 [00:00<00:02, 59.88 examples/s]Map:  15%|█▌        | 30/200 [00:00<00:02, 63.67 examples/s]Map:  20%|██        | 40/200 [00:00<00:02, 70.12 examples/s]Map:  24%|██▍       | 49/200 [00:00<00:02, 72.00 examples/s]Map:  30%|███       | 61/200 [00:00<00:01, 81.26 examples/s]Map:  36%|███▌      | 72/200 [00:01<00:01, 75.63 examples/s]Map:  40%|████      | 80/200 [00:01<00:01, 75.51 examples/s]Map:  45%|████▌     | 90/200 [00:01<00:01, 69.26 examples/s]Map:  50%|████▉     | 99/200 [00:01<00:01, 57.57 examples/s]Map:  54%|█████▎    | 107/200 [00:01<00:01, 60.85 examples/s]Map:  57%|█████▊    | 115/200 [00:01<00:01, 62.61 examples/s]Map:  61%|██████    | 122/200 [00:01<00:01, 63.74 examples/s]Map:  65%|██████▌   | 130/200 [00:02<00:01, 61.18 examples/s]Map:  70%|██████▉   | 139/200 [00:02<00:00, 63.40 examples/s]Map:  74%|███████▎  | 147/200 [00:02<00:00, 64.88 examples/s]Map:  78%|███████▊  | 155/200 [00:02<00:00, 66.05 examples/s]Map:  82%|████████▏ | 163/200 [00:02<00:00, 66.01 examples/s]Map:  85%|████████▌ | 170/200 [00:02<00:00, 64.42 examples/s]Map:  90%|████████▉ | 179/200 [00:02<00:00, 61.19 examples/s]Map:  94%|█████████▎| 187/200 [00:02<00:00, 61.78 examples/s]Map:  97%|█████████▋| 194/200 [00:03<00:00, 61.25 examples/s]Map: 100%|██████████| 200/200 [00:16<00:00, 12.15 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▎         | 5/200 [00:00<00:05, 37.29 examples/s]Map:   6%|▌         | 12/200 [00:00<00:03, 52.01 examples/s]Map:  11%|█         | 22/200 [00:00<00:02, 69.02 examples/s]Map:  16%|█▌        | 31/200 [00:00<00:02, 72.46 examples/s]Map:  20%|██        | 41/200 [00:00<00:02, 79.11 examples/s]Map:  26%|██▌       | 51/200 [00:00<00:01, 84.33 examples/s]Map:  32%|███▎      | 65/200 [00:00<00:01, 97.33 examples/s]Map:  38%|███▊      | 77/200 [00:00<00:01, 86.61 examples/s]Map:  43%|████▎     | 86/200 [00:01<00:01, 84.30 examples/s]Map:  48%|████▊     | 95/200 [00:01<00:01, 69.00 examples/s]Map:  52%|█████▏    | 104/200 [00:01<00:01, 71.38 examples/s]Map:  56%|█████▌    | 112/200 [00:01<00:01, 73.02 examples/s]Map:  60%|██████    | 120/200 [00:01<00:01, 72.29 examples/s]Map:  66%|██████▌   | 131/200 [00:01<00:00, 69.39 examples/s]Map:  71%|███████   | 142/200 [00:01<00:00, 68.39 examples/s]Map:  75%|███████▌  | 150/200 [00:02<00:00, 70.47 examples/s]Map:  79%|███████▉  | 158/200 [00:02<00:00, 72.41 examples/s]Map:  84%|████████▍ | 169/200 [00:02<00:00, 67.48 examples/s]Map:  88%|████████▊ | 176/200 [00:02<00:00, 65.18 examples/s]Map:  92%|█████████▏| 184/200 [00:02<00:00, 66.21 examples/s]Map:  96%|█████████▌| 191/200 [00:02<00:00, 65.09 examples/s]Map: 100%|█████████▉| 199/200 [00:02<00:00, 66.16 examples/s]Map: 100%|██████████| 200/200 [00:14<00:00, 13.46 examples/s]
Loading cn eval data: fold_3...
Preprocess cn fold_3 data for de model
Loading cn test data: fold_3...
Preprocess cn fold_3 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 309.7019, Training accuracy: 0.9319
Macro F1-score: 0.9318
Model performance on Angry speech (in training): 
	Precision: 0.9421, Recall: 0.9350, F1_score: 0.9385
Model performance on Happy speech (in training): 
	Precision: 0.9000, Recall: 0.8775, F1_score: 0.8886
Model performance on Neutral speech (in training): 
	Precision: 0.9036, Recall: 0.9375, F1_score: 0.9202
Model performance on Sad speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|█         | 178/1600 [00:10<01:19, 17.80it/s]Training:  23%|██▎       | 375/1600 [00:20<01:04, 18.87it/s]Training:  36%|███▌      | 574/1600 [00:30<00:53, 19.30it/s]Training:  48%|████▊     | 773/1600 [00:40<00:42, 19.30it/s]Training:  61%|██████    | 969/1600 [00:50<00:32, 19.41it/s]Training:  73%|███████▎  | 1167/1600 [01:00<00:22, 19.54it/s]Training:  85%|████████▌ | 1365/1600 [01:10<00:12, 19.48it/s]Training:  98%|█████████▊| 1567/1600 [01:20<00:01, 19.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 281.0014, Validation accuracy: 0.5200
Macro F1-score: 0.4430
Model performance on Angry speech (in validation): 
	Precision: 0.5065, Recall: 0.7800, F1_score: 0.6142
Model performance on Happy speech (in validation): 
	Precision: 0.5909, Recall: 0.5200, F1_score: 0.5532
Model performance on Neutral speech (in validation): 
	Precision: 0.4937, Recall: 0.7800, F1_score: 0.6047
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
New best accuracy for layer 4 on epoch 1: 0.5200. Model saved.
Epoch 2/100

Training Phase:
Training loss: 103.7583, Training accuracy: 0.9731
Macro F1-score: 0.9731
Model performance on Angry speech (in training): 
	Precision: 0.9773, Recall: 0.9700, F1_score: 0.9737
Model performance on Happy speech (in training): 
	Precision: 0.9574, Recall: 0.9550, F1_score: 0.9562
Model performance on Neutral speech (in training): 
	Precision: 0.9701, Recall: 0.9725, F1_score: 0.9713
Model performance on Sad speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913

Eval Phase: 
Validation loss: 255.3508, Validation accuracy: 0.5850
Macro F1-score: 0.5532
Model performance on Angry speech (in validation): 
	Precision: 0.8667, Recall: 0.5200, F1_score: 0.6500
Model performance on Happy speech (in validation): 
	Precision: 0.5385, Recall: 0.8400, F1_score: 0.6562
Model performance on Neutral speech (in validation): 
	Precision: 0.4819, Recall: 0.8000, F1_score: 0.6015
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1800, F1_score: 0.3051
New best accuracy for layer 4 on epoch 2: 0.5850. Model saved.
Epoch 3/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.93it/s]Training:  25%|██▌       | 400/1600 [00:20<01:00, 19.92it/s]Training:  38%|███▊      | 602/1600 [00:30<00:49, 20.04it/s]Training:  50%|█████     | 804/1600 [00:40<00:40, 19.87it/s]Training:  63%|██████▎   | 1001/1600 [00:50<00:30, 19.71it/s]Training:  75%|███████▌  | 1201/1600 [01:00<00:20, 19.80it/s]Training:  88%|████████▊ | 1401/1600 [01:10<00:10, 19.83it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.58it/s]Training:  24%|██▍       | 392/1600 [00:20<01:02, 19.18it/s]Training:  37%|███▋      | 587/1600 [00:30<00:52, 19.31it/s]Training:  49%|████▉     | 782/1600 Training loss: 92.9347, Training accuracy: 0.9775
Macro F1-score: 0.9775
Model performance on Angry speech (in training): 
	Precision: 0.9848, Recall: 0.9750, F1_score: 0.9799
Model performance on Happy speech (in training): 
	Precision: 0.9581, Recall: 0.9725, F1_score: 0.9653
Model performance on Neutral speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900

Eval Phase: 
Validation loss: 324.3802, Validation accuracy: 0.5650
Macro F1-score: 0.5351
Model performance on Angry speech (in validation): 
	Precision: 0.7368, Recall: 0.5600, F1_score: 0.6364
Model performance on Happy speech (in validation): 
	Precision: 0.4884, Recall: 0.8400, F1_score: 0.6176
Model performance on Neutral speech (in validation): 
	Precision: 0.5075, Recall: 0.6800, F1_score: 0.5812
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1800, F1_score: 0.3051
Epoch 4/100

Training Phase:
[00:40<00:42, 19.26it/s]Training:  61%|██████    | 978/1600 [00:50<00:32, 19.38it/s]Training:  74%|███████▍  | 1183/1600 [01:00<00:21, 19.75it/s]Training:  87%|████████▋ | 1388/1600 [01:10<00:10, 19.99it/s]Training: 100%|█████████▉| 1593/1600 [01:21<00:00, 19.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 199/1600 [00:10<01:10, 19.87it/s]Training:  25%|██▌       | 403/1600 [00:20<00:59, 20.16it/s]Training:  38%|███▊      | 607/1600 [00:30<00:49, 20.02it/s]Training:  51%|█████     | 812/1600 [00:40<00:39, 20.20it/s]Training:  64%|██████▎   | 1017/1600 [00:50<00:29, 19.95it/s]Training:  76%|███████▌  | 1213/1600 [01:01<00:19, 19.66it/s]Training:  88%|████████Training loss: 52.4416, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Neutral speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 342.8003, Validation accuracy: 0.5550
Macro F1-score: 0.5243
Model performance on Angry speech (in validation): 
	Precision: 0.8205, Recall: 0.6400, F1_score: 0.7191
Model performance on Happy speech (in validation): 
	Precision: 0.4787, Recall: 0.9000, F1_score: 0.6250
Model performance on Neutral speech (in validation): 
	Precision: 0.4407, Recall: 0.5200, F1_score: 0.4771
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1600, F1_score: 0.2759
Epoch 5/100

Training Phase:
Training loss: 58.2317, Training accuracy: 0.9881
Macro F1-score: 0.9881
Model performance on Angry speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9800, F1_score: 0.9849
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 246.9666, Validation accuracy: 0.6400
Macro F1-score: 0.6105
Model performance on Angry speech (in validation): 
	Precision: 0.7321, Recall: 0.8200, F1_score: 0.7736
Model performance on Happy speech (in validation): 
	Precision: 0.6230, Recall: 0.7600, F1_score: 0.6847
Model performance on Neutral speech (in validation): 
	Precision: 0.5278, Recall: 0.7600, F1_score: 0.6230
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.2200, F1_score: 0.3607
New best accuracy for layer 4 on epoch 5: 0.6400. Model saved.
Epoch 6/100

Training Phase:
 | 1410/1600 [01:11<00:09, 19.65it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 193/1600 [00:10<01:13, 19.24it/s]Training:  24%|██▍       | 391/1600 [00:20<01:01, 19.57it/s]Training:  24%|██▍       | 391/1600 [00:30<01:01, 19.57it/s]Training:  37%|███▋      | 594/1600 [00:30<00:50, 19.86it/s]Training:  50%|████▉     | 797/1600 [00:40<00:40, 20.01it/s]Training:  62%|██████▎   | 1000/1600 [00:50<00:30, 19.96it/s]Training:  75%|███████▍  | 1199/1600 [01:00<00:20, 19.65it/s]Training:  87%|████████▋ | 1399/1600 [01:10<00:10, 19.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          Training loss: 21.9330, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 364.5011, Validation accuracy: 0.5650
Macro F1-score: 0.5385
Model performance on Angry speech (in validation): 
	Precision: 0.7826, Recall: 0.7200, F1_score: 0.7500
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.8200, F1_score: 0.6212
Model performance on Neutral speech (in validation): 
	Precision: 0.4286, Recall: 0.5400, F1_score: 0.4779
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1800, F1_score: 0.3051
Epoch 7/100

Training Phase:
| 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 203/1600 [00:10<01:09, 20.20it/s]Training:  25%|██▌       | 405/1600 [00:20<01:00, 19.78it/s]Training:  38%|███▊      | 610/1600 [00:30<00:49, 20.08it/s]Training:  51%|█████     | 815/1600 [00:40<00:38, 20.22it/s]Training:  64%|██████▍   | 1020/1600 [00:51<00:29, 19.81it/s]Training:  76%|███████▌  | 1215/1600 [01:01<00:19, 19.68it/s]Training:  88%|████████▊ | 1414/1600 [01:11<00:09, 19.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 198/1600 [00:10<01:11, 19.68it/s]Training:  25%|██▍       | 395/1600 [00:20<01:01, 19.59it/s]Training:  37%|███▋      | 592/1600 [00:30<00:51, 19.63it/s]Training:  50%|████▉     | 799/1600 [00:40<00:40, 20.00it/s]TTraining loss: 36.7463, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 479.9899, Validation accuracy: 0.5400
Macro F1-score: 0.4816
Model performance on Angry speech (in validation): 
	Precision: 0.5679, Recall: 0.9200, F1_score: 0.7023
Model performance on Happy speech (in validation): 
	Precision: 0.4595, Recall: 0.6800, F1_score: 0.5484
Model performance on Neutral speech (in validation): 
	Precision: 0.5854, Recall: 0.4800, F1_score: 0.5275
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.0800, F1_score: 0.1481
Epoch 8/100

Training Phase:
raining:  63%|██████▎   | 1005/1600 [00:50<00:29, 19.87it/s]Training:  76%|███████▌  | 1209/1600 [01:00<00:19, 20.03it/s]Training:  88%|████████▊ | 1413/1600 [01:10<00:09, 20.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.58it/s]Training:  25%|██▍       | 394/1600 [00:20<01:01, 19.68it/s]Training:  37%|███▋      | 593/1600 [00:30<00:50, 19.76it/s]Training:  50%|████▉     | 792/1600 [00:40<00:40, 19.72it/s]Training:  62%|██████▏   | 989/1600 [00:50<00:31, 19.56it/s]Training:  74%|███████▍  | 1182/1600 [01:00<00:21, 19.43it/s]Training:  86%|████████▌ | 1378/1600 [01:10<00:11, 19.46it/s]Training:  98%|█████████▊| 1574/1600 [01:20<00:Training loss: 27.6665, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 696.6693, Validation accuracy: 0.5100
Macro F1-score: 0.4633
Model performance on Angry speech (in validation): 
	Precision: 0.8621, Recall: 0.5000, F1_score: 0.6329
Model performance on Happy speech (in validation): 
	Precision: 0.4174, Recall: 0.9600, F1_score: 0.5818
Model performance on Neutral speech (in validation): 
	Precision: 0.4808, Recall: 0.5000, F1_score: 0.4902
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.0800, F1_score: 0.1481
Epoch 9/100

Training Phase:
Training loss: 34.6118, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 545.7636, Validation accuracy: 0.5300
Macro F1-score: 0.4866
Model performance on Angry speech (in validation): 
	Precision: 0.8108, Recall: 0.6000, F1_score: 0.6897
Model performance on Happy speech (in validation): 
	Precision: 0.4352, Recall: 0.9400, F1_score: 0.5949
Model performance on Neutral speech (in validation): 
	Precision: 0.4800, Recall: 0.4800, F1_score: 0.4800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1000, F1_score: 0.1818
Epoch 10/100

Training Phase:
01, 19.37it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 193/1600 [00:10<01:13, 19.20it/s]Training:  24%|██▍       | 391/1600 [00:20<01:01, 19.52it/s]Training:  37%|███▋      | 590/1600 [00:30<00:51, 19.69it/s]Training:  49%|████▉     | 789/1600 [00:40<00:41, 19.75it/s]Training:  62%|██████▏   | 988/1600 [00:50<00:30, 19.77it/s]Training:  74%|███████▍  | 1187/1600 [01:00<00:20, 19.78it/s]Training:  87%|████████▋ | 1385/1600 [01:10<00:10, 19.78it/s]Training:  99%|█████████▉| 1583/1600 [01:20<00:00, 19.71it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [Training loss: 27.6536, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 477.4393, Validation accuracy: 0.5350
Macro F1-score: 0.5003
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 0.5000, F1_score: 0.6494
Model performance on Happy speech (in validation): 
	Precision: 0.4375, Recall: 0.9800, F1_score: 0.6049
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.5200, F1_score: 0.5098
Model performance on Sad speech (in validation): 
	Precision: 0.7778, Recall: 0.1400, F1_score: 0.2373
Epoch 11/100

Training Phase:
00:00<?, ?it/s]Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.39it/s]Training:  24%|██▍       | 388/1600 [00:20<01:02, 19.33it/s]Training:  36%|███▋      | 582/1600 [00:30<00:52, 19.33it/s]Training:  49%|████▉     | 782/1600 [00:40<00:41, 19.58it/s]Training:  61%|██████▏   | 982/1600 [00:50<00:31, 19.59it/s]Training:  74%|███████▎  | 1179/1600 [01:00<00:21, 19.63it/s]Training:  86%|████████▌ | 1376/1600 [01:10<00:11, 19.58it/s]Training:  98%|█████████▊| 1571/1600 [01:20<00:01, 19.52it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 203/1600 [00:10<01:09, 20.22it/s]Training:  13%|█▎        | 203/1600 [00:20<01:09, 20.22it/s]Training:  25%|██▌       | 402/1600 [00:20<01:01, 19.55it/s]TrainTraining loss: 11.9768, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 770.9064, Validation accuracy: 0.4500
Macro F1-score: 0.3887
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.2000, F1_score: 0.3333
Model performance on Happy speech (in validation): 
	Precision: 0.3769, Recall: 0.9800, F1_score: 0.5444
Model performance on Neutral speech (in validation): 
	Precision: 0.4727, Recall: 0.5200, F1_score: 0.4952
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1000, F1_score: 0.1818
Epoch 12/100

Training Phase:
ing:  37%|███▋      | 597/1600 [00:30<00:51, 19.50it/s]Training:  50%|████▉     | 797/1600 [00:40<00:40, 19.65it/s]Training:  62%|██████▏   | 996/1600 [00:50<00:30, 19.68it/s]Training:  75%|███████▍  | 1194/1600 [01:00<00:20, 19.69it/s]Training:  87%|████████▋ | 1392/1600 [01:10<00:10, 19.60it/s]Training:  99%|█████████▉| 1590/1600 [01:20<00:00, 19.66it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 201/1600 [00:10<01:09, 20.06it/s]Training:  25%|██▌       | 402/1600 [00:20<01:00, 19.70it/s]Training:  37%|███▋      | 597/1600 [00:30<00:51, 19.51it/s]Training:  50%|████▉     | 792/1600 [00:40<00:41, 19.47it/s]Training:  62%|██████▏   | 987/1600 [00:50<00:31, 19.35it/s]TrainingTraining loss: 13.6253, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 690.8135, Validation accuracy: 0.5100
Macro F1-score: 0.4682
Model performance on Angry speech (in validation): 
	Precision: 0.7586, Recall: 0.4400, F1_score: 0.5570
Model performance on Happy speech (in validation): 
	Precision: 0.4065, Recall: 1.0000, F1_score: 0.5780
Model performance on Neutral speech (in validation): 
	Precision: 0.5854, Recall: 0.4800, F1_score: 0.5275
Model performance on Sad speech (in validation): 
	Precision: 0.8571, Recall: 0.1200, F1_score: 0.2105
Epoch 13/100

Training Phase:
Training loss: 32.3131, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
:  74%|███████▍  | 1180/1600 [01:00<00:21, 19.32it/s]Training:  86%|████████▌ | 1376/1600 [01:10<00:11, 19.40it/s]Training:  98%|█████████▊| 1573/1600 [01:20<00:01, 19.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.59it/s]Training:  25%|██▍       | 396/1600 [00:20<01:00, 19.78it/s]Training:  37%|███▋      | 596/1600 [00:30<00:51, 19.46it/s]Training:  50%|████▉     | 797/1600 [00:40<00:40, 19.67it/s]Training:  62%|██████▏   | 997/1600 [00:50<00:30, 19.72it/s]Training:  75%|███████▍  | 1196/1600 [01:00<00:20, 19.74it/s]Training:  87%|████████▋ | 1399/1600 [01:10<00:10, 19.90it/s]                                                             EvaluValidation loss: 579.4012, Validation accuracy: 0.4950
Macro F1-score: 0.4553
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.2000, F1_score: 0.3333
Model performance on Happy speech (in validation): 
	Precision: 0.4087, Recall: 0.9400, F1_score: 0.5697
Model performance on Neutral speech (in validation): 
	Precision: 0.4762, Recall: 0.6000, F1_score: 0.5310
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.2400, F1_score: 0.3871
Epoch 14/100

Training Phase:
Training loss: 19.5281, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9975, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 306.3142, Validation accuracy: 0.6200
Macro F1-score: 0.6181
Model performance on Angry speech (in validation): 
	Precision: 0.8485, Recall: 0.5600, F1_score: 0.6747
Model performance on Happy speech (in validation): 
	Precision: 0.5316, Recall: 0.8400, F1_score: 0.6512
Model performance on Neutral speech (in validation): 
	Precision: 0.5156, Recall: 0.6600, F1_score: 0.5789
Model performance on Sad speech (in validation): 
	Precision: 0.8750, Recall: 0.4200, F1_score: 0.5676
Epoch 15/100

Training Phase:
ating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.36it/s]Training:  24%|██▍       | 390/1600 [00:20<01:02, 19.45it/s]Training:  37%|███▋      | 594/1600 [00:30<00:50, 19.85it/s]Training:  50%|████▉     | 798/1600 [00:40<00:40, 19.95it/s]Training:  63%|██████▎   | 1001/1600 [00:50<00:29, 20.06it/s]Training:  75%|███████▌  | 1204/1600 [01:00<00:19, 19.92it/s]Training:  88%|████████▊ | 1401/1600 [01:10<00:10, 19.77it/s]Training: 100%|█████████▉| 1598/1600 [01:20<00:00, 19.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.99it/s]TTraining loss: 24.0934, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 287.5663, Validation accuracy: 0.6500
Macro F1-score: 0.6447
Model performance on Angry speech (in validation): 
	Precision: 0.9630, Recall: 0.5200, F1_score: 0.6753
Model performance on Happy speech (in validation): 
	Precision: 0.5733, Recall: 0.8600, F1_score: 0.6880
Model performance on Neutral speech (in validation): 
	Precision: 0.5333, Recall: 0.8000, F1_score: 0.6400
Model performance on Sad speech (in validation): 
	Precision: 0.9130, Recall: 0.4200, F1_score: 0.5753
New best accuracy for layer 4 on epoch 15: 0.6500. Model saved.
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6500

Test Phase: 
raining:  25%|██▌       | 404/1600 [00:20<00:59, 20.19it/s]Training:  38%|███▊      | 608/1600 [00:30<00:49, 20.09it/s]Training:  50%|█████     | 808/1600 [00:40<00:39, 19.81it/s]Training:  63%|██████▎   | 1009/1600 [00:50<00:29, 19.90it/s]Training:  76%|███████▌  | 1210/1600 [01:00<00:19, 19.83it/s]Training:  88%|████████▊ | 1408/1600 [01:10<00:09, 19.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   4%|▎         | 7/200 [00:00<00:02, 68.70it/s]Testing:   8%|▊         | 17/200 [00:00<00:02, 85.69it/s]Testing:  14%|█▎        | 27/200 [00:00<00:01, 87.86it/s]Testing:  18%|█▊        | 37/200 [00:00<00:01, 92.37it/s]Testing:  24%|██▍       | 48/200 [00:00<00:01, 95.21it/s]Testing:  29%|██▉       | 58/200 [00:00<00:01, 96.22itTest loss: 311.5953, Test accuracy: 0.6150
Macro F1-score: 0.6044
Model performance on Angry speech (in test): 
	Precision: 0.9474, Recall: 0.3600, F1_score: 0.5217
Model performance on Happy speech (in test): 
	Precision: 0.5250, Recall: 0.8400, F1_score: 0.6462
Model performance on Neutral speech (in test): 
	Precision: 0.5342, Recall: 0.7800, F1_score: 0.6341
Model performance on Sad speech (in test): 
	Precision: 0.8571, Recall: 0.4800, F1_score: 0.6154

======================= This is fold_4 on de =======================

Load dataset: 
Loading de train data: fold_4...
Preprocess de fold_4 data for de model
/s]Testing:  34%|███▍      | 68/200 [00:00<00:01, 97.02it/s]Testing:  39%|███▉      | 78/200 [00:00<00:01, 97.66it/s]Testing:  44%|████▍     | 88/200 [00:00<00:01, 96.85it/s]Testing:  49%|████▉     | 98/200 [00:01<00:01, 96.61it/s]Testing:  55%|█████▍    | 109/200 [00:01<00:00, 98.08it/s]Testing:  60%|█████▉    | 119/200 [00:01<00:00, 97.95it/s]Testing:  64%|██████▍   | 129/200 [00:01<00:00, 97.87it/s]Testing:  70%|███████   | 140/200 [00:01<00:00, 99.04it/s]Testing:  75%|███████▌  | 150/200 [00:01<00:00, 98.57it/s]Testing:  80%|████████  | 160/200 [00:01<00:00, 98.66it/s]Testing:  86%|████████▌ | 171/200 [00:01<00:00, 100.29it/s]Testing:  91%|█████████ | 182/200 [00:01<00:00, 98.68it/s] Testing:  96%|█████████▌| 192/200 [00:01<00:00, 98.10it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 1/1600 [00:00<03:12,  8.32 examples/s]Map:   0%|          | 8/1600 [00:00<00:45, 34.64 examples/s]Map:   1%|          | 18/1600 [00:00<00:29, 54.02 examples/s]Map:   2%|▏         | 28/1600 [00:00<00:23, 68.06 examples/s]Map:   2%|▏         | 38/1600 [00:00<00:20, 75.51 examples/s]Map:   3%|▎         | 48/1600 [00:00<00:19, 81.12 examples/s]Map:   4%|▍         | 60/1600 [00:00<00:17, 89.60 examples/s]Map:   4%|▍         | 71/1600 [00:00<00:16, 90.66 examples/s]Map:   5%|▌         | 81/1600 [00:01<00:16, 91.76 examples/s]Map:   6%|▌         | 92/1600 [00:01<00:20, 74.75 examples/s]Map:   6%|▋         | 101/1600 [00:01<00:19, 76.68 examples/s]Map:   7%|▋         | 111/1600 [00:01<00:18, 78.71 examples/s]Map:   8%|▊         | 123/1600 [00:01<00:18, 78.49 examples/s]Map:   8%|▊         | 135/1600 [00:01<00:19, 75.73 examples/s]Map:   9%|▉         | 143/1600 [00:01<00:19, 73.24 examples/s]Map:  10%|▉         | 154/1600 [00:02<00:20, 72.17 examples/s]Map:  10%|█         | 162/1600 [00:02<00:19, 72.78 examples/s]Map:  11%|█         | 170/1600 [00:02<00:19, 71.75 examples/s]Map:  11%|█         | 179/1600 [00:02<00:19, 74.13 examples/s]Map:  12%|█▏        | 188/1600 [00:02<00:18, 75.42 examples/s]Map:  12%|█▏        | 198/1600 [00:02<00:18, 77.61 examples/s]Map:  13%|█▎        | 209/1600 [00:02<00:22, 61.26 examples/s]Map:  14%|█▎        | 216/1600 [00:03<00:22, 61.50 examples/s]Map:  14%|█▍        | 225/1600 [00:03<00:21, 65.18 examples/s]Map:  15%|█▍        | 235/1600 [00:03<00:19, 70.30 examples/s]Map:  15%|█▌        | 243/1600 [00:03<00:18, 71.94 examples/s]Map:  16%|█▌        | 252/1600 [00:03<00:18, 72.76 examples/s]Map:  16%|█▋        | 261/1600 [00:03<00:18, 73.54 examples/s]Map:  17%|█▋        | 271/1600 [00:03<00:17, 76.48 examples/s]Map:  17%|█▋        | 279/1600 [00:03<00:17, 75.86 examples/s]Map:  18%|█▊        | 289/1600 [00:03<00:16, 78.73 examples/s]Map:  19%|█▊        | 297/1600 [00:04<00:16, 77.30 examples/s]Map:  19%|█▉        | 306/1600 [00:04<00:16, 76.55 examples/s]Map:  20%|█▉        | 315/1600 [00:04<00:19, 65.22 examples/s]Map:  20%|██        | 323/1600 [00:04<00:19, 65.94 examples/s]Map:  21%|██        | 332/1600 [00:04<00:18, 69.03 examples/s]Map:  21%|██▏       | 341/1600 [00:04<00:17, 72.74 examples/s]Map:  22%|██▏       | 351/1600 [00:04<00:16, 76.13 examples/s]Map:  22%|██▏       | 359/1600 [00:04<00:16, 76.12 examples/s]Map:  23%|██▎       | 368/1600 [00:05<00:16, 76.87 examples/s]Map:  24%|██▎       | 378/1600 [00:05<00:15, 79.85 examples/s]Map:  24%|██▍       | 387/1600 [00:05<00:15, 79.24 examples/s]Map:  25%|██▍       | 398/1600 [00:05<00:14, 82.68 examples/s]Map:  26%|██▌       | 411/1600 [00:05<00:12, 92.59 examples/s]Map:  26%|██▋       | 423/1600 [00:05<00:11, 98.80 examples/s]Map:  27%|██▋       | 433/1600 [00:05<00:20, 55.71 examples/s]Map:  28%|██▊       | 444/1600 [00:06<00:17, 65.00 examples/s]Map:  28%|██▊       | 454/1600 [00:06<00:16, 70.29 examples/s]Map:  29%|██▉       | 464/1600 [00:06<00:15, 75.50 examples/s]Map:  30%|██▉       | 477/1600 [00:06<00:12, 86.46 examples/s]Map:  30%|███       | 488/1600 [00:06<00:12, 86.94 examples/s]Map:  31%|███▏      | 501/1600 [00:06<00:13, 83.60 examples/s]Map:  32%|███▏      | 513/1600 [00:06<00:13, 79.91 examples/s]Map:  33%|███▎      | 522/1600 [00:07<00:14, 76.42 examples/s]Map:  33%|███▎      | 530/1600 [00:07<00:16, 65.27 examples/s]Map:  34%|███▎      | 539/1600 [00:07<00:15, 67.94 examples/s]Map:  34%|███▍      | 547/1600 [00:07<00:15, 68.22 examples/s]Map:  35%|███▍      | 555/1600 [00:07<00:14, 70.51 examples/s]Map:  35%|███▌      | 564/1600 [00:07<00:14, 73.22 examples/s]Map:  36%|███▌      | 573/1600 [00:07<00:13, 74.23 examples/s]Map:  36%|███▋      | 581/1600 [00:07<00:14, 72.65 examples/s]Map:  37%|███▋      | 589/1600 [00:08<00:14, 70.18 examples/s]Map:  37%|███▋      | 597/1600 [00:08<00:14, 69.72 examples/s]Map:  38%|███▊      | 605/1600 [00:08<00:14, 68.56 examples/s]Map:  38%|███▊      | 613/1600 [00:08<00:14, 68.65 examples/s]Map:  39%|███▉      | 621/1600 [00:08<00:14, 69.23 examples/s]Map:  39%|███▉      | 629/1600 [00:08<00:16, 57.61 examples/s]Map:  40%|███▉      | 637/1600 [00:08<00:15, 61.05 examples/s]Map:  40%|████      | 645/1600 [00:08<00:15, 62.82 examples/s]Map:  41%|████      | 653/1600 [00:09<00:14, 64.63 examples/s]Map:  41%|████▏     | 661/1600 [00:09<00:14, 66.03 examples/s]Map:  42%|████▏     | 669/1600 [00:09<00:13, 67.45 examples/s]Map:  42%|████▏     | 677/1600 [00:09<00:13, 67.86 examples/s]Map:  43%|████▎     | 684/1600 [00:09<00:14, 64.93 examples/s]Map:  43%|████▎     | 692/1600 [00:09<00:13, 66.52 examples/s]Map:  44%|████▎     | 699/1600 [00:09<00:13, 65.96 examples/s]Map:  44%|████▍     | 707/1600 [00:09<00:13, 66.94 examples/s]Map:  45%|████▍     | 716/1600 [00:09<00:12, 70.74 examples/s]Map:  45%|████▌     | 727/1600 [00:10<00:15, 58.06 examples/s]Map:  46%|████▌     | 735/1600 [00:10<00:14, 60.37 examples/s]Map:  46%|████▋     | 744/1600 [00:10<00:12, 66.13 examples/s]Map:  47%|████▋     | 753/1600 [00:10<00:12, 69.01 examples/s]Map:  48%|████▊     | 764/1600 [00:10<00:12, 68.58 examples/s]Map:  48%|████▊     | 773/1600 [00:10<00:11, 68.98 examples/s]Map:  49%|████▉     | 781/1600 [00:10<00:11, 70.72 examples/s]Map:  50%|████▉     | 792/1600 [00:11<00:11, 68.28 examples/s]Map:  50%|█████     | 801/1600 [00:11<00:11, 69.24 examples/s]Map:  51%|█████     | 812/1600 [00:11<00:12, 62.75 examples/s]Map:  51%|█████     | 819/1600 [00:11<00:12, 61.75 examples/s]Map:  52%|█████▏    | 830/1600 [00:11<00:13, 59.22 examples/s]Map:  52%|█████▏    | 838/1600 [00:11<00:12, 61.15 examples/s]Map:  53%|█████▎    | 848/1600 [00:11<00:11, 66.68 examples/s]Map:  54%|█████▎    | 856/1600 [00:12<00:10, 68.99 examples/s]Map:  54%|█████▍    | 865/1600 [00:12<00:10, 71.99 examples/s]Map:  55%|█████▍    | 875/1600 [00:12<00:09, 76.32 examples/s]Map:  55%|█████▌    | 883/1600 [00:12<00:09, 76.01 examples/s]Map:  56%|█████▌    | 891/1600 [00:12<00:09, 72.56 examples/s]Map:  56%|█████▋    | 901/1600 [00:12<00:10, 69.08 examples/s]Map:  57%|█████▋    | 911/1600 [00:12<00:10, 64.70 examples/s]Map:  58%|█████▊    | 922/1600 [00:13<00:12, 55.89 examples/s]Map:  58%|█████▊    | 930/1600 [00:13<00:11, 58.36 examples/s]Map:  59%|█████▊    | 938/1600 [00:13<00:10, 61.52 examples/s]Map:  59%|█████▉    | 945/1600 [00:13<00:10, 63.12 examples/s]Map:  60%|█████▉    | 952/1600 [00:13<00:10, 63.39 examples/s]Map:  60%|██████    | 960/1600 [00:13<00:09, 66.26 examples/s]Map:  61%|██████    | 970/1600 [00:13<00:09, 63.55 examples/s]Map:  61%|██████    | 977/1600 [00:13<00:10, 62.14 examples/s]Map:  62%|██████▏   | 986/1600 [00:14<00:10, 59.89 examples/s]Map:  62%|██████▏   | 993/1600 [00:14<00:09, 61.27 examples/s]Map:  62%|██████▏   | 997/1600 [00:31<00:09, 61.27 examples/s]Map:  62%|██████▎   | 1000/1600 [01:14<23:38,  2.36s/ examples]Map:  63%|██████▎   | 1007/1600 [01:14<16:50,  1.70s/ examples]Map:  63%|██████▎   | 1015/1600 [01:14<11:25,  1.17s/ examples]Map:  64%|██████▍   | 1023/1600 [01:14<07:49,  1.23 examples/s]Map:  64%|██████▍   | 1031/1600 [01:14<05:22,  1.76 examples/s]Map:  65%|██████▍   | 1039/1600 [01:14<03:43,  2.51 examples/s]Map:  65%|██████▌   | 1047/1600 [01:15<02:35,  3.55 examples/s]Map:  66%|██████▌   | 1055/1600 [01:15<01:49,  4.96 examples/s]Map:  66%|██████▋   | 1063/1600 [01:15<01:18,  6.87 examples/s]Map:  67%|██████▋   | 1071/1600 [01:15<00:56,  9.42 examples/s]Map:  67%|██████▋   | 1079/1600 [01:15<00:41, 12.68 examples/s]Map:  68%|██████▊   | 1086/1600 [01:15<00:31, 16.21 examples/s]Map:  68%|██████▊   | 1096/1600 [01:15<00:23, 21.46 examples/s]Map:  69%|██████▉   | 1104/1600 [01:15<00:18, 26.71 examples/s]Map:  70%|██████▉   | 1114/1600 [01:16<00:14, 33.08 examples/s]Map:  70%|███████   | 1121/1600 [01:16<00:12, 37.77 examples/s]Map:  71%|███████   | 1130/1600 [01:16<00:12, 36.56 examples/s]Map:  71%|███████   | 1137/1600 [01:16<00:11, 41.12 examples/s]Map:  72%|███████▏  | 1145/1600 [01:16<00:09, 46.16 examples/s]Map:  72%|███████▏  | 1152/1600 [01:16<00:09, 49.38 examples/s]Map:  72%|███████▏  | 1159/1600 [01:16<00:08, 53.78 examples/s]Map:  73%|███████▎  | 1166/1600 [01:17<00:07, 56.14 examples/s]Map:  73%|███████▎  | 1174/1600 [01:17<00:07, 58.01 examples/s]Map:  74%|███████▍  | 1183/1600 [01:17<00:07, 55.44 examples/s]Map:  74%|███████▍  | 1190/1600 [01:17<00:07, 56.38 examples/s]Map:  75%|███████▍  | 1199/1600 [01:17<00:06, 61.55 examples/s]Map:  75%|███████▌  | 1207/1600 [01:17<00:06, 64.98 examples/s]Map:  76%|███████▌  | 1214/1600 [01:17<00:06, 62.97 examples/s]Map:  76%|███████▋  | 1221/1600 [01:17<00:06, 62.60 examples/s]Map:  77%|███████▋  | 1231/1600 [01:18<00:06, 60.28 examples/s]Map:  78%|███████▊  | 1241/1600 [01:18<00:07, 49.22 examples/s]Map:  78%|███████▊  | 1248/1600 [01:18<00:07, 50.24 examples/s]Map:  78%|███████▊  | 1255/1600 [01:18<00:06, 52.98 examples/s]Map:  79%|███████▉  | 1263/1600 [01:18<00:06, 55.88 examples/s]Map:  79%|███████▉  | 1270/1600 [01:18<00:05, 57.51 examples/s]Map:  80%|███████▉  | 1276/1600 [01:18<00:05, 56.79 examples/s]Map:  80%|████████  | 1284/1600 [01:19<00:05, 60.61 examples/s]Map:  81%|████████  | 1292/1600 [01:19<00:05, 60.61 examples/s]Map:  81%|████████  | 1299/1600 [01:19<00:04, 60.75 examples/s]Map:  82%|████████▏ | 1307/1600 [01:19<00:04, 62.15 examples/s]Map:  82%|████████▏ | 1317/1600 [01:19<00:04, 61.50 examples/s]Map:  83%|████████▎ | 1325/1600 [01:19<00:04, 63.46 examples/s]Map:  83%|████████▎ | 1333/1600 [01:19<00:04, 64.94 examples/s]Map:  84%|████████▍ | 1341/1600 [01:19<00:03, 66.86 examples/s]Map:  84%|████████▍ | 1348/1600 [01:20<00:04, 51.71 examples/s]Map:  85%|████████▍ | 1356/1600 [01:20<00:04, 55.24 examples/s]Map:  85%|████████▌ | 1364/1600 [01:20<00:04, 57.67 examples/s]Map:  86%|████████▌ | 1371/1600 [01:20<00:03, 58.11 examples/s]Map:  86%|████████▌ | 1378/1600 [01:20<00:03, 59.02 examples/s]Map:  87%|████████▋ | 1385/1600 [01:20<00:03, 58.71 examples/s]Map:  87%|████████▋ | 1392/1600 [01:20<00:03, 59.38 examples/s]Map:  87%|████████▋ | 1399/1600 [01:21<00:03, 59.50 examples/s]Map:  88%|████████▊ | 1407/1600 [01:21<00:03, 62.34 examples/s]Map:  88%|████████▊ | 1415/1600 [01:21<00:03, 61.21 examples/s]Map:  89%|████████▉ | 1423/1600 [01:21<00:02, 63.75 examples/s]Map:  89%|████████▉ | 1430/1600 [01:21<00:02, 63.50 examples/s]Map:  90%|████████▉ | 1439/1600 [01:21<00:02, 63.72 examples/s]Map:  91%|█████████ | 1449/1600 [01:21<00:02, 59.80 examples/s]Map:  91%|█████████ | 1456/1600 [01:21<00:02, 59.82 examples/s]Map:  92%|█████████▏| 1465/1600 [01:22<00:02, 49.24 examples/s]Map:  92%|█████████▏| 1472/1600 [01:22<00:02, 52.43 examples/s]Map:  92%|█████████▎| 1480/1600 [01:22<00:02, 57.95 examples/s]Map:  93%|█████████▎| 1487/1600 [01:22<00:01, 57.28 examples/s]Map:  93%|█████████▎| 1494/1600 [01:22<00:01, 56.30 examples/s]Map:  94%|█████████▍| 1501/1600 [01:22<00:01, 57.64 examples/s]Map:  94%|█████████▍| 1507/1600 [01:22<00:01, 57.28 examples/s]Map:  95%|█████████▍| 1514/1600 [01:22<00:01, 57.40 examples/s]Map:  95%|█████████▌| 1522/1600 [01:23<00:01, 58.93 examples/s]Map:  96%|█████████▌| 1529/1600 [01:23<00:01, 59.34 examples/s]Map:  96%|█████████▌| 1537/1600 [01:23<00:01, 62.24 examples/s]Map:  96%|█████████▋| 1544/1600 [01:23<00:00, 62.58 examples/s]Map:  97%|█████████▋| 1552/1600 [01:23<00:00, 64.05 examples/s]Map:  98%|█████████▊| 1560/1600 [01:23<00:00, 65.48 examples/s]Map:  98%|█████████▊| 1568/1600 [01:23<00:00, 66.67 examples/s]Map:  98%|█████████▊| 1576/1600 [01:24<00:00, 50.04 examples/s]Map:  99%|█████████▉| 1583/1600 [01:24<00:00, 52.98 examples/s]Map:  99%|█████████▉| 1590/1600 [01:24<00:00, 54.79 examples/s]Map: 100%|█████████▉| 1597/1600 [01:24<00:00, 57.45 examples/s]Map: 100%|█████████▉| 1597/1600 [01:41<00:00, 57.45 examples/s]Map: 100%|██████████| 1600/1600 [02:09<00:00,  2.28s/ examples]Map: 100%|██████████| 1600/1600 [02:09<00:00, 12.33 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▏         | 3/200 [00:00<00:09, 20.21 examples/s]Map:   6%|▌         | 11/200 [00:00<00:04, 43.62 examples/s]Map:   9%|▉         | 18/200 [00:00<00:03, 48.45 examples/s]Map:  14%|█▎        | 27/200 [00:00<00:02, 59.73 examples/s]Map:  18%|█▊        | 37/200 [00:00<00:02, 69.54 examples/s]Map:  23%|██▎       | 46/200 [00:00<00:02, 71.28 examples/s]Map:  30%|██▉       | 59/200 [00:00<00:01, 85.00 examples/s]Map:  34%|███▍      | 69/200 [00:00<00:01, 85.49 examples/s]Map:  40%|████      | 80/200 [00:01<00:01, 78.00 examples/s]Map:  45%|████▌     | 90/200 [00:01<00:01, 70.12 examples/s]Map:  50%|█████     | 100/200 [00:01<00:01, 61.80 examples/s]Map:  54%|█████▍    | 108/200 [00:01<00:01, 64.89 examples/s]Map:  58%|█████▊    | 116/200 [00:01<00:01, 65.53 examples/s]Map:  62%|██████▏   | 123/200 [00:01<00:01, 64.04 examples/s]Map:  66%|██████▌   | 131/200 [00:01<00:01, 65.27 examples/s]Map:  69%|██████▉   | 138/200 [00:02<00:00, 64.70 examples/s]Map:  72%|███████▎  | 145/200 [00:02<00:00, 63.28 examples/s]Map:  76%|███████▋  | 153/200 [00:02<00:00, 64.41 examples/s]Map:  81%|████████  | 162/200 [00:02<00:00, 68.09 examples/s]Map:  84%|████████▍ | 169/200 [00:02<00:00, 64.68 examples/s]Map:  88%|████████▊ | 177/200 [00:02<00:00, 64.11 examples/s]Map:  92%|█████████▎| 185/200 [00:02<00:00, 64.47 examples/s]Map:  96%|█████████▋| 193/200 [00:02<00:00, 63.57 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00,  1.92 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 12.55 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▏         | 4/200 [00:00<00:07, 26.09 examples/s]Map:   6%|▌         | 11/200 [00:00<00:04, 45.77 examples/s]Map:  10%|█         | 20/200 [00:00<00:02, 60.68 examples/s]Map:  15%|█▌        | 30/200 [00:00<00:02, 69.70 examples/s]Map:  20%|██        | 40/200 [00:00<00:02, 76.69 examples/s]Map:  25%|██▌       | 50/200 [00:00<00:01, 82.38 examples/s]Map:  31%|███       | 62/200 [00:00<00:01, 91.91 examples/s]Map:  37%|███▋      | 74/200 [00:00<00:01, 81.21 examples/s]Map:  42%|████▎     | 85/200 [00:01<00:01, 74.66 examples/s]Map:  48%|████▊     | 95/200 [00:01<00:01, 63.75 examples/s]Map:  52%|█████▏    | 103/200 [00:01<00:01, 65.84 examples/s]Map:  56%|█████▋    | 113/200 [00:01<00:01, 68.46 examples/s]Map:  62%|██████▏   | 124/200 [00:01<00:01, 67.45 examples/s]Map:  66%|██████▌   | 132/200 [00:01<00:01, 65.98 examples/s]Map:  70%|███████   | 141/200 [00:02<00:00, 61.23 examples/s]Map:  74%|███████▍  | 149/200 [00:02<00:00, 64.66 examples/s]Map:  78%|███████▊  | 156/200 [00:02<00:00, 64.60 examples/s]Map:  82%|████████▏ | 163/200 [00:02<00:00, 64.85 examples/s]Map:  85%|████████▌ | 170/200 [00:02<00:00, 63.81 examples/s]Map:  89%|████████▉ | 178/200 [00:02<00:00, 65.78 examples/s]Map:  93%|█████████▎| 186/200 [00:02<00:00, 66.43 examples/s]Map:  97%|█████████▋| 194/200 [00:02<00:00, 65.70 examples/s]Map:  99%|█████████▉| 198/200 [00:15<00:00, 65.70 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00,  1.87 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 12.80 examples/s]
Loading cn eval data: fold_4...
Preprocess cn fold_4 data for de model
Loading cn test data: fold_4...
Preprocess cn fold_4 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 270.4533, Training accuracy: 0.9419
Macro F1-score: 0.9419
Model performance on Angry speech (in training): 
	Precision: 0.9447, Recall: 0.9400, F1_score: 0.9424
Model performance on Happy speech (in training): 
	Precision: 0.9186, Recall: 0.9025, F1_score: 0.9105
Model performance on Neutral speech (in training): 
	Precision: 0.9205, Recall: 0.9550, F1_score: 0.9374
Model performance on Sad speech (in training): 
	Precision: 0.9848, Recall: 0.9700, F1_score: 0.9773

Eval Phase: 
Validation loss: 320.5391, Validation accuracy: 0.5250
Macro F1-score: 0.5299
Model performance on Angry speech (in validation): 
	Precision: 0.4667, Recall: 0.4200, F1_score: 0.4421
Model performance on Happy speech (in validation): 
	Precision: 0.3836, Recall: 0.5600, F1_score: 0.4553
Model performance on Neutral speech (in validation): 
	Precision: 0.5738, Recall: 0.7000, F1_score: 0.6306
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.4200, F1_score: 0.5915
New best accuracy for layer 4 on epoch 1: 0.5250. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 199/1600 [00:10<01:10, 19.89it/s]Training:  25%|██▍       | 398/1600 [00:20<01:01, 19.69it/s]Training:  37%|███▋      | 594/1600 [00:30<00:51, 19.63it/s]Training:  50%|████▉     | 794/1600 [00:40<00:40, 19.75it/s]Training:  62%|██████▏   | 994/1600 [00:50<00:30, 19.60it/s]Training:  74%|███████▍  | 1189/1600 [01:00<00:21, 19.56it/s]Training:  87%|████████▋ | 1392/1600 [01:10<00:10, 19.76it/s]Training: 100%|█████████▉| 1594/1600 [01:20<00:00, 19.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 198/1600 [00:10<01:11, 19.72it/s]Training:  25%|██▌       | 400/1600 [00:20<01:00, 19.96it/s]Training:  38%|███▊      |Training loss: 96.1988, Training accuracy: 0.9844
Macro F1-score: 0.9844
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9724, Recall: 0.9675, F1_score: 0.9699
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 542.1087, Validation accuracy: 0.3500
Macro F1-score: 0.3128
Model performance on Angry speech (in validation): 
	Precision: 0.3718, Recall: 0.5800, F1_score: 0.4531
Model performance on Happy speech (in validation): 
	Precision: 0.2000, Recall: 0.2600, F1_score: 0.2261
Model performance on Neutral speech (in validation): 
	Precision: 0.4727, Recall: 0.5200, F1_score: 0.4952
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Epoch 3/100

Training Phase:
 603/1600 [00:30<00:49, 20.09it/s]Training:  50%|█████     | 806/1600 [00:40<00:39, 19.99it/s]Training:  63%|██████▎   | 1005/1600 [00:50<00:30, 19.69it/s]Training:  75%|███████▌  | 1205/1600 [01:00<00:19, 19.76it/s]Training:  88%|████████▊ | 1404/1600 [01:10<00:09, 19.70it/s]Training: 100%|██████████| 1600/1600 [01:20<00:00, 19.63it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 198/1600 [00:10<01:10, 19.75it/s]Training:  25%|██▍       | 396/1600 [00:20<01:01, 19.68it/s]Training:  38%|███▊      | 600/1600 [00:30<00:50, 19.97it/s]Training:  50%|█████     | 804/1600 [00:40<00:39, 20.06it/s]Training:  63%|██████▎   | 1006/1600 [00:50<00:29, 19.86it/s]Training:  75%|██████Training loss: 73.3485, Training accuracy: 0.9825
Macro F1-score: 0.9825
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9674, Recall: 0.9650, F1_score: 0.9662
Model performance on Neutral speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 530.3477, Validation accuracy: 0.3950
Macro F1-score: 0.3849
Model performance on Angry speech (in validation): 
	Precision: 0.3944, Recall: 0.5600, F1_score: 0.4628
Model performance on Happy speech (in validation): 
	Precision: 0.2394, Recall: 0.3400, F1_score: 0.2810
Model performance on Neutral speech (in validation): 
	Precision: 0.5200, Recall: 0.5200, F1_score: 0.5200
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1600, F1_score: 0.2759
Epoch 4/100

Training Phase:
Training loss: 40.2761, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
▌  | 1207/1600 [01:00<00:19, 19.94it/s]Training:  88%|████████▊ | 1408/1600 [01:10<00:09, 19.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 202/1600 [00:10<01:09, 20.13it/s]Training:  25%|██▌       | 404/1600 [00:20<00:59, 20.10it/s]Training:  38%|███▊      | 605/1600 [00:30<00:49, 19.91it/s]Training:  50%|█████     | 802/1600 [00:40<00:40, 19.80it/s]Training:  62%|██████▏   | 999/1600 [00:50<00:30, 19.67it/s]Training:  75%|███████▍  | 1195/1600 [01:00<00:20, 19.64it/s]Training:  87%|████████▋ | 1391/1600 [01:10<00:10, 19.56it/s]Training:  99%|█████████▉| 1588/1600 [01:20<00:00, 19.59it/s]                                                             Evaluating:   0%|          | 0/2Validation loss: 456.5428, Validation accuracy: 0.4900
Macro F1-score: 0.5013
Model performance on Angry speech (in validation): 
	Precision: 0.4561, Recall: 0.5200, F1_score: 0.4860
Model performance on Happy speech (in validation): 
	Precision: 0.2958, Recall: 0.4200, F1_score: 0.3471
Model performance on Neutral speech (in validation): 
	Precision: 0.6038, Recall: 0.6400, F1_score: 0.6214
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.3800, F1_score: 0.5507
Epoch 5/100

Training Phase:
Training loss: 49.0592, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925

Eval Phase: 
Validation loss: 449.8183, Validation accuracy: 0.5650
Macro F1-score: 0.5573
Model performance on Angry speech (in validation): 
	Precision: 0.6000, Recall: 0.3000, F1_score: 0.4000
Model performance on Happy speech (in validation): 
	Precision: 0.4227, Recall: 0.8200, F1_score: 0.5578
Model performance on Neutral speech (in validation): 
	Precision: 0.6250, Recall: 0.7000, F1_score: 0.6604
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.4400, F1_score: 0.6111
New best accuracy for layer 4 on epoch 5: 0.5650. Model saved.
Epoch 6/100

Training Phase:
00 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 203/1600 [00:10<01:09, 20.23it/s]Training:  25%|██▌       | 406/1600 [00:20<00:59, 20.21it/s]Training:  38%|███▊      | 608/1600 [00:30<00:49, 20.08it/s]Training:  51%|█████     | 810/1600 [00:40<00:39, 20.09it/s]Training:  63%|██████▎   | 1012/1600 [00:50<00:29, 20.09it/s]Training:  76%|███████▌  | 1213/1600 [01:00<00:19, 20.04it/s]Training:  89%|████████▊ | 1417/1600 [01:10<00:09, 20.12it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.91it/s]Training:  25%|██▌       | 400/1600 [00:20<01:00, 19.86it/s]Training:  38%|███▊      | 601/1600Training loss: 19.0774, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 559.4849, Validation accuracy: 0.5200
Macro F1-score: 0.5299
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.4800, F1_score: 0.4898
Model performance on Happy speech (in validation): 
	Precision: 0.3500, Recall: 0.5600, F1_score: 0.4308
Model performance on Neutral speech (in validation): 
	Precision: 0.6154, Recall: 0.6400, F1_score: 0.6275
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.4000, F1_score: 0.5714
Epoch 7/100

Training Phase:
 [00:30<00:50, 19.94it/s]Training:  50%|█████     | 803/1600 [00:40<00:39, 20.01it/s]Training:  63%|██████▎   | 1005/1600 [00:50<00:29, 19.83it/s]Training:  75%|███████▌  | 1201/1600 [01:00<00:20, 19.65it/s]Training:  87%|████████▋ | 1398/1600 [01:10<00:10, 19.66it/s]Training: 100%|█████████▉| 1595/1600 [01:20<00:00, 19.66it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.39it/s]Training:  25%|██▍       | 395/1600 [00:20<01:01, 19.75it/s]Training:  37%|███▋      | 596/1600 [00:30<00:50, 19.88it/s]Training:  50%|████▉     | 797/1600 [00:40<00:40, 19.67it/s]Training:  62%|██████▏   | 997/1600 [00:50<00:30, 19.76it/s]Training:  75%|███████▍  | 11Training loss: 30.1840, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975

Eval Phase: 
Validation loss: 577.9964, Validation accuracy: 0.5150
Macro F1-score: 0.5198
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.4800, F1_score: 0.4898
Model performance on Happy speech (in validation): 
	Precision: 0.3590, Recall: 0.5600, F1_score: 0.4375
Model performance on Neutral speech (in validation): 
	Precision: 0.5893, Recall: 0.6600, F1_score: 0.6226
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.3600, F1_score: 0.5294
Epoch 8/100

Training Phase:
Training loss: 35.1780, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
98/1600 [01:00<00:20, 19.85it/s]Training:  87%|████████▋ | 1399/1600 [01:10<00:10, 19.64it/s]Training: 100%|█████████▉| 1598/1600 [01:21<00:00, 19.72it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.57it/s]Training:  24%|██▍       | 392/1600 [00:20<01:01, 19.57it/s]Training:  37%|███▋      | 591/1600 [00:30<00:51, 19.68it/s]Training:  49%|████▉     | 790/1600 [00:40<00:41, 19.58it/s]Training:  62%|██████▏   | 988/1600 [00:50<00:31, 19.63it/s]Training:  74%|███████▍  | 1190/1600 [01:00<00:20, 19.81it/s]Training:  87%|████████▋ | 1399/1600 [01:10<00:09, 20.14it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<Validation loss: 893.9680, Validation accuracy: 0.3800
Macro F1-score: 0.3788
Model performance on Angry speech (in validation): 
	Precision: 0.4154, Recall: 0.5400, F1_score: 0.4696
Model performance on Happy speech (in validation): 
	Precision: 0.2159, Recall: 0.3800, F1_score: 0.2754
Model performance on Neutral speech (in validation): 
	Precision: 0.5641, Recall: 0.4400, F1_score: 0.4944
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1600, F1_score: 0.2759
Epoch 9/100

Training Phase:
Training loss: 39.0911, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 498.5990, Validation accuracy: 0.5150
Macro F1-score: 0.5305
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.5200, F1_score: 0.5098
Model performance on Happy speech (in validation): 
	Precision: 0.3291, Recall: 0.5200, F1_score: 0.4031
Model performance on Neutral speech (in validation): 
	Precision: 0.6364, Recall: 0.5600, F1_score: 0.5957
Model performance on Sad speech (in validation): 
	Precision: 0.9200, Recall: 0.4600, F1_score: 0.6133
Epoch 10/100

Training Phase:
?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 205/1600 [00:10<01:08, 20.48it/s]Training:  26%|██▌       | 410/1600 [00:20<00:58, 20.46it/s]Training:  38%|███▊      | 615/1600 [00:30<00:48, 20.34it/s]Training:  51%|█████     | 817/1600 [00:40<00:39, 20.06it/s]Training:  63%|██████▎   | 1014/1600 [00:50<00:29, 19.91it/s]Training:  76%|███████▌  | 1219/1600 [01:00<00:18, 20.07it/s]Training:  89%|████████▉ | 1423/1600 [01:10<00:08, 20.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 203/1600 [00:10<01:09, 20.19it/s]Training:  25%|██▌       | 407/1600 [00:20<00:58, 20.27it/s]Training:  38%|███▊      | 611/1600 [00:30<00Training loss: 16.0573, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 739.0058, Validation accuracy: 0.4700
Macro F1-score: 0.4733
Model performance on Angry speech (in validation): 
	Precision: 0.5435, Recall: 0.5000, F1_score: 0.5208
Model performance on Happy speech (in validation): 
	Precision: 0.3131, Recall: 0.6200, F1_score: 0.4161
Model performance on Neutral speech (in validation): 
	Precision: 0.5952, Recall: 0.5000, F1_score: 0.5435
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.2600, F1_score: 0.4127
Epoch 11/100

Training Phase:
:48, 20.24it/s]Training:  51%|█████     | 814/1600 [00:40<00:39, 19.96it/s]Training:  64%|██████▍   | 1020/1600 [00:50<00:28, 20.15it/s]Training:  77%|███████▋  | 1226/1600 [01:00<00:18, 20.30it/s]Training:  90%|████████▉ | 1432/1600 [01:11<00:08, 20.12it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 192/1600 [00:10<01:13, 19.14it/s]Training:  24%|██▍       | 384/1600 [00:20<01:03, 19.15it/s]Training:  36%|███▋      | 580/1600 [00:30<00:52, 19.33it/s]Training:  49%|████▊     | 778/1600 [00:40<00:42, 19.50it/s]Training:  61%|██████    | 976/1600 [00:50<00:32, 19.39it/s]Training:  73%|███████▎  | 1168/1600 [01:00<00:22, 19.16it/s]Training:  85%|████████▌ | 1362/1600 [01:10Training loss: 15.4106, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 711.8499, Validation accuracy: 0.4300
Macro F1-score: 0.4303
Model performance on Angry speech (in validation): 
	Precision: 0.4694, Recall: 0.4600, F1_score: 0.4646
Model performance on Happy speech (in validation): 
	Precision: 0.2872, Recall: 0.5400, F1_score: 0.3750
Model performance on Neutral speech (in validation): 
	Precision: 0.5435, Recall: 0.5000, F1_score: 0.5208
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.2200, F1_score: 0.3607
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.5650

Test Phase: 
<00:12, 19.23it/s]Training:  97%|█████████▋| 1556/1600 [01:20<00:02, 19.19it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|▌         | 10/200 [00:00<00:02, 92.29it/s]Testing:  10%|█         | 20/200 [00:00<00:01, 91.94it/s]Testing:  15%|█▌        | 30/200 [00:00<00:01, 93.49it/s]Testing:  20%|██        | 40/200 [00:00<00:01, 93.42it/s]Testing:  25%|██▌       | 50/200 [00:00<00:01, 93.74it/s]Testing:  30%|███       | 60/200 [00:00<00:01, 95.51it/s]Testing:  35%|███▌      | 70/200 [00:00<00:01, 95.96it/s]Testing:  40%|████      | 80/200 [00:00<00:01, 94.54it/s]Testing:  45%|████▌     | 90/200 [00:00<00:01, 94.97it/s]Testing:  50%|█████     | 100/200 [00:01<00:01, 94.66it/s]Testing:  55%|█████▌    | 110/200 [00:01<00:00, 94.38Test loss: 463.3541, Test accuracy: 0.5500
Macro F1-score: 0.5543
Model performance on Angry speech (in test): 
	Precision: 0.6552, Recall: 0.3800, F1_score: 0.4810
Model performance on Happy speech (in test): 
	Precision: 0.3883, Recall: 0.8000, F1_score: 0.5229
Model performance on Neutral speech (in test): 
	Precision: 0.6444, Recall: 0.5800, F1_score: 0.6105
Model performance on Sad speech (in test): 
	Precision: 0.9565, Recall: 0.4400, F1_score: 0.6027

de, all folds layer accuracy: ['0.3300', '0.7450', '0.5600', '0.6150', '0.5500']
de, all emo precision: {'Angry': ['0.4369', '1.0000', '0.5789', '0.9474', '0.6552'], 'Happy': ['0.1000', '0.6981', '0.5224', '0.5250', '0.3883'], 'Neutral': ['0.7647', '0.5714', '0.5763', '0.5342', '0.6444'], 'Sad': ['0.0000', '0.7800', '0.5833', '0.8571', '0.9565']}
de, all emo recall: {'Angry': ['0.9000', '0.8200', '0.4400', '0.3600', '0.3800'], 'Happy': ['0.1600', '0.7400', '0.7000', '0.8400', '0.8000'], 'Neutral': ['0.2600', '0.6400', '0.6800', '0.7800', '0.5800'], 'Sad': ['0.0000', '0.7800', '0.4200', '0.4800', '0.4400']}
de, all emo f1score: {'Angry': ['0.5882', '0.9011', '0.5000', '0.5217', '0.4810'], 'Happy': ['0.1231', '0.7184', '0.5983', '0.6462', '0.5229'], 'Neutral': ['0.3881', '0.6038', '0.6239', '0.6341', '0.6105'], 'Sad': ['0.0000', '0.7800', '0.4884', '0.6154', '0.6027']}
it/s]Testing:  60%|██████    | 120/200 [00:01<00:00, 94.97it/s]Testing:  65%|██████▌   | 130/200 [00:01<00:00, 94.67it/s]Testing:  70%|███████   | 140/200 [00:01<00:00, 94.44it/s]Testing:  75%|███████▌  | 150/200 [00:01<00:00, 96.05it/s]Testing:  80%|████████  | 160/200 [00:01<00:00, 96.35it/s]Testing:  85%|████████▌ | 170/200 [00:01<00:00, 96.23it/s]Testing:  90%|█████████ | 180/200 [00:01<00:00, 95.24it/s]Testing:  95%|█████████▌| 190/200 [00:02<00:00, 94.76it/s]Testing: 100%|██████████| 200/200 [00:02<00:00, 94.95it/s]                                                          ------------------NEXT SCRIPT: RUNNER_CN, former setting----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911928/matplotlib-8u6ktnof because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on cn =======================

Load dataset: 
Loading cn train data: fold_0...
Preprocess cn fold_0 data for cn model
Loading de eval data: fold_0...
Preprocess de fold_0 data for cn model
Loading de test data: fold_0...
Preprocess de fold_0 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1314.9543, Training accuracy: 0.6531
Macro F1-score: 0.6448
Model performance on Angry speech (in training): 
	Precision: 0.8783, Recall: 0.2525, F1_score: 0.3922
Model performance on Happy speech (in training): 
	Precision: 0.4319, Recall: 0.8875, F1_score: 0.5810
Model performance on Neutral speech (in training): 
	Precision: 0.8529, Recall: 0.6525, F1_score: 0.7394
Model performance on Sad speech (in training): 
	Precision: 0.9188, Recall: 0.8200, F1_score: 0.8666

Eval Phase: 
Validation loss: 561.2525, Validation accuracy: 0.3950
Macro F1-score: 0.3134
Model performance on Angry speech (in validation): 
	Precision: 0.8929, Recall: 0.5000, F1_score: 0.6410
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.0600, F1_score: 0.1111
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Sad speech (in validation): 
	Precision: 0.3012, Recall: 1.0000, F1_score: 0.4630
New best accuracy for layer 3 on epoch 1: 0.3950. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:14<6:38:26, 14.95s/it]Training:  11%|█▏        | 182/1600 [00:24<02:39,  8.88it/s]Training:  25%|██▌       | 404/1600 [00:34<01:23, 14.33it/s]Training:  40%|████      | 642/1600 [00:45<00:53, 17.81it/s]Training:  56%|█████▋    | 903/1600 [00:55<00:33, 20.65it/s]Training:  73%|███████▎  | 1168/1600 [01:05<00:19, 22.57it/s]Training:  90%|████████▉ | 1439/1600 [01:15<00:06, 23.99it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 283/1600 [00:10<00:46, 28.25it/s]Training:  36%|███▌      | 573/1600 [00:20<00:35, 28.66it/s]Training:  54%|█████▍    | 865/1600 [00:30<00:25, 28.88it/s]Training:  72%|███████▏  | 1157/1600 Training loss: 405.0586, Training accuracy: 0.9256
Macro F1-score: 0.9256
Model performance on Angry speech (in training): 
	Precision: 0.9023, Recall: 0.9000, F1_score: 0.9011
Model performance on Happy speech (in training): 
	Precision: 0.8853, Recall: 0.8875, F1_score: 0.8864
Model performance on Neutral speech (in training): 
	Precision: 0.9500, Recall: 0.9500, F1_score: 0.9500
Model performance on Sad speech (in training): 
	Precision: 0.9650, Recall: 0.9650, F1_score: 0.9650

Eval Phase: 
Validation loss: 419.5408, Validation accuracy: 0.5400
Macro F1-score: 0.4524
Model performance on Angry speech (in validation): 
	Precision: 0.7931, Recall: 0.9200, F1_score: 0.8519
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.7143, Recall: 0.2000, F1_score: 0.3125
Model performance on Sad speech (in validation): 
	Precision: 0.3968, Recall: 1.0000, F1_score: 0.5682
New best accuracy for layer 3 on epoch 2: 0.5400. Model saved.
Epoch 3/100

Training Phase:
Training loss: 236.2861, Training accuracy: 0.9531
Macro F1-score: 0.9531
Model performance on Angry speech (in training): 
	Precision: 0.9352, Recall: 0.9375, F1_score: 0.9363
Model performance on Happy speech (in training): 
	Precision: 0.9246, Recall: 0.9200, F1_score: 0.9223
Model performance on Neutral speech (in training): 
	Precision: 0.9656, Recall: 0.9825, F1_score: 0.9740
Model performance on Sad speech (in training): 
	Precision: 0.9873, Recall: 0.9725, F1_score: 0.9798

Eval Phase: 
Validation loss: 332.7697, Validation accuracy: 0.5900
Macro F1-score: 0.5583
Model performance on Angry speech (in validation): 
	Precision: 0.8409, Recall: 0.7400, F1_score: 0.7872
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1600, F1_score: 0.2759
Model performance on Neutral speech (in validation): 
	Precision: 0.6341, Recall: 0.5200, F1_score: 0.5714
Model performance on Sad speech (in validation): 
	Precision: 0.4393, Recall: 0.9400, F1_score: 0.5987
New best accuracy for layer 3 on epoch 3: 0.5900. Model saved.
Epoch 4/100

Training Phase:
[00:40<00:15, 28.69it/s]Training:  90%|█████████ | 1442/1600 [00:50<00:05, 28.44it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 291/1600 [00:10<00:45, 29.09it/s]Training:  36%|███▋      | 582/1600 [00:20<00:35, 28.74it/s]Training:  55%|█████▍    | 875/1600 [00:30<00:25, 28.96it/s]Training:  73%|███████▎  | 1168/1600 [00:40<00:15, 28.74it/s]Training:  91%|█████████ | 1453/1600 [00:50<00:05, 28.56it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 285/1600 [00:10<00:46, 28.44it/s]Training:  36%|███▌      | 570/1600 [00:20<Training loss: 176.9029, Training accuracy: 0.9656
Macro F1-score: 0.9656
Model performance on Angry speech (in training): 
	Precision: 0.9501, Recall: 0.9525, F1_score: 0.9513
Model performance on Happy speech (in training): 
	Precision: 0.9447, Recall: 0.9400, F1_score: 0.9424
Model performance on Neutral speech (in training): 
	Precision: 0.9703, Recall: 0.9800, F1_score: 0.9751
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937

Eval Phase: 
00:36, 28.39it/s]Training:  53%|█████▎    | 855/1600 [00:30<00:26, 28.41it/s]Training:  71%|███████▏  | 1143/1600 [00:40<00:16, 28.56it/s]Training:  90%|████████▉ | 1432/1600 [00:50<00:05, 28.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 622.2258, Validation accuracy: 0.5200
Macro F1-score: 0.3979
Model performance on Angry speech (in validation): 
	Precision: 0.6957, Recall: 0.9600, F1_score: 0.8067
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.6667, Recall: 0.1200, F1_score: 0.2034
Model performance on Sad speech (in validation): 
	Precision: 0.4098, Recall: 1.0000, F1_score: 0.5814
Epoch 5/100

Training Phase:
Training loss: 121.7060, Training accuracy: 0.9788
Macro F1-score: 0.9787
Model performance on Angry speech (in training): 
	Precision: 0.9676, Recall: 0.9700, F1_score: 0.9688
Model performance on Happy speech (in training): 
	Precision: 0.9724, Recall: 0.9700, F1_score: 0.9712
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 293/1600 [00:10<00:44, 29.23it/s]Training:  37%|███▋      | 586/1600 [00:20<00:34, 29.11it/s]Training:  55%|█████▍    | 877/1600 [00:30<00:24, 28.93it/s]Training:  73%|███████▎  | 1166/1600 [00:40<00:15, 28.90it/s]Training:  91%|█████████ | 1455/1600 [00:50<00:05, 28.80it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 560.9815, Validation accuracy: 0.5700
Macro F1-score: 0.4742
Model performance on Angry speech (in validation): 
	Precision: 0.7460, Recall: 0.9400, F1_score: 0.8319
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.6071, Recall: 0.3400, F1_score: 0.4359
Model performance on Sad speech (in validation): 
	Precision: 0.4587, Recall: 1.0000, F1_score: 0.6289
Epoch 6/100

Training Phase:
Training loss: 102.9602, Training accuracy: 0.9862
Macro F1-score: 0.9862
Model performance on Angry speech (in training): 
	Precision: 0.9773, Recall: 0.9700, F1_score: 0.9737
Model performance on Happy speech (in training): 
	Precision: 0.9751, Recall: 0.9800, F1_score: 0.9776
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 287/1600 [00:10<00:45, 28.60it/s]Training:  36%|███▌      | 574/1600 [00:20<00:35, 28.56it/s]Training:  54%|█████▍    | 860/1600 [00:30<00:25, 28.52it/s]Training:  72%|███████▏  | 1149/1600 [00:40<00:15, 28.64it/s]Training:  90%|████████▉ | 1438/1600 [00:50<00:05, 28.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 754.4800, Validation accuracy: 0.5450
Macro F1-score: 0.4294
Model performance on Angry speech (in validation): 
	Precision: 0.6533, Recall: 0.9800, F1_score: 0.7840
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.7143, Recall: 0.2000, F1_score: 0.3125
Model performance on Sad speech (in validation): 
	Precision: 0.4505, Recall: 1.0000, F1_score: 0.6211
Epoch 7/100

Training Phase:
Training loss: 96.9411, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9773, Recall: 0.9700, F1_score: 0.9737
Model performance on Happy speech (in training): 
	Precision: 0.9654, Recall: 0.9775, F1_score: 0.9714
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 587.4168, Validation accuracy: 0.5250
Macro F1-score: 0.4267
Model performance on Angry speech (in validation): 
	Precision: 0.7500, Recall: 0.9000, F1_score: 0.8182
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.1800, F1_score: 0.2647
Model performance on Sad speech (in validation): 
	Precision: 0.4132, Recall: 1.0000, F1_score: 0.5848
Epoch 8/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 289/1600 [00:10<00:45, 28.85it/s]Training:  37%|███▋      | 585/1600 [00:20<00:34, 29.24it/s]Training:  55%|█████▌    | 881/1600 [00:30<00:24, 29.17it/s]Training:  73%|███████▎  | 1174/1600 [00:40<00:14, 29.19it/s]Training:  92%|█████████▏| 1467/1600 [00:50<00:04, 28.83it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 293/1600 [00:10<00:44, 29.27it/s]Training:  37%|███▋      | 586/1600 [00:20<00:35, 28.70it/s]Training:  55%|█████▌    | 883/1600 [00:30<00:24, 29.13it/s]Training:  74%|███████▍  | 1180/1600 [00:40<00:14, 28.97it/s]Training:  92%|█████████▏| 1468/1600 [00:50<00:04, 28.64it/s]                   Training loss: 101.2111, Training accuracy: 0.9812
Macro F1-score: 0.9813
Model performance on Angry speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Happy speech (in training): 
	Precision: 0.9677, Recall: 0.9725, F1_score: 0.9701
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 874.6159, Validation accuracy: 0.4700
Macro F1-score: 0.3546
Model performance on Angry speech (in validation): 
	Precision: 0.8077, Recall: 0.8400, F1_score: 0.8235
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.3497, Recall: 1.0000, F1_score: 0.5181
Epoch 9/100

Training Phase:
Training loss: 62.9597, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9777, Recall: 0.9875, F1_score: 0.9826
Model performance on Happy speech (in training): 
	Precision: 0.9849, Recall: 0.9800, F1_score: 0.9825
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 294/1600 [00:10<00:44, 29.40it/s]Training:  37%|███▋      | 588/1600 [00:20<00:35, 28.90it/s]Training:  55%|█████▍    | 874/1600 [00:30<00:25, 28.65it/s]Training:  72%|███████▏  | 1158/1600 [00:40<00:15, 28.46it/s]Training:  90%|█████████ | 1443/1600 [00:50<00:05, 28.46it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 831.0483, Validation accuracy: 0.5050
Macro F1-score: 0.3707
Model performance on Angry speech (in validation): 
	Precision: 0.6957, Recall: 0.9600, F1_score: 0.8067
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.7500, Recall: 0.0600, F1_score: 0.1111
Model performance on Sad speech (in validation): 
	Precision: 0.3937, Recall: 1.0000, F1_score: 0.5650
Epoch 10/100

Training Phase:
Training loss: 80.3557, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774
Model performance on Neutral speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925

Eval Phase: 
Validation loss: 568.7724, Validation accuracy: 0.5550
Macro F1-score: 0.4653
Model performance on Angry speech (in validation): 
	Precision: 0.7966, Recall: 0.9400, F1_score: 0.8624
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.6500, Recall: 0.2600, F1_score: 0.3714
Model performance on Sad speech (in validation): 
	Precision: 0.4167, Recall: 1.0000, F1_score: 0.5882
Epoch 11/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 284/1600 [00:10<00:46, 28.37it/s]Training:  36%|███▌      | 568/1600 [00:20<00:36, 28.19it/s]Training:  53%|█████▎    | 849/1600 [00:30<00:26, 28.08it/s]Training:  71%|███████   | 1129/1600 [00:40<00:16, 28.03it/s]Training:  88%|████████▊ | 1409/1600 [00:50<00:06, 27.98it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 287/1600 [00:10<00:45, 28.68it/s]Training:  36%|███▌      | 579/1600 [00:20<00:35, 28.93it/s]Training:  54%|█████▍    | 871/1600 [00:30<00:25, 28.91it/s]Training:  73%|███████▎  | 1167/1600 [00:40<00:14, 29.17it/s]Training:  91%|█████████▏| 1463/1600 [00:50<00:04, 29.17it/s]                       Training loss: 100.6339, Training accuracy: 0.9788
Macro F1-score: 0.9787
Model performance on Angry speech (in training): 
	Precision: 0.9821, Recall: 0.9575, F1_score: 0.9696
Model performance on Happy speech (in training): 
	Precision: 0.9580, Recall: 0.9700, F1_score: 0.9640
Model performance on Neutral speech (in training): 
	Precision: 0.9778, Recall: 0.9900, F1_score: 0.9839
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 717.4383, Validation accuracy: 0.5250
Macro F1-score: 0.3896
Model performance on Angry speech (in validation): 
	Precision: 0.6098, Recall: 1.0000, F1_score: 0.7576
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0800, F1_score: 0.1481
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Sad speech (in validation): 
	Precision: 0.4425, Recall: 1.0000, F1_score: 0.6135
Epoch 12/100

Training Phase:
Training loss: 66.1830, Training accuracy: 0.9881
Macro F1-score: 0.9881
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
                                      Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 289/1600 [00:10<00:45, 28.87it/s]Training:  36%|███▌      | 578/1600 [00:20<00:35, 28.49it/s]Training:  54%|█████▍    | 871/1600 [00:30<00:25, 28.83it/s]Training:  73%|███████▎  | 1164/1600 [00:40<00:15, 28.92it/s]Training:  91%|█████████ | 1455/1600 [00:50<00:05, 28.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 1179.3698, Validation accuracy: 0.4600
Macro F1-score: 0.3321
Model performance on Angry speech (in validation): 
	Precision: 0.7925, Recall: 0.8400, F1_score: 0.8155
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.3448, Recall: 1.0000, F1_score: 0.5128
Epoch 13/100

Training Phase:
Training loss: 64.6630, Training accuracy: 0.9881
Macro F1-score: 0.9881
Model performance on Angry speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 519.2679, Validation accuracy: 0.5550
Macro F1-score: 0.4774
Model performance on Angry speech (in validation): 
	Precision: 0.7667, Recall: 0.9200, F1_score: 0.8364
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1000, F1_score: 0.1818
Model performance on Neutral speech (in validation): 
	Precision: 0.4762, Recall: 0.2000, F1_score: 0.2817
Model performance on Sad speech (in validation): 
	Precision: 0.4386, Recall: 1.0000, F1_score: 0.6098
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.5900

Test Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 291/1600 [00:10<00:45, 29.08it/s]Training:  36%|███▋      | 582/1600 [00:20<00:35, 28.49it/s]Training:  54%|█████▍    | 868/1600 [00:30<00:25, 28.50it/s]Training:  72%|███████▏  | 1156/1600 [00:40<00:15, 28.59it/s]Training:  90%|█████████ | 1447/1600 [00:50<00:05, 28.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:12, 15.32it/s]Testing:   2%|▏         | 4/200 [00:00<00:11, 16.99it/s]Testing:   4%|▎         | 7/200 [00:00<00:10, 18.87it/s]Testing:   4%|▍         | 9/200 [00:00<00:10, 18.93it/s]Testing:   6%|▌         | 12/200 [00:00<00:09, 20.53it/s]Testing:   8%|▊         | 15/200 [00:00<00:08, 22.24it/s]Testing:   9%|▉         | 18/200 [00:00<00:07, 22.88it/s]Testing:  10%|█         | 21/200 [00:00<00:07, 23.18it/s]Testing:  13%|█▎        | 26/200 [00:01<00:06, 28.56it/s]Testing:  16%|█▌        | 31/200 [00:01<00:05, 33.79it/s]Testing:  18%|█▊        | 35/200 [00:01<00:04, 33.78it/s]Testing:  20%|█▉        | 39/200 [00:01<00:04, 32.43it/s]Testing:  22%|██▏       | 43/200 [00:01<00:05, 29.72it/s]Testing:  24%|██▍       | 49/200 [00:01<00:04, 34.21it/s]Testing:  28%|██▊       | 55/200 [00:01<00:03, 40.12it/s]Testing:  30%|███       | 60/200 [00:01<00:03, 41.05it/s]Testing:  33%|███▎      | 66/200 [00:02<00:02, 45.81it/s]Testing:  36%|███▌      | 71/200 [00:02<00:03, 40.60it/s]Testing:  38%|███▊      | 76/200 [00:02<00:03, 38.97it/s]Testing:  40%|████      | 81/200 [00:02<00:02, 41.02it/s]Testing:  44%|████▍     | 89/200 [00:02<00:02, 50.64it/s]Testing:  48%|████▊     | 96/200 [00:02<00:01, 55.23it/s]Testing:  52%|█████▏    Test loss: 297.6185, Test accuracy: 0.6550
Macro F1-score: 0.6256
Model performance on Angry speech (in test): 
	Precision: 0.8400, Recall: 0.8400, F1_score: 0.8400
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.2000, F1_score: 0.3333
Model performance on Neutral speech (in test): 
	Precision: 0.7333, Recall: 0.6600, F1_score: 0.6947
Model performance on Sad speech (in test): 
	Precision: 0.4842, Recall: 0.9200, F1_score: 0.6345

======================= This is fold_1 on cn =======================

Load dataset: 
Loading cn train data: fold_1...
Preprocess cn fold_1 data for cn model
| 103/200 [00:02<00:01, 58.91it/s]Testing:  56%|█████▌    | 112/200 [00:02<00:01, 62.78it/s]Testing:  60%|█████▉    | 119/200 [00:03<00:01, 63.68it/s]Testing:  63%|██████▎   | 126/200 [00:03<00:01, 62.69it/s]Testing:  68%|██████▊   | 135/200 [00:03<00:00, 68.47it/s]Testing:  72%|███████▏  | 143/200 [00:03<00:00, 71.64it/s]Testing:  76%|███████▌  | 151/200 [00:03<00:00, 70.23it/s]Testing:  80%|████████  | 161/200 [00:03<00:00, 76.24it/s]Testing:  86%|████████▌ | 171/200 [00:03<00:00, 81.96it/s]Testing:  90%|█████████ | 181/200 [00:03<00:00, 86.62it/s]Testing:  95%|█████████▌| 190/200 [00:03<00:00, 85.96it/s]Testing: 100%|██████████| 200/200 [00:04<00:00, 81.84it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 5/1600 [00:00<00:41, 38.75 examples/s]Map:   1%|          | 11/1600 [00:00<00:36, 44.08 examples/s]Map:   1%|▏         | 20/1600 [00:00<00:25, 62.16 examples/s]Map:   3%|▎         | 41/1600 [00:00<00:13, 113.10 examples/s]Map:   3%|▎         | 55/1600 [00:00<00:12, 119.86 examples/s]Map:   5%|▍         | 73/1600 [00:00<00:11, 134.24 examples/s]Map:   6%|▌         | 89/1600 [00:00<00:10, 138.95 examples/s]Map:   7%|▋         | 111/1600 [00:00<00:11, 130.38 examples/s]Map:   8%|▊         | 129/1600 [00:01<00:10, 140.87 examples/s]Map:   9%|▉         | 147/1600 [00:01<00:09, 147.63 examples/s]Map:  10%|█         | 167/1600 [00:01<00:08, 159.62 examples/s]Map:  12%|█▏        | 188/1600 [00:01<00:08, 169.67 examples/s]Map:  13%|█▎        | 207/1600 [00:01<00:08, 155.12 examples/s]Map:  14%|█▍        | 229/1600 [00:01<00:09, 149.42 examples/s]Map:  15%|█▌        | 245/1600 [00:01<00:09, 149.30 examples/s]Map:  16%|█▋        | 262/1600 [00:01<00:10, 133.76 examples/s]Map:  17%|█▋        | 279/1600 [00:02<00:10, 121.78 examples/s]Map:  18%|█▊        | 293/1600 [00:02<00:12, 101.74 examples/s]Map:  19%|█▉        | 304/1600 [00:02<00:12, 102.42 examples/s]Map:  20%|█▉        | 318/1600 [00:02<00:11, 108.43 examples/s]Map:  21%|██        | 331/1600 [00:02<00:11, 111.87 examples/s]Map:  22%|██▏       | 344/1600 [00:02<00:10, 114.30 examples/s]Map:  22%|██▏       | 358/1600 [00:02<00:10, 116.49 examples/s]Map:  23%|██▎       | 370/1600 [00:02<00:10, 116.87 examples/s]Map:  24%|██▍       | 382/1600 [00:03<00:10, 116.64 examples/s]Map:  25%|██▍       | 396/1600 [00:03<00:11, 101.30 examples/s]Map:  26%|██▌       | 413/1600 [00:03<00:10, 116.91 examples/s]Map:  27%|██▋       | 431/1600 [00:03<00:08, 131.91 examples/s]Map:  28%|██▊       | 448/1600 [00:03<00:08, 139.99 examples/s]Map:  29%|██▉       | 463/1600 [00:03<00:08, 140.94 examples/s]Map:  30%|███       | 484/1600 [00:03<00:08, 137.18 examples/s]Map:  31%|███▏      | 500/1600 [00:04<00:09, 119.35 examples/s]Map:  32%|███▏      | 519/1600 [00:04<00:08, 134.07 examples/s]Map:  33%|███▎      | 535/1600 [00:04<00:07, 139.51 examples/s]Map:  35%|███▍      | 553/1600 [00:04<00:07, 147.46 examples/s]Map:  36%|███▌      | 573/1600 [00:04<00:06, 158.97 examples/s]Map:  37%|███▋      | 593/1600 [00:04<00:06, 149.59 examples/s]Map:  38%|███▊      | 613/1600 [00:04<00:06, 159.39 examples/s]Map:  39%|███▉      | 631/1600 [00:04<00:05, 164.37 examples/s]Map:  41%|████      | 651/1600 [00:04<00:05, 170.24 examples/s]Map:  42%|████▏     | 674/1600 [00:05<00:05, 158.74 examples/s]Map:  43%|████▎     | 694/1600 [00:05<00:06, 138.68 examples/s]Map:  44%|████▍     | 710/1600 [00:05<00:06, 141.51 examples/s]Map:  46%|████▌     | 728/1600 [00:05<00:05, 147.11 examples/s]Map:  46%|████▋     | 744/1600 [00:05<00:05, 147.71 examples/s]Map:  48%|████▊     | 763/1600 [00:05<00:05, 156.61 examples/s]Map:  49%|████▉     | 787/1600 [00:05<00:05, 149.83 examples/s]Map:  50%|█████     | 808/1600 [00:05<00:04, 163.29 examples/s]Map:  52%|█████▏    | 828/1600 [00:06<00:04, 169.35 examples/s]Map:  53%|█████▎    | 848/1600 [00:06<00:04, 174.27 examples/s]Map:  54%|█████▍    | 871/1600 [00:06<00:04, 164.47 examples/s]Map:  56%|█████▌    | 889/1600 [00:06<00:04, 146.77 examples/s]Map:  57%|█████▋    | 905/1600 [00:06<00:04, 147.95 examples/s]Map:  58%|█████▊    | 925/1600 [00:06<00:04, 158.81 examples/s]Map:  59%|█████▉    | 944/1600 [00:06<00:04, 163.32 examples/s]Map:  60%|██████    | 964/1600 [00:06<00:03, 171.05 examples/s]Map:  62%|██████▏   | 990/1600 [00:07<00:07, 82.33 examples/s] Map:  62%|██████▏   | 990/1600 [00:25<00:07, 82.33 examples/s]Map:  62%|██████▎   | 1000/1600 [00:46<06:35,  1.52 examples/s]Map:  64%|██████▍   | 1020/1600 [00:46<04:19,  2.23 examples/s]Map:  65%|██████▌   | 1041/1600 [00:46<02:49,  3.30 examples/s]Map:  66%|██████▋   | 1063/1600 [00:46<01:50,  4.85 examples/s]Map:  68%|██████▊   | 1082/1600 [00:46<01:17,  6.70 examples/s]Map:  69%|██████▉   | 1102/1600 [00:47<00:53,  9.38 examples/s]Map:  70%|██████▉   | 1119/1600 [00:47<00:38, 12.56 examples/s]Map:  71%|███████   | 1136/1600 [00:47<00:27, 16.90 examples/s]Map:  72%|███████▏  | 1154/1600 [00:47<00:19, 22.38 examples/s]Map:  73%|███████▎  | 1173/1600 [00:47<00:13, 30.66 examples/s]Map:  74%|███████▍  | 1192/1600 [00:47<00:09, 40.99 examples/s]Map:  76%|███████▌  | 1209/1600 [00:47<00:07, 51.57 examples/s]Map:  77%|███████▋  | 1232/1600 [00:47<00:05, 66.59 examples/s]Map:  78%|███████▊  | 1248/1600 [00:48<00:04, 71.84 examples/s]Map:  79%|███████▉  | 1266/1600 [00:48<00:04, 79.80 examples/s]Map:  80%|████████  | 1283/1600 [00:48<00:03, 85.58 examples/s]Map:  81%|████████▏ | 1301/1600 [00:48<00:03, 90.86 examples/s]Map:  82%|████████▏ | 1314/1600 [00:48<00:02, 96.53 examples/s]Map:  83%|████████▎ | 1327/1600 [00:48<00:02, 102.33 examples/s]Map:  84%|████████▍ | 1341/1600 [00:48<00:02, 110.08 examples/s]Map:  85%|████████▍ | 1356/1600 [00:49<00:02, 119.13 examples/s]Map:  86%|████████▌ | 1371/1600 [00:49<00:02, 109.76 examples/s]Map:  87%|████████▋ | 1390/1600 [00:49<00:01, 126.97 examples/s]Map:  88%|████████▊ | 1408/1600 [00:49<00:01, 137.94 examples/s]Map:  89%|████████▉ | 1426/1600 [00:49<00:01, 145.87 examples/s]Map:  90%|█████████ | 1445/1600 [00:49<00:00, 156.16 examples/s]Map:  91%|█████████▏| 1462/1600 [00:49<00:00, 155.73 examples/s]Map:  93%|█████████▎| 1481/1600 [00:49<00:00, 135.51 examples/s]Map:  94%|█████████▎| 1496/1600 [00:50<00:00, 136.94 examples/s]Map:  95%|█████████▍| 1514/1600 [00:50<00:00, 143.05 examples/s]Map:  96%|█████████▌| 1531/1600 [00:50<00:00, 147.38 examples/s]Map:  97%|█████████▋| 1549/1600 [00:50<00:00, 153.19 examples/s]Map:  98%|█████████▊| 1567/1600 [00:50<00:00, 156.24 examples/s]Map:  99%|█████████▉| 1585/1600 [00:50<00:00, 142.20 examples/s]Map: 100%|█████████▉| 1594/1600 [01:05<00:00, 142.20 examples/s]Map: 100%|██████████| 1600/1600 [01:15<00:00,  2.26 examples/s] Map: 100%|██████████| 1600/1600 [01:15<00:00, 21.24 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   5%|▌         | 10/200 [00:00<00:02, 70.77 examples/s]Map:  12%|█▏        | 24/200 [00:00<00:01, 97.66 examples/s]Map:  20%|██        | 40/200 [00:00<00:01, 120.39 examples/s]Map:  28%|██▊       | 56/200 [00:00<00:01, 105.47 examples/s]Map:  36%|███▋      | 73/200 [00:00<00:01, 120.24 examples/s]Map:  46%|████▋     | 93/200 [00:00<00:00, 141.17 examples/s]Map:  55%|█████▍    | 109/200 [00:00<00:00, 134.86 examples/s]Map:  64%|██████▍   | 128/200 [00:00<00:00, 145.75 examples/s]Map:  74%|███████▎  | 147/200 [00:01<00:00, 134.26 examples/s]Map:  81%|████████  | 162/200 [00:01<00:00, 134.38 examples/s]Map:  88%|████████▊ | 177/200 [00:01<00:00, 137.23 examples/s]Map: 100%|█████████▉| 199/200 [00:01<00:00, 156.81 examples/s]Map: 100%|██████████| 200/200 [00:09<00:00, 21.79 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▎         | 5/200 [00:00<00:04, 43.81 examples/s]Map:   6%|▋         | 13/200 [00:00<00:03, 56.87 examples/s]Map:  16%|█▌        | 32/200 [00:00<00:01, 108.37 examples/s]Map:  24%|██▎       | 47/200 [00:00<00:01, 110.84 examples/s]Map:  32%|███▏      | 64/200 [00:00<00:01, 126.40 examples/s]Map:  40%|████      | 81/200 [00:00<00:00, 137.98 examples/s]Map:  50%|█████     | 100/200 [00:00<00:00, 151.46 examples/s]Map:  62%|██████▎   | 125/200 [00:00<00:00, 146.73 examples/s]Map:  72%|███████▏  | 144/200 [00:01<00:00, 136.39 examples/s]Map:  82%|████████▎ | 165/200 [00:01<00:00, 135.11 examples/s]Map:  91%|█████████ | 182/200 [00:01<00:00, 141.31 examples/s]Map: 100%|██████████| 200/200 [00:09<00:00,  7.62 examples/s] Map: 100%|██████████| 200/200 [00:09<00:00, 21.90 examples/s]
Loading de eval data: fold_1...
Preprocess de fold_1 data for cn model
Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 1/1600 [00:00<02:46,  9.59 examples/s]Map:   0%|          | 8/1600 [00:00<00:40, 38.99 examples/s]Map:   1%|          | 19/1600 [00:00<00:24, 64.62 examples/s]Map:   2%|▏         | 28/1600 [00:00<00:21, 72.41 examples/s]Map:   2%|▏         | 38/1600 [00:00<00:20, 77.39 examples/s]Map:   3%|▎         | 51/1600 [00:00<00:20, 75.90 examples/s]Map:   4%|▍         | 60/1600 [00:00<00:19, 78.58 examples/s]Map:   4%|▍         | 70/1600 [00:00<00:18, 80.96 examples/s]Map:   5%|▌         | 80/1600 [00:01<00:18, 83.83 examples/s]Map:   6%|▌         | 90/1600 [00:01<00:17, 83.89 examples/s]Map:   6%|▌         | 99/1600 [00:01<00:18, 79.77 examples/s]Map:   7%|▋         | 112/1600 [00:01<00:18, 79.59 examples/s]Map:   8%|▊         | 122/1600 [00:01<00:18, 80.48 examples/s]Map:   8%|▊         | 131/1600 [00:01<00:18, 80.49 examples/s]Map:   9%|▉         | 140/1600 [00:01<00:18, 78.99 examples/s]Map:   9%|▉         | 149/1600 [00:01<00:18, 78.92 examples/s]Map:  10%|▉         | 157/1600 [00:02<00:18, 76.69 examples/s]Map:  10%|█         | 167/1600 [00:02<00:22, 64.21 examples/s]Map:  11%|█         | 176/1600 [00:02<00:21, 67.76 examples/s]Map:  12%|█▏        | 184/1600 [00:02<00:20, 70.10 examples/s]Map:  12%|█▏        | 194/1600 [00:02<00:19, 73.24 examples/s]Map:  13%|█▎        | 204/1600 [00:02<00:18, 76.52 examples/s]Map:  13%|█▎        | 215/1600 [00:02<00:18, 74.39 examples/s]Map:  14%|█▍        | 225/1600 [00:02<00:17, 78.41 examples/s]Map:  15%|█▍        | 235/1600 [00:03<00:16, 82.47 examples/s]Map:  15%|█▌        | 244/1600 [00:03<00:16, 80.84 examples/s]Map:  16%|█▌        | 253/1600 [00:03<00:16, 79.48 examples/s]Map:  16%|█▋        | 262/1600 [00:03<00:16, 79.61 examples/s]Map:  17%|█▋        | 275/1600 [00:03<00:19, 68.35 examples/s]Map:  18%|█▊        | 284/1600 [00:03<00:18, 70.90 examples/s]Map:  18%|█▊        | 293/1600 [00:03<00:18, 71.59 examples/s]Map:  19%|█▉        | 302/1600 [00:04<00:17, 74.55 examples/s]Map:  19%|█▉        | 311/1600 [00:04<00:16, 76.83 examples/s]Map:  20%|█▉        | 319/1600 [00:04<00:17, 74.63 examples/s]Map:  20%|██        | 327/1600 [00:04<00:17, 74.79 examples/s]Map:  21%|██        | 336/1600 [00:04<00:16, 76.06 examples/s]Map:  22%|██▏       | 345/1600 [00:04<00:16, 76.68 examples/s]Map:  22%|██▏       | 355/1600 [00:04<00:15, 80.41 examples/s]Map:  23%|██▎       | 364/1600 [00:04<00:15, 79.01 examples/s]Map:  23%|██▎       | 372/1600 [00:04<00:15, 78.07 examples/s]Map:  24%|██▍       | 381/1600 [00:05<00:15, 77.17 examples/s]Map:  24%|██▍       | 389/1600 [00:05<00:19, 62.13 examples/s]Map:  25%|██▌       | 400/1600 [00:05<00:16, 71.44 examples/s]Map:  26%|██▌       | 410/1600 [00:05<00:15, 76.99 examples/s]Map:  26%|██▋       | 423/1600 [00:05<00:13, 88.10 examples/s]Map:  27%|██▋       | 436/1600 [00:05<00:12, 93.57 examples/s]Map:  28%|██▊       | 447/1600 [00:05<00:11, 97.16 examples/s]Map:  29%|██▉       | 460/1600 [00:05<00:10, 104.09 examples/s]Map:  29%|██▉       | 471/1600 [00:06<00:10, 104.27 examples/s]Map:  30%|███       | 483/1600 [00:06<00:10, 104.41 examples/s]Map:  31%|███       | 495/1600 [00:06<00:12, 90.71 examples/s] Map:  32%|███▏      | 506/1600 [00:06<00:15, 72.56 examples/s]Map:  32%|███▏      | 515/1600 [00:06<00:14, 72.58 examples/s]Map:  33%|███▎      | 524/1600 [00:06<00:14, 73.91 examples/s]Map:  33%|███▎      | 533/1600 [00:06<00:14, 75.97 examples/s]Map:  34%|███▍      | 542/1600 [00:06<00:13, 76.13 examples/s]Map:  34%|███▍      | 550/1600 [00:07<00:13, 75.73 examples/s]Map:  35%|███▍      | 558/1600 [00:07<00:13, 75.22 examples/s]Map:  35%|███▌      | 566/1600 [00:07<00:13, 73.96 examples/s]Map:  36%|███▌      | 574/1600 [00:07<00:14, 71.90 examples/s]Map:  37%|███▋      | 585/1600 [00:07<00:14, 68.92 examples/s]Map:  37%|███▋      | 593/1600 [00:07<00:14, 70.10 examples/s]Map:  38%|███▊      | 601/1600 [00:07<00:14, 70.44 examples/s]Map:  38%|███▊      | 609/1600 [00:08<00:18, 54.67 examples/s]Map:  39%|███▊      | 617/1600 [00:08<00:16, 57.99 examples/s]Map:  39%|███▉      | 625/1600 [00:08<00:15, 61.91 examples/s]Map:  40%|███▉      | 633/1600 [00:08<00:15, 63.24 examples/s]Map:  40%|████      | 643/1600 [00:08<00:13, 68.83 examples/s]Map:  41%|████      | 651/1600 [00:08<00:13, 69.07 examples/s]Map:  41%|████      | 659/1600 [00:08<00:13, 70.41 examples/s]Map:  42%|████▏     | 668/1600 [00:08<00:12, 73.00 examples/s]Map:  42%|████▏     | 677/1600 [00:08<00:12, 73.97 examples/s]Map:  43%|████▎     | 686/1600 [00:09<00:12, 74.81 examples/s]Map:  43%|████▎     | 695/1600 [00:09<00:11, 76.32 examples/s]Map:  44%|████▍     | 704/1600 [00:09<00:11, 75.53 examples/s]Map:  45%|████▍     | 716/1600 [00:09<00:11, 73.90 examples/s]Map:  45%|████▌     | 727/1600 [00:09<00:14, 60.52 examples/s]Map:  46%|████▌     | 736/1600 [00:09<00:13, 64.29 examples/s]Map:  46%|████▋     | 743/1600 [00:09<00:13, 64.45 examples/s]Map:  47%|████▋     | 750/1600 [00:10<00:12, 65.53 examples/s]Map:  47%|████▋     | 759/1600 [00:10<00:12, 66.87 examples/s]Map:  48%|████▊     | 767/1600 [00:10<00:12, 67.49 examples/s]Map:  48%|████▊     | 775/1600 [00:10<00:12, 67.05 examples/s]Map:  49%|████▉     | 783/1600 [00:10<00:12, 66.37 examples/s]Map:  50%|████▉     | 793/1600 [00:10<00:10, 74.84 examples/s]Map:  50%|█████     | 801/1600 [00:10<00:11, 69.40 examples/s]Map:  51%|█████     | 810/1600 [00:11<00:13, 58.57 examples/s]Map:  51%|█████     | 818/1600 [00:11<00:12, 60.30 examples/s]Map:  52%|█████▏    | 827/1600 [00:11<00:11, 67.12 examples/s]Map:  52%|█████▏    | 835/1600 [00:11<00:11, 68.74 examples/s]Map:  53%|█████▎    | 846/1600 [00:11<00:12, 61.34 examples/s]Map:  53%|█████▎    | 855/1600 [00:11<00:11, 66.66 examples/s]Map:  54%|█████▍    | 864/1600 [00:11<00:10, 70.18 examples/s]Map:  55%|█████▍    | 872/1600 [00:11<00:10, 72.12 examples/s]Map:  55%|█████▌    | 881/1600 [00:11<00:09, 74.22 examples/s]Map:  56%|█████▌    | 889/1600 [00:12<00:09, 72.62 examples/s]Map:  56%|█████▋    | 900/1600 [00:12<00:10, 69.68 examples/s]Map:  57%|█████▋    | 908/1600 [00:12<00:10, 67.90 examples/s]Map:  57%|█████▋    | 916/1600 [00:12<00:09, 68.71 examples/s]Map:  58%|█████▊    | 925/1600 [00:12<00:10, 63.82 examples/s]Map:  58%|█████▊    | 934/1600 [00:12<00:10, 65.01 examples/s]Map:  59%|█████▉    | 941/1600 [00:12<00:10, 62.87 examples/s]Map:  59%|█████▉    | 948/1600 [00:13<00:30, 21.13 examples/s]Map:  60%|█████▉    | 956/1600 [00:13<00:24, 26.78 examples/s]Map:  60%|██████    | 965/1600 [00:14<00:18, 34.10 examples/s]Map:  61%|██████    | 975/1600 [00:14<00:15, 40.07 examples/s]Map:  61%|██████▏   | 983/1600 [00:14<00:13, 45.53 examples/s]Map:  62%|██████▏   | 991/1600 [00:14<00:12, 50.48 examples/s]Map:  62%|██████▏   | 999/1600 [00:14<00:11, 53.94 examples/s]Map:  62%|██████▏   | 999/1600 [00:32<00:11, 53.94 examples/s]Map:  62%|██████▎   | 1000/1600 [01:14<29:47,  2.98s/ examples]Map:  63%|██████▎   | 1008/1600 [01:14<18:51,  1.91s/ examples]Map:  63%|██████▎   | 1015/1600 [01:14<12:54,  1.32s/ examples]Map:  64%|██████▍   | 1023/1600 [01:15<08:29,  1.13 examples/s]Map:  64%|██████▍   | 1031/1600 [01:15<05:42,  1.66 examples/s]Map:  65%|██████▌   | 1041/1600 [01:15<03:35,  2.59 examples/s]Map:  66%|██████▌   | 1051/1600 [01:15<02:22,  3.86 examples/s]Map:  66%|██████▌   | 1059/1600 [01:15<01:43,  5.25 examples/s]Map:  67%|██████▋   | 1066/1600 [01:15<01:17,  6.90 examples/s]Map:  67%|██████▋   | 1074/1600 [01:15<00:55,  9.44 examples/s]Map:  68%|██████▊   | 1084/1600 [01:16<00:38, 13.32 examples/s]Map:  68%|██████▊   | 1091/1600 [01:16<00:30, 16.60 examples/s]Map:  69%|██████▊   | 1099/1600 [01:16<00:23, 21.28 examples/s]Map:  69%|██████▉   | 1107/1600 [01:16<00:18, 26.92 examples/s]Map:  70%|██████▉   | 1114/1600 [01:16<00:15, 31.54 examples/s]Map:  70%|███████   | 1122/1600 [01:16<00:12, 38.28 examples/s]Map:  71%|███████   | 1130/1600 [01:16<00:12, 36.87 examples/s]Map:  71%|███████   | 1137/1600 [01:16<00:11, 41.04 examples/s]Map:  72%|███████▏  | 1145/1600 [01:17<00:09, 47.13 examples/s]Map:  72%|███████▏  | 1153/1600 [01:17<00:08, 52.12 examples/s]Map:  73%|███████▎  | 1161/1600 [01:17<00:07, 57.05 examples/s]Map:  73%|███████▎  | 1169/1600 [01:17<00:07, 60.27 examples/s]Map:  74%|███████▎  | 1176/1600 [01:17<00:06, 61.73 examples/s]Map:  74%|███████▍  | 1184/1600 [01:17<00:06, 62.51 examples/s]Map:  74%|███████▍  | 1192/1600 [01:17<00:06, 65.09 examples/s]Map:  75%|███████▌  | 1202/1600 [01:17<00:05, 68.66 examples/s]Map:  76%|███████▌  | 1210/1600 [01:18<00:05, 68.73 examples/s]Map:  76%|███████▋  | 1220/1600 [01:18<00:06, 62.95 examples/s]Map:  77%|███████▋  | 1228/1600 [01:18<00:05, 65.62 examples/s]Map:  77%|███████▋  | 1235/1600 [01:18<00:05, 65.31 examples/s]Map:  78%|███████▊  | 1243/1600 [01:18<00:08, 42.30 examples/s]Map:  78%|███████▊  | 1251/1600 [01:18<00:07, 47.36 examples/s]Map:  79%|███████▊  | 1258/1600 [01:19<00:06, 51.02 examples/s]Map:  79%|███████▉  | 1266/1600 [01:19<00:05, 56.83 examples/s]Map:  80%|███████▉  | 1275/1600 [01:19<00:05, 61.35 examples/s]Map:  80%|████████  | 1282/1600 [01:19<00:05, 61.27 examples/s]Map:  81%|████████  | 1291/1600 [01:19<00:05, 57.13 examples/s]Map:  81%|████████▏ | 1300/1600 [01:19<00:04, 61.65 examples/s]Map:  82%|████████▏ | 1307/1600 [01:19<00:04, 60.71 examples/s]Map:  82%|████████▏ | 1315/1600 [01:19<00:04, 60.33 examples/s]Map:  83%|████████▎ | 1322/1600 [01:20<00:04, 62.33 examples/s]Map:  83%|████████▎ | 1331/1600 [01:20<00:04, 65.92 examples/s]Map:  84%|████████▎ | 1338/1600 [01:20<00:04, 64.80 examples/s]Map:  84%|████████▍ | 1346/1600 [01:20<00:03, 66.16 examples/s]Map:  85%|████████▍ | 1356/1600 [01:20<00:04, 53.61 examples/s]Map:  85%|████████▌ | 1364/1600 [01:20<00:04, 57.83 examples/s]Map:  86%|████████▌ | 1371/1600 [01:20<00:03, 58.17 examples/s]Map:  86%|████████▌ | 1379/1600 [01:20<00:03, 61.59 examples/s]Map:  87%|████████▋ | 1388/1600 [01:21<00:03, 65.96 examples/s]Map:  87%|████████▋ | 1395/1600 [01:21<00:03, 64.32 examples/s]Map:  88%|████████▊ | 1402/1600 [01:21<00:03, 63.30 examples/s]Map:  88%|████████▊ | 1410/1600 [01:21<00:02, 64.66 examples/s]Map:  89%|████████▊ | 1417/1600 [01:21<00:02, 63.73 examples/s]Map:  89%|████████▉ | 1427/1600 [01:21<00:02, 58.76 examples/s]Map:  90%|████████▉ | 1434/1600 [01:21<00:02, 58.57 examples/s]Map:  90%|█████████ | 1442/1600 [01:21<00:02, 60.24 examples/s]Map:  91%|█████████ | 1450/1600 [01:22<00:02, 62.94 examples/s]Map:  91%|█████████ | 1457/1600 [01:22<00:02, 61.31 examples/s]Map:  92%|█████████▏| 1465/1600 [01:22<00:02, 48.27 examples/s]Map:  92%|█████████▏| 1471/1600 [01:22<00:02, 49.08 examples/s]Map:  92%|█████████▏| 1479/1600 [01:22<00:02, 53.87 examples/s]Map:  93%|█████████▎| 1486/1600 [01:22<00:02, 55.70 examples/s]Map:  93%|█████████▎| 1494/1600 [01:22<00:01, 58.03 examples/s]Map:  94%|█████████▍| 1501/1600 [01:23<00:01, 59.41 examples/s]Map:  94%|█████████▍| 1508/1600 [01:23<00:01, 59.96 examples/s]Map:  95%|█████████▍| 1515/1600 [01:23<00:01, 61.25 examples/s]Map:  95%|█████████▌| 1523/1600 [01:23<00:01, 61.48 examples/s]Map:  96%|█████████▌| 1530/1600 [01:23<00:01, 61.73 examples/s]Map:  96%|█████████▌| 1537/1600 [01:23<00:01, 62.88 examples/s]Map:  96%|█████████▋| 1544/1600 [01:23<00:00, 61.71 examples/s]Map:  97%|█████████▋| 1551/1600 [01:23<00:00, 63.39 examples/s]Map:  97%|█████████▋| 1559/1600 [01:23<00:00, 64.97 examples/s]Map:  98%|█████████▊| 1566/1600 [01:24<00:00, 63.99 examples/s]Map:  98%|█████████▊| 1573/1600 [01:24<00:00, 51.95 examples/s]Map:  99%|█████████▉| 1581/1600 [01:24<00:00, 53.76 examples/s]Map:  99%|█████████▉| 1588/1600 [01:24<00:00, 56.22 examples/s]Map: 100%|█████████▉| 1596/1600 [01:24<00:00, 59.14 examples/s]Map: 100%|██████████| 1600/1600 [01:36<00:00, 59.14 examples/s]Map: 100%|██████████| 1600/1600 [02:09<00:00,  2.09s/ examples]Map: 100%|██████████| 1600/1600 [02:09<00:00, 12.37 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▏         | 3/200 [00:00<00:09, 21.79 examples/s]Map:   3%|▎         | 6/200 [00:00<00:07, 24.28 examples/s]Map:   6%|▋         | 13/200 [00:00<00:04, 40.51 examples/s]Map:  12%|█▏        | 23/200 [00:00<00:03, 57.36 examples/s]Map:  16%|█▌        | 32/200 [00:00<00:02, 64.43 examples/s]Map:  21%|██        | 42/200 [00:00<00:02, 72.84 examples/s]Map:  26%|██▌       | 51/200 [00:00<00:01, 77.18 examples/s]Map:  31%|███       | 62/200 [00:00<00:01, 85.40 examples/s]Map:  36%|███▌      | 71/200 [00:01<00:01, 79.95 examples/s]Map:  42%|████▏     | 83/200 [00:01<00:01, 74.70 examples/s]Map:  48%|████▊     | 95/200 [00:01<00:01, 65.43 examples/s]Map:  52%|█████▏    | 103/200 [00:01<00:01, 66.67 examples/s]Map:  56%|█████▌    | 112/200 [00:01<00:01, 68.89 examples/s]Map:  62%|██████▏   | 123/200 [00:01<00:01, 67.79 examples/s]Map:  66%|██████▌   | 132/200 [00:01<00:00, 69.39 examples/s]Map:  70%|███████   | 140/200 [00:02<00:00, 70.67 examples/s]Map:  76%|███████▌  | 151/200 [00:02<00:00, 67.79 examples/s]Map:  80%|███████▉  | 159/200 [00:02<00:00, 68.08 examples/s]Map:  83%|████████▎ | 166/200 [00:02<00:00, 62.89 examples/s]Map:  87%|████████▋ | 174/200 [00:02<00:00, 65.58 examples/s]Map:  92%|█████████▏| 184/200 [00:02<00:00, 64.23 examples/s]Map:  96%|█████████▌| 192/200 [00:02<00:00, 62.47 examples/s]Map: 100%|██████████| 200/200 [00:03<00:00, 63.10 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 12.68 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▏         | 3/200 [00:00<00:11, 16.97 examples/s]Map:   6%|▌         | 11/200 [00:00<00:05, 37.71 examples/s]Map:  10%|▉         | 19/200 [00:00<00:03, 51.78 examples/s]Map:  14%|█▍        | 29/200 [00:00<00:02, 63.38 examples/s]Map:  18%|█▊        | 37/200 [00:00<00:02, 66.16 examples/s]Map:  24%|██▎       | 47/200 [00:00<00:02, 74.65 examples/s]Map:  28%|██▊       | 57/200 [00:00<00:01, 79.86 examples/s]Map:  34%|███▎      | 67/200 [00:00<00:01, 82.06 examples/s]Map:  38%|███▊      | 76/200 [00:01<00:01, 82.18 examples/s]Map:  44%|████▍     | 88/200 [00:01<00:01, 75.24 examples/s]Map:  50%|████▉     | 99/200 [00:01<00:01, 65.46 examples/s]Map:  54%|█████▍    | 108/200 [00:01<00:01, 69.15 examples/s]Map:  58%|█████▊    | 117/200 [00:01<00:01, 69.57 examples/s]Map:  62%|██████▎   | 125/200 [00:01<00:01, 68.96 examples/s]Map:  66%|██████▋   | 133/200 [00:01<00:00, 70.05 examples/s]Map:  70%|███████   | 141/200 [00:02<00:00, 70.43 examples/s]Map:  76%|███████▌  | 151/200 [00:02<00:00, 73.26 examples/s]Map:  80%|███████▉  | 159/200 [00:02<00:00, 70.03 examples/s]Map:  84%|████████▍ | 168/200 [00:02<00:00, 71.83 examples/s]Map:  88%|████████▊ | 176/200 [00:02<00:00, 70.46 examples/s]Map:  94%|█████████▎| 187/200 [00:02<00:00, 68.69 examples/s]Map:  98%|█████████▊| 195/200 [00:02<00:00, 70.14 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 13.14 examples/s]
Loading de test data: fold_1...
Preprocess de fold_1 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 385.4014, Training accuracy: 0.9250
Macro F1-score: 0.9250
Model performance on Angry speech (in training): 
	Precision: 0.9040, Recall: 0.8950, F1_score: 0.8995
Model performance on Happy speech (in training): 
	Precision: 0.8905, Recall: 0.8950, F1_score: 0.8928
Model performance on Neutral speech (in training): 
	Precision: 0.9312, Recall: 0.9475, F1_score: 0.9393
Model performance on Sad speech (in training): 
	Precision: 0.9747, Recall: 0.9625, F1_score: 0.9686

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 285/1600 [00:10<00:46, 28.41it/s]Training:  36%|███▌      | 570/1600 [00:20<00:36, 28.28it/s]Training:  53%|█████▎    | 855/1600 [00:30<00:26, 28.37it/s]Training:  71%|███████▏  | 1140/1600 [00:40<00:16, 28.33it/s]Training:  89%|████████▉ | 1423/1600 [00:50<00:06, 28.29it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 559.2927, Validation accuracy: 0.5050
Macro F1-score: 0.4119
Model performance on Angry speech (in validation): 
	Precision: 0.9167, Recall: 0.8800, F1_score: 0.8980
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4375, Recall: 0.1400, F1_score: 0.2121
Model performance on Sad speech (in validation): 
	Precision: 0.3676, Recall: 1.0000, F1_score: 0.5376
New best accuracy for layer 3 on epoch 1: 0.5050. Model saved.
Epoch 2/100

Training Phase:
Training loss: 160.3256, Training accuracy: 0.9656
Macro F1-score: 0.9656
Model performance on Angry speech (in training): 
	Precision: 0.9480, Recall: 0.9575, F1_score: 0.9527
Model performance on Happy speech (in training): 
	Precision: 0.9520, Recall: 0.9425, F1_score: 0.9472
Model performance on Neutral speech (in training): 
	Precision: 0.9726, Recall: 0.9775, F1_score: 0.9751
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 287/1600 [00:10<00:45, 28.61it/s]Training:  36%|███▌      | 574/1600 [00:20<00:36, 28.29it/s]Training:  54%|█████▎    | 859/1600 [00:30<00:26, 28.36it/s]Training:  72%|███████▏  | 1144/1600 [00:40<00:16, 28.36it/s]Training:  89%|████████▉ | 1428/1600 [00:50<00:06, 28.30it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 350.5599, Validation accuracy: 0.7050
Macro F1-score: 0.6205
Model performance on Angry speech (in validation): 
	Precision: 0.9388, Recall: 0.9200, F1_score: 0.9293
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.9600, F1_score: 0.6575
Model performance on Sad speech (in validation): 
	Precision: 0.8545, Recall: 0.9400, F1_score: 0.8952
New best accuracy for layer 3 on epoch 2: 0.7050. Model saved.
Epoch 3/100

Training Phase:
Training loss: 109.1191, Training accuracy: 0.9738
Macro F1-score: 0.9737
Model performance on Angry speech (in training): 
	Precision: 0.9653, Recall: 0.9725, F1_score: 0.9689
Model performance on Happy speech (in training): 
	Precision: 0.9722, Recall: 0.9600, F1_score: 0.9660
Model performance on Neutral speech (in training): 
	Precision: 0.9703, Recall: 0.9800, F1_score: 0.9751
Model performance on Sad speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 286/1600 [00:10<00:45, 28.57it/s]Training:  36%|███▌      | 573/1600 [00:20<00:35, 28.63it/s]Training:  54%|█████▍    | 860/1600 [00:30<00:26, 28.38it/s]Training:  72%|███████▏  | 1145/1600 [00:40<00:16, 28.42it/s]Training:  89%|████████▉ | 1430/1600 [00:50<00:05, 28.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 343.3663, Validation accuracy: 0.6950
Macro F1-score: 0.6065
Model performance on Angry speech (in validation): 
	Precision: 0.9057, Recall: 0.9600, F1_score: 0.9320
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.8400, F1_score: 0.6269
Model performance on Sad speech (in validation): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Epoch 4/100

Training Phase:
Training loss: 109.6314, Training accuracy: 0.9812
Macro F1-score: 0.9813
Model performance on Angry speech (in training): 
	Precision: 0.9749, Recall: 0.9700, F1_score: 0.9724
Model performance on Happy speech (in training): 
	Precision: 0.9653, Recall: 0.9750, F1_score: 0.9701
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9949, Recall: 0.9850, F1_score: 0.9899

Eval Phase: 
Validation loss: 378.9180, Validation accuracy: 0.6700
Macro F1-score: 0.5887
Model performance on Angry speech (in validation): 
	Precision: 0.8448, Recall: 0.9800, F1_score: 0.9074
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.5323, Recall: 0.6600, F1_score: 0.5893
Model performance on Sad speech (in validation): 
	Precision: 0.6410, Recall: 1.0000, F1_score: 0.7812
Epoch 5/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 273/1600 [00:10<00:48, 27.22it/s]Training:  34%|███▍      | 551/1600 [00:20<00:38, 27.56it/s]Training:  52%|█████▏    | 837/1600 [00:30<00:27, 28.00it/s]Training:  70%|███████   | 1123/1600 [00:40<00:17, 27.80it/s]Training:  88%|████████▊ | 1405/1600 [00:50<00:06, 27.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 282/1600 [00:10<00:46, 28.17it/s]Training:  35%|███▌      | 564/1600 [00:20<00:36, 28.03it/s]Training:  35%|███▌      | 564/1600 [00:30<00:36, 28.03it/s]Training:  53%|█████▎    | 841/1600 [00:30<00:27, 27.75it/s]Training:  70%|███████   | 1125/1600 [00:40<00:16, 28.00it/s]Training:  88%|███████Training loss: 89.2636, Training accuracy: 0.9812
Macro F1-score: 0.9812
Model performance on Angry speech (in training): 
	Precision: 0.9678, Recall: 0.9775, F1_score: 0.9726
Model performance on Happy speech (in training): 
	Precision: 0.9698, Recall: 0.9625, F1_score: 0.9661
Model performance on Neutral speech (in training): 
	Precision: 0.9949, Recall: 0.9850, F1_score: 0.9899
Model performance on Sad speech (in training): 
	Precision: 0.9926, Recall: 1.0000, F1_score: 0.9963

Eval Phase: 
▊ | 1409/1600 [00:50<00:06, 27.99it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 681.4372, Validation accuracy: 0.5150
Macro F1-score: 0.4428
Model performance on Angry speech (in validation): 
	Precision: 0.9750, Recall: 0.7800, F1_score: 0.8667
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4516, Recall: 0.2800, F1_score: 0.3457
Model performance on Sad speech (in validation): 
	Precision: 0.3876, Recall: 1.0000, F1_score: 0.5587
Epoch 6/100

Training Phase:
Training loss: 63.3382, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 282/1600 [00:10<00:46, 28.09it/s]Training:  35%|███▌      | 565/1600 [00:20<00:36, 28.21it/s]Training:  53%|█████▎    | 848/1600 [00:30<00:26, 28.11it/s]Training:  71%|███████   | 1129/1600 [00:40<00:16, 28.10it/s]Training:  88%|████████▊ | 1410/1600 [00:50<00:06, 28.00it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 909.9916, Validation accuracy: 0.4450
Macro F1-score: 0.3549
Model performance on Angry speech (in validation): 
	Precision: 0.9730, Recall: 0.7200, F1_score: 0.8276
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.0600, F1_score: 0.1017
Model performance on Sad speech (in validation): 
	Precision: 0.3247, Recall: 1.0000, F1_score: 0.4902
Epoch 7/100

Training Phase:
Training loss: 57.5626, Training accuracy: 0.9888
Macro F1-score: 0.9888
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9875, F1_score: 0.9912

Eval Phase: 
Validation loss: 465.9487, Validation accuracy: 0.6200
Macro F1-score: 0.5502
Model performance on Angry speech (in validation): 
	Precision: 0.7538, Recall: 0.9800, F1_score: 0.8522
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1000, F1_score: 0.1818
Model performance on Neutral speech (in validation): 
	Precision: 0.6667, Recall: 0.4000, F1_score: 0.5000
Model performance on Sad speech (in validation): 
	Precision: 0.5000, Recall: 1.0000, F1_score: 0.6667
Epoch 8/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 284/1600 [00:10<00:46, 28.35it/s]Training:  36%|███▌      | 568/1600 [00:20<00:36, 27.90it/s]Training:  53%|█████▎    | 855/1600 [00:30<00:26, 28.21it/s]Training:  71%|███████▏  | 1141/1600 [00:40<00:16, 28.13it/s]Training:  89%|████████▉ | 1421/1600 [00:50<00:06, 27.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 282/1600 [00:10<00:46, 28.13it/s]Training:  35%|███▌      | 564/1600 [00:20<00:37, 27.77it/s]Training:  53%|█████▎    | 848/1600 [00:30<00:26, 28.03it/s]Training:  71%|███████   | 1132/1600 [00:40<00:16, 28.11it/s]Training:  88%|████████▊ | 1415/1600 [00:50<00:06, 27.98it/s]                         Training loss: 51.9827, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 504.6237, Validation accuracy: 0.6050
Macro F1-score: 0.5440
Model performance on Angry speech (in validation): 
	Precision: 0.8448, Recall: 0.9800, F1_score: 0.9074
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1200, F1_score: 0.2143
Model performance on Neutral speech (in validation): 
	Precision: 0.7273, Recall: 0.3200, F1_score: 0.4444
Model performance on Sad speech (in validation): 
	Precision: 0.4386, Recall: 1.0000, F1_score: 0.6098
Epoch 9/100

Training Phase:
Training loss: 40.1061, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 533.8600, Validation accuracy: 0.6800
Macro F1-score: 0.5924
Model performance on Angry speech (in validation): 
	Precision: 0.8305, Recall: 0.9800, F1_score: 0.8991
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.5217, Recall: 0.7200, F1_score: 0.6050
Model performance on Sad speech (in validation): 
	Precision: 0.7042, Recall: 1.0000, F1_score: 0.8264
Epoch 10/100

Training Phase:
                                    Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 281/1600 [00:10<00:47, 28.06it/s]Training:  35%|███▌      | 562/1600 [00:20<00:38, 27.22it/s]Training:  53%|█████▎    | 847/1600 [00:30<00:27, 27.76it/s]Training:  71%|███████   | 1132/1600 [00:40<00:16, 27.84it/s]Training:  88%|████████▊ | 1415/1600 [00:50<00:06, 28.00it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 276/1600 [00:10<00:48, 27.53it/s]Training:  35%|███▌      | 561/1600 [00:20<00:36, 28.09it/s]Training:  53%|█████▎    | 846/1600 [00:30<00:27, 27.89it/s]Training:  71%|███████   | Training loss: 41.3404, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925

Eval Phase: 
1133/1600 [00:40<00:16, 28.19it/s]Training:  89%|████████▉ | 1420/1600 [00:50<00:06, 28.02it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 637.0944, Validation accuracy: 0.6850
Macro F1-score: 0.6021
Model performance on Angry speech (in validation): 
	Precision: 0.8519, Recall: 0.9200, F1_score: 0.8846
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4896, Recall: 0.9400, F1_score: 0.6438
Model performance on Sad speech (in validation): 
	Precision: 0.8800, Recall: 0.8800, F1_score: 0.8800
Epoch 11/100

Training Phase:
Training loss: 37.1351, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 280/1600 [00:10<00:47, 27.95it/s]Training:  35%|███▌      | 563/1600 [00:20<00:36, 28.11it/s]Training:  53%|█████▎    | 846/1600 [00:30<00:26, 27.94it/s]Training:  70%|███████   | 1127/1600 [00:40<00:16, 27.99it/s]Training:  88%|████████▊ | 1410/1600 [00:50<00:06, 28.10it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 522.8569, Validation accuracy: 0.6750
Macro F1-score: 0.5892
Model performance on Angry speech (in validation): 
	Precision: 0.8033, Recall: 0.9800, F1_score: 0.8829
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.9400, F1_score: 0.6528
Model performance on Sad speech (in validation): 
	Precision: 0.8667, Recall: 0.7800, F1_score: 0.8211
Epoch 12/100

Training Phase:
Training loss: 29.5883, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 286/1600 [00:10<00:46, 28.54it/s]Training:  36%|███▌      | 572/1600 [00:20<00:36, 27.84it/s]Training:  53%|█████▎    | 852/1600 [00:30<00:26, 27.87it/s]Training:  71%|███████   | 1132/1600 [00:40<00:16, 27.86it/s]Training:  88%|████████▊ | 1411/1600 [00:50<00:06, 27.83it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 813.9596, Validation accuracy: 0.6050
Macro F1-score: 0.5173
Model performance on Angry speech (in validation): 
	Precision: 0.8727, Recall: 0.9600, F1_score: 0.9143
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5610, Recall: 0.4600, F1_score: 0.5055
Model performance on Sad speech (in validation): 
	Precision: 0.4808, Recall: 1.0000, F1_score: 0.6494
Epoch 13/100

Training Phase:
Training loss: 61.0205, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888

Eval Phase: 
Validation loss: 473.0573, Validation accuracy: 0.6250
Macro F1-score: 0.5531
Model performance on Angry speech (in validation): 
	Precision: 0.9189, Recall: 0.6800, F1_score: 0.7816
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.4505, Recall: 0.8200, F1_score: 0.5816
Model performance on Sad speech (in validation): 
	Precision: 0.6901, Recall: 0.9800, F1_score: 0.8099
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7050

Test Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 273/1600 [00:10<00:48, 27.26it/s]Training:  34%|███▍      | 547/1600 [00:20<00:38, 27.34it/s]Training:  51%|█████▏    | 822/1600 [00:30<00:28, 27.38it/s]Training:  69%|██████▉   | 1103/1600 [00:40<00:17, 27.66it/s]Training:  86%|████████▋ | 1384/1600 [00:50<00:07, 27.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|▌         | 10/200 [00:00<00:01, 98.75it/s]Testing:  10%|█         | 20/200 [00:00<00:01, 99.03it/s]Testing:  16%|█▌        | 31/200 [00:00<00:01, 103.58it/s]Testing:  21%|██        | 42/200 [00:00<00:01, 103.10it/s]Testing:  26%|██▋       | 53/200 [00:00<00:01, 103.37it/s]Testing:  32%|███▏      | 64/200 [00:00<00:01, 104.58it/s]Testing:  38%|Test loss: 333.0327, Test accuracy: 0.7050
Macro F1-score: 0.6289
Model performance on Angry speech (in test): 
	Precision: 0.9375, Recall: 0.9000, F1_score: 0.9184
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in test): 
	Precision: 0.5341, Recall: 0.9400, F1_score: 0.6812
Model performance on Sad speech (in test): 
	Precision: 0.7581, Recall: 0.9400, F1_score: 0.8393

======================= This is fold_2 on cn =======================

Load dataset: 
Loading cn train data: fold_2...
Preprocess cn fold_2 data for cn model
███▊      | 75/200 [00:00<00:01, 104.26it/s]Testing:  43%|████▎     | 86/200 [00:00<00:01, 104.41it/s]Testing:  48%|████▊     | 97/200 [00:00<00:00, 105.16it/s]Testing:  54%|█████▍    | 108/200 [00:01<00:00, 103.61it/s]Testing:  60%|█████▉    | 119/200 [00:01<00:00, 102.55it/s]Testing:  65%|██████▌   | 130/200 [00:01<00:00, 102.14it/s]Testing:  70%|███████   | 141/200 [00:01<00:00, 101.54it/s]Testing:  76%|███████▌  | 152/200 [00:01<00:00, 101.44it/s]Testing:  82%|████████▏ | 163/200 [00:01<00:00, 103.13it/s]Testing:  87%|████████▋ | 174/200 [00:01<00:00, 102.82it/s]Testing:  92%|█████████▎| 185/200 [00:01<00:00, 102.07it/s]Testing:  98%|█████████▊| 197/200 [00:01<00:00, 105.08it/s]                                                           Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 4/1600 [00:00<00:44, 35.70 examples/s]Map:   1%|          | 13/1600 [00:00<00:28, 55.85 examples/s]Map:   2%|▏         | 34/1600 [00:00<00:14, 111.31 examples/s]Map:   3%|▎         | 47/1600 [00:00<00:13, 115.58 examples/s]Map:   4%|▍         | 63/1600 [00:00<00:12, 124.66 examples/s]Map:   5%|▍         | 76/1600 [00:00<00:12, 122.63 examples/s]Map:   6%|▌         | 90/1600 [00:00<00:12, 124.81 examples/s]Map:   6%|▋         | 104/1600 [00:00<00:11, 125.98 examples/s]Map:   8%|▊         | 121/1600 [00:01<00:10, 134.94 examples/s]Map:   9%|▊         | 139/1600 [00:01<00:12, 119.55 examples/s]Map:  10%|▉         | 156/1600 [00:01<00:11, 130.51 examples/s]Map:  11%|█         | 176/1600 [00:01<00:09, 146.91 examples/s]Map:  12%|█▏        | 196/1600 [00:01<00:08, 157.85 examples/s]Map:  14%|█▎        | 217/1600 [00:01<00:08, 168.64 examples/s]Map:  15%|█▌        | 240/1600 [00:01<00:08, 160.01 examples/s]Map:  16%|█▋        | 262/1600 [00:01<00:08, 151.61 examples/s]Map:  18%|█▊        | 283/1600 [00:02<00:09, 141.04 examples/s]Map:  19%|█▉        | 302/1600 [00:02<00:09, 133.58 examples/s]Map:  20%|█▉        | 317/1600 [00:02<00:09, 135.35 examples/s]Map:  21%|██        | 333/1600 [00:02<00:10, 120.44 examples/s]Map:  22%|██▏       | 347/1600 [00:02<00:10, 122.94 examples/s]Map:  23%|██▎       | 367/1600 [00:02<00:08, 139.97 examples/s]Map:  24%|██▍       | 390/1600 [00:02<00:07, 158.61 examples/s]Map:  26%|██▌       | 408/1600 [00:02<00:07, 160.57 examples/s]Map:  27%|██▋       | 429/1600 [00:03<00:08, 137.69 examples/s]Map:  28%|██▊       | 445/1600 [00:03<00:08, 140.64 examples/s]Map:  29%|██▉       | 463/1600 [00:03<00:08, 129.49 examples/s]Map:  30%|███       | 481/1600 [00:03<00:09, 120.06 examples/s]Map:  31%|███       | 499/1600 [00:03<00:09, 112.86 examples/s]Map:  32%|███▏      | 514/1600 [00:03<00:09, 117.65 examples/s]Map:  33%|███▎      | 532/1600 [00:04<00:10, 105.02 examples/s]Map:  34%|███▍      | 546/1600 [00:04<00:09, 109.78 examples/s]Map:  35%|███▌      | 560/1600 [00:04<00:09, 113.85 examples/s]Map:  36%|███▌      | 573/1600 [00:04<00:08, 114.72 examples/s]Map:  37%|███▋      | 586/1600 [00:04<00:08, 115.66 examples/s]Map:  38%|███▊      | 600/1600 [00:04<00:08, 120.14 examples/s]Map:  38%|███▊      | 616/1600 [00:04<00:08, 115.60 examples/s]Map:  40%|███▉      | 635/1600 [00:04<00:07, 131.35 examples/s]Map:  41%|████      | 652/1600 [00:05<00:06, 137.09 examples/s]Map:  42%|████▏     | 668/1600 [00:05<00:06, 138.27 examples/s]Map:  43%|████▎     | 689/1600 [00:05<00:06, 135.35 examples/s]Map:  44%|████▍     | 703/1600 [00:05<00:06, 134.97 examples/s]Map:  45%|████▌     | 720/1600 [00:05<00:06, 128.52 examples/s]Map:  46%|████▌     | 737/1600 [00:05<00:06, 136.32 examples/s]Map:  47%|████▋     | 756/1600 [00:05<00:05, 147.15 examples/s]Map:  48%|████▊     | 776/1600 [00:05<00:05, 158.79 examples/s]Map:  50%|████▉     | 796/1600 [00:06<00:04, 167.59 examples/s]Map:  51%|█████     | 817/1600 [00:06<00:05, 155.13 examples/s]Map:  52%|█████▏    | 838/1600 [00:06<00:04, 164.65 examples/s]Map:  54%|█████▎    | 856/1600 [00:06<00:04, 165.67 examples/s]Map:  55%|█████▌    | 880/1600 [00:06<00:04, 157.32 examples/s]Map:  56%|█████▋    | 903/1600 [00:06<00:04, 152.32 examples/s]Map:  58%|█████▊    | 922/1600 [00:06<00:04, 140.55 examples/s]Map:  59%|█████▊    | 938/1600 [00:06<00:04, 143.43 examples/s]Map:  60%|█████▉    | 956/1600 [00:07<00:04, 150.99 examples/s]Map:  61%|██████    | 975/1600 [00:07<00:03, 160.06 examples/s]Map:  62%|██████▏   | 997/1600 [00:07<00:03, 173.24 examples/s]Map:  62%|██████▏   | 997/1600 [00:20<00:03, 173.24 examples/s]Map:  62%|██████▎   | 1000/1600 [00:48<08:30,  1.18 examples/s]Map:  64%|██████▍   | 1023/1600 [00:49<04:55,  1.95 examples/s]Map:  65%|██████▌   | 1047/1600 [00:49<02:58,  3.10 examples/s]Map:  67%|██████▋   | 1074/1600 [00:49<01:47,  4.91 examples/s]Map:  69%|██████▊   | 1097/1600 [00:49<01:11,  7.04 examples/s]Map:  70%|██████▉   | 1116/1600 [00:49<00:50,  9.50 examples/s]Map:  71%|███████   | 1136/1600 [00:49<00:35, 13.10 examples/s]Map:  72%|███████▏  | 1156/1600 [00:49<00:25, 17.68 examples/s]Map:  74%|███████▎  | 1178/1600 [00:49<00:16, 24.84 examples/s]Map:  75%|███████▌  | 1200/1600 [00:50<00:11, 34.14 examples/s]Map:  76%|███████▌  | 1219/1600 [00:50<00:08, 43.85 examples/s]Map:  78%|███████▊  | 1245/1600 [00:50<00:06, 59.04 examples/s]Map:  79%|███████▉  | 1268/1600 [00:50<00:04, 67.31 examples/s]Map:  80%|████████  | 1287/1600 [00:50<00:04, 76.40 examples/s]Map:  82%|████████▏ | 1307/1600 [00:50<00:03, 86.47 examples/s]Map:  83%|████████▎ | 1325/1600 [00:51<00:02, 99.25 examples/s]Map:  84%|████████▍ | 1343/1600 [00:51<00:02, 111.94 examples/s]Map:  85%|████████▌ | 1360/1600 [00:51<00:01, 122.81 examples/s]Map:  86%|████████▋ | 1381/1600 [00:51<00:01, 121.06 examples/s]Map:  87%|████████▋ | 1399/1600 [00:51<00:01, 132.44 examples/s]Map:  88%|████████▊ | 1415/1600 [00:51<00:01, 136.24 examples/s]Map:  89%|████████▉ | 1431/1600 [00:51<00:01, 138.19 examples/s]Map:  90%|█████████ | 1447/1600 [00:51<00:01, 137.19 examples/s]Map:  92%|█████████▏| 1466/1600 [00:52<00:01, 127.86 examples/s]Map:  93%|█████████▎| 1482/1600 [00:52<00:01, 104.59 examples/s]Map:  93%|█████████▎| 1494/1600 [00:52<00:01, 105.76 examples/s]Map:  94%|█████████▍| 1507/1600 [00:52<00:00, 107.80 examples/s]Map:  95%|█████████▌| 1520/1600 [00:52<00:00, 111.45 examples/s]Map:  96%|█████████▌| 1535/1600 [00:52<00:00, 117.56 examples/s]Map:  97%|█████████▋| 1549/1600 [00:52<00:00, 121.47 examples/s]Map:  98%|█████████▊| 1566/1600 [00:52<00:00, 133.22 examples/s]Map:  99%|█████████▉| 1585/1600 [00:52<00:00, 146.23 examples/s]Map: 100%|█████████▉| 1597/1600 [01:04<00:00, 146.23 examples/s]Map: 100%|██████████| 1600/1600 [01:17<00:00,  2.14 examples/s] Map: 100%|██████████| 1600/1600 [01:17<00:00, 20.64 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   4%|▍         | 8/200 [00:00<00:02, 65.83 examples/s]Map:  12%|█▏        | 23/200 [00:00<00:01, 101.69 examples/s]Map:  22%|██▏       | 43/200 [00:00<00:01, 138.73 examples/s]Map:  34%|███▎      | 67/200 [00:00<00:00, 146.14 examples/s]Map:  43%|████▎     | 86/200 [00:00<00:00, 158.57 examples/s]Map:  52%|█████▎    | 105/200 [00:00<00:00, 146.50 examples/s]Map:  62%|██████▏   | 123/200 [00:00<00:00, 150.38 examples/s]Map:  70%|██████▉   | 139/200 [00:00<00:00, 150.21 examples/s]Map:  78%|███████▊  | 155/200 [00:01<00:00, 150.85 examples/s]Map:  86%|████████▌ | 172/200 [00:01<00:00, 153.37 examples/s]Map:  94%|█████████▍| 189/200 [00:01<00:00, 155.48 examples/s]Map: 100%|██████████| 200/200 [00:08<00:00, 23.49 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▎         | 5/200 [00:00<00:05, 38.84 examples/s]Map:   6%|▌         | 12/200 [00:00<00:03, 52.77 examples/s]Map:  15%|█▌        | 30/200 [00:00<00:01, 104.54 examples/s]Map:  23%|██▎       | 46/200 [00:00<00:01, 113.20 examples/s]Map:  32%|███▎      | 65/200 [00:00<00:00, 136.00 examples/s]Map:  42%|████▏     | 84/200 [00:00<00:00, 148.97 examples/s]Map:  52%|█████▎    | 105/200 [00:00<00:00, 144.66 examples/s]Map:  62%|██████▏   | 124/200 [00:00<00:00, 154.75 examples/s]Map:  70%|███████   | 140/200 [00:01<00:00, 153.54 examples/s]Map:  82%|████████▏ | 164/200 [00:01<00:00, 153.46 examples/s]Map:  91%|█████████ | 182/200 [00:01<00:00, 158.47 examples/s]Map: 100%|██████████| 200/200 [00:08<00:00,  8.44 examples/s] Map: 100%|██████████| 200/200 [00:08<00:00, 23.96 examples/s]
Loading de eval data: fold_2...
Preprocess de fold_2 data for cn model
Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 3/1600 [00:00<01:36, 16.50 examples/s]Map:   1%|          | 13/1600 [00:00<00:33, 47.06 examples/s]Map:   1%|▏         | 23/1600 [00:00<00:24, 64.72 examples/s]Map:   2%|▏         | 34/1600 [00:00<00:20, 77.36 examples/s]Map:   3%|▎         | 44/1600 [00:00<00:18, 83.72 examples/s]Map:   3%|▎         | 54/1600 [00:00<00:18, 84.07 examples/s]Map:   4%|▍         | 65/1600 [00:00<00:19, 78.46 examples/s]Map:   5%|▍         | 74/1600 [00:01<00:19, 79.50 examples/s]Map:   5%|▌         | 83/1600 [00:01<00:18, 81.29 examples/s]Map:   6%|▌         | 93/1600 [00:01<00:17, 83.77 examples/s]Map:   6%|▋         | 102/1600 [00:01<00:18, 80.58 examples/s]Map:   7%|▋         | 111/1600 [00:01<00:18, 79.55 examples/s]Map:   8%|▊         | 120/1600 [00:01<00:18, 80.24 examples/s]Map:   8%|▊         | 132/1600 [00:01<00:18, 78.43 examples/s]Map:   9%|▉         | 143/1600 [00:01<00:19, 74.75 examples/s]Map:   9%|▉         | 151/1600 [00:02<00:19, 75.28 examples/s]Map:  10%|▉         | 159/1600 [00:02<00:19, 74.87 examples/s]Map:  10%|█         | 167/1600 [00:02<00:22, 62.57 examples/s]Map:  11%|█         | 176/1600 [00:02<00:21, 66.34 examples/s]Map:  12%|█▏        | 186/1600 [00:02<00:20, 70.61 examples/s]Map:  12%|█▏        | 194/1600 [00:02<00:19, 70.57 examples/s]Map:  13%|█▎        | 203/1600 [00:02<00:19, 73.00 examples/s]Map:  13%|█▎        | 212/1600 [00:02<00:18, 76.37 examples/s]Map:  14%|█▍        | 222/1600 [00:02<00:17, 79.77 examples/s]Map:  14%|█▍        | 231/1600 [00:03<00:17, 80.37 examples/s]Map:  15%|█▌        | 240/1600 [00:03<00:16, 81.21 examples/s]Map:  16%|█▌        | 249/1600 [00:03<00:16, 80.14 examples/s]Map:  16%|█▌        | 259/1600 [00:03<00:16, 82.13 examples/s]Map:  17%|█▋        | 272/1600 [00:03<00:16, 80.88 examples/s]Map:  18%|█▊        | 281/1600 [00:03<00:19, 66.98 examples/s]Map:  18%|█▊        | 291/1600 [00:03<00:18, 71.64 examples/s]Map:  19%|█▊        | 299/1600 [00:04<00:17, 72.46 examples/s]Map:  19%|█▉        | 308/1600 [00:04<00:17, 73.61 examples/s]Map:  20%|█▉        | 316/1600 [00:04<00:17, 74.33 examples/s]Map:  20%|██        | 326/1600 [00:04<00:16, 76.67 examples/s]Map:  21%|██        | 334/1600 [00:04<00:16, 76.74 examples/s]Map:  21%|██▏       | 342/1600 [00:04<00:16, 74.91 examples/s]Map:  22%|██▏       | 352/1600 [00:04<00:15, 78.88 examples/s]Map:  22%|██▎       | 360/1600 [00:04<00:16, 77.28 examples/s]Map:  23%|██▎       | 369/1600 [00:04<00:16, 76.66 examples/s]Map:  24%|██▎       | 378/1600 [00:05<00:15, 76.70 examples/s]Map:  24%|██▍       | 386/1600 [00:05<00:15, 77.48 examples/s]Map:  25%|██▍       | 398/1600 [00:05<00:17, 69.33 examples/s]Map:  26%|██▌       | 409/1600 [00:05<00:15, 74.74 examples/s]Map:  26%|██▋       | 420/1600 [00:05<00:14, 82.99 examples/s]Map:  27%|██▋       | 433/1600 [00:05<00:12, 91.42 examples/s]Map:  28%|██▊       | 445/1600 [00:05<00:12, 95.88 examples/s]Map:  29%|██▊       | 457/1600 [00:05<00:11, 98.53 examples/s]Map:  29%|██▉       | 470/1600 [00:06<00:10, 103.66 examples/s]Map:  30%|███       | 482/1600 [00:06<00:11, 100.72 examples/s]Map:  31%|███       | 495/1600 [00:06<00:12, 89.70 examples/s] Map:  32%|███▏      | 509/1600 [00:06<00:14, 74.25 examples/s]Map:  33%|███▎      | 522/1600 [00:06<00:14, 72.91 examples/s]Map:  33%|███▎      | 531/1600 [00:06<00:14, 72.08 examples/s]Map:  34%|███▎      | 539/1600 [00:07<00:14, 70.82 examples/s]Map:  34%|███▍      | 548/1600 [00:07<00:14, 73.51 examples/s]Map:  35%|███▍      | 556/1600 [00:07<00:14, 72.79 examples/s]Map:  35%|███▌      | 565/1600 [00:07<00:13, 74.04 examples/s]Map:  36%|███▌      | 573/1600 [00:07<00:14, 72.17 examples/s]Map:  36%|███▋      | 581/1600 [00:07<00:14, 69.97 examples/s]Map:  37%|███▋      | 589/1600 [00:07<00:14, 70.96 examples/s]Map:  38%|███▊      | 600/1600 [00:07<00:14, 68.84 examples/s]Map:  38%|███▊      | 608/1600 [00:07<00:13, 71.14 examples/s]Map:  38%|███▊      | 616/1600 [00:08<00:13, 71.61 examples/s]Map:  39%|███▉      | 626/1600 [00:08<00:16, 57.86 examples/s]Map:  40%|███▉      | 633/1600 [00:08<00:16, 60.05 examples/s]Map:  40%|████      | 641/1600 [00:08<00:15, 61.95 examples/s]Map:  41%|████      | 650/1600 [00:08<00:14, 66.47 examples/s]Map:  41%|████      | 659/1600 [00:08<00:13, 69.08 examples/s]Map:  42%|████▏     | 669/1600 [00:08<00:12, 72.08 examples/s]Map:  42%|████▏     | 677/1600 [00:09<00:12, 71.65 examples/s]Map:  43%|████▎     | 689/1600 [00:09<00:12, 70.15 examples/s]Map:  44%|████▎     | 697/1600 [00:09<00:12, 70.62 examples/s]Map:  44%|████▍     | 705/1600 [00:09<00:12, 71.62 examples/s]Map:  45%|████▍     | 713/1600 [00:09<00:12, 69.96 examples/s]Map:  45%|████▌     | 721/1600 [00:09<00:12, 71.07 examples/s]Map:  46%|████▌     | 729/1600 [00:09<00:14, 58.43 examples/s]Map:  46%|████▌     | 737/1600 [00:09<00:13, 62.14 examples/s]Map:  47%|████▋     | 745/1600 [00:10<00:13, 65.12 examples/s]Map:  47%|████▋     | 753/1600 [00:10<00:12, 68.35 examples/s]Map:  48%|████▊     | 762/1600 [00:10<00:11, 72.69 examples/s]Map:  48%|████▊     | 771/1600 [00:10<00:10, 75.54 examples/s]Map:  49%|████▉     | 780/1600 [00:10<00:10, 77.18 examples/s]Map:  49%|████▉     | 789/1600 [00:10<00:10, 77.31 examples/s]Map:  50%|█████     | 801/1600 [00:10<00:09, 80.41 examples/s]Map:  51%|█████     | 810/1600 [00:10<00:12, 64.90 examples/s]Map:  51%|█████     | 818/1600 [00:11<00:11, 66.15 examples/s]Map:  52%|█████▏    | 827/1600 [00:11<00:10, 70.34 examples/s]Map:  52%|█████▏    | 837/1600 [00:11<00:10, 76.13 examples/s]Map:  53%|█████▎    | 848/1600 [00:11<00:11, 64.13 examples/s]Map:  54%|█████▎    | 857/1600 [00:11<00:10, 68.28 examples/s]Map:  54%|█████▍    | 867/1600 [00:11<00:09, 74.05 examples/s]Map:  55%|█████▍    | 876/1600 [00:11<00:09, 76.40 examples/s]Map:  55%|█████▌    | 887/1600 [00:12<00:09, 71.76 examples/s]Map:  56%|█████▌    | 895/1600 [00:12<00:09, 71.97 examples/s]Map:  57%|█████▋    | 906/1600 [00:12<00:10, 68.43 examples/s]Map:  57%|█████▋    | 917/1600 [00:12<00:10, 64.82 examples/s]Map:  58%|█████▊    | 924/1600 [00:12<00:10, 64.41 examples/s]Map:  58%|█████▊    | 934/1600 [00:12<00:10, 62.17 examples/s]Map:  59%|█████▉    | 942/1600 [00:12<00:10, 63.74 examples/s]Map:  59%|█████▉    | 949/1600 [00:13<00:10, 60.83 examples/s]Map:  60%|█████▉    | 956/1600 [00:13<00:10, 61.25 examples/s]Map:  60%|██████    | 965/1600 [00:13<00:13, 47.54 examples/s]Map:  61%|██████    | 972/1600 [00:13<00:12, 50.49 examples/s]Map:  61%|██████▏   | 980/1600 [00:13<00:11, 55.02 examples/s]Map:  62%|██████▏   | 987/1600 [00:13<00:10, 56.65 examples/s]Map:  62%|██████▏   | 994/1600 [00:13<00:10, 58.88 examples/s]Map:  62%|██████▏   | 997/1600 [00:25<00:10, 58.88 examples/s]Map:  62%|██████▎   | 1000/1600 [01:14<26:07,  2.61s/ examples]Map:  63%|██████▎   | 1008/1600 [01:14<17:22,  1.76s/ examples]Map:  64%|██████▎   | 1016/1600 [01:14<11:42,  1.20s/ examples]Map:  64%|██████▍   | 1023/1600 [01:15<08:18,  1.16 examples/s]Map:  64%|██████▍   | 1031/1600 [01:15<05:38,  1.68 examples/s]Map:  65%|██████▌   | 1041/1600 [01:15<03:35,  2.60 examples/s]Map:  66%|██████▌   | 1048/1600 [01:15<02:38,  3.48 examples/s]Map:  66%|██████▌   | 1055/1600 [01:15<01:55,  4.70 examples/s]Map:  66%|██████▋   | 1063/1600 [01:15<01:21,  6.60 examples/s]Map:  67%|██████▋   | 1072/1600 [01:15<00:56,  9.33 examples/s]Map:  67%|██████▋   | 1079/1600 [01:15<00:42, 12.14 examples/s]Map:  68%|██████▊   | 1086/1600 [01:16<00:33, 15.46 examples/s]Map:  68%|██████▊   | 1093/1600 [01:16<00:26, 19.47 examples/s]Map:  69%|██████▉   | 1100/1600 [01:16<00:20, 24.45 examples/s]Map:  69%|██████▉   | 1108/1600 [01:16<00:15, 31.17 examples/s]Map:  70%|██████▉   | 1116/1600 [01:16<00:12, 37.62 examples/s]Map:  70%|███████   | 1123/1600 [01:16<00:13, 35.56 examples/s]Map:  71%|███████   | 1131/1600 [01:16<00:11, 42.15 examples/s]Map:  71%|███████   | 1138/1600 [01:16<00:09, 46.29 examples/s]Map:  72%|███████▏  | 1147/1600 [01:17<00:08, 53.52 examples/s]Map:  72%|███████▏  | 1155/1600 [01:17<00:07, 57.64 examples/s]Map:  73%|███████▎  | 1163/1600 [01:17<00:07, 62.04 examples/s]Map:  73%|███████▎  | 1171/1600 [01:17<00:06, 63.69 examples/s]Map:  74%|███████▎  | 1178/1600 [01:17<00:06, 62.56 examples/s]Map:  74%|███████▍  | 1186/1600 [01:17<00:06, 64.62 examples/s]Map:  75%|███████▍  | 1195/1600 [01:17<00:06, 66.59 examples/s]Map:  75%|███████▌  | 1204/1600 [01:17<00:05, 69.66 examples/s]Map:  76%|███████▌  | 1212/1600 [01:18<00:05, 70.15 examples/s]Map:  76%|███████▋  | 1220/1600 [01:18<00:05, 69.22 examples/s]Map:  77%|███████▋  | 1228/1600 [01:18<00:05, 66.58 examples/s]Map:  77%|███████▋  | 1237/1600 [01:18<00:07, 48.70 examples/s]Map:  78%|███████▊  | 1244/1600 [01:18<00:06, 51.42 examples/s]Map:  78%|███████▊  | 1253/1600 [01:18<00:06, 56.96 examples/s]Map:  79%|███████▉  | 1260/1600 [01:18<00:05, 57.62 examples/s]Map:  79%|███████▉  | 1268/1600 [01:19<00:05, 61.04 examples/s]Map:  80%|███████▉  | 1276/1600 [01:19<00:05, 64.26 examples/s]Map:  80%|████████  | 1284/1600 [01:19<00:04, 65.97 examples/s]Map:  81%|████████  | 1293/1600 [01:19<00:04, 62.57 examples/s]Map:  81%|████████▏ | 1301/1600 [01:19<00:04, 61.64 examples/s]Map:  82%|████████▏ | 1308/1600 [01:19<00:04, 62.27 examples/s]Map:  82%|████████▏ | 1316/1600 [01:19<00:04, 63.92 examples/s]Map:  83%|████████▎ | 1324/1600 [01:19<00:04, 64.96 examples/s]Map:  83%|████████▎ | 1331/1600 [01:20<00:04, 63.73 examples/s]Map:  84%|████████▍ | 1341/1600 [01:20<00:04, 60.29 examples/s]Map:  84%|████████▍ | 1348/1600 [01:20<00:05, 48.39 examples/s]Map:  85%|████████▍ | 1356/1600 [01:20<00:04, 52.44 examples/s]Map:  85%|████████▌ | 1363/1600 [01:20<00:04, 53.80 examples/s]Map:  86%|████████▌ | 1371/1600 [01:20<00:03, 57.33 examples/s]Map:  86%|████████▌ | 1378/1600 [01:20<00:03, 57.87 examples/s]Map:  87%|████████▋ | 1386/1600 [01:21<00:03, 57.57 examples/s]Map:  87%|████████▋ | 1394/1600 [01:21<00:03, 60.01 examples/s]Map:  88%|████████▊ | 1401/1600 [01:21<00:03, 59.00 examples/s]Map:  88%|████████▊ | 1407/1600 [01:21<00:03, 57.65 examples/s]Map:  88%|████████▊ | 1415/1600 [01:21<00:03, 59.53 examples/s]Map:  89%|████████▉ | 1423/1600 [01:21<00:02, 62.42 examples/s]Map:  90%|████████▉ | 1433/1600 [01:21<00:02, 58.77 examples/s]Map:  90%|█████████ | 1441/1600 [01:21<00:02, 62.39 examples/s]Map:  90%|█████████ | 1448/1600 [01:22<00:02, 62.94 examples/s]Map:  91%|█████████ | 1458/1600 [01:22<00:02, 62.14 examples/s]Map:  92%|█████████▏| 1466/1600 [01:22<00:02, 49.88 examples/s]Map:  92%|█████████▏| 1473/1600 [01:22<00:02, 52.87 examples/s]Map:  93%|█████████▎| 1481/1600 [01:22<00:02, 56.16 examples/s]Map:  93%|█████████▎| 1489/1600 [01:22<00:01, 58.47 examples/s]Map:  94%|█████████▎| 1496/1600 [01:22<00:01, 59.37 examples/s]Map:  94%|█████████▍| 1503/1600 [01:23<00:01, 58.39 examples/s]Map:  94%|█████████▍| 1511/1600 [01:23<00:01, 61.01 examples/s]Map:  95%|█████████▍| 1519/1600 [01:23<00:01, 62.57 examples/s]Map:  95%|█████████▌| 1527/1600 [01:23<00:01, 63.30 examples/s]Map:  96%|█████████▌| 1535/1600 [01:23<00:01, 64.41 examples/s]Map:  96%|█████████▋| 1543/1600 [01:23<00:00, 64.34 examples/s]Map:  97%|█████████▋| 1551/1600 [01:23<00:00, 64.21 examples/s]Map:  97%|█████████▋| 1558/1600 [01:23<00:00, 61.77 examples/s]Map:  98%|█████████▊| 1565/1600 [01:24<00:00, 61.20 examples/s]Map:  98%|█████████▊| 1572/1600 [01:24<00:00, 62.45 examples/s]Map:  99%|█████████▉| 1580/1600 [01:24<00:00, 49.33 examples/s]Map:  99%|█████████▉| 1588/1600 [01:24<00:00, 53.54 examples/s]Map: 100%|█████████▉| 1595/1600 [01:24<00:00, 56.44 examples/s]Map: 100%|█████████▉| 1598/1600 [01:36<00:00, 56.44 examples/s]Map: 100%|██████████| 1600/1600 [02:09<00:00,  2.02s/ examples]Map: 100%|██████████| 1600/1600 [02:09<00:00, 12.36 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   1%|          | 2/200 [00:00<00:12, 16.14 examples/s]Map:   2%|▎         | 5/200 [00:00<00:08, 21.73 examples/s]Map:   8%|▊         | 16/200 [00:00<00:03, 48.08 examples/s]Map:  12%|█▏        | 24/200 [00:00<00:03, 57.68 examples/s]Map:  17%|█▋        | 34/200 [00:00<00:02, 69.40 examples/s]Map:  22%|██▏       | 43/200 [00:00<00:02, 72.35 examples/s]Map:  26%|██▋       | 53/200 [00:00<00:01, 75.18 examples/s]Map:  32%|███▎      | 65/200 [00:00<00:01, 84.09 examples/s]Map:  37%|███▋      | 74/200 [00:01<00:01, 80.96 examples/s]Map:  42%|████▏     | 83/200 [00:01<00:01, 80.41 examples/s]Map:  46%|████▌     | 92/200 [00:01<00:01, 68.41 examples/s]Map:  50%|█████     | 100/200 [00:01<00:01, 68.67 examples/s]Map:  54%|█████▍    | 108/200 [00:01<00:01, 71.14 examples/s]Map:  58%|█████▊    | 116/200 [00:01<00:01, 69.64 examples/s]Map:  62%|██████▎   | 125/200 [00:01<00:01, 70.52 examples/s]Map:  66%|██████▋   | 133/200 [00:01<00:00, 68.74 examples/s]Map:  70%|███████   | 141/200 [00:02<00:00, 67.28 examples/s]Map:  74%|███████▍  | 148/200 [00:02<00:00, 66.65 examples/s]Map:  78%|███████▊  | 155/200 [00:02<00:00, 64.51 examples/s]Map:  82%|████████▏ | 163/200 [00:02<00:00, 62.77 examples/s]Map:  86%|████████▌ | 171/200 [00:02<00:00, 62.58 examples/s]Map:  90%|█████████ | 180/200 [00:02<00:00, 64.90 examples/s]Map:  94%|█████████▍| 188/200 [00:02<00:00, 65.71 examples/s]Map:  98%|█████████▊| 195/200 [00:02<00:00, 65.04 examples/s]Map: 100%|██████████| 200/200 [00:16<00:00, 12.27 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   1%|          | 2/200 [00:00<00:11, 16.55 examples/s]Map:   2%|▎         | 5/200 [00:00<00:08, 21.74 examples/s]Map:   6%|▋         | 13/200 [00:00<00:04, 42.59 examples/s]Map:  11%|█         | 22/200 [00:00<00:03, 55.38 examples/s]Map:  16%|█▌        | 31/200 [00:00<00:02, 64.34 examples/s]Map:  20%|██        | 41/200 [00:00<00:02, 72.17 examples/s]Map:  26%|██▌       | 52/200 [00:00<00:01, 79.13 examples/s]Map:  32%|███▏      | 63/200 [00:00<00:01, 84.84 examples/s]Map:  37%|███▋      | 74/200 [00:01<00:01, 74.91 examples/s]Map:  42%|████▏     | 84/200 [00:01<00:01, 69.21 examples/s]Map:  46%|████▌     | 92/200 [00:01<00:01, 60.55 examples/s]Map:  50%|█████     | 101/200 [00:01<00:01, 65.57 examples/s]Map:  56%|█████▌    | 111/200 [00:01<00:01, 71.30 examples/s]Map:  60%|█████▉    | 119/200 [00:01<00:01, 70.75 examples/s]Map:  64%|██████▎   | 127/200 [00:01<00:01, 70.61 examples/s]Map:  68%|██████▊   | 135/200 [00:02<00:00, 69.88 examples/s]Map:  72%|███████▏  | 143/200 [00:02<00:00, 69.96 examples/s]Map:  76%|███████▌  | 151/200 [00:02<00:00, 71.36 examples/s]Map:  80%|████████  | 160/200 [00:02<00:00, 72.56 examples/s]Map:  84%|████████▍ | 168/200 [00:02<00:00, 71.64 examples/s]Map:  88%|████████▊ | 176/200 [00:02<00:00, 69.11 examples/s]Map:  92%|█████████▏| 183/200 [00:02<00:00, 67.01 examples/s]Map:  96%|█████████▌| 191/200 [00:02<00:00, 67.19 examples/s]Map:  99%|█████████▉| 198/200 [00:02<00:00, 67.02 examples/s]Map:  99%|█████████▉| 198/200 [00:14<00:00, 67.02 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00,  1.58 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 13.15 examples/s]
Loading de test data: fold_2...
Preprocess de fold_2 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 199.1468, Training accuracy: 0.9656
Macro F1-score: 0.9656
Model performance on Angry speech (in training): 
	Precision: 0.9653, Recall: 0.9725, F1_score: 0.9689
Model performance on Happy speech (in training): 
	Precision: 0.9593, Recall: 0.9425, F1_score: 0.9508
Model performance on Neutral speech (in training): 
	Precision: 0.9629, Recall: 0.9725, F1_score: 0.9677
Model performance on Sad speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 277/1600 [00:10<00:47, 27.70it/s]Training:  35%|███▍      | 554/1600 [00:20<00:37, 27.67it/s]Training:  52%|█████▏    | 831/1600 [00:30<00:27, 27.67it/s]Training:  69%|██████▉   | 1108/1600 [00:40<00:17, 27.38it/s]Training:  86%|████████▌ | 1378/1600 [00:50<00:08, 27.24it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 375.7090, Validation accuracy: 0.6750
Macro F1-score: 0.6024
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4466, Recall: 0.9200, F1_score: 0.6013
Model performance on Sad speech (in validation): 
	Precision: 0.8545, Recall: 0.9400, F1_score: 0.8952
New best accuracy for layer 3 on epoch 1: 0.6750. Model saved.
Epoch 2/100

Training Phase:
Training loss: 77.2700, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9924, Recall: 0.9825, F1_score: 0.9874
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 520.9830, Validation accuracy: 0.6000
Macro F1-score: 0.5381
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.5000, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3902, Recall: 0.9600, F1_score: 0.5549
Model performance on Sad speech (in validation): 
	Precision: 0.9216, Recall: 0.9400, F1_score: 0.9307
Epoch 3/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 272/1600 [00:10<00:48, 27.18it/s]Training:  35%|███▍      | 554/1600 [00:20<00:37, 27.73it/s]Training:  52%|█████▏    | 836/1600 [00:30<00:28, 27.21it/s]Training:  70%|██████▉   | 1119/1600 [00:40<00:17, 27.63it/s]Training:  88%|████████▊ | 1402/1600 [00:50<00:07, 27.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 280/1600 [00:10<00:47, 27.91it/s]Training:  35%|███▌      | 560/1600 [00:20<00:37, 27.71it/s]Training:  52%|█████▏    | 836/1600 [00:30<00:28, 27.21it/s]Training:  70%|██████▉   | 1117/1600 [00:40<00:17, 27.54it/s]Training:  87%|████████▋ | 1399/1600 [00:50<00:07, 27.74it/s]                           Training loss: 53.9675, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925

Eval Phase: 
Validation loss: 501.8300, Validation accuracy: 0.6650
Macro F1-score: 0.6006
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7600, F1_score: 0.8636
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.4356, Recall: 0.8800, F1_score: 0.5828
Model performance on Sad speech (in validation): 
	Precision: 0.8475, Recall: 1.0000, F1_score: 0.9174
Epoch 4/100

Training Phase:
Training loss: 58.5562, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9825, F1_score: 0.9825
Model performance on Neutral speech (in training): 
	Precision: 0.9802, Recall: 0.9900, F1_score: 0.9851
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925

Eval Phase: 
Validation loss: 506.7976, Validation accuracy: 0.6000
Macro F1-score: 0.5357
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6000, F1_score: 0.7500
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3868, Recall: 0.8200, F1_score: 0.5256
Model performance on Sad speech (in validation): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Epoch 5/100

Training Phase:
                                  Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 285/1600 [00:10<00:46, 28.49it/s]Training:  36%|███▌      | 570/1600 [00:20<00:36, 28.31it/s]Training:  53%|█████▎    | 852/1600 [00:30<00:26, 27.83it/s]Training:  71%|███████   | 1138/1600 [00:40<00:16, 28.09it/s]Training:  89%|████████▉ | 1424/1600 [00:50<00:06, 28.25it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 277/1600 [00:10<00:47, 27.68it/s]Training:  35%|███▌      | 564/1600 [00:20<00:36, 28.27it/s]Training:  53%|█████▎    | 851/1600 [00:30<00:26, 28.29it/s]Training:  71%|███████   | 11Training loss: 47.4143, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 522.0774, Validation accuracy: 0.6250
Macro F1-score: 0.5759
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6600, F1_score: 0.7952
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.4000, Recall: 0.9600, F1_score: 0.5647
Model performance on Sad speech (in validation): 
	Precision: 0.9556, Recall: 0.8600, F1_score: 0.9053
Epoch 6/100

Training Phase:
Training loss: 35.4295, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 523.6612, Validation accuracy: 0.6450
Macro F1-score: 0.5907
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6600, F1_score: 0.7952
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.4128, Recall: 0.9000, F1_score: 0.5660
Model performance on Sad speech (in validation): 
	Precision: 0.8750, Recall: 0.9800, F1_score: 0.9245
Epoch 7/100

Training Phase:
36/1600 [00:40<00:16, 28.34it/s]Training:  89%|████████▉ | 1421/1600 [00:50<00:06, 28.27it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 284/1600 [00:10<00:46, 28.33it/s]Training:  36%|███▌      | 568/1600 [00:20<00:37, 27.78it/s]Training:  36%|███▌      | 568/1600 [00:30<00:37, 27.78it/s]Training:  53%|█████▎    | 842/1600 [00:30<00:27, 27.16it/s]Training:  70%|███████   | 1120/1600 [00:40<00:17, 27.39it/s]Training:  88%|████████▊ | 1402/1600 [00:50<00:07, 27.65it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 281/1600 [Training loss: 33.0007, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 388.0474, Validation accuracy: 0.7000
Macro F1-score: 0.6450
Model performance on Angry speech (in validation): 
	Precision: 0.9778, Recall: 0.8800, F1_score: 0.9263
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.0600, F1_score: 0.1111
Model performance on Neutral speech (in validation): 
	Precision: 0.4653, Recall: 0.9400, F1_score: 0.6225
Model performance on Sad speech (in validation): 
	Precision: 0.9200, Recall: 0.9200, F1_score: 0.9200
New best accuracy for layer 3 on epoch 7: 0.7000. Model saved.
Epoch 8/100

Training Phase:
Training loss: 32.1924, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
00:10<00:47, 28.04it/s]Training:  35%|███▌      | 562/1600 [00:20<00:37, 27.88it/s]Training:  52%|█████▎    | 840/1600 [00:30<00:27, 27.55it/s]Training:  70%|██████▉   | 1113/1600 [00:40<00:17, 27.43it/s]Training:  87%|████████▋ | 1394/1600 [00:50<00:07, 27.66it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 278/1600 [00:10<00:47, 27.71it/s]Training:  35%|███▍      | 557/1600 [00:20<00:37, 27.82it/s]Training:  52%|█████▏    | 838/1600 [00:30<00:27, 27.92it/s]Training:  70%|██████▉   | 1119/1600 [00:40<00:17, 27.72it/s]Training:  87%|████████▋ | 1394/1600 [00:50<00:07, 27.49it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]     Validation loss: 372.9272, Validation accuracy: 0.6950
Macro F1-score: 0.6269
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.0400, F1_score: 0.0755
Model performance on Neutral speech (in validation): 
	Precision: 0.4889, Recall: 0.8800, F1_score: 0.6286
Model performance on Sad speech (in validation): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Epoch 9/100

Training Phase:
Training loss: 8.1873, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 484.7811, Validation accuracy: 0.7000
Macro F1-score: 0.6295
Model performance on Angry speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.4889, Recall: 0.8800, F1_score: 0.6286
Model performance on Sad speech (in validation): 
	Precision: 0.7903, Recall: 0.9800, F1_score: 0.8750
Epoch 10/100

Training Phase:
                                              Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 276/1600 [00:10<00:47, 27.58it/s]Training:  34%|███▍      | 552/1600 [00:20<00:38, 27.46it/s]Training:  52%|█████▏    | 827/1600 [00:30<00:28, 27.46it/s]Training:  69%|██████▉   | 1109/1600 [00:40<00:17, 27.73it/s]Training:  87%|████████▋ | 1394/1600 [00:50<00:07, 27.99it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 277/1600 [00:10<00:47, 27.67it/s]Training:  35%|███▍      | 554/1600 [00:20<00:37, 27.66it/s]Training:  52%|█████▏    | 831/1600 [00:30<00:27, 27.67it/s]Training:  69%|██████▉   | 1108/1600 [00:40<00:17, 27.45it/s]Training:  87%|████████▋ | 1385/1600 [00:5Training loss: 34.2232, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
0<00:07, 27.53it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 582.4638, Validation accuracy: 0.5250
Macro F1-score: 0.4592
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3373, Recall: 0.5600, F1_score: 0.4211
Model performance on Sad speech (in validation): 
	Precision: 0.5556, Recall: 1.0000, F1_score: 0.7143
Epoch 11/100

Training Phase:
Training loss: 18.6367, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 285/1600 [00:10<00:46, 28.44it/s]Training:  36%|███▌      | 570/1600 [00:20<00:36, 28.08it/s]Training:  53%|█████▎    | 849/1600 [00:30<00:26, 27.90it/s]Training:  70%|███████   | 1126/1600 [00:40<00:17, 27.77it/s]Training:  88%|████████▊ | 1404/1600 [00:50<00:07, 27.77it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 592.1943, Validation accuracy: 0.6600
Macro F1-score: 0.5963
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8000, F1_score: 0.8889
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4248, Recall: 0.9600, F1_score: 0.5890
Model performance on Sad speech (in validation): 
	Precision: 0.9362, Recall: 0.8800, F1_score: 0.9072
Epoch 12/100

Training Phase:
Training loss: 29.7978, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 286/1600 [00:10<00:46, 28.54it/s]Training:  36%|███▌      | 572/1600 [00:20<00:36, 28.41it/s]Training:  54%|█████▎    | 856/1600 [00:30<00:26, 27.81it/s]Training:  71%|███████▏  | 1140/1600 [00:40<00:16, 28.02it/s]Training:  89%|████████▉ | 1424/1600 [00:50<00:06, 28.02it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 967.2577, Validation accuracy: 0.4450
Macro F1-score: 0.3494
Model performance on Angry speech (in validation): 
	Precision: 0.9737, Recall: 0.7400, F1_score: 0.8409
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.0400, F1_score: 0.0714
Model performance on Sad speech (in validation): 
	Precision: 0.3205, Recall: 1.0000, F1_score: 0.4854
Epoch 13/100

Training Phase:
Training loss: 7.2257, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 574.0277, Validation accuracy: 0.6600
Macro F1-score: 0.5824
Model performance on Angry speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.4789, Recall: 0.6800, F1_score: 0.5620
Model performance on Sad speech (in validation): 
	Precision: 0.6250, Recall: 1.0000, F1_score: 0.7692
Epoch 14/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 279/1600 [00:10<00:47, 27.86it/s]Training:  35%|███▍      | 558/1600 [00:20<00:37, 27.68it/s]Training:  35%|███▍      | 558/1600 [00:30<00:37, 27.68it/s]Training:  52%|█████▏    | 831/1600 [00:30<00:28, 27.42it/s]Training:  69%|██████▉   | 1104/1600 [00:40<00:18, 27.38it/s]Training:  86%|████████▌ | 1378/1600 [00:50<00:08, 27.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 277/1600 [00:10<00:47, 27.69it/s]Training:  35%|███▍      | 559/1600 [00:20<00:37, 27.97it/s]Training:  35%|███▍      | 559/1600 [00:30<00:37, 27.97it/s]Training:  52%|█████▎    | 840/1600 [00:30<00:27, 27.99it/s]Training:  70%|███████   | 1121Training loss: 34.2247, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
/1600 [00:40<00:17, 27.66it/s]Training:  87%|████████▋ | 1393/1600 [00:50<00:07, 27.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 775.8758, Validation accuracy: 0.4650
Macro F1-score: 0.3993
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.4600, F1_score: 0.6301
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.2985, Recall: 0.4000, F1_score: 0.3419
Model performance on Sad speech (in validation): 
	Precision: 0.4545, Recall: 1.0000, F1_score: 0.6250
Epoch 15/100

Training Phase:
Training loss: 23.8529, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 280/1600 [00:10<00:47, 27.98it/s]Training:  35%|███▌      | 560/1600 [00:20<00:37, 27.94it/s]Training:  53%|█████▎    | 842/1600 [00:30<00:27, 28.03it/s]Training:  70%|███████   | 1124/1600 [00:40<00:17, 27.63it/s]Training:  87%|████████▋ | 1396/1600 [00:50<00:07, 27.45it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 597.7634, Validation accuracy: 0.6150
Macro F1-score: 0.5365
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4000, Recall: 0.5200, F1_score: 0.4522
Model performance on Sad speech (in validation): 
	Precision: 0.5682, Recall: 1.0000, F1_score: 0.7246
Epoch 16/100

Training Phase:
Training loss: 31.0433, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 276/1600 [00:10<00:48, 27.57it/s]Training:  35%|███▍      | 558/1600 [00:20<00:37, 27.88it/s]Training:  52%|█████▎    | 840/1600 [00:30<00:27, 27.51it/s]Training:  70%|███████   | 1122/1600 [00:40<00:17, 27.77it/s]Training:  88%|████████▊ | 1407/1600 [00:50<00:06, 28.01it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 611.1407, Validation accuracy: 0.5750
Macro F1-score: 0.5048
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3919, Recall: 0.5800, F1_score: 0.4677
Model performance on Sad speech (in validation): 
	Precision: 0.5556, Recall: 1.0000, F1_score: 0.7143
Epoch 17/100

Training Phase:
Training loss: 14.8315, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 267/1600 [00:10<00:50, 26.60it/s]Training:  34%|███▍      | 541/1600 [00:20<00:39, 27.03it/s]Training:  51%|█████     | 818/1600 [00:30<00:28, 27.32it/s]Training:  51%|█████     | 818/1600 [00:40<00:28, 27.32it/s]Training:  68%|██████▊   | 1094/1600 [00:40<00:18, 27.39it/s]Training:  86%|████████▌ | 1376/1600 [00:50<00:08, 27.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 605.1979, Validation accuracy: 0.7000
Macro F1-score: 0.6102
Model performance on Angry speech (in validation): 
	Precision: 0.9074, Recall: 0.9800, F1_score: 0.9423
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.8200, F1_score: 0.6212
Model performance on Sad speech (in validation): 
	Precision: 0.7812, Recall: 1.0000, F1_score: 0.8772
Epoch 18/100

Training Phase:
Training loss: 18.4592, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 571.6420, Validation accuracy: 0.7200
Macro F1-score: 0.6474
Model performance on Angry speech (in validation): 
	Precision: 0.9231, Recall: 0.9600, F1_score: 0.9412
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.4945, Recall: 0.9000, F1_score: 0.6383
Model performance on Sad speech (in validation): 
	Precision: 0.8909, Recall: 0.9800, F1_score: 0.9333
New best accuracy for layer 3 on epoch 18: 0.7200. Model saved.
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7200

Test Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 281/1600 [00:10<00:47, 28.03it/s]Training:  35%|███▌      | 562/1600 [00:20<00:37, 28.04it/s]Training:  53%|█████▎    | 843/1600 [00:30<00:27, 27.99it/s]Training:  70%|███████   | 1124/1600 [00:40<00:16, 28.03it/s]Training:  88%|████████▊ | 1406/1600 [00:50<00:06, 28.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|▌         | 10/200 [00:00<00:01, 98.04it/s]Testing:  10%|█         | 20/200 [00:00<00:01, 98.04it/s]Testing:  16%|█▌        | 31/200 [00:00<00:01, 100.01it/s]Testing:  21%|██        | 42/200 [00:00<00:01, 102.13it/s]Testing:  26%|██▋       | 53/200 [00:00<00:01, 103.18it/s]Testing:  32%|███▏      | 64/200 [00:00<00:01, 97.33it/s] Testing:  37%|███▋      | 74/200 [00:00<00:01, 95.69it/s]Testing:  42%|████▎     | 85/200 [00:00<00:01, 97.87it/s]Testing:  48%|████▊     | 96/200 [00:00<00:01, 98.77it/s]Testing:  54%|█████▎    | 107/200 [00:01<00:00, 100.17it/s]Testing:  59%|█████▉    | 118/200 [00:01<00:00, 101.58it/s]Testing:  64%|██████▍   | 129/200 [00:01<00:00, 101.36it/s]Testing:  70%|███████   | 140/200 [00:01<00:00, 102.72it/s]Testing:  76%|███████▌  | 151/200 [00:01<00:00, 103.98it/s]Testing:  81%|████████  | 162/200 [00:01<00:00, 104.90it/s]Testing:  86%|████████▋ | 173/200 [00:01<00:00, 104.91it/s]Testing:  92%|█████████▏| 184/200 [00:01<00:00, 106.09it/s]Testing:  98%|█████████▊| 195/200 [00:01<00:00, 101.73it/s]                                                           /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 523.1462, Test accuracy: 0.7300
Macro F1-score: 0.6391
Model performance on Angry speech (in test): 
	Precision: 0.9074, Recall: 0.9800, F1_score: 0.9423
Model performance on Happy speech (in test): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in test): 
	Precision: 0.5275, Recall: 0.9600, F1_score: 0.6809
Model performance on Sad speech (in test): 
	Precision: 0.8909, Recall: 0.9800, F1_score: 0.9333

======================= This is fold_3 on cn =======================

Load dataset: 
Loading cn train data: fold_3...
Preprocess cn fold_3 data for cn model
Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 6/1600 [00:00<00:31, 50.90 examples/s]Map:   1%|          | 13/1600 [00:00<00:27, 56.86 examples/s]Map:   2%|▏         | 34/1600 [00:00<00:13, 118.89 examples/s]Map:   3%|▎         | 48/1600 [00:00<00:12, 123.96 examples/s]Map:   4%|▍         | 64/1600 [00:00<00:11, 135.29 examples/s]Map:   5%|▍         | 78/1600 [00:00<00:11, 132.18 examples/s]Map:   6%|▌         | 95/1600 [00:00<00:13, 112.02 examples/s]Map:   7%|▋         | 109/1600 [00:00<00:12, 116.44 examples/s]Map:   8%|▊         | 126/1600 [00:01<00:11, 127.80 examples/s]Map:   9%|▉         | 142/1600 [00:01<00:10, 134.72 examples/s]Map:  10%|█         | 160/1600 [00:01<00:09, 144.31 examples/s]Map:  11%|█▏        | 180/1600 [00:01<00:08, 158.07 examples/s]Map:  12%|█▎        | 200/1600 [00:01<00:08, 166.40 examples/s]Map:  14%|█▍        | 226/1600 [00:01<00:08, 154.63 examples/s]Map:  15%|█▌        | 247/1600 [00:01<00:08, 166.67 examples/s]Map:  17%|█▋        | 265/1600 [00:01<00:09, 147.11 examples/s]Map:  18%|█▊        | 284/1600 [00:02<00:09, 139.28 examples/s]Map:  19%|█▉        | 303/1600 [00:02<00:09, 131.74 examples/s]Map:  20%|█▉        | 318/1600 [00:02<00:09, 134.28 examples/s]Map:  21%|██        | 335/1600 [00:02<00:10, 119.20 examples/s]Map:  22%|██▏       | 350/1600 [00:02<00:10, 123.07 examples/s]Map:  23%|██▎       | 371/1600 [00:02<00:08, 143.25 examples/s]Map:  25%|██▍       | 393/1600 [00:02<00:07, 161.11 examples/s]Map:  26%|██▌       | 414/1600 [00:02<00:06, 171.57 examples/s]Map:  28%|██▊       | 442/1600 [00:03<00:07, 162.31 examples/s]Map:  29%|██▊       | 459/1600 [00:03<00:07, 162.91 examples/s]Map:  30%|██▉       | 476/1600 [00:03<00:06, 162.26 examples/s]Map:  31%|███       | 499/1600 [00:03<00:07, 154.90 examples/s]Map:  32%|███▏      | 517/1600 [00:03<00:06, 160.04 examples/s]Map:  34%|███▎      | 538/1600 [00:03<00:07, 145.91 examples/s]Map:  35%|███▍      | 555/1600 [00:03<00:06, 151.46 examples/s]Map:  36%|███▌      | 574/1600 [00:04<00:06, 158.88 examples/s]Map:  37%|███▋      | 596/1600 [00:04<00:05, 172.11 examples/s]Map:  39%|███▉      | 621/1600 [00:04<00:05, 166.06 examples/s]Map:  40%|████      | 642/1600 [00:04<00:06, 145.43 examples/s]Map:  41%|████▏     | 661/1600 [00:04<00:06, 134.97 examples/s]Map:  42%|████▏     | 678/1600 [00:04<00:07, 123.83 examples/s]Map:  43%|████▎     | 695/1600 [00:04<00:07, 116.42 examples/s]Map:  44%|████▍     | 708/1600 [00:05<00:07, 117.69 examples/s]Map:  45%|████▌     | 722/1600 [00:05<00:07, 118.86 examples/s]Map:  46%|████▌     | 737/1600 [00:05<00:08, 104.44 examples/s]Map:  47%|████▋     | 751/1600 [00:05<00:07, 109.28 examples/s]Map:  48%|████▊     | 764/1600 [00:05<00:07, 112.15 examples/s]Map:  49%|████▊     | 778/1600 [00:05<00:07, 114.94 examples/s]Map:  49%|████▉     | 790/1600 [00:05<00:07, 114.61 examples/s]Map:  50%|█████     | 806/1600 [00:05<00:06, 124.31 examples/s]Map:  51%|█████▏    | 820/1600 [00:06<00:06, 116.21 examples/s]Map:  52%|█████▏    | 838/1600 [00:06<00:05, 130.24 examples/s]Map:  53%|█████▎    | 854/1600 [00:06<00:05, 134.99 examples/s]Map:  54%|█████▍    | 870/1600 [00:06<00:05, 138.51 examples/s]Map:  56%|█████▌    | 891/1600 [00:06<00:05, 134.41 examples/s]Map:  57%|█████▋    | 905/1600 [00:06<00:05, 134.80 examples/s]Map:  58%|█████▊    | 926/1600 [00:06<00:05, 128.35 examples/s]Map:  59%|█████▉    | 943/1600 [00:06<00:04, 135.34 examples/s]Map:  60%|██████    | 962/1600 [00:07<00:04, 147.00 examples/s]Map:  61%|██████▏   | 982/1600 [00:07<00:03, 158.00 examples/s]Map:  62%|██████▏   | 992/1600 [00:18<00:03, 158.00 examples/s]Map:  62%|██████▎   | 1000/1600 [00:47<06:34,  1.52 examples/s]Map:  64%|██████▍   | 1023/1600 [00:47<04:08,  2.33 examples/s]Map:  65%|██████▌   | 1046/1600 [00:47<02:39,  3.47 examples/s]Map:  67%|██████▋   | 1066/1600 [00:47<01:49,  4.86 examples/s]Map:  68%|██████▊   | 1085/1600 [00:47<01:16,  6.71 examples/s]Map:  69%|██████▉   | 1111/1600 [00:47<00:47, 10.21 examples/s]Map:  71%|███████   | 1132/1600 [00:47<00:33, 14.10 examples/s]Map:  72%|███████▏  | 1156/1600 [00:48<00:22, 19.72 examples/s]Map:  74%|███████▎  | 1178/1600 [00:48<00:15, 26.95 examples/s]Map:  75%|███████▌  | 1200/1600 [00:48<00:10, 36.39 examples/s]Map:  76%|███████▋  | 1222/1600 [00:48<00:08, 46.50 examples/s]Map:  78%|███████▊  | 1246/1600 [00:48<00:05, 59.32 examples/s]Map:  79%|███████▉  | 1266/1600 [00:48<00:05, 64.83 examples/s]Map:  80%|████████  | 1284/1600 [00:49<00:04, 72.24 examples/s]Map:  81%|████████▏ | 1303/1600 [00:49<00:03, 80.65 examples/s]Map:  83%|████████▎ | 1322/1600 [00:49<00:03, 88.74 examples/s]Map:  84%|████████▎ | 1337/1600 [00:49<00:02, 96.94 examples/s]Map:  85%|████████▍ | 1353/1600 [00:49<00:02, 106.47 examples/s]Map:  86%|████████▌ | 1371/1600 [00:49<00:02, 104.16 examples/s]Map:  87%|████████▋ | 1390/1600 [00:49<00:01, 119.84 examples/s]Map:  88%|████████▊ | 1408/1600 [00:50<00:01, 131.02 examples/s]Map:  89%|████████▉ | 1426/1600 [00:50<00:01, 139.86 examples/s]Map:  90%|█████████ | 1446/1600 [00:50<00:01, 152.51 examples/s]Map:  91%|█████████▏| 1463/1600 [00:50<00:00, 153.36 examples/s]Map:  93%|█████████▎| 1485/1600 [00:50<00:00, 136.82 examples/s]Map:  94%|█████████▍| 1501/1600 [00:50<00:00, 139.29 examples/s]Map:  95%|█████████▍| 1518/1600 [00:50<00:00, 145.89 examples/s]Map:  96%|█████████▌| 1535/1600 [00:50<00:00, 150.75 examples/s]Map:  97%|█████████▋| 1553/1600 [00:50<00:00, 156.01 examples/s]Map:  98%|█████████▊| 1570/1600 [00:51<00:00, 157.98 examples/s]Map:  99%|█████████▉| 1587/1600 [00:51<00:00, 88.76 examples/s] Map: 100%|█████████▉| 1597/1600 [01:01<00:00, 88.76 examples/s]Map: 100%|██████████| 1600/1600 [01:14<00:00,  2.25 examples/s]Map: 100%|██████████| 1600/1600 [01:14<00:00, 21.41 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   4%|▍         | 8/200 [00:00<00:02, 67.33 examples/s]Map:  12%|█▎        | 25/200 [00:00<00:01, 95.37 examples/s]Map:  22%|██▏       | 43/200 [00:00<00:01, 112.14 examples/s]Map:  31%|███       | 62/200 [00:00<00:01, 135.46 examples/s]Map:  42%|████▏     | 83/200 [00:00<00:00, 156.34 examples/s]Map:  52%|█████▏    | 103/200 [00:00<00:00, 166.77 examples/s]Map:  62%|██████▏   | 123/200 [00:00<00:00, 146.20 examples/s]Map:  71%|███████   | 142/200 [00:01<00:00, 136.36 examples/s]Map:  79%|███████▉  | 158/200 [00:01<00:00, 138.39 examples/s]Map:  88%|████████▊ | 177/200 [00:01<00:00, 146.95 examples/s]Map:  98%|█████████▊| 196/200 [00:01<00:00, 154.48 examples/s]Map: 100%|██████████| 200/200 [00:08<00:00, 22.57 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   3%|▎         | 6/200 [00:00<00:03, 49.15 examples/s]Map:   7%|▋         | 14/200 [00:00<00:03, 56.41 examples/s]Map:  17%|█▋        | 34/200 [00:00<00:01, 111.11 examples/s]Map:  24%|██▎       | 47/200 [00:00<00:01, 114.80 examples/s]Map:  33%|███▎      | 66/200 [00:00<00:00, 136.25 examples/s]Map:  44%|████▍     | 88/200 [00:00<00:00, 160.10 examples/s]Map:  52%|█████▎    | 105/200 [00:00<00:00, 143.23 examples/s]Map:  62%|██████▏   | 123/200 [00:00<00:00, 150.97 examples/s]Map:  71%|███████   | 142/200 [00:01<00:00, 139.54 examples/s]Map:  82%|████████▏ | 164/200 [00:01<00:00, 140.55 examples/s]Map:  91%|█████████ | 182/200 [00:01<00:00, 148.00 examples/s]Map: 100%|██████████| 200/200 [00:01<00:00, 153.87 examples/s]Map: 100%|██████████| 200/200 [00:08<00:00, 22.30 examples/s] 
Loading de eval data: fold_3...
Preprocess de fold_3 data for cn model
Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 3/1600 [00:00<01:33, 17.03 examples/s]Map:   1%|          | 12/1600 [00:00<00:33, 47.94 examples/s]Map:   1%|▏         | 20/1600 [00:00<00:27, 57.05 examples/s]Map:   2%|▏         | 31/1600 [00:00<00:21, 71.64 examples/s]Map:   3%|▎         | 45/1600 [00:00<00:17, 88.46 examples/s]Map:   4%|▎         | 56/1600 [00:00<00:18, 84.43 examples/s]Map:   4%|▍         | 69/1600 [00:00<00:16, 92.89 examples/s]Map:   5%|▌         | 83/1600 [00:01<00:17, 89.19 examples/s]Map:   6%|▌         | 96/1600 [00:01<00:17, 85.05 examples/s]Map:   7%|▋         | 105/1600 [00:01<00:17, 84.86 examples/s]Map:   7%|▋         | 114/1600 [00:01<00:17, 82.62 examples/s]Map:   8%|▊         | 127/1600 [00:01<00:18, 79.78 examples/s]Map:   9%|▉         | 140/1600 [00:01<00:18, 77.77 examples/s]Map:   9%|▉         | 148/1600 [00:01<00:18, 76.60 examples/s]Map:  10%|▉         | 158/1600 [00:02<00:18, 79.28 examples/s]Map:  10%|█         | 167/1600 [00:02<00:21, 66.87 examples/s]Map:  11%|█         | 176/1600 [00:02<00:20, 69.75 examples/s]Map:  12%|█▏        | 185/1600 [00:02<00:19, 72.27 examples/s]Map:  12%|█▏        | 195/1600 [00:02<00:18, 76.11 examples/s]Map:  13%|█▎        | 204/1600 [00:02<00:18, 76.60 examples/s]Map:  13%|█▎        | 214/1600 [00:02<00:17, 78.12 examples/s]Map:  14%|█▍        | 223/1600 [00:02<00:17, 79.94 examples/s]Map:  14%|█▍        | 232/1600 [00:03<00:17, 79.41 examples/s]Map:  15%|█▌        | 244/1600 [00:03<00:17, 78.02 examples/s]Map:  16%|█▌        | 252/1600 [00:03<00:17, 77.96 examples/s]Map:  16%|█▋        | 262/1600 [00:03<00:17, 78.18 examples/s]Map:  17%|█▋        | 273/1600 [00:03<00:15, 84.19 examples/s]Map:  18%|█▊        | 285/1600 [00:03<00:19, 68.25 examples/s]Map:  18%|█▊        | 295/1600 [00:03<00:17, 74.20 examples/s]Map:  19%|█▉        | 308/1600 [00:04<00:17, 74.09 examples/s]Map:  20%|█▉        | 317/1600 [00:04<00:17, 75.18 examples/s]Map:  20%|██        | 327/1600 [00:04<00:16, 77.58 examples/s]Map:  21%|██        | 339/1600 [00:04<00:16, 76.86 examples/s]Map:  22%|██▏       | 348/1600 [00:04<00:16, 77.39 examples/s]Map:  22%|██▏       | 357/1600 [00:04<00:15, 79.38 examples/s]Map:  23%|██▎       | 366/1600 [00:04<00:15, 80.94 examples/s]Map:  24%|██▎       | 376/1600 [00:04<00:15, 80.67 examples/s]Map:  24%|██▍       | 385/1600 [00:04<00:15, 80.50 examples/s]Map:  25%|██▍       | 395/1600 [00:05<00:17, 68.39 examples/s]Map:  25%|██▌       | 404/1600 [00:05<00:16, 71.29 examples/s]Map:  26%|██▌       | 418/1600 [00:05<00:13, 87.11 examples/s]Map:  27%|██▋       | 429/1600 [00:05<00:12, 92.35 examples/s]Map:  28%|██▊       | 442/1600 [00:05<00:11, 100.25 examples/s]Map:  28%|██▊       | 454/1600 [00:05<00:11, 103.28 examples/s]Map:  29%|██▉       | 467/1600 [00:05<00:10, 107.38 examples/s]Map:  30%|██▉       | 479/1600 [00:05<00:10, 106.95 examples/s]Map:  31%|███       | 492/1600 [00:06<00:11, 95.35 examples/s] Map:  32%|███▏      | 505/1600 [00:06<00:12, 85.87 examples/s]Map:  32%|███▏      | 518/1600 [00:06<00:14, 72.90 examples/s]Map:  33%|███▎      | 526/1600 [00:06<00:14, 73.48 examples/s]Map:  34%|███▎      | 537/1600 [00:06<00:14, 71.27 examples/s]Map:  34%|███▍      | 547/1600 [00:06<00:14, 73.22 examples/s]Map:  35%|███▍      | 556/1600 [00:07<00:14, 73.62 examples/s]Map:  35%|███▌      | 565/1600 [00:07<00:13, 75.02 examples/s]Map:  36%|███▌      | 575/1600 [00:07<00:14, 70.28 examples/s]Map:  37%|███▋      | 586/1600 [00:07<00:14, 68.53 examples/s]Map:  37%|███▋      | 596/1600 [00:07<00:14, 71.26 examples/s]Map:  38%|███▊      | 604/1600 [00:07<00:14, 70.88 examples/s]Map:  38%|███▊      | 612/1600 [00:07<00:14, 70.52 examples/s]Map:  39%|███▉      | 620/1600 [00:08<00:17, 57.58 examples/s]Map:  39%|███▉      | 628/1600 [00:08<00:15, 61.25 examples/s]Map:  40%|███▉      | 636/1600 [00:08<00:15, 63.54 examples/s]Map:  40%|████      | 644/1600 [00:08<00:14, 65.93 examples/s]Map:  41%|████      | 652/1600 [00:08<00:13, 68.43 examples/s]Map:  41%|████▏     | 660/1600 [00:08<00:13, 68.16 examples/s]Map:  42%|████▏     | 668/1600 [00:08<00:13, 68.96 examples/s]Map:  42%|████▏     | 676/1600 [00:08<00:12, 71.17 examples/s]Map:  43%|████▎     | 687/1600 [00:09<00:13, 66.38 examples/s]Map:  43%|████▎     | 695/1600 [00:09<00:13, 66.32 examples/s]Map:  44%|████▍     | 703/1600 [00:09<00:13, 66.16 examples/s]Map:  44%|████▍     | 711/1600 [00:09<00:13, 66.97 examples/s]Map:  45%|████▍     | 719/1600 [00:09<00:12, 69.49 examples/s]Map:  46%|████▌     | 731/1600 [00:09<00:15, 57.88 examples/s]Map:  46%|████▋     | 740/1600 [00:09<00:13, 63.20 examples/s]Map:  47%|████▋     | 748/1600 [00:09<00:13, 65.09 examples/s]Map:  47%|████▋     | 757/1600 [00:10<00:12, 66.88 examples/s]Map:  48%|████▊     | 765/1600 [00:10<00:12, 65.97 examples/s]Map:  48%|████▊     | 773/1600 [00:10<00:12, 67.63 examples/s]Map:  49%|████▉     | 781/1600 [00:10<00:11, 68.41 examples/s]Map:  49%|████▉     | 789/1600 [00:10<00:11, 70.09 examples/s]Map:  50%|█████     | 801/1600 [00:10<00:11, 70.30 examples/s]Map:  51%|█████     | 810/1600 [00:10<00:12, 62.24 examples/s]Map:  51%|█████▏    | 820/1600 [00:11<00:12, 62.25 examples/s]Map:  52%|█████▏    | 830/1600 [00:11<00:11, 68.09 examples/s]Map:  52%|█████▎    | 840/1600 [00:11<00:10, 73.08 examples/s]Map:  53%|█████▎    | 848/1600 [00:11<00:11, 63.68 examples/s]Map:  54%|█████▎    | 857/1600 [00:11<00:11, 66.77 examples/s]Map:  54%|█████▍    | 865/1600 [00:11<00:10, 69.12 examples/s]Map:  55%|█████▍    | 875/1600 [00:11<00:09, 75.87 examples/s]Map:  55%|█████▌    | 883/1600 [00:11<00:09, 73.78 examples/s]Map:  56%|█████▌    | 892/1600 [00:12<00:09, 71.98 examples/s]Map:  56%|█████▋    | 900/1600 [00:12<00:10, 68.97 examples/s]Map:  57%|█████▋    | 911/1600 [00:12<00:10, 67.13 examples/s]Map:  57%|█████▊    | 920/1600 [00:12<00:11, 61.42 examples/s]Map:  58%|█████▊    | 927/1600 [00:12<00:11, 61.03 examples/s]Map:  58%|█████▊    | 934/1600 [00:12<00:10, 60.91 examples/s]Map:  59%|█████▉    | 944/1600 [00:12<00:10, 59.84 examples/s]Map:  60%|█████▉    | 952/1600 [00:13<00:10, 61.53 examples/s]Map:  60%|██████    | 960/1600 [00:13<00:12, 50.08 examples/s]Map:  60%|██████    | 968/1600 [00:13<00:11, 54.29 examples/s]Map:  61%|██████    | 976/1600 [00:13<00:10, 57.81 examples/s]Map:  62%|██████▏   | 984/1600 [00:13<00:10, 60.80 examples/s]Map:  62%|██████▏   | 992/1600 [00:13<00:09, 62.81 examples/s]Map:  62%|██████▏   | 998/1600 [00:25<00:09, 62.81 examples/s]Map:  62%|██████▎   | 1000/1600 [01:13<22:32,  2.25s/ examples]Map:  63%|██████▎   | 1007/1600 [01:14<16:16,  1.65s/ examples]Map:  64%|██████▎   | 1016/1600 [01:14<10:43,  1.10s/ examples]Map:  64%|██████▍   | 1023/1600 [01:14<07:46,  1.24 examples/s]Map:  64%|██████▍   | 1030/1600 [01:14<05:35,  1.70 examples/s]Map:  65%|██████▍   | 1039/1600 [01:14<03:40,  2.54 examples/s]Map:  65%|██████▌   | 1047/1600 [01:14<02:34,  3.58 examples/s]Map:  66%|██████▌   | 1058/1600 [01:14<01:38,  5.49 examples/s]Map:  67%|██████▋   | 1065/1600 [01:14<01:14,  7.13 examples/s]Map:  67%|██████▋   | 1073/1600 [01:15<00:54,  9.61 examples/s]Map:  68%|██████▊   | 1082/1600 [01:15<00:38, 13.30 examples/s]Map:  68%|██████▊   | 1090/1600 [01:15<00:29, 17.40 examples/s]Map:  69%|██████▊   | 1097/1600 [01:15<00:23, 21.60 examples/s]Map:  69%|██████▉   | 1105/1600 [01:15<00:18, 27.16 examples/s]Map:  70%|██████▉   | 1112/1600 [01:15<00:15, 32.26 examples/s]Map:  70%|██████▉   | 1119/1600 [01:15<00:12, 37.96 examples/s]Map:  70%|███████   | 1127/1600 [01:15<00:13, 35.98 examples/s]Map:  71%|███████   | 1135/1600 [01:16<00:10, 42.84 examples/s]Map:  72%|███████▏  | 1144/1600 [01:16<00:09, 49.59 examples/s]Map:  72%|███████▏  | 1151/1600 [01:16<00:08, 51.85 examples/s]Map:  72%|███████▏  | 1159/1600 [01:16<00:07, 56.12 examples/s]Map:  73%|███████▎  | 1167/1600 [01:16<00:07, 59.16 examples/s]Map:  73%|███████▎  | 1175/1600 [01:16<00:06, 62.84 examples/s]Map:  74%|███████▍  | 1182/1600 [01:16<00:06, 62.38 examples/s]Map:  74%|███████▍  | 1191/1600 [01:16<00:06, 64.85 examples/s]Map:  75%|███████▌  | 1201/1600 [01:17<00:05, 70.75 examples/s]Map:  76%|███████▌  | 1210/1600 [01:17<00:05, 70.65 examples/s]Map:  76%|███████▌  | 1218/1600 [01:17<00:05, 70.05 examples/s]Map:  77%|███████▋  | 1227/1600 [01:17<00:05, 70.90 examples/s]Map:  77%|███████▋  | 1235/1600 [01:17<00:05, 67.22 examples/s]Map:  78%|███████▊  | 1244/1600 [01:17<00:06, 53.19 examples/s]Map:  78%|███████▊  | 1251/1600 [01:17<00:06, 55.42 examples/s]Map:  79%|███████▊  | 1257/1600 [01:17<00:06, 54.85 examples/s]Map:  79%|███████▉  | 1264/1600 [01:18<00:05, 57.90 examples/s]Map:  80%|███████▉  | 1273/1600 [01:18<00:05, 62.07 examples/s]Map:  80%|████████  | 1281/1600 [01:18<00:05, 62.84 examples/s]Map:  81%|████████  | 1291/1600 [01:18<00:05, 61.67 examples/s]Map:  81%|████████  | 1299/1600 [01:18<00:04, 62.35 examples/s]Map:  82%|████████▏ | 1306/1600 [01:18<00:04, 63.61 examples/s]Map:  82%|████████▏ | 1314/1600 [01:18<00:04, 66.51 examples/s]Map:  83%|████████▎ | 1321/1600 [01:18<00:04, 65.31 examples/s]Map:  83%|████████▎ | 1329/1600 [01:19<00:04, 67.23 examples/s]Map:  84%|████████▎ | 1337/1600 [01:19<00:03, 69.27 examples/s]Map:  84%|████████▍ | 1345/1600 [01:19<00:03, 69.26 examples/s]Map:  84%|████████▍ | 1352/1600 [01:19<00:04, 52.49 examples/s]Map:  85%|████████▌ | 1360/1600 [01:19<00:04, 57.11 examples/s]Map:  86%|████████▌ | 1368/1600 [01:19<00:03, 59.49 examples/s]Map:  86%|████████▌ | 1376/1600 [01:19<00:03, 61.43 examples/s]Map:  86%|████████▋ | 1384/1600 [01:19<00:03, 63.12 examples/s]Map:  87%|████████▋ | 1393/1600 [01:20<00:03, 66.28 examples/s]Map:  88%|████████▊ | 1401/1600 [01:20<00:03, 65.02 examples/s]Map:  88%|████████▊ | 1408/1600 [01:20<00:02, 64.72 examples/s]Map:  89%|████████▊ | 1417/1600 [01:20<00:02, 68.21 examples/s]Map:  89%|████████▉ | 1425/1600 [01:20<00:02, 69.61 examples/s]Map:  90%|████████▉ | 1433/1600 [01:20<00:02, 70.71 examples/s]Map:  90%|█████████ | 1443/1600 [01:20<00:02, 66.25 examples/s]Map:  91%|█████████ | 1453/1600 [01:21<00:02, 61.82 examples/s]Map:  91%|█████████▏| 1462/1600 [01:21<00:02, 49.67 examples/s]Map:  92%|█████████▏| 1470/1600 [01:21<00:02, 53.71 examples/s]Map:  92%|█████████▏| 1477/1600 [01:21<00:02, 55.47 examples/s]Map:  93%|█████████▎| 1484/1600 [01:21<00:02, 56.65 examples/s]Map:  93%|█████████▎| 1491/1600 [01:21<00:01, 58.83 examples/s]Map:  94%|█████████▎| 1499/1600 [01:21<00:01, 62.09 examples/s]Map:  94%|█████████▍| 1506/1600 [01:21<00:01, 62.77 examples/s]Map:  95%|█████████▍| 1513/1600 [01:22<00:01, 61.91 examples/s]Map:  95%|█████████▌| 1520/1600 [01:22<00:01, 61.69 examples/s]Map:  95%|█████████▌| 1527/1600 [01:22<00:01, 63.42 examples/s]Map:  96%|█████████▌| 1534/1600 [01:22<00:01, 63.62 examples/s]Map:  96%|█████████▋| 1542/1600 [01:22<00:00, 64.48 examples/s]Map:  97%|█████████▋| 1549/1600 [01:22<00:00, 64.69 examples/s]Map:  97%|█████████▋| 1556/1600 [01:22<00:00, 64.52 examples/s]Map:  98%|█████████▊| 1564/1600 [01:22<00:00, 64.13 examples/s]Map:  98%|█████████▊| 1571/1600 [01:23<00:00, 63.22 examples/s]Map:  99%|█████████▊| 1579/1600 [01:23<00:00, 48.80 examples/s]Map:  99%|█████████▉| 1585/1600 [01:23<00:00, 50.69 examples/s]Map: 100%|█████████▉| 1593/1600 [01:23<00:00, 55.37 examples/s]Map: 100%|█████████▉| 1597/1600 [01:36<00:00, 55.37 examples/s]Map: 100%|██████████| 1600/1600 [02:08<00:00,  1.88s/ examples]Map: 100%|██████████| 1600/1600 [02:08<00:00, 12.46 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▏         | 3/200 [00:00<00:10, 19.20 examples/s]Map:   6%|▌         | 12/200 [00:00<00:03, 51.61 examples/s]Map:  10%|█         | 20/200 [00:00<00:03, 58.86 examples/s]Map:  14%|█▍        | 28/200 [00:00<00:02, 63.81 examples/s]Map:  18%|█▊        | 36/200 [00:00<00:02, 68.25 examples/s]Map:  22%|██▏       | 44/200 [00:00<00:02, 68.37 examples/s]Map:  28%|██▊       | 55/200 [00:00<00:01, 76.62 examples/s]Map:  33%|███▎      | 66/200 [00:00<00:01, 82.08 examples/s]Map:  39%|███▉      | 78/200 [00:01<00:01, 78.62 examples/s]Map:  44%|████▎     | 87/200 [00:01<00:01, 77.13 examples/s]Map:  48%|████▊     | 95/200 [00:01<00:01, 62.03 examples/s]Map:  52%|█████▏    | 103/200 [00:01<00:01, 64.26 examples/s]Map:  56%|█████▌    | 111/200 [00:01<00:01, 66.79 examples/s]Map:  60%|█████▉    | 119/200 [00:01<00:01, 67.73 examples/s]Map:  63%|██████▎   | 126/200 [00:01<00:01, 66.63 examples/s]Map:  67%|██████▋   | 134/200 [00:02<00:00, 66.02 examples/s]Map:  72%|███████▏  | 143/200 [00:02<00:00, 68.85 examples/s]Map:  76%|███████▌  | 151/200 [00:02<00:00, 70.37 examples/s]Map:  80%|███████▉  | 159/200 [00:02<00:00, 71.15 examples/s]Map:  85%|████████▌ | 170/200 [00:02<00:00, 70.08 examples/s]Map:  90%|█████████ | 180/200 [00:02<00:00, 66.13 examples/s]Map:  94%|█████████▍| 188/200 [00:02<00:00, 67.71 examples/s]Map:  98%|█████████▊| 195/200 [00:02<00:00, 66.52 examples/s]Map: 100%|██████████| 200/200 [00:16<00:00, 12.48 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   3%|▎         | 6/200 [00:00<00:04, 48.45 examples/s]Map:   9%|▉         | 18/200 [00:00<00:02, 69.45 examples/s]Map:  14%|█▎        | 27/200 [00:00<00:02, 73.27 examples/s]Map:  18%|█▊        | 36/200 [00:00<00:02, 77.28 examples/s]Map:  23%|██▎       | 46/200 [00:00<00:01, 82.69 examples/s]Map:  30%|███       | 61/200 [00:00<00:01, 98.89 examples/s]Map:  36%|███▌      | 71/200 [00:00<00:01, 92.88 examples/s]Map:  42%|████▏     | 83/200 [00:00<00:01, 85.28 examples/s]Map:  48%|████▊     | 95/200 [00:01<00:01, 70.99 examples/s]Map:  52%|█████▏    | 103/200 [00:01<00:01, 72.69 examples/s]Map:  56%|█████▌    | 112/200 [00:01<00:01, 73.41 examples/s]Map:  60%|██████    | 120/200 [00:01<00:01, 72.77 examples/s]Map:  66%|██████▌   | 131/200 [00:01<00:00, 69.98 examples/s]Map:  70%|██████▉   | 139/200 [00:01<00:00, 69.86 examples/s]Map:  76%|███████▌  | 151/200 [00:02<00:00, 71.49 examples/s]Map:  80%|████████  | 160/200 [00:02<00:00, 72.62 examples/s]Map:  86%|████████▌ | 171/200 [00:02<00:00, 69.26 examples/s]Map:  91%|█████████ | 182/200 [00:02<00:00, 66.38 examples/s]Map:  95%|█████████▌| 190/200 [00:02<00:00, 65.37 examples/s]Map:  99%|█████████▉| 198/200 [00:02<00:00, 66.64 examples/s]Map:  99%|█████████▉| 198/200 [00:14<00:00, 66.64 examples/s]Map: 100%|██████████| 200/200 [00:14<00:00,  1.88 examples/s]Map: 100%|██████████| 200/200 [00:14<00:00, 13.46 examples/s]
Loading de test data: fold_3...
Preprocess de fold_3 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 179.6447, Training accuracy: 0.9594
Macro F1-score: 0.9593
Model performance on Angry speech (in training): 
	Precision: 0.9425, Recall: 0.9425, F1_score: 0.9425
Model performance on Happy speech (in training): 
	Precision: 0.9421, Recall: 0.9350, F1_score: 0.9385
Model performance on Neutral speech (in training): 
	Precision: 0.9749, Recall: 0.9725, F1_score: 0.9737
Model performance on Sad speech (in training): 
	Precision: 0.9777, Recall: 0.9875, F1_score: 0.9826

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 275/1600 [00:10<00:48, 27.47it/s]Training:  34%|███▍      | 552/1600 [00:20<00:37, 27.58it/s]Training:  52%|█████▏    | 834/1600 [00:30<00:27, 27.86it/s]Training:  70%|██████▉   | 1116/1600 [00:40<00:17, 27.73it/s]Training:  88%|████████▊ | 1403/1600 [00:50<00:07, 28.04it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 341.3031, Validation accuracy: 0.5500
Macro F1-score: 0.4738
Model performance on Angry speech (in validation): 
	Precision: 0.9091, Recall: 0.4000, F1_score: 0.5556
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3704, Recall: 0.8000, F1_score: 0.5063
Model performance on Sad speech (in validation): 
	Precision: 0.7143, Recall: 1.0000, F1_score: 0.8333
New best accuracy for layer 3 on epoch 1: 0.5500. Model saved.
Epoch 2/100

Training Phase:
Training loss: 92.1979, Training accuracy: 0.9781
Macro F1-score: 0.9781
Model performance on Angry speech (in training): 
	Precision: 0.9679, Recall: 0.9800, F1_score: 0.9739
Model performance on Happy speech (in training): 
	Precision: 0.9747, Recall: 0.9625, F1_score: 0.9686
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888

Eval Phase: 
Validation loss: 336.4047, Validation accuracy: 0.4900
Macro F1-score: 0.4287
Model performance on Angry speech (in validation): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.3273, Recall: 0.3600, F1_score: 0.3429
Model performance on Sad speech (in validation): 
	Precision: 0.4464, Recall: 1.0000, F1_score: 0.6173
Epoch 3/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 271/1600 [00:10<00:49, 27.06it/s]Training:  34%|███▍      | 550/1600 [00:20<00:38, 27.54it/s]Training:  52%|█████▏    | 829/1600 [00:30<00:27, 27.54it/s]Training:  70%|██████▉   | 1115/1600 [00:40<00:17, 27.95it/s]Training:  88%|████████▊ | 1401/1600 [00:50<00:07, 28.09it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 282/1600 [00:10<00:46, 28.17it/s]Training:  35%|███▌      | 564/1600 [00:20<00:37, 27.90it/s]Training:  53%|█████▎    | 846/1600 [00:30<00:26, 28.01it/s]Training:  71%|███████   | 1129/1600 [00:40<00:16, 28.12it/s]Training:  88%|████████▊ | 1412/1600 [00:50<00:06, 27.82it/s]                           Training loss: 77.6730, Training accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9700, F1_score: 0.9749
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9926, Recall: 1.0000, F1_score: 0.9963

Eval Phase: 
Validation loss: 375.1485, Validation accuracy: 0.4750
Macro F1-score: 0.4058
Model performance on Angry speech (in validation): 
	Precision: 0.9444, Recall: 0.3400, F1_score: 0.5000
Model performance on Happy speech (in validation): 
	Precision: 0.3333, Recall: 0.0200, F1_score: 0.0377
Model performance on Neutral speech (in validation): 
	Precision: 0.3418, Recall: 0.5400, F1_score: 0.4186
Model performance on Sad speech (in validation): 
	Precision: 0.5000, Recall: 1.0000, F1_score: 0.6667
Epoch 4/100

Training Phase:
Training loss: 50.0394, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
                                  Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 280/1600 [00:10<00:47, 27.90it/s]Training:  35%|███▌      | 561/1600 [00:20<00:37, 28.00it/s]Training:  53%|█████▎    | 842/1600 [00:30<00:27, 27.64it/s]Training:  70%|███████   | 1127/1600 [00:40<00:16, 27.95it/s]Training:  88%|████████▊ | 1412/1600 [00:50<00:06, 27.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 487.1722, Validation accuracy: 0.4350
Macro F1-score: 0.3643
Model performance on Angry speech (in validation): 
	Precision: 0.9500, Recall: 0.3800, F1_score: 0.5429
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.3600, F1_score: 0.3462
Model performance on Sad speech (in validation): 
	Precision: 0.3968, Recall: 1.0000, F1_score: 0.5682
Epoch 5/100

Training Phase:
Training loss: 62.7867, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9775, F1_score: 0.9849
Model performance on Happy speech (in training): 
	Precision: 0.9777, Recall: 0.9875, F1_score: 0.9826
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 346.7439, Validation accuracy: 0.5100
Macro F1-score: 0.4539
Model performance on Angry speech (in validation): 
	Precision: 0.9412, Recall: 0.6400, F1_score: 0.7619
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0400, F1_score: 0.0741
Model performance on Neutral speech (in validation): 
	Precision: 0.4286, Recall: 0.3600, F1_score: 0.3913
Model performance on Sad speech (in validation): 
	Precision: 0.4167, Recall: 1.0000, F1_score: 0.5882
Epoch 6/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 273/1600 [00:10<00:48, 27.21it/s]Training:  34%|███▍      | 546/1600 [00:20<00:38, 27.18it/s]Training:  51%|█████▏    | 823/1600 [00:30<00:28, 27.41it/s]Training:  69%|██████▉   | 1106/1600 [00:40<00:17, 27.73it/s]Training:  87%|████████▋ | 1389/1600 [00:50<00:07, 27.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 276/1600 [00:10<00:47, 27.59it/s]Training:  35%|███▌      | 560/1600 [00:20<00:37, 28.04it/s]Training:  53%|█████▎    | 844/1600 [00:30<00:26, 28.12it/s]Training:  70%|███████   | 1127/1600 [00:40<00:16, 27.94it/s]Training:  88%|████████▊ | 1404/1600 [00:50<00:07, 27.64it/s]                           Training loss: 24.3744, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 365.5797, Validation accuracy: 0.5000
Macro F1-score: 0.4388
Model performance on Angry speech (in validation): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.4255, Recall: 0.4000, F1_score: 0.4124
Model performance on Sad speech (in validation): 
	Precision: 0.4167, Recall: 1.0000, F1_score: 0.5882
Epoch 7/100

Training Phase:
Training loss: 32.2576, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9875, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 440.8759, Validation accuracy: 0.5100
Macro F1-score: 0.4376
Model performance on Angry speech (in validation): 
	Precision: 0.9200, Recall: 0.4600, F1_score: 0.6133
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3766, Recall: 0.5800, F1_score: 0.4567
Model performance on Sad speech (in validation): 
	Precision: 0.5155, Recall: 1.0000, F1_score: 0.6803
Epoch 8/100

Training Phase:
                                  Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 273/1600 [00:10<00:48, 27.24it/s]Training:  34%|███▍      | 546/1600 [00:20<00:38, 27.14it/s]Training:  51%|█████     | 817/1600 [00:30<00:28, 27.09it/s]Training:  68%|██████▊   | 1094/1600 [00:40<00:18, 27.29it/s]Training:  86%|████████▌ | 1370/1600 [00:50<00:08, 27.38it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 277/1600 [00:10<00:47, 27.67it/s]Training:  35%|███▍      | 557/1600 [00:20<00:37, 27.86it/s]Training:  52%|█████▏    | 838/1600 [00:30<00:27, 27.96it/s]Training:  70%|██████▉   | 1119Training loss: 36.1355, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 511.0908, Validation accuracy: 0.4400
Macro F1-score: 0.3729
Model performance on Angry speech (in validation): 
	Precision: 0.9630, Recall: 0.5200, F1_score: 0.6753
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3077, Recall: 0.2400, F1_score: 0.2697
Model performance on Sad speech (in validation): 
	Precision: 0.3759, Recall: 1.0000, F1_score: 0.5464
Epoch 9/100

Training Phase:
Training loss: 27.3831, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9949, Recall: 0.9850, F1_score: 0.9899
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 389.6528, Validation accuracy: 0.5150
Macro F1-score: 0.4872
Model performance on Angry speech (in validation): 
	Precision: 0.9545, Recall: 0.4200, F1_score: 0.5833
Model performance on Happy speech (in validation): 
	Precision: 0.9091, Recall: 0.2000, F1_score: 0.3279
Model performance on Neutral speech (in validation): 
	Precision: 0.3607, Recall: 0.4400, F1_score: 0.3964
Model performance on Sad speech (in validation): 
	Precision: 0.4717, Recall: 1.0000, F1_score: 0.6410
Epoch 10/100

Training Phase:
/1600 [00:40<00:17, 27.93it/s]Training:  88%|████████▊ | 1405/1600 [00:50<00:06, 28.15it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 283/1600 [00:10<00:46, 28.25it/s]Training:  35%|███▌      | 566/1600 [00:21<00:39, 26.20it/s]Training:  53%|█████▎    | 845/1600 [00:31<00:28, 26.91it/s]Training:  70%|███████   | 1124/1600 [00:41<00:17, 27.20it/s]Training:  88%|████████▊ | 1401/1600 [00:51<00:07, 27.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 275/1600 [00:10<00:48, 27.44it/s]Training:  34%|███▍      | 551/1600 [00Training loss: 38.4638, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9926, Recall: 1.0000, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 327.3604, Validation accuracy: 0.6200
Macro F1-score: 0.5706
Model performance on Angry speech (in validation): 
	Precision: 0.9500, Recall: 0.7600, F1_score: 0.8444
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1000, F1_score: 0.1818
Model performance on Neutral speech (in validation): 
	Precision: 0.4493, Recall: 0.6200, F1_score: 0.5210
Model performance on Sad speech (in validation): 
	Precision: 0.5814, Recall: 1.0000, F1_score: 0.7353
New best accuracy for layer 3 on epoch 10: 0.6200. Model saved.
Epoch 11/100

Training Phase:
Training loss: 36.4789, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 245.2554, Validation accuracy: 0.6400
Macro F1-score: 0.6283
Model performance on Angry speech (in validation): 
	Precision: 0.9545, Recall: 0.4200, F1_score: 0.5833
Model performance on Happy speech (in validation): 
	Precision: 0.7429, Recall: 0.5200, F1_score: 0.6118
Model performance on Neutral speech (in validation): 
	Precision: 0.4921, Recall: 0.6200, F1_score: 0.5487
Model performance on Sad speech (in validation): 
	Precision: 0.6250, Recall: 1.0000, F1_score: 0.7692
New best accuracy for layer 3 on epoch 11: 0.6400. Model saved.
Epoch 12/100

Training Phase:
:20<00:38, 27.53it/s]Training:  52%|█████▏    | 831/1600 [00:30<00:27, 27.72it/s]Training:  69%|██████▉   | 1111/1600 [00:40<00:17, 27.80it/s]Training:  87%|████████▋ | 1391/1600 [00:50<00:07, 27.81it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 280/1600 [00:10<00:47, 27.98it/s]Training:  35%|███▌      | 560/1600 [00:20<00:37, 27.87it/s]Training:  53%|█████▎    | 846/1600 [00:30<00:26, 28.20it/s]Training:  71%|███████   | 1132/1600 [00:40<00:16, 28.33it/s]Training:  89%|████████▊ | 1418/1600 [00:50<00:06, 28.23it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|        Training loss: 36.5636, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 477.1230, Validation accuracy: 0.4600
Macro F1-score: 0.4254
Model performance on Angry speech (in validation): 
	Precision: 0.9500, Recall: 0.3800, F1_score: 0.5429
Model performance on Happy speech (in validation): 
	Precision: 0.5625, Recall: 0.1800, F1_score: 0.2727
Model performance on Neutral speech (in validation): 
	Precision: 0.3256, Recall: 0.2800, F1_score: 0.3011
Model performance on Sad speech (in validation): 
	Precision: 0.4132, Recall: 1.0000, F1_score: 0.5848
Epoch 13/100

Training Phase:
  | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 285/1600 [00:10<00:46, 28.47it/s]Training:  36%|███▌      | 570/1600 [00:20<00:36, 28.08it/s]Training:  53%|█████▎    | 853/1600 [00:30<00:26, 28.15it/s]Training:  71%|███████   | 1136/1600 [00:40<00:16, 27.95it/s]Training:  88%|████████▊ | 1415/1600 [00:50<00:06, 27.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 285/1600 [00:10<00:46, 28.46it/s]Training:  36%|███▌      | 570/1600 [00:20<00:36, 28.21it/s]Training:  53%|█████▎    | 851/1600 [00:30<00:26, 28.07it/s]Training:  71%|███████   | 1134/1600 [00:40<00:16, 28.16it/s]Training:  89%|████████▊ | 1417/1600 [00:50<00:06, 27.91it/s]                                                   Training loss: 36.4719, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 407.4901, Validation accuracy: 0.5200
Macro F1-score: 0.4945
Model performance on Angry speech (in validation): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Happy speech (in validation): 
	Precision: 0.6190, Recall: 0.2600, F1_score: 0.3662
Model performance on Neutral speech (in validation): 
	Precision: 0.4286, Recall: 0.2400, F1_score: 0.3077
Model performance on Sad speech (in validation): 
	Precision: 0.4167, Recall: 1.0000, F1_score: 0.5882
Epoch 14/100

Training Phase:
Training loss: 25.3021, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 314.0959, Validation accuracy: 0.5950
Macro F1-score: 0.5861
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 0.5000, F1_score: 0.6579
Model performance on Happy speech (in validation): 
	Precision: 0.7586, Recall: 0.4400, F1_score: 0.5570
Model performance on Neutral speech (in validation): 
	Precision: 0.4400, Recall: 0.4400, F1_score: 0.4400
Model performance on Sad speech (in validation): 
	Precision: 0.5263, Recall: 1.0000, F1_score: 0.6897
Epoch 15/100

Training Phase:
          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|█         | 168/1600 [00:10<01:25, 16.77it/s]Training:  23%|██▎       | 371/1600 [00:20<01:05, 18.82it/s]Training:  37%|███▋      | 587/1600 [00:30<00:50, 20.08it/s]Training:  52%|█████▏    | 829/1600 [00:40<00:35, 21.68it/s]Training:  68%|██████▊   | 1082/1600 [00:50<00:22, 22.97it/s]Training:  84%|████████▎ | 1337/1600 [01:00<00:11, 23.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 273/1600 [00:10<00:48, 27.26it/s]Training:  34%|███▍      | 547/1600 [00:20<00:38, 27.31it/s]Training:  52%|█████▏    | 832/1600 [00:30<00:27, 27.85it/s]TrTraining loss: 24.0069, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 170.5470, Validation accuracy: 0.7450
Macro F1-score: 0.7425
Model performance on Angry speech (in validation): 
	Precision: 0.9643, Recall: 0.5400, F1_score: 0.6923
Model performance on Happy speech (in validation): 
	Precision: 0.7907, Recall: 0.6800, F1_score: 0.7312
Model performance on Neutral speech (in validation): 
	Precision: 0.5652, Recall: 0.7800, F1_score: 0.6555
Model performance on Sad speech (in validation): 
	Precision: 0.8167, Recall: 0.9800, F1_score: 0.8909
New best accuracy for layer 3 on epoch 15: 0.7450. Model saved.
Epoch 16/100

Training Phase:
Training loss: 18.4221, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 482.1983, Validation accuracy: 0.5100
Macro F1-score: 0.4753
Model performance on Angry speech (in validation): 
	Precision: 0.9091, Recall: 0.6000, F1_score: 0.7229
Model performance on Happy speech (in validation): 
	Precision: 0.5769, Recall: 0.3000, F1_score: 0.3947
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.1400, F1_score: 0.2188
Model performance on Sad speech (in validation): 
	Precision: 0.3937, Recall: 1.0000, F1_score: 0.5650
Epoch 17/100

Training Phase:
aining:  70%|██████▉   | 1117/1600 [00:40<00:17, 27.85it/s]Training:  87%|████████▋ | 1396/1600 [00:50<00:07, 27.56it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 278/1600 [00:10<00:47, 27.75it/s]Training:  35%|███▍      | 556/1600 [00:20<00:38, 27.08it/s]Training:  52%|█████▏    | 830/1600 [00:30<00:28, 27.18it/s]Training:  69%|██████▉   | 1103/1600 [00:40<00:18, 27.21it/s]Training:  87%|████████▋ | 1386/1600 [00:50<00:07, 27.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 279/1600 [00:10<00:47, 27.89it/s]TraiTraining loss: 33.7910, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925

Eval Phase: 
Validation loss: 297.2307, Validation accuracy: 0.6450
Macro F1-score: 0.6221
Model performance on Angry speech (in validation): 
	Precision: 0.9000, Recall: 0.7200, F1_score: 0.8000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.2600, F1_score: 0.4127
Model performance on Neutral speech (in validation): 
	Precision: 0.4918, Recall: 0.6000, F1_score: 0.5405
Model performance on Sad speech (in validation): 
	Precision: 0.5814, Recall: 1.0000, F1_score: 0.7353
Epoch 18/100

Training Phase:
Training loss: 19.5289, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
ning:  35%|███▌      | 560/1600 [00:20<00:37, 27.99it/s]Training:  53%|█████▎    | 841/1600 [00:30<00:27, 27.83it/s]Training:  70%|██████▉   | 1118/1600 [00:40<00:17, 27.71it/s]Training:  88%|████████▊ | 1402/1600 [00:50<00:07, 27.96it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 284/1600 [00:10<00:46, 28.39it/s]Training:  36%|███▌      | 568/1600 [00:20<00:36, 27.99it/s]Training:  53%|█████▎    | 851/1600 [00:30<00:26, 28.10it/s]Training:  71%|███████   | 1136/1600 [00:40<00:16, 28.23it/s]Training:  89%|████████▉ | 1421/1600 [00:50<00:06, 28.05it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                 Validation loss: 314.4539, Validation accuracy: 0.6500
Macro F1-score: 0.6336
Model performance on Angry speech (in validation): 
	Precision: 0.8936, Recall: 0.8400, F1_score: 0.8660
Model performance on Happy speech (in validation): 
	Precision: 0.9524, Recall: 0.4000, F1_score: 0.5634
Model performance on Neutral speech (in validation): 
	Precision: 0.5455, Recall: 0.3600, F1_score: 0.4337
Model performance on Sad speech (in validation): 
	Precision: 0.5051, Recall: 1.0000, F1_score: 0.6711
Epoch 19/100

Training Phase:
Training loss: 23.0211, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 569.1669, Validation accuracy: 0.4600
Macro F1-score: 0.3944
Model performance on Angry speech (in validation): 
	Precision: 0.9500, Recall: 0.3800, F1_score: 0.5429
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.3385, Recall: 0.4400, F1_score: 0.3826
Model performance on Sad speech (in validation): 
	Precision: 0.4425, Recall: 1.0000, F1_score: 0.6135
Epoch 20/100

Training Phase:
                  Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 285/1600 [00:10<00:46, 28.40it/s]Training:  36%|███▌      | 570/1600 [00:20<00:36, 28.24it/s]Training:  53%|█████▎    | 852/1600 [00:30<00:26, 27.97it/s]Training:  71%|███████▏  | 1140/1600 [00:40<00:16, 28.28it/s]Training:  89%|████████▉ | 1428/1600 [00:50<00:06, 28.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 290/1600 [00:10<00:45, 28.92it/s]Training:  36%|███▋      | 580/1600 [00:20<00:35, 28.94it/s]Training:  54%|█████▍    | 870/1600 [00:30<00:25, 28.82it/s]Training:  73%|███████▎  | 1161/1600 [00:40<00:15, 28.91it/s]Training:  91%|█████████ | 1452/1600 [00:50<00:05, 28.84it/s]    Training loss: 36.3512, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
                                                         Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 598.5095, Validation accuracy: 0.5050
Macro F1-score: 0.4120
Model performance on Angry speech (in validation): 
	Precision: 0.9167, Recall: 0.8800, F1_score: 0.8980
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4667, Recall: 0.1400, F1_score: 0.2154
Model performance on Sad speech (in validation): 
	Precision: 0.3650, Recall: 1.0000, F1_score: 0.5348
Epoch 21/100

Training Phase:
Training loss: 14.9264, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 1067.1067, Validation accuracy: 0.3550
Macro F1-score: 0.2648
Model performance on Angry speech (in validation): 
	Precision: 0.9524, Recall: 0.4000, F1_score: 0.5634
Model performance on Happy speech (in validation): 
	Precision: 0.1429, Recall: 0.0200, F1_score: 0.0351
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.2994, Recall: 1.0000, F1_score: 0.4608
Epoch 22/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 291/1600 [00:10<00:45, 29.00it/s]Training:  37%|███▋      | 587/1600 [00:20<00:34, 29.32it/s]Training:  55%|█████▌    | 883/1600 [00:30<00:24, 29.21it/s]Training:  73%|███████▎  | 1174/1600 [00:40<00:14, 29.05it/s]Training:  92%|█████████▏| 1466/1600 [00:50<00:04, 29.08it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▊        | 298/1600 [00:10<00:43, 29.71it/s]Training:  37%|███▋      | 596/1600 [00:20<00:34, 29.14it/s]Training:  55%|█████▌    | 884/1600 [00:30<00:25, 28.22it/s]Training:  72%|███████▏  | 1156/1600 [00:41<00:16, 27.26it/s]Training:  89%|████████▊ | 1418/1600 [00:51<00:06, 26.85it/s]                     Training loss: 5.1529, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 590.8392, Validation accuracy: 0.5250
Macro F1-score: 0.4448
Model performance on Angry speech (in validation): 
	Precision: 0.8800, Recall: 0.8800, F1_score: 0.8800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.5625, Recall: 0.1800, F1_score: 0.2727
Model performance on Sad speech (in validation): 
	Precision: 0.3788, Recall: 1.0000, F1_score: 0.5495
Epoch 23/100

Training Phase:
Training loss: 23.5080, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 682.4663, Validation accuracy: 0.4750
Macro F1-score: 0.3966
Model performance on Angry speech (in validation): 
	Precision: 0.9750, Recall: 0.7800, F1_score: 0.8667
Model performance on Happy speech (in validation): 
	Precision: 0.4000, Recall: 0.0400, F1_score: 0.0727
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.0800, F1_score: 0.1290
Model performance on Sad speech (in validation): 
	Precision: 0.3497, Recall: 1.0000, F1_score: 0.5181
Epoch 24/100

Training Phase:
                                        Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 276/1600 [00:10<00:48, 27.55it/s]Training:  35%|███▍      | 558/1600 [00:20<00:37, 27.90it/s]Training:  53%|█████▎    | 841/1600 [00:30<00:27, 28.05it/s]Training:  70%|███████   | 1124/1600 [00:40<00:16, 28.14it/s]Training:  88%|████████▊ | 1409/1600 [00:50<00:06, 28.26it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 292/1600 [00:10<00:44, 29.19it/s]Training:  36%|███▋      | 584/1600 [00:20<00:35, 28.95it/s]Training:  55%|█████▍    | 872/1600 [00:30<00:25, 28.24it/s]Training:  73%|███████Training loss: 17.7437, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
  | 1161/1600 [00:40<00:15, 28.46it/s]Training:  91%|█████████ | 1453/1600 [00:50<00:05, 28.72it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 744.7687, Validation accuracy: 0.4550
Macro F1-score: 0.3731
Model performance on Angry speech (in validation): 
	Precision: 0.9333, Recall: 0.2800, F1_score: 0.4308
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.5400, F1_score: 0.4122
Model performance on Sad speech (in validation): 
	Precision: 0.4808, Recall: 1.0000, F1_score: 0.6494
Epoch 25/100

Training Phase:
Training loss: 13.8608, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 479.4347, Validation accuracy: 0.4850
Macro F1-score: 0.4508
Model performance on Angry speech (in validation): 
	Precision: 0.9444, Recall: 0.3400, F1_score: 0.5000
Model performance on Happy speech (in validation): 
	Precision: 0.5263, Recall: 0.2000, F1_score: 0.2899
Model performance on Neutral speech (in validation): 
	Precision: 0.3774, Recall: 0.4000, F1_score: 0.3883
Model performance on Sad speech (in validation): 
	Precision: 0.4545, Recall: 1.0000, F1_score: 0.6250
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7450

Test Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 293/1600 [00:10<00:44, 29.27it/s]Training:  37%|███▋      | 586/1600 [00:20<00:34, 29.28it/s]Training:  55%|█████▍    | 879/1600 [00:30<00:24, 29.24it/s]Training:  73%|███████▎  | 1175/1600 [00:40<00:14, 29.37it/s]Training:  92%|█████████▏| 1471/1600 [00:50<00:04, 29.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:13, 14.82it/s]Testing:   2%|▎         | 5/200 [00:00<00:09, 20.78it/s]Testing:   4%|▍         | 8/200 [00:00<00:09, 20.22it/s]Testing:   6%|▌         | 11/200 [00:00<00:09, 20.63it/s]Testing:   8%|▊         | 15/200 [00:00<00:07, 26.00it/s]Testing:  10%|█         | 20/200 [00:00<00:06, 29.24it/s]Testing:  12%|█▏        | 23/200 [00:00<00:06, 25.92it/s]Testing:  13%|█▎        | 26/200 [00:01<00:06, 25.90it/s]Testing:  15%|█▌        | 30/200 [00:01<00:05, 29.02it/s]Testing:  16%|█▋        | 33/200 [00:01<00:05, 28.24it/s]Testing:  18%|█▊        | 37/200 [00:01<00:05, 31.00it/s]Testing:  20%|██        | 41/200 [00:01<00:04, 33.35it/s]Testing:  24%|██▎       | 47/200 [00:01<00:03, 40.64it/s]Testing:  26%|██▌       | 52/200 [00:01<00:03, 40.82it/s]Testing:  28%|██▊       | 57/200 [00:01<00:03, 40.77it/s]Testing:  31%|███       | 62/200 [00:01<00:03, 40.18it/s]Testing:  34%|███▎      | 67/200 [00:02<00:03, 42.05it/s]Testing:  37%|███▋      | 74/200 [00:02<00:02, 46.97it/s]Testing:  40%|████      | 81/200 [00:02<00:02, 51.01it/s]Testing:  44%|████▍     | 88/200 [00:02<00:02, 54.90it/s]Testing:  48%|████▊     | 97/200 [00:02<00:01, 57.29it/s]Testing:  52%|█████▏    | 103/200 [00:02<00:01, 54.56it/s]Testing:  55%|███Test loss: 183.9260, Test accuracy: 0.7200
Macro F1-score: 0.7113
Model performance on Angry speech (in test): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Happy speech (in test): 
	Precision: 0.7188, Recall: 0.4600, F1_score: 0.5610
Model performance on Neutral speech (in test): 
	Precision: 0.5584, Recall: 0.8600, F1_score: 0.6772
Model performance on Sad speech (in test): 
	Precision: 0.8167, Recall: 0.9800, F1_score: 0.8909

======================= This is fold_4 on cn =======================

Load dataset: 
Loading cn train data: fold_4...
Preprocess cn fold_4 data for cn model
█▍    | 109/200 [00:02<00:01, 55.56it/s]Testing:  60%|█████▉    | 119/200 [00:02<00:01, 65.70it/s]Testing:  63%|██████▎   | 126/200 [00:03<00:01, 63.70it/s]Testing:  68%|██████▊   | 135/200 [00:03<00:00, 69.34it/s]Testing:  71%|███████   | 142/200 [00:03<00:00, 68.72it/s]Testing:  76%|███████▌  | 151/200 [00:03<00:00, 72.31it/s]Testing:  80%|███████▉  | 159/200 [00:03<00:00, 73.42it/s]Testing:  84%|████████▍ | 168/200 [00:03<00:00, 77.35it/s]Testing:  89%|████████▉ | 178/200 [00:03<00:00, 83.27it/s]Testing:  94%|█████████▍| 188/200 [00:03<00:00, 87.45it/s]Testing: 100%|█████████▉| 199/200 [00:03<00:00, 91.15it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 8/1600 [00:00<00:25, 63.64 examples/s]Map:   2%|▏         | 25/1600 [00:00<00:17, 92.18 examples/s]Map:   3%|▎         | 47/1600 [00:00<00:12, 125.54 examples/s]Map:   4%|▍         | 65/1600 [00:00<00:10, 141.29 examples/s]Map:   5%|▌         | 80/1600 [00:00<00:10, 141.32 examples/s]Map:   6%|▌         | 98/1600 [00:00<00:12, 124.90 examples/s]Map:   7%|▋         | 113/1600 [00:00<00:11, 129.84 examples/s]Map:   8%|▊         | 131/1600 [00:01<00:10, 140.23 examples/s]Map:   9%|▉         | 147/1600 [00:01<00:10, 143.71 examples/s]Map:  10%|█         | 166/1600 [00:01<00:09, 155.14 examples/s]Map:  12%|█▏        | 187/1600 [00:01<00:08, 167.62 examples/s]Map:  13%|█▎        | 213/1600 [00:01<00:08, 157.79 examples/s]Map:  15%|█▍        | 234/1600 [00:01<00:08, 167.51 examples/s]Map:  16%|█▌        | 253/1600 [00:01<00:07, 169.26 examples/s]Map:  17%|█▋        | 272/1600 [00:01<00:08, 150.45 examples/s]Map:  18%|█▊        | 292/1600 [00:02<00:09, 138.45 examples/s]Map:  20%|█▉        | 314/1600 [00:02<00:09, 137.00 examples/s]Map:  21%|██        | 331/1600 [00:02<00:10, 123.03 examples/s]Map:  22%|██▏       | 347/1600 [00:02<00:09, 127.32 examples/s]Map:  23%|██▎       | 368/1600 [00:02<00:08, 145.00 examples/s]Map:  24%|██▍       | 391/1600 [00:02<00:07, 164.19 examples/s]Map:  26%|██▌       | 413/1600 [00:02<00:06, 174.42 examples/s]Map:  27%|██▋       | 437/1600 [00:02<00:07, 163.46 examples/s]Map:  29%|██▊       | 457/1600 [00:03<00:06, 168.88 examples/s]Map:  30%|███       | 482/1600 [00:03<00:06, 166.80 examples/s]Map:  32%|███▏      | 507/1600 [00:03<00:06, 162.49 examples/s]Map:  33%|███▎      | 526/1600 [00:03<00:06, 167.01 examples/s]Map:  34%|███▍      | 545/1600 [00:03<00:06, 151.84 examples/s]Map:  35%|███▌      | 565/1600 [00:03<00:06, 160.74 examples/s]Map:  37%|███▋      | 586/1600 [00:03<00:05, 171.11 examples/s]Map:  38%|███▊      | 606/1600 [00:03<00:05, 175.88 examples/s]Map:  39%|███▉      | 624/1600 [00:04<00:06, 152.02 examples/s]Map:  40%|████      | 640/1600 [00:04<00:06, 151.80 examples/s]Map:  41%|████▏     | 660/1600 [00:04<00:06, 141.36 examples/s]Map:  42%|████▏     | 677/1600 [00:04<00:07, 128.42 examples/s]Map:  43%|████▎     | 694/1600 [00:04<00:07, 120.30 examples/s]Map:  44%|████▍     | 707/1600 [00:04<00:07, 120.18 examples/s]Map:  45%|████▌     | 721/1600 [00:05<00:08, 106.55 examples/s]Map:  46%|████▌     | 735/1600 [00:05<00:07, 111.32 examples/s]Map:  47%|████▋     | 749/1600 [00:05<00:07, 116.97 examples/s]Map:  48%|████▊     | 762/1600 [00:05<00:07, 118.95 examples/s]Map:  48%|████▊     | 776/1600 [00:05<00:06, 121.98 examples/s]Map:  49%|████▉     | 789/1600 [00:05<00:06, 121.10 examples/s]Map:  50%|█████     | 806/1600 [00:05<00:06, 130.97 examples/s]Map:  52%|█████▏    | 828/1600 [00:05<00:06, 128.03 examples/s]Map:  53%|█████▎    | 848/1600 [00:05<00:05, 144.05 examples/s]Map:  54%|█████▍    | 864/1600 [00:06<00:05, 146.04 examples/s]Map:  55%|█████▌    | 880/1600 [00:06<00:04, 147.02 examples/s]Map:  56%|█████▌    | 896/1600 [00:06<00:04, 148.38 examples/s]Map:  57%|█████▋    | 913/1600 [00:06<00:04, 152.07 examples/s]Map:  58%|█████▊    | 933/1600 [00:06<00:04, 139.50 examples/s]Map:  59%|█████▉    | 950/1600 [00:06<00:04, 145.57 examples/s]Map:  61%|██████    | 970/1600 [00:06<00:03, 159.62 examples/s]Map:  62%|██████▏   | 991/1600 [00:06<00:03, 172.82 examples/s]Map:  62%|██████▏   | 991/1600 [00:18<00:03, 172.82 examples/s]Map:  62%|██████▎   | 1000/1600 [00:47<07:33,  1.32 examples/s]Map:  64%|██████▍   | 1022/1600 [00:47<04:36,  2.09 examples/s]Map:  65%|██████▌   | 1044/1600 [00:47<02:54,  3.18 examples/s]Map:  67%|██████▋   | 1069/1600 [00:47<01:48,  4.91 examples/s]Map:  68%|██████▊   | 1091/1600 [00:47<01:12,  7.01 examples/s]Map:  70%|██████▉   | 1115/1600 [00:47<00:47, 10.18 examples/s]Map:  71%|███████   | 1133/1600 [00:47<00:34, 13.47 examples/s]Map:  72%|███████▏  | 1155/1600 [00:48<00:24, 18.52 examples/s]Map:  73%|███████▎  | 1174/1600 [00:48<00:17, 24.70 examples/s]Map:  75%|███████▍  | 1194/1600 [00:48<00:12, 33.14 examples/s]Map:  76%|███████▌  | 1211/1600 [00:48<00:09, 41.84 examples/s]Map:  77%|███████▋  | 1234/1600 [00:48<00:06, 55.09 examples/s]Map:  78%|███████▊  | 1255/1600 [00:48<00:05, 64.21 examples/s]Map:  80%|███████▉  | 1273/1600 [00:48<00:04, 71.50 examples/s]Map:  81%|████████  | 1292/1600 [00:49<00:03, 80.85 examples/s]Map:  82%|████████▏ | 1305/1600 [00:49<00:03, 86.90 examples/s]Map:  82%|████████▏ | 1319/1600 [00:49<00:02, 94.63 examples/s]Map:  83%|████████▎ | 1334/1600 [00:49<00:02, 103.95 examples/s]Map:  84%|████████▍ | 1349/1600 [00:49<00:02, 112.02 examples/s]Map:  86%|████████▌ | 1371/1600 [00:49<00:02, 90.65 examples/s] Map:  87%|████████▋ | 1390/1600 [00:49<00:01, 107.57 examples/s]Map:  88%|████████▊ | 1409/1600 [00:50<00:01, 122.53 examples/s]Map:  89%|████████▉ | 1427/1600 [00:50<00:01, 133.78 examples/s]Map:  90%|█████████ | 1446/1600 [00:50<00:01, 145.88 examples/s]Map:  91%|█████████▏| 1463/1600 [00:50<00:00, 149.57 examples/s]Map:  93%|█████████▎| 1484/1600 [00:50<00:00, 135.38 examples/s]Map:  94%|█████████▎| 1499/1600 [00:50<00:00, 138.11 examples/s]Map:  95%|█████████▍| 1517/1600 [00:50<00:00, 147.10 examples/s]Map:  96%|█████████▌| 1534/1600 [00:50<00:00, 151.83 examples/s]Map:  97%|█████████▋| 1553/1600 [00:50<00:00, 159.55 examples/s]Map:  98%|█████████▊| 1571/1600 [00:51<00:00, 162.45 examples/s]Map: 100%|█████████▉| 1598/1600 [00:51<00:00, 155.80 examples/s]Map: 100%|█████████▉| 1598/1600 [01:04<00:00, 155.80 examples/s]Map: 100%|██████████| 1600/1600 [01:15<00:00,  2.08 examples/s] Map: 100%|██████████| 1600/1600 [01:15<00:00, 21.28 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   4%|▍         | 8/200 [00:00<00:02, 65.15 examples/s]Map:  10%|█         | 20/200 [00:00<00:01, 91.07 examples/s]Map:  20%|██        | 40/200 [00:00<00:01, 131.09 examples/s]Map:  28%|██▊       | 55/200 [00:00<00:01, 135.17 examples/s]Map:  38%|███▊      | 77/200 [00:00<00:00, 161.37 examples/s]Map:  50%|█████     | 101/200 [00:00<00:00, 183.76 examples/s]Map:  63%|██████▎   | 126/200 [00:00<00:00, 167.75 examples/s]Map:  76%|███████▌  | 151/200 [00:00<00:00, 163.20 examples/s]Map:  86%|████████▌ | 172/200 [00:01<00:00, 173.45 examples/s]Map:  97%|█████████▋| 194/200 [00:01<00:00, 184.36 examples/s]Map: 100%|██████████| 200/200 [00:07<00:00, 25.03 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   6%|▋         | 13/200 [00:00<00:01, 93.51 examples/s]Map:  16%|█▌        | 32/200 [00:00<00:01, 136.16 examples/s]Map:  24%|██▎       | 47/200 [00:00<00:01, 130.55 examples/s]Map:  34%|███▎      | 67/200 [00:00<00:00, 149.67 examples/s]Map:  44%|████▍     | 89/200 [00:00<00:00, 170.86 examples/s]Map:  57%|█████▊    | 115/200 [00:00<00:00, 164.76 examples/s]Map:  66%|██████▋   | 133/200 [00:00<00:00, 166.88 examples/s]Map:  80%|███████▉  | 159/200 [00:01<00:00, 167.93 examples/s]Map:  89%|████████▉ | 178/200 [00:01<00:00, 171.81 examples/s]Map: 100%|██████████| 200/200 [00:01<00:00, 182.88 examples/s]Map: 100%|██████████| 200/200 [00:08<00:00, 24.06 examples/s] 
Loading de eval data: fold_4...
Preprocess de fold_4 data for cn model
Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 1/1600 [00:00<05:04,  5.25 examples/s]Map:   0%|          | 4/1600 [00:00<02:00, 13.24 examples/s]Map:   1%|          | 12/1600 [00:00<00:49, 32.19 examples/s]Map:   2%|▏         | 24/1600 [00:00<00:27, 57.20 examples/s]Map:   2%|▏         | 35/1600 [00:00<00:22, 69.28 examples/s]Map:   3%|▎         | 46/1600 [00:00<00:19, 79.93 examples/s]Map:   4%|▎         | 56/1600 [00:00<00:19, 80.98 examples/s]Map:   4%|▍         | 68/1600 [00:01<00:17, 89.51 examples/s]Map:   5%|▌         | 80/1600 [00:01<00:16, 94.00 examples/s]Map:   6%|▌         | 93/1600 [00:01<00:16, 88.65 examples/s]Map:   6%|▋         | 103/1600 [00:01<00:16, 88.57 examples/s]Map:   7%|▋         | 113/1600 [00:01<00:16, 88.63 examples/s]Map:   8%|▊         | 122/1600 [00:01<00:17, 86.46 examples/s]Map:   8%|▊         | 134/1600 [00:01<00:18, 80.85 examples/s]Map:   9%|▉         | 146/1600 [00:01<00:18, 76.96 examples/s]Map:  10%|▉         | 154/1600 [00:02<00:18, 77.14 examples/s]Map:  10%|█         | 163/1600 [00:02<00:19, 75.62 examples/s]Map:  11%|█         | 172/1600 [00:02<00:21, 65.06 examples/s]Map:  11%|█▏        | 181/1600 [00:02<00:20, 69.85 examples/s]Map:  12%|█▏        | 192/1600 [00:02<00:18, 75.15 examples/s]Map:  12%|█▎        | 200/1600 [00:02<00:18, 74.98 examples/s]Map:  13%|█▎        | 208/1600 [00:02<00:18, 74.26 examples/s]Map:  14%|█▎        | 216/1600 [00:02<00:19, 72.05 examples/s]Map:  14%|█▍        | 225/1600 [00:03<00:18, 74.46 examples/s]Map:  15%|█▍        | 236/1600 [00:03<00:17, 78.65 examples/s]Map:  15%|█▌        | 245/1600 [00:03<00:17, 79.05 examples/s]Map:  16%|█▌        | 255/1600 [00:03<00:16, 81.93 examples/s]Map:  17%|█▋        | 268/1600 [00:03<00:16, 81.85 examples/s]Map:  17%|█▋        | 277/1600 [00:03<00:16, 81.63 examples/s]Map:  18%|█▊        | 286/1600 [00:03<00:18, 69.34 examples/s]Map:  18%|█▊        | 295/1600 [00:03<00:18, 71.58 examples/s]Map:  19%|█▉        | 305/1600 [00:04<00:17, 75.14 examples/s]Map:  20%|█▉        | 314/1600 [00:04<00:16, 78.08 examples/s]Map:  20%|██        | 322/1600 [00:04<00:16, 75.99 examples/s]Map:  21%|██        | 331/1600 [00:04<00:16, 77.49 examples/s]Map:  21%|██▏       | 341/1600 [00:04<00:15, 80.45 examples/s]Map:  22%|██▏       | 351/1600 [00:04<00:14, 83.83 examples/s]Map:  22%|██▎       | 360/1600 [00:04<00:14, 83.39 examples/s]Map:  23%|██▎       | 370/1600 [00:04<00:14, 83.13 examples/s]Map:  24%|██▍       | 380/1600 [00:05<00:14, 86.27 examples/s]Map:  24%|██▍       | 390/1600 [00:05<00:13, 87.55 examples/s]Map:  25%|██▌       | 401/1600 [00:05<00:15, 76.10 examples/s]Map:  26%|██▌       | 416/1600 [00:05<00:12, 92.60 examples/s]Map:  27%|██▋       | 430/1600 [00:05<00:11, 102.17 examples/s]Map:  28%|██▊       | 442/1600 [00:05<00:11, 104.62 examples/s]Map:  28%|██▊       | 453/1600 [00:05<00:11, 103.84 examples/s]Map:  29%|██▉       | 464/1600 [00:05<00:11, 101.66 examples/s]Map:  30%|██▉       | 478/1600 [00:05<00:10, 109.60 examples/s]Map:  31%|███       | 494/1600 [00:06<00:10, 101.65 examples/s]Map:  32%|███▏      | 506/1600 [00:06<00:13, 83.19 examples/s] Map:  32%|███▏      | 519/1600 [00:06<00:13, 81.09 examples/s]Map:  33%|███▎      | 528/1600 [00:06<00:13, 80.65 examples/s]Map:  34%|███▎      | 537/1600 [00:06<00:13, 80.66 examples/s]Map:  34%|███▍      | 546/1600 [00:06<00:13, 79.35 examples/s]Map:  35%|███▍      | 555/1600 [00:06<00:13, 79.79 examples/s]Map:  35%|███▌      | 565/1600 [00:07<00:12, 82.08 examples/s]Map:  36%|███▌      | 574/1600 [00:07<00:12, 79.68 examples/s]Map:  37%|███▋      | 586/1600 [00:07<00:12, 78.76 examples/s]Map:  37%|███▋      | 594/1600 [00:07<00:13, 74.54 examples/s]Map:  38%|███▊      | 602/1600 [00:07<00:13, 73.66 examples/s]Map:  38%|███▊      | 611/1600 [00:07<00:13, 75.85 examples/s]Map:  39%|███▉      | 620/1600 [00:07<00:16, 60.25 examples/s]Map:  39%|███▉      | 628/1600 [00:08<00:15, 63.52 examples/s]Map:  40%|███▉      | 636/1600 [00:08<00:14, 66.18 examples/s]Map:  40%|████      | 645/1600 [00:08<00:14, 67.52 examples/s]Map:  41%|████      | 654/1600 [00:08<00:13, 69.39 examples/s]Map:  41%|████▏     | 662/1600 [00:08<00:13, 70.86 examples/s]Map:  42%|████▏     | 670/1600 [00:08<00:12, 72.64 examples/s]Map:  42%|████▏     | 678/1600 [00:08<00:12, 71.04 examples/s]Map:  43%|████▎     | 686/1600 [00:08<00:12, 70.33 examples/s]Map:  43%|████▎     | 695/1600 [00:08<00:13, 69.39 examples/s]Map:  44%|████▍     | 703/1600 [00:09<00:12, 70.62 examples/s]Map:  44%|████▍     | 712/1600 [00:09<00:12, 73.10 examples/s]Map:  45%|████▌     | 721/1600 [00:09<00:11, 73.38 examples/s]Map:  46%|████▌     | 729/1600 [00:09<00:11, 73.79 examples/s]Map:  46%|████▌     | 739/1600 [00:09<00:14, 60.27 examples/s]Map:  47%|████▋     | 748/1600 [00:09<00:13, 65.16 examples/s]Map:  47%|████▋     | 757/1600 [00:09<00:12, 67.51 examples/s]Map:  48%|████▊     | 766/1600 [00:09<00:11, 70.16 examples/s]Map:  48%|████▊     | 775/1600 [00:10<00:11, 71.58 examples/s]Map:  49%|████▉     | 784/1600 [00:10<00:11, 72.86 examples/s]Map:  50%|████▉     | 792/1600 [00:10<00:11, 71.68 examples/s]Map:  50%|█████     | 801/1600 [00:10<00:11, 69.15 examples/s]Map:  51%|█████     | 813/1600 [00:10<00:12, 63.24 examples/s]Map:  51%|█████▏    | 822/1600 [00:10<00:11, 67.75 examples/s]Map:  52%|█████▏    | 832/1600 [00:10<00:10, 74.18 examples/s]Map:  52%|█████▎    | 840/1600 [00:11<00:10, 74.34 examples/s]Map:  53%|█████▎    | 850/1600 [00:11<00:11, 65.18 examples/s]Map:  54%|█████▍    | 860/1600 [00:11<00:10, 71.09 examples/s]Map:  54%|█████▍    | 870/1600 [00:11<00:09, 74.38 examples/s]Map:  55%|█████▌    | 880/1600 [00:11<00:09, 79.45 examples/s]Map:  56%|█████▌    | 892/1600 [00:11<00:09, 74.72 examples/s]Map:  56%|█████▋    | 900/1600 [00:11<00:09, 73.00 examples/s]Map:  57%|█████▋    | 910/1600 [00:12<00:10, 67.94 examples/s]Map:  58%|█████▊    | 922/1600 [00:12<00:10, 66.64 examples/s]Map:  58%|█████▊    | 930/1600 [00:12<00:09, 67.77 examples/s]Map:  59%|█████▊    | 939/1600 [00:12<00:09, 68.08 examples/s]Map:  59%|█████▉    | 947/1600 [00:12<00:09, 68.16 examples/s]Map:  60%|█████▉    | 956/1600 [00:12<00:09, 70.65 examples/s]Map:  60%|██████    | 966/1600 [00:12<00:11, 56.19 examples/s]Map:  61%|██████    | 974/1600 [00:13<00:10, 60.26 examples/s]Map:  61%|██████▏   | 981/1600 [00:13<00:10, 61.16 examples/s]Map:  62%|██████▏   | 989/1600 [00:13<00:09, 62.97 examples/s]Map:  62%|██████▏   | 997/1600 [00:13<00:09, 65.14 examples/s]Map:  62%|██████▏   | 997/1600 [00:23<00:09, 65.14 examples/s]Map:  62%|██████▎   | 1000/1600 [01:12<26:42,  2.67s/ examples]Map:  63%|██████▎   | 1009/1600 [01:12<16:39,  1.69s/ examples]Map:  64%|██████▎   | 1018/1600 [01:12<10:47,  1.11s/ examples]Map:  64%|██████▍   | 1026/1600 [01:12<07:26,  1.29 examples/s]Map:  65%|██████▍   | 1036/1600 [01:12<04:46,  1.97 examples/s]Map:  65%|██████▌   | 1045/1600 [01:12<03:16,  2.83 examples/s]Map:  66%|██████▌   | 1053/1600 [01:12<02:20,  3.88 examples/s]Map:  66%|██████▋   | 1062/1600 [01:12<01:37,  5.53 examples/s]Map:  67%|██████▋   | 1073/1600 [01:12<01:04,  8.17 examples/s]Map:  68%|██████▊   | 1081/1600 [01:13<00:48, 10.70 examples/s]Map:  68%|██████▊   | 1089/1600 [01:13<00:36, 13.97 examples/s]Map:  69%|██████▉   | 1100/1600 [01:13<00:25, 19.26 examples/s]Map:  69%|██████▉   | 1108/1600 [01:13<00:20, 23.91 examples/s]Map:  70%|██████▉   | 1115/1600 [01:13<00:16, 28.59 examples/s]Map:  70%|███████   | 1123/1600 [01:13<00:15, 30.65 examples/s]Map:  71%|███████   | 1131/1600 [01:13<00:12, 36.36 examples/s]Map:  71%|███████   | 1139/1600 [01:14<00:10, 42.60 examples/s]Map:  72%|███████▏  | 1147/1600 [01:14<00:09, 48.60 examples/s]Map:  72%|███████▏  | 1155/1600 [01:14<00:08, 53.52 examples/s]Map:  73%|███████▎  | 1164/1600 [01:14<00:07, 59.38 examples/s]Map:  73%|███████▎  | 1172/1600 [01:14<00:06, 61.69 examples/s]Map:  74%|███████▎  | 1179/1600 [01:14<00:07, 60.12 examples/s]Map:  74%|███████▍  | 1187/1600 [01:14<00:06, 59.89 examples/s]Map:  75%|███████▍  | 1194/1600 [01:14<00:06, 60.51 examples/s]Map:  75%|███████▌  | 1203/1600 [01:14<00:05, 67.72 examples/s]Map:  76%|███████▌  | 1212/1600 [01:15<00:05, 70.07 examples/s]Map:  76%|███████▋  | 1224/1600 [01:15<00:05, 68.99 examples/s]Map:  77%|███████▋  | 1232/1600 [01:15<00:05, 66.54 examples/s]Map:  78%|███████▊  | 1241/1600 [01:15<00:06, 53.71 examples/s]Map:  78%|███████▊  | 1249/1600 [01:15<00:06, 57.74 examples/s]Map:  78%|███████▊  | 1256/1600 [01:15<00:05, 57.46 examples/s]Map:  79%|███████▉  | 1265/1600 [01:16<00:05, 60.15 examples/s]Map:  80%|███████▉  | 1274/1600 [01:16<00:05, 63.42 examples/s]Map:  80%|████████  | 1281/1600 [01:16<00:04, 63.93 examples/s]Map:  81%|████████  | 1289/1600 [01:16<00:04, 65.72 examples/s]Map:  81%|████████  | 1297/1600 [01:16<00:04, 65.70 examples/s]Map:  82%|████████▏ | 1304/1600 [01:16<00:04, 65.01 examples/s]Map:  82%|████████▏ | 1312/1600 [01:16<00:04, 66.58 examples/s]Map:  82%|████████▎ | 1320/1600 [01:16<00:04, 66.11 examples/s]Map:  83%|████████▎ | 1328/1600 [01:16<00:03, 68.63 examples/s]Map:  84%|████████▎ | 1337/1600 [01:17<00:03, 70.60 examples/s]Map:  84%|████████▍ | 1345/1600 [01:17<00:03, 70.55 examples/s]Map:  85%|████████▍ | 1356/1600 [01:17<00:04, 56.35 examples/s]Map:  85%|████████▌ | 1364/1600 [01:17<00:03, 59.20 examples/s]Map:  86%|████████▌ | 1371/1600 [01:17<00:03, 60.94 examples/s]Map:  86%|████████▌ | 1379/1600 [01:17<00:03, 62.85 examples/s]Map:  87%|████████▋ | 1386/1600 [01:17<00:03, 62.23 examples/s]Map:  87%|████████▋ | 1393/1600 [01:18<00:03, 62.23 examples/s]Map:  88%|████████▊ | 1400/1600 [01:18<00:03, 63.90 examples/s]Map:  88%|████████▊ | 1408/1600 [01:18<00:02, 64.87 examples/s]Map:  88%|████████▊ | 1415/1600 [01:18<00:02, 63.75 examples/s]Map:  89%|████████▉ | 1423/1600 [01:18<00:02, 66.61 examples/s]Map:  89%|████████▉ | 1431/1600 [01:18<00:02, 67.98 examples/s]Map:  90%|████████▉ | 1439/1600 [01:18<00:02, 66.57 examples/s]Map:  91%|█████████ | 1449/1600 [01:18<00:02, 63.10 examples/s]Map:  91%|█████████ | 1456/1600 [01:18<00:02, 62.85 examples/s]Map:  92%|█████████▏| 1466/1600 [01:19<00:04, 31.46 examples/s]Map:  92%|█████████▏| 1474/1600 [01:19<00:03, 37.32 examples/s]Map:  93%|█████████▎| 1482/1600 [01:19<00:02, 43.77 examples/s]Map:  93%|█████████▎| 1491/1600 [01:19<00:02, 47.47 examples/s]Map:  94%|█████████▎| 1498/1600 [01:20<00:02, 50.35 examples/s]Map:  94%|█████████▍| 1508/1600 [01:20<00:01, 52.05 examples/s]Map:  95%|█████████▍| 1516/1600 [01:20<00:01, 56.38 examples/s]Map:  95%|█████████▌| 1524/1600 [01:20<00:01, 58.70 examples/s]Map:  96%|█████████▌| 1532/1600 [01:20<00:01, 61.97 examples/s]Map:  96%|█████████▋| 1540/1600 [01:20<00:00, 64.22 examples/s]Map:  97%|█████████▋| 1547/1600 [01:20<00:00, 65.10 examples/s]Map:  97%|█████████▋| 1555/1600 [01:20<00:00, 67.77 examples/s]Map:  98%|█████████▊| 1563/1600 [01:21<00:00, 68.77 examples/s]Map:  98%|█████████▊| 1571/1600 [01:21<00:00, 68.45 examples/s]Map:  99%|█████████▊| 1579/1600 [01:21<00:00, 51.16 examples/s]Map:  99%|█████████▉| 1587/1600 [01:21<00:00, 55.81 examples/s]Map: 100%|█████████▉| 1595/1600 [01:21<00:00, 59.33 examples/s]Map: 100%|█████████▉| 1599/1600 [01:34<00:00, 59.33 examples/s]Map: 100%|██████████| 1600/1600 [02:05<00:00,  1.87s/ examples]Map: 100%|██████████| 1600/1600 [02:05<00:00, 12.75 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▏         | 4/200 [00:00<00:07, 24.69 examples/s]Map:   6%|▋         | 13/200 [00:00<00:04, 44.09 examples/s]Map:  12%|█▏        | 23/200 [00:00<00:02, 59.81 examples/s]Map:  16%|█▋        | 33/200 [00:00<00:02, 71.12 examples/s]Map:  21%|██        | 42/200 [00:00<00:02, 76.17 examples/s]Map:  26%|██▌       | 52/200 [00:00<00:01, 82.48 examples/s]Map:  32%|███▎      | 65/200 [00:00<00:01, 94.45 examples/s]Map:  39%|███▉      | 78/200 [00:01<00:01, 86.90 examples/s]Map:  45%|████▌     | 90/200 [00:01<00:01, 81.16 examples/s]Map:  50%|█████     | 100/200 [00:01<00:01, 69.17 examples/s]Map:  55%|█████▌    | 110/200 [00:01<00:01, 74.03 examples/s]Map:  60%|██████    | 121/200 [00:01<00:01, 70.96 examples/s]Map:  64%|██████▍   | 129/200 [00:01<00:00, 72.00 examples/s]Map:  68%|██████▊   | 137/200 [00:01<00:00, 71.74 examples/s]Map:  72%|███████▎  | 145/200 [00:02<00:00, 71.18 examples/s]Map:  76%|███████▋  | 153/200 [00:02<00:00, 71.18 examples/s]Map:  81%|████████  | 162/200 [00:02<00:00, 74.54 examples/s]Map:  86%|████████▋ | 173/200 [00:02<00:00, 70.12 examples/s]Map:  90%|█████████ | 181/200 [00:02<00:00, 70.01 examples/s]Map:  94%|█████████▍| 189/200 [00:02<00:00, 69.76 examples/s]Map: 100%|██████████| 200/200 [00:02<00:00, 66.38 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 13.21 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|▏         | 3/200 [00:00<00:11, 17.66 examples/s]Map:   4%|▍         | 8/200 [00:00<00:06, 29.68 examples/s]Map:   8%|▊         | 16/200 [00:00<00:04, 44.70 examples/s]Map:  14%|█▎        | 27/200 [00:00<00:02, 62.50 examples/s]Map:  18%|█▊        | 37/200 [00:00<00:02, 72.51 examples/s]Map:  24%|██▍       | 49/200 [00:00<00:01, 82.56 examples/s]Map:  32%|███▏      | 63/200 [00:00<00:01, 96.00 examples/s]Map:  38%|███▊      | 76/200 [00:01<00:01, 86.55 examples/s]Map:  44%|████▎     | 87/200 [00:01<00:01, 77.93 examples/s]Map:  50%|████▉     | 99/200 [00:01<00:01, 67.89 examples/s]Map:  54%|█████▍    | 108/200 [00:01<00:01, 71.12 examples/s]Map:  58%|█████▊    | 117/200 [00:01<00:01, 71.41 examples/s]Map:  62%|██████▎   | 125/200 [00:01<00:01, 70.93 examples/s]Map:  68%|██████▊   | 136/200 [00:01<00:00, 66.91 examples/s]Map:  72%|███████▏  | 144/200 [00:02<00:00, 67.34 examples/s]Map:  76%|███████▋  | 153/200 [00:02<00:00, 71.01 examples/s]Map:  80%|████████  | 161/200 [00:02<00:00, 70.01 examples/s]Map:  84%|████████▍ | 169/200 [00:02<00:00, 70.86 examples/s]Map:  89%|████████▉ | 178/200 [00:02<00:00, 70.31 examples/s]Map:  93%|█████████▎| 186/200 [00:02<00:00, 71.08 examples/s]Map:  97%|█████████▋| 194/200 [00:02<00:00, 70.75 examples/s]Map:  99%|█████████▉| 198/200 [00:14<00:00, 70.75 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00,  1.99 examples/s]Map: 100%|██████████| 200/200 [00:15<00:00, 13.05 examples/s]
Loading de test data: fold_4...
Preprocess de fold_4 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 131.1607, Training accuracy: 0.9644
Macro F1-score: 0.9644
Model performance on Angry speech (in training): 
	Precision: 0.9598, Recall: 0.9550, F1_score: 0.9574
Model performance on Happy speech (in training): 
	Precision: 0.9428, Recall: 0.9475, F1_score: 0.9451
Model performance on Neutral speech (in training): 
	Precision: 0.9678, Recall: 0.9775, F1_score: 0.9726
Model performance on Sad speech (in training): 
	Precision: 0.9874, Recall: 0.9775, F1_score: 0.9824

Eval Phase: 
Validation loss: 566.2461, Validation accuracy: 0.4850
Macro F1-score: 0.4128
Model performance on Angry speech (in validation): 
	Precision: 0.9487, Recall: 0.7400, F1_score: 0.8315
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.3000, Recall: 0.1800, F1_score: 0.2250
Model performance on Sad speech (in validation): 
	Precision: 0.3846, Recall: 1.0000, F1_score: 0.5556
New best accuracy for layer 3 on epoch 1: 0.4850. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 277/1600 [00:10<00:47, 27.64it/s]Training:  35%|███▌      | 567/1600 [00:20<00:36, 28.41it/s]Training:  54%|█████▍    | 860/1600 [00:30<00:25, 28.81it/s]Training:  72%|███████▏  | 1153/1600 [00:40<00:15, 28.87it/s]Training:  90%|█████████ | 1444/1600 [00:50<00:05, 28.94it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 292/1600 [00:10<00:44, 29.15it/s]Training:  36%|███▋      | 584/1600 [00:20<00:35, 28.96it/s]Training:  55%|█████▌    | 881/1600 [00:30<00:24, 29.27it/s]Training:  74%|███████▎  | 1178/1600 [00:40<00:14, 29.28it/s]Training:  92%|█████████▏| 1472/1600 [00:50<00:04, 29.22it/s]                     Training loss: 84.4163, Training accuracy: 0.9788
Macro F1-score: 0.9788
Model performance on Angry speech (in training): 
	Precision: 0.9557, Recall: 0.9700, F1_score: 0.9628
Model performance on Happy speech (in training): 
	Precision: 0.9649, Recall: 0.9625, F1_score: 0.9637
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9925, F1_score: 0.9962

Eval Phase: 
Validation loss: 536.4156, Validation accuracy: 0.4900
Macro F1-score: 0.4272
Model performance on Angry speech (in validation): 
	Precision: 0.9130, Recall: 0.4200, F1_score: 0.5753
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.3059, Recall: 0.5200, F1_score: 0.3852
Model performance on Sad speech (in validation): 
	Precision: 0.5495, Recall: 1.0000, F1_score: 0.7092
New best accuracy for layer 3 on epoch 2: 0.4900. Model saved.
Epoch 3/100

Training Phase:
Training loss: 50.8993, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9801, Recall: 0.9850, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9849, Recall: 0.9775, F1_score: 0.9812
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 677.3457, Validation accuracy: 0.4350
Macro F1-score: 0.3599
Model performance on Angry speech (in validation): 
	Precision: 0.9697, Recall: 0.6400, F1_score: 0.7711
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0400, F1_score: 0.0741
Model performance on Neutral speech (in validation): 
	Precision: 0.2500, Recall: 0.0600, F1_score: 0.0968
Model performance on Sad speech (in validation): 
	Precision: 0.3311, Recall: 1.0000, F1_score: 0.4975
Epoch 4/100

Training Phase:
                                        Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 290/1600 [00:10<00:45, 28.98it/s]Training:  36%|███▋      | 581/1600 [00:20<00:35, 29.01it/s]Training:  55%|█████▍    | 875/1600 [00:30<00:24, 29.17it/s]Training:  73%|███████▎  | 1169/1600 [00:40<00:14, 29.24it/s]Training:  91%|█████████▏| 1463/1600 [00:50<00:04, 29.29it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 292/1600 [00:10<00:44, 29.12it/s]Training:  37%|███▋      | 589/1600 [00:20<00:34, 29.42it/s]Training:  55%|█████▌    | 886/1600 [00:30<00:24, 29.33it/s]Training:  74%|██████Training loss: 49.0924, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9825, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 436.8945, Validation accuracy: 0.5250
Macro F1-score: 0.4705
Model performance on Angry speech (in validation): 
	Precision: 0.9750, Recall: 0.7800, F1_score: 0.8667
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0800, F1_score: 0.1481
Model performance on Neutral speech (in validation): 
	Precision: 0.3529, Recall: 0.2400, F1_score: 0.2857
Model performance on Sad speech (in validation): 
	Precision: 0.4098, Recall: 1.0000, F1_score: 0.5814
New best accuracy for layer 3 on epoch 4: 0.5250. Model saved.
Epoch 5/100

Training Phase:
Training loss: 51.5668, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 474.5138, Validation accuracy: 0.5050
Macro F1-score: 0.4534
Model performance on Angry speech (in validation): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.0600, F1_score: 0.1111
Model performance on Neutral speech (in validation): 
	Precision: 0.3519, Recall: 0.3800, F1_score: 0.3654
Model performance on Sad speech (in validation): 
	Precision: 0.4505, Recall: 1.0000, F1_score: 0.6211
Epoch 6/100

Training Phase:
█▎  | 1179/1600 [00:40<00:14, 29.26it/s]Training:  92%|█████████▏| 1473/1600 [00:50<00:04, 29.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 294/1600 [00:10<00:44, 29.37it/s]Training:  37%|███▋      | 591/1600 [00:20<00:34, 29.54it/s]Training:  56%|█████▌    | 888/1600 [00:30<00:24, 28.94it/s]Training:  73%|███████▎  | 1171/1600 [00:40<00:14, 28.65it/s]Training:  73%|███████▎  | 1171/1600 [00:50<00:14, 28.65it/s]Training:  91%|█████████ | 1450/1600 [00:50<00:05, 28.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|Training loss: 26.5678, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
▊        | 292/1600 [00:10<00:44, 29.13it/s]Training:  37%|███▋      | 589/1600 [00:20<00:34, 29.39it/s]Training:  55%|█████▌    | 885/1600 [00:30<00:24, 29.31it/s]Training:  74%|███████▎  | 1178/1600 [00:40<00:14, 29.30it/s]Training:  92%|█████████▏| 1471/1600 [00:50<00:04, 29.25it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 507.3148, Validation accuracy: 0.6000
Macro F1-score: 0.5227
Model performance on Angry speech (in validation): 
	Precision: 0.9318, Recall: 0.8200, F1_score: 0.8723
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4143, Recall: 0.5800, F1_score: 0.4833
Model performance on Sad speech (in validation): 
	Precision: 0.5814, Recall: 1.0000, F1_score: 0.7353
New best accuracy for layer 3 on epoch 6: 0.6000. Model saved.
Epoch 7/100

Training Phase:
Training loss: 28.1134, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 567.0203, Validation accuracy: 0.4700
Macro F1-score: 0.4236
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.4400, F1_score: 0.6111
Model performance on Happy speech (in validation): 
	Precision: 0.8000, Recall: 0.0800, F1_score: 0.1455
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.3600, F1_score: 0.3462
Model performance on Sad speech (in validation): 
	Precision: 0.4202, Recall: 1.0000, F1_score: 0.5917
Epoch 8/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 291/1600 [00:10<00:45, 29.04it/s]Training:  37%|███▋      | 586/1600 [00:20<00:34, 29.27it/s]Training:  55%|█████▌    | 881/1600 [00:30<00:24, 29.09it/s]Training:  73%|███████▎  | 1170/1600 [00:40<00:14, 28.71it/s]Training:  91%|█████████ | 1452/1600 [00:50<00:05, 28.51it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 278/1600 [00:10<00:47, 27.73it/s]Training:  35%|███▌      | 561/1600 [00:20<00:37, 28.04it/s]Training:  53%|█████▎    | 844/1600 [00:30<00:27, 27.88it/s]Training:  70%|███████   | 1124/1600 [00:40<00:17, 27.91it/s]Training:  88%|████████▊ | 1404/1600 [00:50<00:07, 27.89it/s]                         Training loss: 38.1427, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
                                    Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 721.6778, Validation accuracy: 0.4950
Macro F1-score: 0.4166
Model performance on Angry speech (in validation): 
	Precision: 0.8571, Recall: 0.7200, F1_score: 0.7826
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.2600, F1_score: 0.2921
Model performance on Sad speech (in validation): 
	Precision: 0.4202, Recall: 1.0000, F1_score: 0.5917
Epoch 9/100

Training Phase:
Training loss: 25.8476, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 509.6531, Validation accuracy: 0.5600
Macro F1-score: 0.5009
Model performance on Angry speech (in validation): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.3780, Recall: 0.6200, F1_score: 0.4697
Model performance on Sad speech (in validation): 
	Precision: 0.5882, Recall: 1.0000, F1_score: 0.7407
Epoch 10/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 281/1600 [00:10<00:46, 28.07it/s]Training:  35%|███▌      | 562/1600 [00:20<00:37, 27.82it/s]Training:  53%|█████▎    | 842/1600 [00:30<00:27, 27.87it/s]Training:  71%|███████   | 1129/1600 [00:40<00:16, 28.17it/s]Training:  89%|████████▊ | 1418/1600 [00:50<00:06, 28.43it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 293/1600 [00:10<00:44, 29.20it/s]Training:  37%|███▋      | 586/1600 [00:20<00:34, 29.07it/s]Training:  55%|█████▌    | 880/1600 [00:30<00:24, 29.21it/s]Training:  73%|███████▎  | 1174/1600 [00:40<00:14, 29.04it/s]Training:  73%|███████▎  | 1174/1600 [00:50<00:14, 29.04it/s]Training:  91%|████Training loss: 20.2065, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 510.7630, Validation accuracy: 0.6150
Macro F1-score: 0.5437
Model performance on Angry speech (in validation): 
	Precision: 0.8776, Recall: 0.8600, F1_score: 0.8687
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.4590, Recall: 0.5600, F1_score: 0.5045
Model performance on Sad speech (in validation): 
	Precision: 0.5682, Recall: 1.0000, F1_score: 0.7246
New best accuracy for layer 3 on epoch 10: 0.6150. Model saved.
Epoch 11/100

Training Phase:
Training loss: 46.7127, Training accuracy: 0.9888
Macro F1-score: 0.9888
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9827, Recall: 0.9925, F1_score: 0.9876
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900

Eval Phase: 
█████ | 1454/1600 [00:50<00:05, 28.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 289/1600 [00:10<00:45, 28.87it/s]Training:  36%|███▌      | 578/1600 [00:20<00:35, 28.88it/s]Training:  54%|█████▍    | 867/1600 [00:30<00:25, 28.81it/s]Training:  72%|███████▏  | 1155/1600 [00:40<00:15, 28.63it/s]Training:  90%|█████████ | 1442/1600 [00:50<00:05, 28.62it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 679.1202, Validation accuracy: 0.6250
Macro F1-score: 0.5370
Model performance on Angry speech (in validation): 
	Precision: 0.7963, Recall: 0.8600, F1_score: 0.8269
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4848, Recall: 0.6400, F1_score: 0.5517
Model performance on Sad speech (in validation): 
	Precision: 0.6250, Recall: 1.0000, F1_score: 0.7692
New best accuracy for layer 3 on epoch 11: 0.6250. Model saved.
Epoch 12/100

Training Phase:
Training loss: 25.4190, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Validation loss: 458.6381, Validation accuracy: 0.7200
Macro F1-score: 0.6507
Model performance on Angry speech (in validation): 
	Precision: 0.8704, Recall: 0.9400, F1_score: 0.9038
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0600, F1_score: 0.1132
Model performance on Neutral speech (in validation): 
	Precision: 0.5057, Recall: 0.8800, F1_score: 0.6423
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
New best accuracy for layer 3 on epoch 12: 0.7200. Model saved.
Epoch 13/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 289/1600 [00:10<00:45, 28.82it/s]Training:  36%|███▋      | 581/1600 [00:20<00:35, 29.03it/s]Training:  55%|█████▍    | 875/1600 [00:30<00:24, 29.15it/s]Training:  73%|███████▎  | 1169/1600 [00:40<00:14, 29.14it/s]Training:  91%|█████████▏| 1461/1600 [00:50<00:04, 29.02it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  17%|█▋        | 279/1600 [00:10<00:47, 27.87it/s]Training:  35%|███▍      | 559/1600 [00:20<00:37, 27.89it/s]Training:  52%|█████▎    | 840/1600 [00:30<00:27, 27.98it/s]Training:  52%|█████▎    | 840/1600 [00:40<00:27, 27.98it/s]Training:  70%|███████   | 1121/1600 [00:40<00:17, 27.89it/s]Training:  88%|█████Training loss: 25.8212, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 548.1926, Validation accuracy: 0.6800
Macro F1-score: 0.6052
Model performance on Angry speech (in validation): 
	Precision: 0.8600, Recall: 0.8600, F1_score: 0.8600
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.4615, Recall: 0.8400, F1_score: 0.5957
Model performance on Sad speech (in validation): 
	Precision: 0.8621, Recall: 1.0000, F1_score: 0.9259
Epoch 14/100

Training Phase:
Training loss: 16.4178, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
███▊ | 1407/1600 [00:50<00:06, 28.13it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 291/1600 [00:10<00:45, 29.06it/s]Training:  36%|███▋      | 582/1600 [00:20<00:35, 28.89it/s]Training:  55%|█████▍    | 874/1600 [00:30<00:25, 29.03it/s]Training:  73%|███████▎  | 1166/1600 [00:40<00:14, 28.94it/s]Training:  91%|█████████ | 1455/1600 [00:50<00:05, 28.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 543.3900, Validation accuracy: 0.7000
Macro F1-score: 0.6050
Model performance on Angry speech (in validation): 
	Precision: 0.7273, Recall: 0.9600, F1_score: 0.8276
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5526, Recall: 0.8400, F1_score: 0.6667
Model performance on Sad speech (in validation): 
	Precision: 0.8621, Recall: 1.0000, F1_score: 0.9259
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7200

Test Phase: 
Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   6%|▌         | 11/200 [00:00<00:01, 104.47it/s]Testing:  11%|█         | 22/200 [00:00<00:01, 98.95it/s] Testing:  17%|█▋        | 34/200 [00:00<00:01, 104.64it/s]Testing:  22%|██▎       | 45/200 [00:00<00:01, 106.05it/s]Testing:  28%|██▊       | 56/200 [00:00<00:01, 104.12it/s]Testing:  34%|███▎      | 67/200 [00:00<00:01, 102.72it/s]Testing:  39%|███▉      | 78/200 [00:00<00:01, 103.13it/s]Testing:  44%|████▍     | 89/200 [00:00<00:01, 103.80it/s]Testing:  50%|█████     | 100/200 [00:00<00:00, 103.86it/s]Testing:  56%|█████▌    | 111/200 [00:01<00:00, 102.04it/s]Testing:  61%|██████    | 122/200 [00:01<00:00, 101.82it/s]Testing:  66%|██████▋   | 133/200 [00:01<00:00, 101.41it/s]Testing:  72%|███████▏  | 144/200 [00:01<00:00, 102.37it/s]Testing:  78%|███████▊  | 156/200 [00:01<00:00, 105.27it/s]Testing:  84%|████████▎ | 167/200 [00:01<00:00, 104.92it/s]Testing:  89%|████████▉ | 178/200 [00:01<00:00, 104.84it/s]Testing:  94%|█████████▍| 189/200 [00:01<00:00, 105.24it/s]                                                           /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Test loss: 397.6371, Test accuracy: 0.7250
Macro F1-score: 0.6269
Model performance on Angry speech (in test): 
	Precision: 0.8333, Recall: 1.0000, F1_score: 0.9091
Model performance on Happy speech (in test): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in test): 
	Precision: 0.5696, Recall: 0.9000, F1_score: 0.6977
Model performance on Sad speech (in test): 
	Precision: 0.8197, Recall: 1.0000, F1_score: 0.9009

cn, all folds layer accuracy: ['0.6550', '0.7050', '0.7300', '0.7200', '0.7250']
cn, all emo precision: {'Angry': ['0.8400', '0.9375', '0.9074', '0.9355', '0.8333'], 'Happy': ['1.0000', '1.0000', '0.0000', '0.7188', '0.0000'], 'Neutral': ['0.7333', '0.5341', '0.5275', '0.5584', '0.5696'], 'Sad': ['0.4842', '0.7581', '0.8909', '0.8167', '0.8197']}
cn, all emo recall: {'Angry': ['0.8400', '0.9000', '0.9800', '0.5800', '1.0000'], 'Happy': ['0.2000', '0.0400', '0.0000', '0.4600', '0.0000'], 'Neutral': ['0.6600', '0.9400', '0.9600', '0.8600', '0.9000'], 'Sad': ['0.9200', '0.9400', '0.9800', '0.9800', '1.0000']}
cn, all emo f1score: {'Angry': ['0.8400', '0.9184', '0.9423', '0.7160', '0.9091'], 'Happy': ['0.3333', '0.0769', '0.0000', '0.5610', '0.0000'], 'Neutral': ['0.6947', '0.6812', '0.6809', '0.6772', '0.6977'], 'Sad': ['0.6345', '0.8393', '0.9333', '0.8909', '0.9009']}
