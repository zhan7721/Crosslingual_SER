Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
------------------NEXT SCRIPT: RUNNER_DE, former setting----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5912052/matplotlib-81mr_g24 because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

======================= This is fold_0 on de =======================

Load dataset: 
Loading de train data: fold_0...
Preprocess de fold_0 data for de model
Loading cn eval data: fold_0...
Preprocess cn fold_0 data for de model
Loading cn test data: fold_0...
Preprocess cn fold_0 data for de model
Use de model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1363.4093, Training accuracy: 0.6125
Macro F1-score: 0.6112
Model performance on Angry speech (in training): 
	Precision: 0.9275, Recall: 0.3200, F1_score: 0.4758
Model performance on Happy speech (in training): 
	Precision: 0.3887, Recall: 0.8075, F1_score: 0.5248
Model performance on Neutral speech (in training): 
	Precision: 0.7056, Recall: 0.4375, F1_score: 0.5401
Model performance on Sad speech (in training): 
	Precision: 0.9243, Recall: 0.8850, F1_score: 0.9042

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:14<6:14:17, 14.05s/it]Training:  11%|█         | 173/1600 [00:24<02:44,  8.70it/s]Training:  22%|██▏       | 347/1600 [00:34<01:41, 12.31it/s]Training:  33%|███▎      | 528/1600 [00:44<01:14, 14.46it/s]Training:  45%|████▌     | 723/1600 [00:54<00:54, 16.19it/s]Training:  57%|█████▋    | 919/1600 [01:04<00:39, 17.30it/s]Training:  70%|███████   | 1124/1600 [01:14<00:25, 18.31it/s]Training:  83%|████████▎ | 1329/1600 [01:24<00:14, 18.81it/s]Training:  96%|█████████▌| 1528/1600 [01:34<00:03, 19.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 435.9809, Validation accuracy: 0.2500
Macro F1-score: 0.1042
Model performance on Angry speech (in validation): 
	Precision: 0.2632, Recall: 1.0000, F1_score: 0.4167
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
New best accuracy for layer 4 on epoch 1: 0.2500. Model saved.
Epoch 2/100

Training Phase:
Training loss: 310.1669, Training accuracy: 0.9444
Macro F1-score: 0.9444
Model performance on Angry speech (in training): 
	Precision: 0.9593, Recall: 0.9425, F1_score: 0.9508
Model performance on Happy speech (in training): 
	Precision: 0.9007, Recall: 0.9300, F1_score: 0.9151
Model performance on Neutral speech (in training): 
	Precision: 0.9532, Recall: 0.9175, F1_score: 0.9350
Model performance on Sad speech (in training): 
	Precision: 0.9658, Recall: 0.9875, F1_score: 0.9765

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 223/1600 [00:10<01:01, 22.24it/s]Training:  28%|██▊       | 446/1600 [00:20<00:52, 22.06it/s]Training:  42%|████▏     | 666/1600 [00:30<00:42, 22.01it/s]Training:  56%|█████▌    | 889/1600 [00:40<00:32, 22.09it/s]Training:  70%|██████▉   | 1112/1600 [00:50<00:22, 22.13it/s]Training:  83%|████████▎ | 1334/1600 [01:00<00:12, 22.11it/s]Training:  97%|█████████▋| 1555/1600 [01:10<00:02, 21.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 550.6734, Validation accuracy: 0.4200
Macro F1-score: 0.3712
Model performance on Angry speech (in validation): 
	Precision: 0.4937, Recall: 0.7800, F1_score: 0.6047
Model performance on Happy speech (in validation): 
	Precision: 0.1711, Recall: 0.2600, F1_score: 0.2063
Model performance on Neutral speech (in validation): 
	Precision: 0.7111, Recall: 0.6400, F1_score: 0.6737
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
New best accuracy for layer 4 on epoch 2: 0.4200. Model saved.
Epoch 3/100

Training Phase:
Training loss: 171.8967, Training accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in training): 
	Precision: 0.9823, Recall: 0.9700, F1_score: 0.9761
Model performance on Happy speech (in training): 
	Precision: 0.9407, Recall: 0.9525, F1_score: 0.9466
Model performance on Neutral speech (in training): 
	Precision: 0.9649, Recall: 0.9625, F1_score: 0.9637
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 220/1600 [00:10<01:02, 21.98it/s]Training:  28%|██▊       | 445/1600 [00:20<00:51, 22.27it/s]Training:  42%|████▏     | 670/1600 [00:30<00:41, 22.19it/s]Training:  56%|█████▌    | 893/1600 [00:40<00:31, 22.20it/s]Training:  70%|██████▉   | 1116/1600 [00:50<00:21, 22.10it/s]Training:  84%|████████▎ | 1336/1600 [01:00<00:11, 22.03it/s]Training:  97%|█████████▋| 1556/1600 [01:10<00:02, 22.00it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 643.3215, Validation accuracy: 0.2900
Macro F1-score: 0.2496
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.5200, F1_score: 0.5098
Model performance on Happy speech (in validation): 
	Precision: 0.1880, Recall: 0.5000, F1_score: 0.2732
Model performance on Neutral speech (in validation): 
	Precision: 0.4667, Recall: 0.1400, F1_score: 0.2154
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 4/100

Training Phase:
Training loss: 123.8862, Training accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in training): 
	Precision: 0.9776, Recall: 0.9800, F1_score: 0.9788
Model performance on Happy speech (in training): 
	Precision: 0.9627, Recall: 0.9675, F1_score: 0.9651
Model performance on Neutral speech (in training): 
	Precision: 0.9873, Recall: 0.9725, F1_score: 0.9798
Model performance on Sad speech (in training): 
	Precision: 0.9926, Recall: 1.0000, F1_score: 0.9963

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 218/1600 [00:10<01:03, 21.69it/s]Training:  27%|██▋       | 435/1600 [00:20<00:54, 21.54it/s]Training:  41%|████      | 655/1600 [00:30<00:43, 21.74it/s]Training:  55%|█████▍    | 875/1600 [00:40<00:33, 21.66it/s]Training:  68%|██████▊   | 1093/1600 [00:50<00:23, 21.69it/s]Training:  68%|██████▊   | 1093/1600 [01:00<00:23, 21.69it/s]Training:  82%|████████▏ | 1309/1600 [01:00<00:13, 21.60it/s]Training:  95%|█████████▌| 1524/1600 [01:10<00:03, 21.38it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 683.9856, Validation accuracy: 0.2450
Macro F1-score: 0.2055
Model performance on Angry speech (in validation): 
	Precision: 0.3882, Recall: 0.6600, F1_score: 0.4889
Model performance on Happy speech (in validation): 
	Precision: 0.0909, Recall: 0.1800, F1_score: 0.1208
Model performance on Neutral speech (in validation): 
	Precision: 0.4375, Recall: 0.1400, F1_score: 0.2121
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 5/100

Training Phase:
Training loss: 114.0908, Training accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in training): 
	Precision: 0.9727, Recall: 0.9800, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9822, Recall: 0.9650, F1_score: 0.9735
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9850, F1_score: 0.9825
Model performance on Sad speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 211/1600 [00:10<01:05, 21.09it/s]Training:  27%|██▋       | 431/1600 [00:20<00:54, 21.60it/s]Training:  41%|████      | 651/1600 [00:30<00:43, 21.70it/s]Training:  55%|█████▍    | 877/1600 [00:40<00:32, 22.05it/s]Training:  69%|██████▉   | 1103/1600 [00:50<00:22, 22.12it/s]Training:  83%|████████▎ | 1326/1600 [01:00<00:12, 22.07it/s]Training:  97%|█████████▋| 1546/1600 [01:10<00:02, 21.99it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 918.0795, Validation accuracy: 0.1900
Macro F1-score: 0.1495
Model performance on Angry speech (in validation): 
	Precision: 0.3692, Recall: 0.4800, F1_score: 0.4174
Model performance on Happy speech (in validation): 
	Precision: 0.0985, Recall: 0.2600, F1_score: 0.1429
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.0200, F1_score: 0.0377
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 6/100

Training Phase:
Training loss: 91.7356, Training accuracy: 0.9812
Macro F1-score: 0.9813
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9627, Recall: 0.9675, F1_score: 0.9651
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 217/1600 [00:10<01:03, 21.64it/s]Training:  27%|██▋       | 438/1600 [00:20<00:53, 21.90it/s]Training:  41%|████▏     | 662/1600 [00:30<00:42, 22.12it/s]Training:  55%|█████▌    | 886/1600 [00:40<00:32, 21.95it/s]Training:  69%|██████▉   | 1105/1600 [00:50<00:22, 21.92it/s]Training:  83%|████████▎ | 1324/1600 [01:00<00:12, 21.68it/s]Training:  97%|█████████▋| 1548/1600 [01:10<00:02, 21.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 709.1424, Validation accuracy: 0.2600
Macro F1-score: 0.2118
Model performance on Angry speech (in validation): 
	Precision: 0.4565, Recall: 0.4200, F1_score: 0.4375
Model performance on Happy speech (in validation): 
	Precision: 0.1915, Recall: 0.5400, F1_score: 0.2827
Model performance on Neutral speech (in validation): 
	Precision: 0.3077, Recall: 0.0800, F1_score: 0.1270
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 7/100

Training Phase:
Training loss: 75.7732, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9777, Recall: 0.9875, F1_score: 0.9826
Model performance on Neutral speech (in training): 
	Precision: 0.9899, Recall: 0.9800, F1_score: 0.9849
Model performance on Sad speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 219/1600 [00:10<01:03, 21.90it/s]Training:  27%|██▋       | 438/1600 [00:20<00:53, 21.85it/s]Training:  41%|████▏     | 660/1600 [00:30<00:42, 22.00it/s]Training:  55%|█████▌    | 884/1600 [00:40<00:32, 22.12it/s]Training:  69%|██████▉   | 1108/1600 [00:50<00:22, 22.00it/s]Training:  83%|████████▎ | 1331/1600 [01:00<00:12, 22.09it/s]Training:  97%|█████████▋| 1554/1600 [01:10<00:02, 22.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 942.3137, Validation accuracy: 0.2250
Macro F1-score: 0.1747
Model performance on Angry speech (in validation): 
	Precision: 0.4237, Recall: 0.5000, F1_score: 0.4587
Model performance on Happy speech (in validation): 
	Precision: 0.1387, Recall: 0.3800, F1_score: 0.2032
Model performance on Neutral speech (in validation): 
	Precision: 0.2500, Recall: 0.0200, F1_score: 0.0370
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 8/100

Training Phase:
Training loss: 74.7711, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9751, Recall: 0.9800, F1_score: 0.9776
Model performance on Neutral speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 219/1600 [00:10<01:03, 21.89it/s]Training:  28%|██▊       | 440/1600 [00:20<00:52, 21.98it/s]Training:  41%|████▏     | 661/1600 [00:30<00:42, 21.93it/s]Training:  55%|█████▌    | 880/1600 [00:40<00:32, 21.88it/s]Training:  69%|██████▊   | 1098/1600 [00:50<00:23, 21.78it/s]Training:  83%|████████▎ | 1322/1600 [01:00<00:12, 21.96it/s]Training:  97%|█████████▋| 1546/1600 [01:10<00:02, 21.99it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 963.0123, Validation accuracy: 0.2150
Macro F1-score: 0.1542
Model performance on Angry speech (in validation): 
	Precision: 0.4118, Recall: 0.7000, F1_score: 0.5185
Model performance on Happy speech (in validation): 
	Precision: 0.0708, Recall: 0.1600, F1_score: 0.0982
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 9/100

Training Phase:
Training loss: 78.9773, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9801, Recall: 0.9850, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 215/1600 [00:10<01:04, 21.45it/s]Training:  27%|██▋       | 433/1600 [00:20<00:53, 21.61it/s]Training:  41%|████      | 655/1600 [00:30<00:43, 21.86it/s]Training:  55%|█████▍    | 878/1600 [00:40<00:32, 22.00it/s]Training:  69%|██████▉   | 1101/1600 [00:50<00:22, 21.86it/s]Training:  82%|████████▎ | 1320/1600 [01:00<00:12, 21.85it/s]Training:  96%|█████████▋| 1543/1600 [01:10<00:02, 21.97it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 713.8054, Validation accuracy: 0.2400
Macro F1-score: 0.1968
Model performance on Angry speech (in validation): 
	Precision: 0.4070, Recall: 0.7000, F1_score: 0.5147
Model performance on Happy speech (in validation): 
	Precision: 0.0729, Recall: 0.1400, F1_score: 0.0959
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.1200, F1_score: 0.1765
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 10/100

Training Phase:
Training loss: 45.6809, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9825, F1_score: 0.9825
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 220/1600 [00:10<01:02, 21.93it/s]Training:  28%|██▊       | 445/1600 [00:20<00:51, 22.22it/s]Training:  28%|██▊       | 445/1600 [00:30<00:51, 22.22it/s]Training:  42%|████▏     | 666/1600 [00:30<00:42, 22.04it/s]Training:  56%|█████▌    | 888/1600 [00:40<00:32, 22.09it/s]Training:  69%|██████▉   | 1110/1600 [00:50<00:22, 22.00it/s]Training:  83%|████████▎ | 1329/1600 [01:00<00:12, 21.96it/s]Training:  97%|█████████▋| 1552/1600 [01:10<00:02, 22.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 836.4074, Validation accuracy: 0.2400
Macro F1-score: 0.1143
Model performance on Angry speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in validation): 
	Precision: 0.2486, Recall: 0.9200, F1_score: 0.3915
Model performance on Neutral speech (in validation): 
	Precision: 0.1818, Recall: 0.0400, F1_score: 0.0656
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Epoch 11/100

Training Phase:
Training loss: 49.7516, Training accuracy: 0.9912
Macro F1-score: 0.9913
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 222/1600 [00:10<01:02, 22.17it/s]Training:  28%|██▊       | 444/1600 [00:20<00:52, 21.85it/s]Training:  42%|████▏     | 668/1600 [00:30<00:42, 22.08it/s]Training:  56%|█████▌    | 892/1600 [00:40<00:31, 22.18it/s]Training:  70%|██████▉   | 1116/1600 [00:50<00:21, 22.07it/s]Training:  84%|████████▎ | 1337/1600 [01:00<00:11, 22.04it/s]Training:  98%|█████████▊| 1561/1600 [01:10<00:01, 22.15it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 1008.9425, Validation accuracy: 0.2100
Macro F1-score: 0.1539
Model performance on Angry speech (in validation): 
	Precision: 0.3793, Recall: 0.4400, F1_score: 0.4074
Model performance on Happy speech (in validation): 
	Precision: 0.1408, Recall: 0.4000, F1_score: 0.2083
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.4200

Test Phase: 
Testing:   0%|          | 0/200 [00:00<?, ?it/s]                                                /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Test loss: 545.2328, Test accuracy: 0.3750
Macro F1-score: 0.3438
Model performance on Angry speech (in test): 
	Precision: 0.4578, Recall: 0.7600, F1_score: 0.5714
Model performance on Happy speech (in test): 
	Precision: 0.1279, Recall: 0.2200, F1_score: 0.1618
Model performance on Neutral speech (in test): 
	Precision: 0.8387, Recall: 0.5200, F1_score: 0.6420
Model performance on Sad speech (in test): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000

======================= This is fold_1 on de =======================

Load dataset: 
Loading de train data: fold_1...
Preprocess de fold_1 data for de model
Loading cn eval data: fold_1...
Preprocess cn fold_1 data for de model
Loading cn test data: fold_1...
Preprocess cn fold_1 data for de model
Use de model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 555.9005, Training accuracy: 0.8700
Macro F1-score: 0.8692
Model performance on Angry speech (in training): 
	Precision: 0.8796, Recall: 0.8950, F1_score: 0.8872
Model performance on Happy speech (in training): 
	Precision: 0.8155, Recall: 0.7625, F1_score: 0.7881
Model performance on Neutral speech (in training): 
	Precision: 0.8406, Recall: 0.8700, F1_score: 0.8550
Model performance on Sad speech (in training): 
	Precision: 0.9407, Recall: 0.9525, F1_score: 0.9466

Eval Phase: 
Validation loss: 152.0957, Validation accuracy: 0.6700
Macro F1-score: 0.6411
Model performance on Angry speech (in validation): 
	Precision: 0.6806, Recall: 0.9800, F1_score: 0.8033
Model performance on Happy speech (in validation): 
	Precision: 0.8387, Recall: 0.5200, F1_score: 0.6420
Model performance on Neutral speech (in validation): 
	Precision: 0.4667, Recall: 0.2800, F1_score: 0.3500
Model performance on Sad speech (in validation): 
	Precision: 0.6716, Recall: 0.9000, F1_score: 0.7692
New best accuracy for layer 4 on epoch 1: 0.6700. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   9%|▉         | 141/1600 [00:10<01:43, 14.10it/s]Training:  19%|█▉        | 309/1600 [00:20<01:22, 15.65it/s]Training:  31%|███       | 493/1600 [00:30<01:05, 16.90it/s]Training:  42%|████▎     | 680/1600 [00:40<00:52, 17.60it/s]Training:  55%|█████▍    | 877/1600 [00:50<00:39, 18.35it/s]Training:  67%|██████▋   | 1079/1600 [01:00<00:27, 18.95it/s]Training:  80%|████████  | 1282/1600 [01:10<00:16, 19.37it/s]Training:  93%|█████████▎| 1490/1600 [01:20<00:05, 19.81it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 214/1600 [00:10<01:05, 21.31it/s]Training:  28%|██▊       | 443/1600 [00:20<00:52, 22.21it/s]Training:  28%|██▊       | 443/1600 [00:Training loss: 175.8254, Training accuracy: 0.9650
Macro F1-score: 0.9650
Model performance on Angry speech (in training): 
	Precision: 0.9627, Recall: 0.9675, F1_score: 0.9651
Model performance on Happy speech (in training): 
	Precision: 0.9474, Recall: 0.9450, F1_score: 0.9462
Model performance on Neutral speech (in training): 
	Precision: 0.9649, Recall: 0.9625, F1_score: 0.9637
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850

Eval Phase: 
Validation loss: 126.7613, Validation accuracy: 0.7550
Macro F1-score: 0.7462
Model performance on Angry speech (in validation): 
	Precision: 0.9773, Recall: 0.8600, F1_score: 0.9149
Model performance on Happy speech (in validation): 
	Precision: 0.6522, Recall: 0.9000, F1_score: 0.7563
Model performance on Neutral speech (in validation): 
	Precision: 0.6364, Recall: 0.4200, F1_score: 0.5060
Model performance on Sad speech (in validation): 
	Precision: 0.7778, Recall: 0.8400, F1_score: 0.8077
New best accuracy for layer 4 on epoch 2: 0.7550. Model saved.
Epoch 3/100

Training Phase:
30<00:52, 22.21it/s]Training:  41%|████      | 658/1600 [00:30<00:43, 21.71it/s]Training:  54%|█████▍    | 870/1600 [00:40<00:33, 21.48it/s]Training:  68%|██████▊   | 1082/1600 [00:50<00:24, 21.27it/s]Training:  81%|████████  | 1293/1600 [01:00<00:14, 21.21it/s]Training:  94%|█████████▍| 1510/1600 [01:10<00:04, 21.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 214/1600 [00:10<01:04, 21.34it/s]Training:  27%|██▋       | 431/1600 [00:20<00:54, 21.52it/s]Training:  40%|████      | 648/1600 [00:30<00:44, 21.50it/s]Training:  54%|█████▍    | 864/1600 [00:40<00:34, 21.51it/s]Training:  68%|██████▊   | 1080/1600 [00:50<00:24, 21.41it/s]Training:  81%|████████▏ | 1302/1600 [Training loss: 104.2061, Training accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in training): 
	Precision: 0.9775, Recall: 0.9775, F1_score: 0.9775
Model performance on Happy speech (in training): 
	Precision: 0.9576, Recall: 0.9600, F1_score: 0.9588
Model performance on Neutral speech (in training): 
	Precision: 0.9749, Recall: 0.9700, F1_score: 0.9724
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 234.3394, Validation accuracy: 0.6700
Macro F1-score: 0.6646
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Happy speech (in validation): 
	Precision: 0.5789, Recall: 0.6600, F1_score: 0.6168
Model performance on Neutral speech (in validation): 
	Precision: 0.4615, Recall: 0.3600, F1_score: 0.4045
Model performance on Sad speech (in validation): 
	Precision: 0.6866, Recall: 0.9200, F1_score: 0.7863
Epoch 4/100

Training Phase:
Training loss: 72.4188, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9849, Recall: 0.9800, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9726, Recall: 0.9775, F1_score: 0.9751
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 147.9277, Validation accuracy: 0.7450
Macro F1-score: 0.7401
Model performance on Angry speech (in validation): 
	Precision: 0.9231, Recall: 0.9600, F1_score: 0.9412
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.6600, F1_score: 0.7021
Model performance on Neutral speech (in validation): 
	Precision: 0.5106, Recall: 0.4800, F1_score: 0.4948
Model performance on Sad speech (in validation): 
	Precision: 0.7719, Recall: 0.8800, F1_score: 0.8224
Epoch 5/100

Training Phase:
01:00<00:13, 21.65it/s]Training:  95%|█████████▌| 1524/1600 [01:10<00:03, 21.72it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 216/1600 [00:10<01:04, 21.57it/s]Training:  27%|██▋       | 438/1600 [00:20<00:53, 21.91it/s]Training:  41%|████▏     | 660/1600 [00:30<00:43, 21.83it/s]Training:  55%|█████▍    | 878/1600 [00:40<00:33, 21.53it/s]Training:  69%|██████▊   | 1099/1600 [00:50<00:23, 21.73it/s]Training:  83%|████████▎ | 1322/1600 [01:00<00:12, 21.90it/s]Training:  97%|█████████▋| 1545/1600 [01:10<00:02, 22.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|  Training loss: 60.1468, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Happy speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 235.7792, Validation accuracy: 0.7200
Macro F1-score: 0.7084
Model performance on Angry speech (in validation): 
	Precision: 0.9231, Recall: 0.9600, F1_score: 0.9412
Model performance on Happy speech (in validation): 
	Precision: 0.8056, Recall: 0.5800, F1_score: 0.6744
Model performance on Neutral speech (in validation): 
	Precision: 0.4762, Recall: 0.4000, F1_score: 0.4348
Model performance on Sad speech (in validation): 
	Precision: 0.6714, Recall: 0.9400, F1_score: 0.7833
Epoch 6/100

Training Phase:
        | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 221/1600 [00:10<01:02, 22.01it/s]Training:  28%|██▊       | 442/1600 [00:20<00:53, 21.46it/s]Training:  42%|████▏     | 664/1600 [00:30<00:43, 21.77it/s]Training:  55%|█████▌    | 886/1600 [00:40<00:32, 21.79it/s]Training:  69%|██████▉   | 1105/1600 [00:50<00:22, 21.75it/s]Training:  83%|████████▎ | 1326/1600 [01:00<00:12, 21.84it/s]Training:  97%|█████████▋| 1547/1600 [01:10<00:02, 21.84it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 214/1600 [00:10<01:05, 21.31it/s]Training:  27%|██▋       | 433/1600 [00:20<00:54, 21.60it/s]Training:  41%|████      | 653/1600 [00:30<00:43, 21.77it/s]Training:  55%|█████▍    | 874/1600 [00:40<0Training loss: 50.4535, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 184.4434, Validation accuracy: 0.6950
Macro F1-score: 0.7042
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6000, F1_score: 0.7500
Model performance on Happy speech (in validation): 
	Precision: 0.5507, Recall: 0.7600, F1_score: 0.6387
Model performance on Neutral speech (in validation): 
	Precision: 0.5763, Recall: 0.6800, F1_score: 0.6239
Model performance on Sad speech (in validation): 
	Precision: 0.8810, Recall: 0.7400, F1_score: 0.8043
Epoch 7/100

Training Phase:
0:33, 21.90it/s]Training:  69%|██████▊   | 1098/1600 [00:50<00:22, 22.07it/s]Training:  83%|████████▎ | 1322/1600 [01:00<00:12, 22.00it/s]Training:  96%|█████████▋| 1541/1600 [01:10<00:02, 21.85it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 219/1600 [00:10<01:03, 21.87it/s]Training:  27%|██▋       | 439/1600 [00:20<00:53, 21.87it/s]Training:  41%|████▏     | 662/1600 [00:30<00:42, 22.06it/s]Training:  56%|█████▌    | 889/1600 [00:40<00:31, 22.28it/s]Training:  70%|██████▉   | 1118/1600 [00:50<00:21, 22.49it/s]Training:  84%|████████▍ | 1347/1600 [01:00<00:11, 22.21it/s]Training:  98%|█████████▊| 1564/1600 [01:10<00:01, 21.96it/s]                                    Training loss: 56.3953, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 215.3817, Validation accuracy: 0.6550
Macro F1-score: 0.6481
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7000, F1_score: 0.8235
Model performance on Happy speech (in validation): 
	Precision: 0.5781, Recall: 0.7400, F1_score: 0.6491
Model performance on Neutral speech (in validation): 
	Precision: 0.4444, Recall: 0.3200, F1_score: 0.3721
Model performance on Sad speech (in validation): 
	Precision: 0.6615, Recall: 0.8600, F1_score: 0.7478
Epoch 8/100

Training Phase:
Training loss: 20.9984, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 226.9344, Validation accuracy: 0.7100
Macro F1-score: 0.6984
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8000, F1_score: 0.8889
Model performance on Happy speech (in validation): 
	Precision: 0.6452, Recall: 0.8000, F1_score: 0.7143
Model performance on Neutral speech (in validation): 
	Precision: 0.5312, Recall: 0.3400, F1_score: 0.4146
Model performance on Sad speech (in validation): 
	Precision: 0.6818, Recall: 0.9000, F1_score: 0.7759
Epoch 9/100

Training Phase:
                         Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 215/1600 [00:10<01:04, 21.43it/s]Training:  27%|██▋       | 430/1600 [00:20<00:54, 21.34it/s]Training:  40%|████      | 643/1600 [00:30<00:45, 21.18it/s]Training:  53%|█████▎    | 853/1600 [00:40<00:35, 21.10it/s]Training:  67%|██████▋   | 1066/1600 [00:50<00:25, 21.15it/s]Training:  80%|███████▉  | 1279/1600 [01:00<00:15, 21.06it/s]Training:  94%|█████████▎| 1498/1600 [01:10<00:04, 21.32it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 226/1600 [00:10<01:00, 22.58it/s]Training:  14%|█▍        | 226/1600 [00:20<Training loss: 38.2392, Training accuracy: 0.9912
Macro F1-score: 0.9913
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9801, Recall: 0.9850, F1_score: 0.9825
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 407.7252, Validation accuracy: 0.6150
Macro F1-score: 0.6035
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.4800, F1_score: 0.6486
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.6800, F1_score: 0.5763
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.3600, F1_score: 0.4186
Model performance on Sad speech (in validation): 
	Precision: 0.6528, Recall: 0.9400, F1_score: 0.7705
Epoch 10/100

Training Phase:
01:00, 22.58it/s]Training:  28%|██▊       | 448/1600 [00:20<00:53, 21.63it/s]Training:  42%|████▏     | 664/1600 [00:30<00:43, 21.58it/s]Training:  55%|█████▌    | 883/1600 [00:40<00:33, 21.68it/s]Training:  69%|██████▉   | 1106/1600 [00:50<00:22, 21.88it/s]Training:  83%|████████▎ | 1329/1600 [01:00<00:12, 21.86it/s]Training:  97%|█████████▋| 1552/1600 [01:10<00:02, 21.98it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 220/1600 [00:10<01:02, 21.99it/s]Training:  28%|██▊       | 440/1600 [00:20<00:52, 21.89it/s]Training:  41%|████▏     | 662/1600 [00:30<00:42, 22.00it/s]Training:  55%|█████▌    | 884/1600 [00:40<00:32, 21.99it/s]Training:  69%|██████▉   | 1104/1600 [00:50<00:2Training loss: 50.4375, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 124.4811, Validation accuracy: 0.8150
Macro F1-score: 0.8102
Model performance on Angry speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Happy speech (in validation): 
	Precision: 0.7667, Recall: 0.9200, F1_score: 0.8364
Model performance on Neutral speech (in validation): 
	Precision: 0.7179, Recall: 0.5600, F1_score: 0.6292
Model performance on Sad speech (in validation): 
	Precision: 0.7963, Recall: 0.8600, F1_score: 0.8269
New best accuracy for layer 4 on epoch 10: 0.8150. Model saved.
Epoch 11/100

Training Phase:
Training loss: 35.0896, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
3, 21.44it/s]Training:  82%|████████▏ | 1319/1600 [01:00<00:13, 21.46it/s]Training:  96%|█████████▌| 1534/1600 [01:11<00:03, 21.42it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 212/1600 [00:10<01:05, 21.13it/s]Training:  26%|██▋       | 424/1600 [00:20<00:55, 21.14it/s]Training:  40%|███▉      | 638/1600 [00:30<00:45, 21.23it/s]Training:  53%|█████▎    | 852/1600 [00:40<00:35, 21.17it/s]Training:  67%|██████▋   | 1075/1600 [00:50<00:24, 21.55it/s]Training:  81%|████████  | 1298/1600 [01:00<00:14, 21.55it/s]Training:  95%|█████████▍| 1514/1600 [01:10<00:04, 21.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]    Validation loss: 270.2835, Validation accuracy: 0.6450
Macro F1-score: 0.6139
Model performance on Angry speech (in validation): 
	Precision: 0.9375, Recall: 0.6000, F1_score: 0.7317
Model performance on Happy speech (in validation): 
	Precision: 0.5610, Recall: 0.9200, F1_score: 0.6970
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.2000, F1_score: 0.2857
Model performance on Sad speech (in validation): 
	Precision: 0.6515, Recall: 0.8600, F1_score: 0.7414
Epoch 12/100

Training Phase:
Training loss: 17.0147, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 239.0834, Validation accuracy: 0.7350
Macro F1-score: 0.7237
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7600, F1_score: 0.8636
Model performance on Happy speech (in validation): 
	Precision: 0.6104, Recall: 0.9400, F1_score: 0.7402
Model performance on Neutral speech (in validation): 
	Precision: 0.6786, Recall: 0.3800, F1_score: 0.4872
Model performance on Sad speech (in validation): 
	Precision: 0.7544, Recall: 0.8600, F1_score: 0.8037
Epoch 13/100

Training Phase:
                                               Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 215/1600 [00:10<01:04, 21.49it/s]Training:  27%|██▋       | 430/1600 [00:20<00:55, 21.10it/s]Training:  40%|████      | 640/1600 [00:30<00:45, 21.02it/s]Training:  53%|█████▎    | 854/1600 [00:40<00:35, 21.14it/s]Training:  67%|██████▋   | 1068/1600 [00:50<00:25, 21.12it/s]Training:  80%|████████  | 1285/1600 [01:00<00:14, 21.30it/s]Training:  94%|█████████▍| 1502/1600 [01:10<00:04, 21.24it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 213/1600 [00:10<01:05, 21.24it/s]Training:  27%|██▋       | 431/1600 [00:20<00:54, 21.54it/s]Training:  41%|████      | 649/1600 [00:30<00:44, 21.14Training loss: 41.1314, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 273.0228, Validation accuracy: 0.7150
Macro F1-score: 0.7081
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Happy speech (in validation): 
	Precision: 0.7000, Recall: 0.7000, F1_score: 0.7000
Model performance on Neutral speech (in validation): 
	Precision: 0.5263, Recall: 0.4000, F1_score: 0.4545
Model performance on Sad speech (in validation): 
	Precision: 0.6620, Recall: 0.9400, F1_score: 0.7769
Epoch 14/100

Training Phase:
it/s]Training:  54%|█████▍    | 860/1600 [00:40<00:35, 21.11it/s]Training:  67%|██████▋   | 1071/1600 [00:50<00:25, 21.09it/s]Training:  80%|████████  | 1283/1600 [01:00<00:15, 21.12it/s]Training:  94%|█████████▎| 1497/1600 [01:10<00:04, 21.21it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 215/1600 [00:10<01:04, 21.47it/s]Training:  27%|██▋       | 430/1600 [00:20<00:55, 21.12it/s]Training:  40%|███▉      | 639/1600 [00:30<00:45, 20.96it/s]Training:  53%|█████▎    | 850/1600 [00:40<00:35, 20.98it/s]Training:  67%|██████▋   | 1067/1600 [00:50<00:25, 21.20it/s]Training:  81%|████████  | 1290/1600 [01:00<00:14, 21.57it/s]Training:  95%|█████████▍| 1514/1600 [01:1Training loss: 13.6029, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 412.4932, Validation accuracy: 0.6900
Macro F1-score: 0.6836
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8000, F1_score: 0.8889
Model performance on Happy speech (in validation): 
	Precision: 0.6531, Recall: 0.6400, F1_score: 0.6465
Model performance on Neutral speech (in validation): 
	Precision: 0.4750, Recall: 0.3800, F1_score: 0.4222
Model performance on Sad speech (in validation): 
	Precision: 0.6620, Recall: 0.9400, F1_score: 0.7769
Epoch 15/100

Training Phase:
Training loss: 33.7165, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 191.8112, Validation accuracy: 0.7350
Macro F1-score: 0.7216
Model performance on Angry speech (in validation): 
	Precision: 0.9565, Recall: 0.8800, F1_score: 0.9167
Model performance on Happy speech (in validation): 
	Precision: 0.7451, Recall: 0.7600, F1_score: 0.7525
Model performance on Neutral speech (in validation): 
	Precision: 0.5455, Recall: 0.3600, F1_score: 0.4337
Model performance on Sad speech (in validation): 
	Precision: 0.6714, Recall: 0.9400, F1_score: 0.7833
Epoch 16/100

Training Phase:
0<00:03, 21.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 215/1600 [00:10<01:04, 21.42it/s]Training:  27%|██▋       | 430/1600 [00:20<00:55, 21.25it/s]Training:  40%|████      | 645/1600 [00:30<00:44, 21.32it/s]Training:  54%|█████▍    | 863/1600 [00:40<00:34, 21.48it/s]Training:  68%|██████▊   | 1081/1600 [00:50<00:24, 21.54it/s]Training:  81%|████████  | 1298/1600 [01:00<00:14, 21.25it/s]Training:  94%|█████████▍| 1510/1600 [01:10<00:04, 21.22it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 214/1600 [00:10<01:04, 21Training loss: 3.2081, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 262.9613, Validation accuracy: 0.6950
Macro F1-score: 0.6893
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7000, F1_score: 0.8235
Model performance on Happy speech (in validation): 
	Precision: 0.6316, Recall: 0.7200, F1_score: 0.6729
Model performance on Neutral speech (in validation): 
	Precision: 0.5526, Recall: 0.4200, F1_score: 0.4773
Model performance on Sad speech (in validation): 
	Precision: 0.6714, Recall: 0.9400, F1_score: 0.7833
Epoch 17/100

Training Phase:
.34it/s]Training:  27%|██▋       | 432/1600 [00:20<00:54, 21.59it/s]Training:  41%|████      | 650/1600 [00:30<00:44, 21.49it/s]Training:  54%|█████▍    | 864/1600 [00:40<00:34, 21.30it/s]Training:  67%|██████▋   | 1076/1600 [00:50<00:24, 21.26it/s]Training:  81%|████████  | 1297/1600 [01:00<00:14, 21.52it/s]Training:  95%|█████████▍| 1518/1600 [01:10<00:03, 21.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 212/1600 [00:10<01:05, 21.18it/s]Training:  27%|██▋       | 430/1600 [00:20<00:54, 21.49it/s]Training:  40%|████      | 648/1600 [00:30<00:44, 21.38it/s]Training:  54%|█████▍    | 861/1600 [00:40<00:34, 21.20it/s]Training:  67%|██████▋   | 1073/1600 [00:50<00:24, 21.18it/s]TTraining loss: 37.0310, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937

Eval Phase: 
Validation loss: 119.8918, Validation accuracy: 0.7650
Macro F1-score: 0.7514
Model performance on Angry speech (in validation): 
	Precision: 0.8654, Recall: 0.9000, F1_score: 0.8824
Model performance on Happy speech (in validation): 
	Precision: 0.8000, Recall: 0.8000, F1_score: 0.8000
Model performance on Neutral speech (in validation): 
	Precision: 0.6364, Recall: 0.4200, F1_score: 0.5060
Model performance on Sad speech (in validation): 
	Precision: 0.7231, Recall: 0.9400, F1_score: 0.8174
Epoch 18/100

Training Phase:
Training loss: 10.0616, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
raining:  80%|████████  | 1285/1600 [01:00<00:14, 21.10it/s]Training:  94%|█████████▍| 1500/1600 [01:10<00:04, 21.22it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 213/1600 [00:10<01:05, 21.26it/s]Training:  27%|██▋       | 428/1600 [00:20<00:54, 21.35it/s]Training:  40%|████      | 643/1600 [00:30<00:44, 21.30it/s]Training:  54%|█████▎    | 856/1600 [00:40<00:35, 21.18it/s]Training:  67%|██████▋   | 1075/1600 [00:50<00:24, 21.44it/s]Training:  81%|████████  | 1298/1600 [01:00<00:13, 21.71it/s]Training:  95%|█████████▌| 1521/1600 [01:10<00:03, 21.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                     Validation loss: 181.1119, Validation accuracy: 0.7450
Macro F1-score: 0.7347
Model performance on Angry speech (in validation): 
	Precision: 0.9565, Recall: 0.8800, F1_score: 0.9167
Model performance on Happy speech (in validation): 
	Precision: 0.7917, Recall: 0.7600, F1_score: 0.7755
Model performance on Neutral speech (in validation): 
	Precision: 0.5882, Recall: 0.4000, F1_score: 0.4762
Model performance on Sad speech (in validation): 
	Precision: 0.6528, Recall: 0.9400, F1_score: 0.7705
Epoch 19/100

Training Phase:
Training loss: 18.5313, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 208.8578, Validation accuracy: 0.7000
Macro F1-score: 0.6987
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7000, F1_score: 0.8235
Model performance on Happy speech (in validation): 
	Precision: 0.6111, Recall: 0.6600, F1_score: 0.6346
Model performance on Neutral speech (in validation): 
	Precision: 0.5556, Recall: 0.5000, F1_score: 0.5263
Model performance on Sad speech (in validation): 
	Precision: 0.7121, Recall: 0.9400, F1_score: 0.8103
Epoch 20/100

Training Phase:
                              Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 214/1600 [00:10<01:05, 21.31it/s]Training:  27%|██▋       | 437/1600 [00:20<00:53, 21.87it/s]Training:  41%|████▏     | 660/1600 [00:30<00:43, 21.60it/s]Training:  55%|█████▍    | 878/1600 [00:40<00:33, 21.67it/s]Training:  68%|██████▊   | 1096/1600 [00:50<00:23, 21.43it/s]Training:  82%|████████▏ | 1312/1600 [01:00<00:13, 21.48it/s]Training:  96%|█████████▌| 1528/1600 [01:11<00:03, 21.20it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 210/1600 [00:10<01:06, 20.94it/s]Training:  27%|██▋       | 427/1600 [00:20<00:54, 21.36it/s]Training:  41%|████      | 650/1600 [00:30<00:43, 21.75it/s]TraininTraining loss: 19.4241, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 440.2114, Validation accuracy: 0.5700
Macro F1-score: 0.5326
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.2000, F1_score: 0.3333
Model performance on Happy speech (in validation): 
	Precision: 0.4234, Recall: 0.9400, F1_score: 0.5839
Model performance on Neutral speech (in validation): 
	Precision: 0.6667, Recall: 0.3200, F1_score: 0.4324
Model performance on Sad speech (in validation): 
	Precision: 0.7455, Recall: 0.8200, F1_score: 0.7810
Epoch 21/100

Training Phase:
g:  55%|█████▍    | 873/1600 [00:40<00:33, 21.79it/s]Training:  68%|██████▊   | 1096/1600 [00:50<00:22, 21.94it/s]Training:  83%|████████▎ | 1321/1600 [01:00<00:12, 22.10it/s]Training:  97%|█████████▋| 1546/1600 [01:10<00:02, 22.04it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 222/1600 [00:10<01:02, 22.17it/s]Training:  28%|██▊       | 444/1600 [00:20<00:52, 21.83it/s]Training:  42%|████▏     | 665/1600 [00:30<00:42, 21.92it/s]Training:  55%|█████▌    | 886/1600 [00:40<00:32, 21.95it/s]Training:  69%|██████▉   | 1110/1600 [00:50<00:22, 22.10it/s]Training:  83%|████████▎ | 1334/1600 [01:00<00:12, 21.91it/s]Training:  97%|█████████▋| 1550/1600 [01:10<00:02Training loss: 29.9321, Training accuracy: 0.9938
Macro F1-score: 0.9938
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 226.6097, Validation accuracy: 0.7050
Macro F1-score: 0.6782
Model performance on Angry speech (in validation): 
	Precision: 0.9714, Recall: 0.6800, F1_score: 0.8000
Model performance on Happy speech (in validation): 
	Precision: 0.5714, Recall: 0.9600, F1_score: 0.7164
Model performance on Neutral speech (in validation): 
	Precision: 0.7222, Recall: 0.2600, F1_score: 0.3824
Model performance on Sad speech (in validation): 
	Precision: 0.7302, Recall: 0.9200, F1_score: 0.8142
Epoch 22/100

Training Phase:
Training loss: 13.6471, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 201.3131, Validation accuracy: 0.7250
Macro F1-score: 0.7178
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.7632, Recall: 0.5800, F1_score: 0.6591
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.4400, F1_score: 0.4681
Model performance on Sad speech (in validation): 
	Precision: 0.6712, Recall: 0.9800, F1_score: 0.7967
Epoch 23/100

Training Phase:
, 21.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 218/1600 [00:10<01:03, 21.71it/s]Training:  27%|██▋       | 437/1600 [00:20<00:53, 21.81it/s]Training:  41%|████      | 656/1600 [00:30<00:43, 21.62it/s]Training:  54%|█████▍    | 870/1600 [00:40<00:33, 21.51it/s]Training:  68%|██████▊   | 1084/1600 [00:50<00:24, 21.45it/s]Training:  81%|████████  | 1298/1600 [01:00<00:14, 21.41it/s]Training:  95%|█████████▍| 1514/1600 [01:10<00:04, 21.45it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 210/1600 [00:10<01:06, 20.98it/sTraining loss: 13.9207, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 180.9872, Validation accuracy: 0.7400
Macro F1-score: 0.7381
Model performance on Angry speech (in validation): 
	Precision: 0.9474, Recall: 0.7200, F1_score: 0.8182
Model performance on Happy speech (in validation): 
	Precision: 0.6379, Recall: 0.7400, F1_score: 0.6852
Model performance on Neutral speech (in validation): 
	Precision: 0.6512, Recall: 0.5600, F1_score: 0.6022
Model performance on Sad speech (in validation): 
	Precision: 0.7705, Recall: 0.9400, F1_score: 0.8468
Epoch 24/100

Training Phase:
]Training:  26%|██▋       | 420/1600 [00:20<00:56, 20.96it/s]Training:  40%|████      | 644/1600 [00:30<00:44, 21.62it/s]Training:  54%|█████▍    | 868/1600 [00:40<00:33, 21.58it/s]Training:  68%|██████▊   | 1084/1600 [00:50<00:23, 21.54it/s]Training:  81%|████████▏ | 1300/1600 [01:00<00:13, 21.56it/s]Training:  95%|█████████▍| 1516/1600 [01:10<00:03, 21.49it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 212/1600 [00:10<01:05, 21.14it/s]Training:  27%|██▋       | 425/1600 [00:20<00:55, 21.21it/s]Training:  40%|████      | 642/1600 [00:30<00:44, 21.42it/s]Training:  54%|█████▎    | 859/1600 [00:40<00:34, 21.52it/s]Training:  67%|██████▋   | 1076/1600 [00:50<00:24, 21.43it/s]TrainiTraining loss: 14.5670, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 179.4955, Validation accuracy: 0.7500
Macro F1-score: 0.7348
Model performance on Angry speech (in validation): 
	Precision: 0.8846, Recall: 0.9200, F1_score: 0.9020
Model performance on Happy speech (in validation): 
	Precision: 0.7647, Recall: 0.7800, F1_score: 0.7723
Model performance on Neutral speech (in validation): 
	Precision: 0.5758, Recall: 0.3800, F1_score: 0.4578
Model performance on Sad speech (in validation): 
	Precision: 0.7188, Recall: 0.9200, F1_score: 0.8070
Epoch 25/100

Training Phase:
Training loss: 1.8756, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
ng:  81%|████████  | 1291/1600 [01:00<00:14, 21.44it/s]Training:  94%|█████████▍| 1506/1600 [01:10<00:04, 21.38it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 222/1600 [00:10<01:02, 22.17it/s]Training:  14%|█▍        | 222/1600 [00:20<01:02, 22.17it/s]Training:  27%|██▋       | 438/1600 [00:20<00:53, 21.62it/s]Training:  41%|████      | 651/1600 [00:30<00:44, 21.25it/s]Training:  54%|█████▍    | 867/1600 [00:40<00:34, 21.35it/s]Training:  68%|██████▊   | 1089/1600 [00:50<00:23, 21.62it/s]Training:  82%|████████▏ | 1311/1600 [01:00<00:13, 21.78it/s]Training:  96%|█████████▌| 1532/1600 [01:10<00:03, 21.82it/s]                                                             Evaluating:Validation loss: 224.7410, Validation accuracy: 0.7300
Macro F1-score: 0.7134
Model performance on Angry speech (in validation): 
	Precision: 0.8679, Recall: 0.9200, F1_score: 0.8932
Model performance on Happy speech (in validation): 
	Precision: 0.7778, Recall: 0.7000, F1_score: 0.7368
Model performance on Neutral speech (in validation): 
	Precision: 0.5455, Recall: 0.3600, F1_score: 0.4337
Model performance on Sad speech (in validation): 
	Precision: 0.6812, Recall: 0.9400, F1_score: 0.7899
Epoch 26/100

Training Phase:
Training loss: 25.4364, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 249.6543, Validation accuracy: 0.7450
Macro F1-score: 0.7173
Model performance on Angry speech (in validation): 
	Precision: 0.9057, Recall: 0.9600, F1_score: 0.9320
Model performance on Happy speech (in validation): 
	Precision: 0.8200, Recall: 0.8200, F1_score: 0.8200
Model performance on Neutral speech (in validation): 
	Precision: 0.5200, Recall: 0.2600, F1_score: 0.3467
Model performance on Sad speech (in validation): 
	Precision: 0.6528, Recall: 0.9400, F1_score: 0.7705
Epoch 27/100

Training Phase:
   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 216/1600 [00:10<01:04, 21.53it/s]Training:  27%|██▋       | 432/1600 [00:20<00:54, 21.40it/s]Training:  41%|████      | 650/1600 [00:30<00:44, 21.55it/s]Training:  54%|█████▍    | 868/1600 [00:40<00:34, 21.52it/s]Training:  68%|██████▊   | 1087/1600 [00:50<00:23, 21.64it/s]Training:  82%|████████▏ | 1312/1600 [01:00<00:13, 21.92it/s]Training:  96%|█████████▋| 1540/1600 [01:10<00:02, 22.19it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 226/1600 [00:10<01:00, 22.56it/s]Training:  28%|██▊       | 452/1600 [00:20<00:51, 22.47it/s]Training:  42%|Training loss: 13.8776, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 316.3161, Validation accuracy: 0.6900
Macro F1-score: 0.6810
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Model performance on Happy speech (in validation): 
	Precision: 0.6471, Recall: 0.6600, F1_score: 0.6535
Model performance on Neutral speech (in validation): 
	Precision: 0.4865, Recall: 0.3600, F1_score: 0.4138
Model performance on Sad speech (in validation): 
	Precision: 0.6575, Recall: 0.9600, F1_score: 0.7805
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.8150

Test Phase: 
███▏     | 677/1600 [00:30<00:41, 22.48it/s]Training:  42%|████▏     | 677/1600 [00:40<00:41, 22.48it/s]Training:  56%|█████▋    | 902/1600 [00:40<00:31, 22.18it/s]Training:  70%|███████   | 1123/1600 [00:50<00:21, 22.14it/s]Training:  84%|████████▍ | 1349/1600 [01:00<00:11, 22.27it/s]Training:  98%|█████████▊| 1575/1600 [01:10<00:01, 22.17it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]                                                Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Test loss: 170.4417, Test accuracy: 0.7500
Macro F1-score: 0.7566
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Happy speech (in test): 
	Precision: 0.6441, Recall: 0.7600, F1_score: 0.6972
Model performance on Neutral speech (in test): 
	Precision: 0.5926, Recall: 0.6400, F1_score: 0.6154
Model performance on Sad speech (in test): 
	Precision: 0.8478, Recall: 0.7800, F1_score: 0.8125

======================= This is fold_2 on de =======================

Load dataset: 
Loading de train data: fold_2...
Preprocess de fold_2 data for de model
Loading cn eval data: fold_2...
Preprocess cn fold_2 data for de model
Loading cn test data: fold_2...
Preprocess cn fold_2 data for de model
Use de model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 431.7640, Training accuracy: 0.9031
Macro F1-score: 0.9034
Model performance on Angry speech (in training): 
	Precision: 0.9059, Recall: 0.8900, F1_score: 0.8979
Model performance on Happy speech (in training): 
	Precision: 0.8539, Recall: 0.8475, F1_score: 0.8507
Model performance on Neutral speech (in training): 
	Precision: 0.8706, Recall: 0.9250, F1_score: 0.8970
Model performance on Sad speech (in training): 
	Precision: 0.9870, Recall: 0.9500, F1_score: 0.9682

Eval Phase: 
Validation loss: 380.9902, Validation accuracy: 0.4700
Macro F1-score: 0.4150
Model performance on Angry speech (in validation): 
	Precision: 0.4653, Recall: 0.9400, F1_score: 0.6225
Model performance on Happy speech (in validation): 
	Precision: 0.2500, Recall: 0.0600, F1_score: 0.0968
Model performance on Neutral speech (in validation): 
	Precision: 0.5122, Recall: 0.4200, F1_score: 0.4615
Model performance on Sad speech (in validation): 
	Precision: 0.5000, Recall: 0.4600, F1_score: 0.4792
New best accuracy for layer 4 on epoch 1: 0.4700. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   9%|▊         | 138/1600 [00:10<01:46, 13.77it/s]Training:  19%|█▉        | 301/1600 [00:20<01:25, 15.23it/s]Training:  30%|██▉       | 477/1600 [00:30<01:08, 16.29it/s]Training:  42%|████▏     | 670/1600 [00:40<00:53, 17.45it/s]Training:  54%|█████▍    | 869/1600 [00:50<00:39, 18.31it/s]Training:  67%|██████▋   | 1068/1600 [01:00<00:28, 18.64it/s]Training:  79%|███████▉  | 1261/1600 [01:10<00:18, 18.68it/s]Training:  91%|█████████ | 1455/1600 [01:20<00:07, 18.86it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 211/1600 [00:10<01:06, 21.02it/s]Training:  26%|██▋       | 423/1600 [00:20<00:55, 21.11it/s]Training:  40%|███▉      | 637/1600 [00:Training loss: 104.4256, Training accuracy: 0.9769
Macro F1-score: 0.9769
Model performance on Angry speech (in training): 
	Precision: 0.9774, Recall: 0.9725, F1_score: 0.9749
Model performance on Happy speech (in training): 
	Precision: 0.9575, Recall: 0.9575, F1_score: 0.9575
Model performance on Neutral speech (in training): 
	Precision: 0.9752, Recall: 0.9825, F1_score: 0.9788
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 309.1072, Validation accuracy: 0.5450
Macro F1-score: 0.5029
Model performance on Angry speech (in validation): 
	Precision: 0.5056, Recall: 0.9000, F1_score: 0.6475
Model performance on Happy speech (in validation): 
	Precision: 0.4667, Recall: 0.1400, F1_score: 0.2154
Model performance on Neutral speech (in validation): 
	Precision: 0.6471, Recall: 0.4400, F1_score: 0.5238
Model performance on Sad speech (in validation): 
	Precision: 0.5645, Recall: 0.7000, F1_score: 0.6250
New best accuracy for layer 4 on epoch 2: 0.5450. Model saved.
Epoch 3/100

Training Phase:
30<00:45, 21.23it/s]Training:  54%|█████▎    | 859/1600 [00:40<00:34, 21.60it/s]Training:  68%|██████▊   | 1081/1600 [00:50<00:24, 21.36it/s]Training:  81%|████████  | 1295/1600 [01:00<00:14, 21.37it/s]Training:  94%|█████████▍| 1510/1600 [01:10<00:04, 21.41it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 215/1600 [00:10<01:04, 21.48it/s]Training:  27%|██▋       | 430/1600 [00:20<00:54, 21.36it/s]Training:  41%|████      | 651/1600 [00:30<00:43, 21.68it/s]Training:  55%|█████▍    | 874/1600 [00:40<00:33, 21.88it/s]Training:  69%|██████▊   | 1097/1600 [00:50<00:22, 22.02it/s]Training:  82%|████████▎ | 1320/1600 [01:00<00:12, 22.08it/s]Training:  96%|█████████▋Training loss: 74.4047, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in training): 
	Precision: 0.9724, Recall: 0.9675, F1_score: 0.9699
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 317.7730, Validation accuracy: 0.5350
Macro F1-score: 0.5024
Model performance on Angry speech (in validation): 
	Precision: 0.5176, Recall: 0.8800, F1_score: 0.6519
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.1800, F1_score: 0.2647
Model performance on Neutral speech (in validation): 
	Precision: 0.5714, Recall: 0.4800, F1_score: 0.5217
Model performance on Sad speech (in validation): 
	Precision: 0.5455, Recall: 0.6000, F1_score: 0.5714
Epoch 4/100

Training Phase:
Training loss: 43.6368, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 295.4439, Validation accuracy: 0.5750
Macro F1-score: 0.5729
Model performance on Angry speech (in validation): 
	Precision: 0.5455, Recall: 0.6000, F1_score: 0.5714
Model performance on Happy speech (in validation): 
	Precision: 0.5319, Recall: 0.5000, F1_score: 0.5155
Model performance on Neutral speech (in validation): 
	Precision: 0.6579, Recall: 0.5000, F1_score: 0.5682
Model performance on Sad speech (in validation): 
	Precision: 0.5833, Recall: 0.7000, F1_score: 0.6364
New best accuracy for layer 4 on epoch 4: 0.5750. Model saved.
Epoch 5/100

Training Phase:
| 1542/1600 [01:10<00:02, 22.01it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 219/1600 [00:10<01:03, 21.89it/s]Training:  27%|██▋       | 439/1600 [00:20<00:52, 21.95it/s]Training:  42%|████▏     | 664/1600 [00:30<00:42, 22.19it/s]Training:  42%|████▏     | 664/1600 [00:40<00:42, 22.19it/s]Training:  55%|█████▌    | 886/1600 [00:40<00:32, 21.95it/s]Training:  69%|██████▉   | 1102/1600 [00:50<00:22, 21.82it/s]Training:  82%|████████▏ | 1318/1600 [01:00<00:13, 21.69it/s]Training:  96%|█████████▌| 1533/1600 [01:10<00:03, 21.56it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|Training loss: 54.2463, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Validation loss: 346.5214, Validation accuracy: 0.5450
Macro F1-score: 0.5436
Model performance on Angry speech (in validation): 
	Precision: 0.5294, Recall: 0.5400, F1_score: 0.5347
Model performance on Happy speech (in validation): 
	Precision: 0.4902, Recall: 0.5000, F1_score: 0.4950
Model performance on Neutral speech (in validation): 
	Precision: 0.6154, Recall: 0.4800, F1_score: 0.5393
Model performance on Sad speech (in validation): 
	Precision: 0.5593, Recall: 0.6600, F1_score: 0.6055
Epoch 6/100

Training Phase:
          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 213/1600 [00:10<01:05, 21.29it/s]Training:  27%|██▋       | 427/1600 [00:20<00:54, 21.34it/s]Training:  40%|████      | 642/1600 [00:30<00:44, 21.38it/s]Training:  54%|█████▎    | 857/1600 [00:40<00:34, 21.34it/s]Training:  67%|██████▋   | 1070/1600 [00:50<00:24, 21.21it/s]Training:  80%|████████  | 1285/1600 [01:00<00:14, 21.30it/s]Training:  94%|█████████▍| 1510/1600 [01:10<00:04, 21.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 218/1600 [00:10<01:03, 21.76it/s]Training:  27%|██▋       | 436/1600 [00:20<00:54, 21.40it/s]Training:  40%|████      | 648/1600 [00:30<00:44, 21.28it/s]Training:  54%|█████▍    | 863/1600 [00:40<00:Training loss: 33.2767, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 322.6758, Validation accuracy: 0.6050
Macro F1-score: 0.6028
Model performance on Angry speech (in validation): 
	Precision: 0.5636, Recall: 0.6200, F1_score: 0.5905
Model performance on Happy speech (in validation): 
	Precision: 0.5208, Recall: 0.5000, F1_score: 0.5102
Model performance on Neutral speech (in validation): 
	Precision: 0.6552, Recall: 0.7600, F1_score: 0.7037
Model performance on Sad speech (in validation): 
	Precision: 0.6923, Recall: 0.5400, F1_score: 0.6067
New best accuracy for layer 4 on epoch 6: 0.6050. Model saved.
Epoch 7/100

Training Phase:
34, 21.35it/s]Training:  68%|██████▊   | 1084/1600 [00:50<00:23, 21.60it/s]Training:  82%|████████▏ | 1305/1600 [01:00<00:13, 21.62it/s]Training:  95%|█████████▌| 1522/1600 [01:10<00:03, 21.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 217/1600 [00:10<01:03, 21.66it/s]Training:  27%|██▋       | 437/1600 [00:20<00:53, 21.84it/s]Training:  41%|████▏     | 661/1600 [00:30<00:42, 22.08it/s]Training:  55%|█████▌    | 885/1600 [00:40<00:32, 22.09it/s]Training:  69%|██████▉   | 1110/1600 [00:50<00:22, 22.21it/s]Training:  83%|████████▎ | 1335/1600 [01:00<00:11, 22.14it/s]Training:  97%|█████████▋| 1557/1600 [01:10<00:01, 22.15it/s]                                      Training loss: 36.7555, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 366.7215, Validation accuracy: 0.5900
Macro F1-score: 0.5852
Model performance on Angry speech (in validation): 
	Precision: 0.5469, Recall: 0.7000, F1_score: 0.6140
Model performance on Happy speech (in validation): 
	Precision: 0.5385, Recall: 0.4200, F1_score: 0.4719
Model performance on Neutral speech (in validation): 
	Precision: 0.6923, Recall: 0.5400, F1_score: 0.6067
Model performance on Sad speech (in validation): 
	Precision: 0.6034, Recall: 0.7000, F1_score: 0.6481
Epoch 8/100

Training Phase:
Training loss: 29.6712, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 370.2452, Validation accuracy: 0.5550
Macro F1-score: 0.5508
Model performance on Angry speech (in validation): 
	Precision: 0.5814, Recall: 0.5000, F1_score: 0.5376
Model performance on Happy speech (in validation): 
	Precision: 0.4783, Recall: 0.6600, F1_score: 0.5546
Model performance on Neutral speech (in validation): 
	Precision: 0.6111, Recall: 0.6600, F1_score: 0.6346
Model performance on Sad speech (in validation): 
	Precision: 0.5882, Recall: 0.4000, F1_score: 0.4762
Epoch 9/100

Training Phase:
                       Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 228/1600 [00:10<01:00, 22.76it/s]Training:  28%|██▊       | 456/1600 [00:20<00:51, 22.03it/s]Training:  42%|████▏     | 672/1600 [00:30<00:42, 21.76it/s]Training:  55%|█████▌    | 887/1600 [00:40<00:33, 21.53it/s]Training:  69%|██████▉   | 1107/1600 [00:50<00:22, 21.69it/s]Training:  83%|████████▎ | 1327/1600 [01:00<00:12, 21.76it/s]Training:  97%|█████████▋| 1547/1600 [01:11<00:02, 21.69it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 218/1600 [00:10<01:03, 21.72it/s]Training:  27%|██▋       | 437/1600 [00Training loss: 25.7506, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 404.3123, Validation accuracy: 0.5900
Macro F1-score: 0.5839
Model performance on Angry speech (in validation): 
	Precision: 0.5484, Recall: 0.6800, F1_score: 0.6071
Model performance on Happy speech (in validation): 
	Precision: 0.5366, Recall: 0.4400, F1_score: 0.4835
Model performance on Neutral speech (in validation): 
	Precision: 0.7273, Recall: 0.4800, F1_score: 0.5783
Model performance on Sad speech (in validation): 
	Precision: 0.5938, Recall: 0.7600, F1_score: 0.6667
Epoch 10/100

Training Phase:
:20<00:53, 21.77it/s]Training:  41%|████      | 657/1600 [00:30<00:43, 21.84it/s]Training:  55%|█████▍    | 877/1600 [00:40<00:33, 21.82it/s]Training:  69%|██████▉   | 1100/1600 [00:50<00:22, 21.97it/s]Training:  83%|████████▎ | 1323/1600 [01:00<00:12, 22.01it/s]Training:  97%|█████████▋| 1545/1600 [01:10<00:02, 22.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 214/1600 [00:10<01:04, 21.34it/s]Training:  27%|██▋       | 438/1600 [00:20<00:52, 21.94it/s]Training:  41%|████▏     | 662/1600 [00:30<00:42, 21.98it/s]Training:  55%|█████▌    | 883/1600 [00:40<00:32, 21.90it/s]Training:  69%|██████▉   | 1102/1600 [00:50<00:22, 21.86it/s]Training:  83%|████████▎ | 1322/1Training loss: 18.1918, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 453.2563, Validation accuracy: 0.5500
Macro F1-score: 0.5245
Model performance on Angry speech (in validation): 
	Precision: 0.5195, Recall: 0.8000, F1_score: 0.6299
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.2600, F1_score: 0.3421
Model performance on Neutral speech (in validation): 
	Precision: 0.6786, Recall: 0.3800, F1_score: 0.4872
Model performance on Sad speech (in validation): 
	Precision: 0.5507, Recall: 0.7600, F1_score: 0.6387
Epoch 11/100

Training Phase:
Training loss: 8.5873, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 452.0120, Validation accuracy: 0.5700
Macro F1-score: 0.5589
Model performance on Angry speech (in validation): 
	Precision: 0.5410, Recall: 0.6600, F1_score: 0.5946
Model performance on Happy speech (in validation): 
	Precision: 0.5116, Recall: 0.4400, F1_score: 0.4731
Model performance on Neutral speech (in validation): 
	Precision: 0.7600, Recall: 0.3800, F1_score: 0.5067
Model performance on Sad speech (in validation): 
	Precision: 0.5634, Recall: 0.8000, F1_score: 0.6612
Epoch 12/100

Training Phase:
600 [01:00<00:12, 21.90it/s]Training:  96%|█████████▋| 1542/1600 [01:10<00:02, 21.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 221/1600 [00:10<01:02, 22.03it/s]Training:  28%|██▊       | 442/1600 [00:20<00:53, 21.51it/s]Training:  42%|████▏     | 668/1600 [00:30<00:42, 21.96it/s]Training:  56%|█████▌    | 894/1600 [00:40<00:32, 22.00it/s]Training:  70%|██████▉   | 1115/1600 [00:50<00:22, 22.03it/s]Training:  84%|████████▎ | 1336/1600 [01:00<00:11, 22.01it/s]Training:  97%|█████████▋| 1556/1600 [01:10<00:02, 21.95it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   Training loss: 14.8477, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 538.0179, Validation accuracy: 0.5400
Macro F1-score: 0.4953
Model performance on Angry speech (in validation): 
	Precision: 0.6667, Recall: 0.1200, F1_score: 0.2034
Model performance on Happy speech (in validation): 
	Precision: 0.4694, Recall: 0.9200, F1_score: 0.6216
Model performance on Neutral speech (in validation): 
	Precision: 0.6667, Recall: 0.4800, F1_score: 0.5581
Model performance on Sad speech (in validation): 
	Precision: 0.5614, Recall: 0.6400, F1_score: 0.5981
Epoch 13/100

Training Phase:
0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 217/1600 [00:10<01:03, 21.69it/s]Training:  27%|██▋       | 436/1600 [00:20<00:53, 21.80it/s]Training:  41%|████      | 655/1600 [00:30<00:43, 21.73it/s]Training:  55%|█████▍    | 879/1600 [00:40<00:32, 21.98it/s]Training:  69%|██████▉   | 1103/1600 [00:50<00:22, 21.73it/s]Training:  83%|████████▎ | 1323/1600 [01:00<00:12, 21.81it/s]Training:  97%|█████████▋| 1545/1600 [01:10<00:02, 21.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 225/1600 [00:10<01:01, 22.39it/s]Training:  28%|██▊       | 449/1600 [00:20<00:52, 22.11it/s]Training:  42%|████▏     | 669/1600 [00:30<00:42, 21.80it/s]Training:  56%|█████▌    | 888/1600 [00Training loss: 25.6831, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 424.4982, Validation accuracy: 0.5700
Macro F1-score: 0.5663
Model performance on Angry speech (in validation): 
	Precision: 0.5833, Recall: 0.4200, F1_score: 0.4884
Model performance on Happy speech (in validation): 
	Precision: 0.4789, Recall: 0.6800, F1_score: 0.5620
Model performance on Neutral speech (in validation): 
	Precision: 0.7273, Recall: 0.4800, F1_score: 0.5783
Model performance on Sad speech (in validation): 
	Precision: 0.5833, Recall: 0.7000, F1_score: 0.6364
Epoch 14/100

Training Phase:
:40<00:32, 21.81it/s]Training:  69%|██████▉   | 1110/1600 [00:50<00:22, 21.93it/s]Training:  83%|████████▎ | 1332/1600 [01:00<00:12, 21.87it/s]Training:  97%|█████████▋| 1550/1600 [01:11<00:02, 21.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|█▏        | 187/1600 [00:10<01:15, 18.68it/s]Training:  24%|██▎       | 379/1600 [00:20<01:04, 18.94it/s]Training:  37%|███▋      | 598/1600 [00:30<00:49, 20.27it/s]Training:  51%|█████     | 817/1600 [00:40<00:37, 20.68it/s]Training:  65%|██████▍   | 1034/1600 [00:50<00:26, 21.04it/s]Training:  78%|███████▊  | 1252/1600 [01:00<00:16, 21.30it/s]Training:  92%|█████████▏| 1473/1600 [01:10<00:05, 21.55it/s]                                     Training loss: 1.8107, Training accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 494.1602, Validation accuracy: 0.5950
Macro F1-score: 0.5937
Model performance on Angry speech (in validation): 
	Precision: 0.5769, Recall: 0.6000, F1_score: 0.5882
Model performance on Happy speech (in validation): 
	Precision: 0.5091, Recall: 0.5600, F1_score: 0.5333
Model performance on Neutral speech (in validation): 
	Precision: 0.7742, Recall: 0.4800, F1_score: 0.5926
Model performance on Sad speech (in validation): 
	Precision: 0.5968, Recall: 0.7400, F1_score: 0.6607
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6050

Test Phase: 
                        Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]                                                Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Test loss: 332.6989, Test accuracy: 0.6350
Macro F1-score: 0.6355
Model performance on Angry speech (in test): 
	Precision: 0.5273, Recall: 0.5800, F1_score: 0.5524
Model performance on Happy speech (in test): 
	Precision: 0.5000, Recall: 0.5000, F1_score: 0.5000
Model performance on Neutral speech (in test): 
	Precision: 0.7368, Recall: 0.8400, F1_score: 0.7850
Model performance on Sad speech (in test): 
	Precision: 0.8158, Recall: 0.6200, F1_score: 0.7045

======================= This is fold_3 on de =======================

Load dataset: 
Loading de train data: fold_3...
Preprocess de fold_3 data for de model
Loading cn eval data: fold_3...
Preprocess cn fold_3 data for de model
Loading cn test data: fold_3...
Preprocess cn fold_3 data for de model
Use de model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 331.4323, Training accuracy: 0.9300
Macro F1-score: 0.9302
Model performance on Angry speech (in training): 
	Precision: 0.9388, Recall: 0.9200, F1_score: 0.9293
Model performance on Happy speech (in training): 
	Precision: 0.8809, Recall: 0.8875, F1_score: 0.8842
Model performance on Neutral speech (in training): 
	Precision: 0.9082, Recall: 0.9400, F1_score: 0.9238
Model performance on Sad speech (in training): 
	Precision: 0.9949, Recall: 0.9725, F1_score: 0.9836

Eval Phase: 
Validation loss: 300.9455, Validation accuracy: 0.5200
Macro F1-score: 0.5070
Model performance on Angry speech (in validation): 
	Precision: 0.6129, Recall: 0.3800, F1_score: 0.4691
Model performance on Happy speech (in validation): 
	Precision: 0.4144, Recall: 0.9200, F1_score: 0.5714
Model performance on Neutral speech (in validation): 
	Precision: 0.5862, Recall: 0.3400, F1_score: 0.4304
Model performance on Sad speech (in validation): 
	Precision: 0.7586, Recall: 0.4400, F1_score: 0.5570
New best accuracy for layer 4 on epoch 1: 0.5200. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   8%|▊         | 132/1600 [00:10<01:52, 13.07it/s]Training:  19%|█▉        | 303/1600 [00:20<01:24, 15.42it/s]Training:  30%|███       | 487/1600 [00:30<01:06, 16.75it/s]Training:  42%|████▏     | 671/1600 [00:40<00:53, 17.23it/s]Training:  54%|█████▎    | 858/1600 [00:50<00:41, 17.74it/s]Training:  66%|██████▌   | 1057/1600 [01:00<00:29, 18.47it/s]Training:  79%|███████▊  | 1258/1600 [01:10<00:18, 19.00it/s]Training:  91%|█████████▏| 1463/1600 [01:20<00:07, 19.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 221/1600 [00:10<01:02, 22.04it/s]Training:  28%|██▊       | 442/1600 [00:20<00:52, 21.91it/s]Training:  42%|████▏     | 665/1600 Training loss: 95.1802, Training accuracy: 0.9781
Macro F1-score: 0.9781
Model performance on Angry speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774
Model performance on Happy speech (in training): 
	Precision: 0.9648, Recall: 0.9600, F1_score: 0.9624
Model performance on Neutral speech (in training): 
	Precision: 0.9727, Recall: 0.9800, F1_score: 0.9763
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 191.7916, Validation accuracy: 0.6350
Macro F1-score: 0.6289
Model performance on Angry speech (in validation): 
	Precision: 0.6667, Recall: 0.7600, F1_score: 0.7103
Model performance on Happy speech (in validation): 
	Precision: 0.6000, Recall: 0.7800, F1_score: 0.6783
Model performance on Neutral speech (in validation): 
	Precision: 0.5682, Recall: 0.5000, F1_score: 0.5319
Model performance on Sad speech (in validation): 
	Precision: 0.7353, Recall: 0.5000, F1_score: 0.5952
New best accuracy for layer 4 on epoch 2: 0.6350. Model saved.
Epoch 3/100

Training Phase:
[00:30<00:42, 22.09it/s]Training:  56%|█████▌    | 888/1600 [00:40<00:32, 22.06it/s]Training:  69%|██████▉   | 1110/1600 [00:50<00:22, 22.09it/s]Training:  83%|████████▎ | 1332/1600 [01:00<00:12, 22.04it/s]Training:  97%|█████████▋| 1552/1600 [01:10<00:02, 22.01it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 222/1600 [00:10<01:02, 22.16it/s]Training:  14%|█▍        | 222/1600 [00:20<01:02, 22.16it/s]Training:  28%|██▊       | 443/1600 [00:20<00:52, 21.96it/s]Training:  41%|████▏     | 663/1600 [00:30<00:42, 21.96it/s]Training:  55%|█████▌    | 883/1600 [00:40<00:33, 21.65it/s]Training:  68%|██████▊   | 1096/1600 [00:50<00:23, 21.53it/s]Training:  82%|████████▏ | 1317/16Training loss: 71.0937, Training accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Happy speech (in training): 
	Precision: 0.9848, Recall: 0.9725, F1_score: 0.9786
Model performance on Neutral speech (in training): 
	Precision: 0.9802, Recall: 0.9900, F1_score: 0.9851
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 280.1342, Validation accuracy: 0.5850
Macro F1-score: 0.5706
Model performance on Angry speech (in validation): 
	Precision: 0.8235, Recall: 0.5600, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.4752, Recall: 0.9600, F1_score: 0.6358
Model performance on Neutral speech (in validation): 
	Precision: 0.5306, Recall: 0.5200, F1_score: 0.5253
Model performance on Sad speech (in validation): 
	Precision: 0.9375, Recall: 0.3000, F1_score: 0.4545
Epoch 4/100

Training Phase:
Training loss: 65.8327, Training accuracy: 0.9881
Macro F1-score: 0.9881
Model performance on Angry speech (in training): 
	Precision: 0.9827, Recall: 0.9925, F1_score: 0.9876
Model performance on Happy speech (in training): 
	Precision: 0.9849, Recall: 0.9775, F1_score: 0.9812
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 197.1393, Validation accuracy: 0.6450
Macro F1-score: 0.6429
Model performance on Angry speech (in validation): 
	Precision: 0.8621, Recall: 0.5000, F1_score: 0.6329
Model performance on Happy speech (in validation): 
	Precision: 0.5513, Recall: 0.8600, F1_score: 0.6719
Model performance on Neutral speech (in validation): 
	Precision: 0.5763, Recall: 0.6800, F1_score: 0.6239
Model performance on Sad speech (in validation): 
	Precision: 0.7941, Recall: 0.5400, F1_score: 0.6429
New best accuracy for layer 4 on epoch 4: 0.6450. Model saved.
Epoch 5/100

Training Phase:
00 [01:00<00:13, 21.71it/s]Training:  96%|█████████▋| 1540/1600 [01:10<00:02, 21.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 223/1600 [00:10<01:01, 22.21it/s]Training:  28%|██▊       | 447/1600 [00:20<00:51, 22.26it/s]Training:  42%|████▏     | 672/1600 [00:30<00:41, 22.33it/s]Training:  56%|█████▌    | 897/1600 [00:40<00:32, 21.96it/s]Training:  70%|██████▉   | 1112/1600 [00:50<00:22, 21.68it/s]Training:  83%|████████▎ | 1328/1600 [01:00<00:12, 21.63it/s]Training:  96%|█████████▋| 1544/1600 [01:10<00:02, 21.56it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0Training loss: 32.7458, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 316.6739, Validation accuracy: 0.5900
Macro F1-score: 0.5546
Model performance on Angry speech (in validation): 
	Precision: 0.9032, Recall: 0.5600, F1_score: 0.6914
Model performance on Happy speech (in validation): 
	Precision: 0.5465, Recall: 0.9400, F1_score: 0.6912
Model performance on Neutral speech (in validation): 
	Precision: 0.4667, Recall: 0.7000, F1_score: 0.5600
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1600, F1_score: 0.2759
Epoch 6/100

Training Phase:
%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 216/1600 [00:10<01:04, 21.50it/s]Training:  27%|██▋       | 434/1600 [00:20<00:53, 21.60it/s]Training:  27%|██▋       | 434/1600 [00:30<00:53, 21.60it/s]Training:  40%|████      | 648/1600 [00:30<00:44, 21.44it/s]Training:  54%|█████▍    | 865/1600 [00:40<00:34, 21.51it/s]Training:  68%|██████▊   | 1085/1600 [00:50<00:23, 21.68it/s]Training:  82%|████████▏ | 1305/1600 [01:00<00:13, 21.52it/s]Training:  96%|█████████▌| 1533/1600 [01:10<00:03, 21.91it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 225/1600 [00:10<01:01, 22.43it/s]Training:  28%|██▊       | 451/1600 [00:20<00:51, 22.48it/s]Training:  42%|████▏     | 677/1600 [00:30<00:Training loss: 50.1503, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 352.7016, Validation accuracy: 0.5950
Macro F1-score: 0.5792
Model performance on Angry speech (in validation): 
	Precision: 0.8750, Recall: 0.5600, F1_score: 0.6829
Model performance on Happy speech (in validation): 
	Precision: 0.4800, Recall: 0.9600, F1_score: 0.6400
Model performance on Neutral speech (in validation): 
	Precision: 0.5472, Recall: 0.5800, F1_score: 0.5631
Model performance on Sad speech (in validation): 
	Precision: 0.9333, Recall: 0.2800, F1_score: 0.4308
Epoch 7/100

Training Phase:
42, 21.97it/s]Training:  56%|█████▌    | 891/1600 [00:40<00:32, 21.69it/s]Training:  69%|██████▉   | 1104/1600 [00:50<00:23, 21.48it/s]Training:  82%|████████▎ | 1320/1600 [01:00<00:13, 21.50it/s]Training:  96%|█████████▌| 1536/1600 [01:11<00:02, 21.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 216/1600 [00:10<01:04, 21.48it/s]Training:  27%|██▋       | 433/1600 [00:20<00:54, 21.58it/s]Training:  41%|████      | 655/1600 [00:30<00:43, 21.84it/s]Training:  55%|█████▍    | 877/1600 [00:40<00:33, 21.61it/s]Training:  69%|██████▊   | 1097/1600 [00:50<00:23, 21.73it/s]Training:  82%|████████▏ | 1317/1600 [01:00<00:13, 21.74it/s]Training:  96%|█████████▌| 15Training loss: 9.8589, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 510.8392, Validation accuracy: 0.5300
Macro F1-score: 0.4912
Model performance on Angry speech (in validation): 
	Precision: 0.7778, Recall: 0.5600, F1_score: 0.6512
Model performance on Happy speech (in validation): 
	Precision: 0.4083, Recall: 0.9800, F1_score: 0.5765
Model performance on Neutral speech (in validation): 
	Precision: 0.6053, Recall: 0.4600, F1_score: 0.5227
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.1200, F1_score: 0.2143
Epoch 8/100

Training Phase:
Training loss: 39.9315, Training accuracy: 0.9912
Macro F1-score: 0.9913
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 413.1144, Validation accuracy: 0.5650
Macro F1-score: 0.5291
Model performance on Angry speech (in validation): 
	Precision: 0.7805, Recall: 0.6400, F1_score: 0.7033
Model performance on Happy speech (in validation): 
	Precision: 0.4706, Recall: 0.9600, F1_score: 0.6316
Model performance on Neutral speech (in validation): 
	Precision: 0.5208, Recall: 0.5000, F1_score: 0.5102
Model performance on Sad speech (in validation): 
	Precision: 0.8889, Recall: 0.1600, F1_score: 0.2712
Epoch 9/100

Training Phase:
39/1600 [01:10<00:02, 21.86it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 223/1600 [00:10<01:01, 22.30it/s]Training:  28%|██▊       | 446/1600 [00:20<00:52, 22.06it/s]Training:  42%|████▏     | 665/1600 [00:30<00:43, 21.32it/s]Training:  55%|█████▌    | 884/1600 [00:40<00:33, 21.54it/s]Training:  69%|██████▉   | 1103/1600 [00:51<00:23, 21.56it/s]Training:  83%|████████▎ | 1329/1600 [01:01<00:12, 21.88it/s]Training:  97%|█████████▋| 1555/1600 [01:11<00:02, 21.77it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 214/1600Training loss: 35.5110, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 329.5672, Validation accuracy: 0.5800
Macro F1-score: 0.5649
Model performance on Angry speech (in validation): 
	Precision: 0.8235, Recall: 0.5600, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.4947, Recall: 0.9400, F1_score: 0.6483
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.5200, F1_score: 0.5098
Model performance on Sad speech (in validation): 
	Precision: 0.7895, Recall: 0.3000, F1_score: 0.4348
Epoch 10/100

Training Phase:
 [00:10<01:05, 21.32it/s]Training:  27%|██▋       | 433/1600 [00:20<00:53, 21.61it/s]Training:  41%|████      | 656/1600 [00:30<00:43, 21.90it/s]Training:  55%|█████▍    | 879/1600 [00:40<00:32, 22.00it/s]Training:  69%|██████▉   | 1101/1600 [00:50<00:22, 21.97it/s]Training:  83%|████████▎ | 1321/1600 [01:00<00:12, 21.93it/s]Training:  96%|█████████▋| 1544/1600 [01:10<00:02, 22.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 220/1600 [00:10<01:03, 21.89it/s]Training:  27%|██▋       | 439/1600 [00:20<00:53, 21.86it/s]Training:  41%|████      | 658/1600 [00:30<00:43, 21.79it/s]Training:  55%|█████▌    | 881/1600 [00:40<00:32, 21.97it/s]Training:  69%|██████▉   | 1104/1600 [00:50<Training loss: 18.1406, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 402.7833, Validation accuracy: 0.5800
Macro F1-score: 0.5642
Model performance on Angry speech (in validation): 
	Precision: 0.8750, Recall: 0.5600, F1_score: 0.6829
Model performance on Happy speech (in validation): 
	Precision: 0.4608, Recall: 0.9400, F1_score: 0.6184
Model performance on Neutral speech (in validation): 
	Precision: 0.5385, Recall: 0.5600, F1_score: 0.5490
Model performance on Sad speech (in validation): 
	Precision: 0.9286, Recall: 0.2600, F1_score: 0.4062
Epoch 11/100

Training Phase:
Training loss: 22.9489, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
00:22, 21.59it/s]Training:  83%|████████▎ | 1327/1600 [01:00<00:12, 21.81it/s]Training:  97%|█████████▋| 1550/1600 [01:11<00:02, 21.80it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 227/1600 [00:10<01:00, 22.63it/s]Training:  28%|██▊       | 454/1600 [00:20<00:51, 22.36it/s]Training:  42%|████▏     | 677/1600 [00:30<00:41, 22.30it/s]Training:  56%|█████▋    | 900/1600 [00:40<00:31, 22.09it/s]Training:  70%|██████▉   | 1118/1600 [00:50<00:21, 21.92it/s]Training:  83%|████████▎ | 1335/1600 [01:00<00:12, 21.67it/s]Training:  97%|█████████▋| 1555/1600 [01:10<00:02, 21.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?itValidation loss: 385.1585, Validation accuracy: 0.5700
Macro F1-score: 0.5444
Model performance on Angry speech (in validation): 
	Precision: 0.5938, Recall: 0.7600, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.4756, Recall: 0.7800, F1_score: 0.5909
Model performance on Neutral speech (in validation): 
	Precision: 0.6190, Recall: 0.5200, F1_score: 0.5652
Model performance on Sad speech (in validation): 
	Precision: 0.9167, Recall: 0.2200, F1_score: 0.3548
Epoch 12/100

Training Phase:
Training loss: 11.9407, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 405.2665, Validation accuracy: 0.5800
Macro F1-score: 0.5651
Model performance on Angry speech (in validation): 
	Precision: 0.7556, Recall: 0.6800, F1_score: 0.7158
Model performance on Happy speech (in validation): 
	Precision: 0.4731, Recall: 0.8800, F1_score: 0.6154
Model performance on Neutral speech (in validation): 
	Precision: 0.5349, Recall: 0.4600, F1_score: 0.4946
Model performance on Sad speech (in validation): 
	Precision: 0.7895, Recall: 0.3000, F1_score: 0.4348
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6450

Test Phase: 
/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 218/1600 [00:10<01:03, 21.67it/s]Training:  27%|██▋       | 435/1600 [00:20<00:54, 21.52it/s]Training:  41%|████      | 650/1600 [00:30<00:44, 21.49it/s]Training:  54%|█████▍    | 869/1600 [00:40<00:33, 21.64it/s]Training:  68%|██████▊   | 1088/1600 [00:50<00:23, 21.40it/s]Training:  81%|████████▏ | 1303/1600 [01:00<00:13, 21.43it/s]Training:  95%|█████████▍| 1518/1600 [01:10<00:03, 21.36it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]                                                Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Test loss: 174.0036, Test accuracy: 0.7050
Macro F1-score: 0.7017
Model performance on Angry speech (in test): 
	Precision: 0.8387, Recall: 0.5200, F1_score: 0.6420
Model performance on Happy speech (in test): 
	Precision: 0.6111, Recall: 0.8800, F1_score: 0.7213
Model performance on Neutral speech (in test): 
	Precision: 0.6452, Recall: 0.8000, F1_score: 0.7143
Model performance on Sad speech (in test): 
	Precision: 0.8857, Recall: 0.6200, F1_score: 0.7294

======================= This is fold_4 on de =======================

Load dataset: 
Loading de train data: fold_4...
Preprocess de fold_4 data for de model
Loading cn eval data: fold_4...
Preprocess cn fold_4 data for de model
Loading cn test data: fold_4...
Preprocess cn fold_4 data for de model
Use de model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 314.5667, Training accuracy: 0.9269
Macro F1-score: 0.9271
Model performance on Angry speech (in training): 
	Precision: 0.9359, Recall: 0.9125, F1_score: 0.9241
Model performance on Happy speech (in training): 
	Precision: 0.8750, Recall: 0.8925, F1_score: 0.8837
Model performance on Neutral speech (in training): 
	Precision: 0.9095, Recall: 0.9300, F1_score: 0.9197
Model performance on Sad speech (in training): 
	Precision: 0.9898, Recall: 0.9725, F1_score: 0.9811

Eval Phase: 
Validation loss: 356.3984, Validation accuracy: 0.5200
Macro F1-score: 0.5381
Model performance on Angry speech (in validation): 
	Precision: 0.4259, Recall: 0.4600, F1_score: 0.4423
Model performance on Happy speech (in validation): 
	Precision: 0.3133, Recall: 0.5200, F1_score: 0.3910
Model performance on Neutral speech (in validation): 
	Precision: 0.8636, Recall: 0.3800, F1_score: 0.5278
Model performance on Sad speech (in validation): 
	Precision: 0.8780, Recall: 0.7200, F1_score: 0.7912
New best accuracy for layer 4 on epoch 1: 0.5200. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   8%|▊         | 130/1600 [00:10<01:53, 12.99it/s]Training:  18%|█▊        | 295/1600 [00:20<01:26, 15.04it/s]Training:  30%|███       | 480/1600 [00:30<01:07, 16.57it/s]Training:  42%|████▏     | 666/1600 [00:40<00:53, 17.37it/s]Training:  54%|█████▎    | 857/1600 [00:50<00:41, 17.96it/s]Training:  66%|██████▌   | 1048/1600 [01:00<00:30, 18.25it/s]Training:  78%|███████▊  | 1246/1600 [01:10<00:18, 18.74it/s]Training:  90%|█████████ | 1446/1600 [01:20<00:08, 19.14it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 221/1600 [00:10<01:02, 22.00it/s]Training:  28%|██▊       | 441/1600 [00:20<00:53, 21.85it/s]Training:  42%|████▏     | 665/1600 [0Training loss: 96.0490, Training accuracy: 0.9775
Macro F1-score: 0.9775
Model performance on Angry speech (in training): 
	Precision: 0.9823, Recall: 0.9725, F1_score: 0.9774
Model performance on Happy speech (in training): 
	Precision: 0.9603, Recall: 0.9675, F1_score: 0.9639
Model performance on Neutral speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925

Eval Phase: 
Validation loss: 434.8776, Validation accuracy: 0.4850
Macro F1-score: 0.5035
Model performance on Angry speech (in validation): 
	Precision: 0.4762, Recall: 0.4000, F1_score: 0.4348
Model performance on Happy speech (in validation): 
	Precision: 0.3158, Recall: 0.6000, F1_score: 0.4138
Model performance on Neutral speech (in validation): 
	Precision: 0.5897, Recall: 0.4600, F1_score: 0.5169
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.4800, F1_score: 0.6486
Epoch 3/100

Training Phase:
0:30<00:42, 22.06it/s]Training:  56%|█████▌    | 889/1600 [00:40<00:32, 22.19it/s]Training:  70%|██████▉   | 1113/1600 [00:50<00:22, 22.13it/s]Training:  83%|████████▎ | 1334/1600 [01:00<00:12, 22.08it/s]Training:  97%|█████████▋| 1554/1600 [01:10<00:02, 22.01it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▎        | 219/1600 [00:10<01:03, 21.87it/s]Training:  27%|██▋       | 438/1600 [00:20<00:53, 21.54it/s]Training:  41%|████      | 652/1600 [00:30<00:44, 21.45it/s]Training:  54%|█████▍    | 869/1600 [00:40<00:33, 21.54it/s]Training:  69%|██████▊   | 1098/1600 [00:50<00:22, 22.01it/s]Training:  83%|████████▎ | 1327/1600 [01:00<00:12, 22.06it/s]Training:  97%|████████Training loss: 62.9607, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9725, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 577.8928, Validation accuracy: 0.4000
Macro F1-score: 0.4028
Model performance on Angry speech (in validation): 
	Precision: 0.3167, Recall: 0.7600, F1_score: 0.4471
Model performance on Happy speech (in validation): 
	Precision: 0.1860, Recall: 0.1600, F1_score: 0.1720
Model performance on Neutral speech (in validation): 
	Precision: 0.8333, Recall: 0.3000, F1_score: 0.4412
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.3800, F1_score: 0.5507
Epoch 4/100

Training Phase:
Training loss: 49.6083, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9752, Recall: 0.9850, F1_score: 0.9801
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 581.5227, Validation accuracy: 0.4650
Macro F1-score: 0.4846
Model performance on Angry speech (in validation): 
	Precision: 0.4364, Recall: 0.4800, F1_score: 0.4571
Model performance on Happy speech (in validation): 
	Precision: 0.2841, Recall: 0.5000, F1_score: 0.3623
Model performance on Neutral speech (in validation): 
	Precision: 0.6579, Recall: 0.5000, F1_score: 0.5682
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.3800, F1_score: 0.5507
Epoch 5/100

Training Phase:
▋| 1549/1600 [01:10<00:02, 22.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 224/1600 [00:10<01:01, 22.32it/s]Training:  28%|██▊       | 448/1600 [00:20<00:52, 22.01it/s]Training:  42%|████▏     | 669/1600 [00:30<00:42, 22.05it/s]Training:  56%|█████▌    | 890/1600 [00:40<00:32, 22.05it/s]Training:  70%|██████▉   | 1114/1600 [00:50<00:21, 22.14it/s]Training:  84%|████████▎ | 1338/1600 [01:00<00:11, 22.15it/s]Training:  98%|█████████▊| 1562/1600 [01:10<00:01, 22.21it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | Training loss: 36.1785, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 536.5821, Validation accuracy: 0.5050
Macro F1-score: 0.5238
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.4600, F1_score: 0.4792
Model performance on Happy speech (in validation): 
	Precision: 0.3222, Recall: 0.5800, F1_score: 0.4143
Model performance on Neutral speech (in validation): 
	Precision: 0.6341, Recall: 0.5200, F1_score: 0.5714
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.4600, F1_score: 0.6301
Epoch 6/100

Training Phase:
223/1600 [00:10<01:01, 22.25it/s]Training:  28%|██▊       | 446/1600 [00:20<00:52, 22.05it/s]Training:  42%|████▏     | 668/1600 [00:30<00:42, 22.08it/s]Training:  56%|█████▌    | 890/1600 [00:40<00:32, 22.07it/s]Training:  69%|██████▉   | 1111/1600 [00:50<00:22, 21.90it/s]Training:  83%|████████▎ | 1335/1600 [01:00<00:12, 22.06it/s]Training:  98%|█████████▊| 1560/1600 [01:10<00:01, 22.20it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 220/1600 [00:10<01:02, 21.95it/s]Training:  28%|██▊       | 442/1600 [00:20<00:52, 22.07it/s]Training:  42%|████▏     | 664/1600 [00:30<00:42, 21.92it/s]Training:  55%|█████▌    | 882/1600 [00:40<00:32, 21.78it/s]Training:  69%|██████▊   | 1098/Training loss: 40.7482, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900

Eval Phase: 
Validation loss: 536.9889, Validation accuracy: 0.4900
Macro F1-score: 0.5129
Model performance on Angry speech (in validation): 
	Precision: 0.4265, Recall: 0.5800, F1_score: 0.4915
Model performance on Happy speech (in validation): 
	Precision: 0.2537, Recall: 0.3400, F1_score: 0.2906
Model performance on Neutral speech (in validation): 
	Precision: 0.6579, Recall: 0.5000, F1_score: 0.5682
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Epoch 7/100

Training Phase:
Training loss: 23.3991, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
1600 [00:50<00:23, 21.57it/s]Training:  82%|████████▏ | 1316/1600 [01:00<00:13, 21.62it/s]Training:  96%|█████████▌| 1536/1600 [01:10<00:02, 21.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 213/1600 [00:10<01:05, 21.27it/s]Training:  27%|██▋       | 426/1600 [00:20<00:55, 21.27it/s]Training:  40%|████      | 641/1600 [00:30<00:44, 21.37it/s]Training:  54%|█████▍    | 861/1600 [00:40<00:34, 21.60it/s]Training:  68%|██████▊   | 1081/1600 [00:50<00:24, 21.58it/s]Training:  81%|████████  | 1299/1600 [01:00<00:13, 21.63it/s]Training:  95%|█████████▌| 1526/1600 [01:10<00:03, 21.95it/s]                                                             Evaluating:   0%|          | 0/200 [00:0Validation loss: 680.0695, Validation accuracy: 0.4400
Macro F1-score: 0.4561
Model performance on Angry speech (in validation): 
	Precision: 0.4464, Recall: 0.5000, F1_score: 0.4717
Model performance on Happy speech (in validation): 
	Precision: 0.2697, Recall: 0.4800, F1_score: 0.3453
Model performance on Neutral speech (in validation): 
	Precision: 0.5789, Recall: 0.4400, F1_score: 0.5000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.3400, F1_score: 0.5075
Epoch 8/100

Training Phase:
Training loss: 31.0765, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9926, Recall: 1.0000, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 506.6857, Validation accuracy: 0.4950
Macro F1-score: 0.5151
Model performance on Angry speech (in validation): 
	Precision: 0.4545, Recall: 0.6000, F1_score: 0.5172
Model performance on Happy speech (in validation): 
	Precision: 0.2647, Recall: 0.3600, F1_score: 0.3051
Model performance on Neutral speech (in validation): 
	Precision: 0.6341, Recall: 0.5200, F1_score: 0.5714
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.5000, F1_score: 0.6667
Epoch 9/100

Training Phase:
0<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 220/1600 [00:10<01:02, 21.92it/s]Training:  28%|██▊       | 443/1600 [00:20<00:52, 22.14it/s]Training:  42%|████▏     | 666/1600 [00:30<00:42, 22.17it/s]Training:  56%|█████▌    | 889/1600 [00:40<00:32, 21.96it/s]Training:  70%|██████▉   | 1112/1600 [00:50<00:22, 22.07it/s]Training:  84%|████████▎ | 1336/1600 [01:00<00:11, 22.17it/s]Training:  98%|█████████▊| 1562/1600 [01:10<00:01, 22.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 220/1600 [00:10<01:02, 21.99it/s]Training:  28%|██▊       | 446/1600 [00:20<00:51, 22.35it/s]Training:  42%|████▏     | 672/16Training loss: 13.3503, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 530.9551, Validation accuracy: 0.5050
Macro F1-score: 0.5108
Model performance on Angry speech (in validation): 
	Precision: 0.4194, Recall: 0.7800, F1_score: 0.5455
Model performance on Happy speech (in validation): 
	Precision: 0.2222, Recall: 0.2000, F1_score: 0.2105
Model performance on Neutral speech (in validation): 
	Precision: 0.7297, Recall: 0.5400, F1_score: 0.6207
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.5000, F1_score: 0.6667
Epoch 10/100

Training Phase:
00 [00:30<00:41, 22.40it/s]Training:  56%|█████▌    | 898/1600 [00:40<00:31, 22.46it/s]Training:  70%|███████   | 1124/1600 [00:50<00:21, 22.46it/s]Training:  84%|████████▍ | 1349/1600 [01:00<00:11, 22.39it/s]Training:  98%|█████████▊| 1576/1600 [01:10<00:01, 22.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|█▍        | 224/1600 [00:10<01:01, 22.37it/s]Training:  28%|██▊       | 448/1600 [00:20<00:51, 22.32it/s]Training:  42%|████▏     | 671/1600 [00:30<00:41, 22.28it/s]Training:  56%|█████▌    | 894/1600 [00:40<00:31, 22.15it/s]Training:  70%|██████▉   | 1116/1600 [00:50<00:21, 22.16it/s]Training:  84%|████████▎ | 1339/1600 [01:00<00:11, 22.18it/s]Training:  98%|██████Training loss: 30.6444, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 489.8708, Validation accuracy: 0.5050
Macro F1-score: 0.5228
Model performance on Angry speech (in validation): 
	Precision: 0.4342, Recall: 0.6600, F1_score: 0.5238
Model performance on Happy speech (in validation): 
	Precision: 0.2414, Recall: 0.2800, F1_score: 0.2593
Model performance on Neutral speech (in validation): 
	Precision: 0.6923, Recall: 0.5400, F1_score: 0.6067
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Epoch 11/100

Training Phase:
Training loss: 6.4445, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 705.8882, Validation accuracy: 0.4700
Macro F1-score: 0.4950
Model performance on Angry speech (in validation): 
	Precision: 0.4167, Recall: 0.7000, F1_score: 0.5224
Model performance on Happy speech (in validation): 
	Precision: 0.1940, Recall: 0.2600, F1_score: 0.2222
Model performance on Neutral speech (in validation): 
	Precision: 0.8846, Recall: 0.4600, F1_score: 0.6053
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.4600, F1_score: 0.6301
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.5200

Test Phase: 
██▊| 1562/1600 [01:10<00:01, 22.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|█▎        | 208/1600 [00:10<01:07, 20.74it/s]Training:  27%|██▋       | 425/1600 [00:20<00:55, 21.29it/s]Training:  40%|████      | 642/1600 [00:30<00:45, 21.24it/s]Training:  54%|█████▍    | 861/1600 [00:40<00:34, 21.47it/s]Training:  68%|██████▊   | 1088/1600 [00:50<00:23, 21.89it/s]Training:  82%|████████▏ | 1315/1600 [01:00<00:12, 22.04it/s]Training:  96%|█████████▌| 1539/1600 [01:10<00:02, 22.14it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]                            Test loss: 373.1065, Test accuracy: 0.4600
Macro F1-score: 0.4849
Model performance on Angry speech (in test): 
	Precision: 0.3548, Recall: 0.4400, F1_score: 0.3929
Model performance on Happy speech (in test): 
	Precision: 0.2716, Recall: 0.4400, F1_score: 0.3359
Model performance on Neutral speech (in test): 
	Precision: 0.8261, Recall: 0.3800, F1_score: 0.5205
Model performance on Sad speech (in test): 
	Precision: 0.8529, Recall: 0.5800, F1_score: 0.6905

de, all folds accuracy: ['0.3750', '0.7500', '0.6350', '0.7050', '0.4600']
de, all folds emo precision: {'Angry': ['0.4578', '1.0000', '0.5273', '0.8387', '0.3548'], 'Happy': ['0.1279', '0.6441', '0.5000', '0.6111', '0.2716'], 'Neutral': ['0.8387', '0.5926', '0.7368', '0.6452', '0.8261'], 'Sad': ['0.0000', '0.8478', '0.8158', '0.8857', '0.8529']}
de, all folds emo recall: {'Angry': ['0.7600', '0.8200', '0.5800', '0.5200', '0.4400'], 'Happy': ['0.2200', '0.7600', '0.5000', '0.8800', '0.4400'], 'Neutral': ['0.5200', '0.6400', '0.8400', '0.8000', '0.3800'], 'Sad': ['0.0000', '0.7800', '0.6200', '0.6200', '0.5800']}
de, all folds emo f1score: {'Angry': ['0.5714', '0.9011', '0.5524', '0.6420', '0.3929'], 'Happy': ['0.1618', '0.6972', '0.5000', '0.7213', '0.3359'], 'Neutral': ['0.6420', '0.6154', '0.7850', '0.7143', '0.5205'], 'Sad': ['0.0000', '0.8125', '0.7045', '0.7294', '0.6905']}
                    ------------------NEXT SCRIPT: RUNNER_CN, former setting----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5912052/matplotlib-vd19ojh3 because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.4.attention.k_proj.bias', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.4.attention.out_proj.bias', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.4.attention.q_proj.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

======================= This is fold_0 on cn =======================

Load dataset: 
Loading cn train data: fold_0...
Preprocess cn fold_0 data for cn model
Loading de eval data: fold_0...
Preprocess de fold_0 data for cn model
Loading de test data: fold_0...
Preprocess de fold_0 data for cn model
Use cn model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:27<12:24:51, 27.95s/it]Training:  12%|█▎        | 200/1600 [00:37<03:27,  6.75it/s] Training:  28%|██▊       | 450/1600 [00:48<01:30, 12.68it/s]Training:  44%|████▍     | 706/1600 [00:58<00:53, 16.73it/s]Training:  62%|██████▏   | 988/1600 [01:08<00:30, 20.29it/s]Training:  80%|████████  | 1287/1600 [01:18<00:13, 23.24it/s]Training:  99%|█████████▉| 1586/1600 [01:28<00:00, 24.83it/s]                                                             /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Training loss: 1810.6420, Training accuracy: 0.4294
Macro F1-score: 0.3507
Model performance on Angry speech (in training): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in training): 
	Precision: 0.3832, Recall: 0.9475, F1_score: 0.5457
Model performance on Neutral speech (in training): 
	Precision: 0.5000, Recall: 0.2800, F1_score: 0.3590
Model performance on Sad speech (in training): 
	Precision: 0.5065, Recall: 0.4900, F1_score: 0.4981

Eval Phase: 
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 622.8981, Validation accuracy: 0.2550
Macro F1-score: 0.1240
Model performance on Angry speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Sad speech (in validation): 
	Precision: 0.2959, Recall: 1.0000, F1_score: 0.4566
New best accuracy for layer 3 on epoch 1: 0.2550. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 308/1600 [00:10<00:41, 30.78it/s]Training:  39%|███▊      | 618/1600 [00:20<00:31, 30.87it/s]Training:  58%|█████▊    | 928/1600 [00:30<00:21, 30.76it/s]Training:  77%|███████▋  | 1235/1600 [00:40<00:11, 30.59it/s]Training:  96%|█████████▋| 1541/1600 [00:50<00:01, 30.57it/s]                                                             /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Training loss: 1303.5185, Training accuracy: 0.6019
Macro F1-score: 0.5229
Model performance on Angry speech (in training): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in training): 
	Precision: 0.4693, Recall: 0.9375, F1_score: 0.6255
Model performance on Neutral speech (in training): 
	Precision: 0.7275, Recall: 0.6675, F1_score: 0.6962
Model performance on Sad speech (in training): 
	Precision: 0.7396, Recall: 0.8025, F1_score: 0.7698

Eval Phase: 
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 493.1662, Validation accuracy: 0.4550
Macro F1-score: 0.3188
Model performance on Angry speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3281, Recall: 0.8400, F1_score: 0.4719
Model performance on Sad speech (in validation): 
	Precision: 0.6806, Recall: 0.9800, F1_score: 0.8033
New best accuracy for layer 3 on epoch 2: 0.4550. Model saved.
Epoch 3/100

Training Phase:
Training loss: 854.4701, Training accuracy: 0.7281
Macro F1-score: 0.7246
Model performance on Angry speech (in training): 
	Precision: 0.5897, Recall: 0.4275, F1_score: 0.4957
Model performance on Happy speech (in training): 
	Precision: 0.5218, Recall: 0.6575, F1_score: 0.5819
Model performance on Neutral speech (in training): 
	Precision: 0.8892, Recall: 0.9025, F1_score: 0.8958
Model performance on Sad speech (in training): 
	Precision: 0.9250, Recall: 0.9250, F1_score: 0.9250

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 302/1600 [00:10<00:43, 30.11it/s]Training:  38%|███▊      | 604/1600 [00:20<00:33, 30.13it/s]Training:  57%|█████▋    | 913/1600 [00:30<00:22, 30.45it/s]Training:  76%|███████▋  | 1222/1600 [00:40<00:12, 30.48it/s]Training:  96%|█████████▌| 1530/1600 [00:50<00:02, 30.56it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 419.4111, Validation accuracy: 0.4950
Macro F1-score: 0.3845
Model performance on Angry speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in validation): 
	Precision: 0.4444, Recall: 0.0800, F1_score: 0.1356
Model performance on Neutral speech (in validation): 
	Precision: 0.3534, Recall: 0.9400, F1_score: 0.5137
Model performance on Sad speech (in validation): 
	Precision: 0.8276, Recall: 0.9600, F1_score: 0.8889
New best accuracy for layer 3 on epoch 3: 0.4950. Model saved.
Epoch 4/100

Training Phase:
Training loss: 700.8914, Training accuracy: 0.7844
Macro F1-score: 0.7841
Model performance on Angry speech (in training): 
	Precision: 0.6432, Recall: 0.6625, F1_score: 0.6527
Model performance on Happy speech (in training): 
	Precision: 0.6227, Recall: 0.6025, F1_score: 0.6125
Model performance on Neutral speech (in training): 
	Precision: 0.9238, Recall: 0.9400, F1_score: 0.9318
Model performance on Sad speech (in training): 
	Precision: 0.9467, Recall: 0.9325, F1_score: 0.9395

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.59it/s]Training:  38%|███▊      | 616/1600 [00:20<00:31, 30.82it/s]Training:  58%|█████▊    | 926/1600 [00:30<00:21, 30.79it/s]Training:  77%|███████▋  | 1234/1600 [00:40<00:12, 30.50it/s]Training:  96%|█████████▋| 1540/1600 [00:50<00:01, 30.52it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 462.1496, Validation accuracy: 0.4150
Macro F1-score: 0.3205
Model performance on Angry speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.1600, F1_score: 0.2424
Model performance on Neutral speech (in validation): 
	Precision: 0.4561, Recall: 0.5200, F1_score: 0.4860
Model performance on Sad speech (in validation): 
	Precision: 0.3858, Recall: 0.9800, F1_score: 0.5537
Epoch 5/100

Training Phase:
Training loss: 562.3432, Training accuracy: 0.8538
Macro F1-score: 0.8536
Model performance on Angry speech (in training): 
	Precision: 0.7556, Recall: 0.7650, F1_score: 0.7602
Model performance on Happy speech (in training): 
	Precision: 0.7386, Recall: 0.7275, F1_score: 0.7330
Model performance on Neutral speech (in training): 
	Precision: 0.9527, Recall: 0.9575, F1_score: 0.9551
Model performance on Sad speech (in training): 
	Precision: 0.9674, Recall: 0.9650, F1_score: 0.9662

Eval Phase: 
Validation loss: 205.1245, Validation accuracy: 0.6400
Macro F1-score: 0.5809
Model performance on Angry speech (in validation): 
	Precision: 0.8333, Recall: 0.1000, F1_score: 0.1786
Model performance on Happy speech (in validation): 
	Precision: 0.5686, Recall: 0.5800, F1_score: 0.5743
Model performance on Neutral speech (in validation): 
	Precision: 0.5357, Recall: 0.9000, F1_score: 0.6716
Model performance on Sad speech (in validation): 
	Precision: 0.8305, Recall: 0.9800, F1_score: 0.8991
New best accuracy for layer 3 on epoch 5: 0.6400. Model saved.
Epoch 6/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 303/1600 [00:10<00:42, 30.26it/s]Training:  38%|███▊      | 612/1600 [00:20<00:32, 30.62it/s]Training:  58%|█████▊    | 923/1600 [00:30<00:21, 30.82it/s]Training:  77%|███████▋  | 1238/1600 [00:40<00:11, 31.09it/s]Training:  97%|█████████▋| 1557/1600 [00:50<00:01, 31.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 309/1600 [00:10<00:41, 30.90it/s]Training:  39%|███▉      | 626/1600 [00:20<00:31, 31.35it/s]Training:  59%|█████▉    | 951/1600 [00:30<00:20, 31.85it/s]Training:  80%|███████▉  | 1276/1600 [00:40<00:10, 31.87it/s]Training: 100%|█████████▉| 1598/1600 [00:50<00:00, 31.97it/s]                   Training loss: 483.0706, Training accuracy: 0.8788
Macro F1-score: 0.8786
Model performance on Angry speech (in training): 
	Precision: 0.7967, Recall: 0.8425, F1_score: 0.8190
Model performance on Happy speech (in training): 
	Precision: 0.8042, Recall: 0.7600, F1_score: 0.7815
Model performance on Neutral speech (in training): 
	Precision: 0.9431, Recall: 0.9525, F1_score: 0.9478
Model performance on Sad speech (in training): 
	Precision: 0.9722, Recall: 0.9600, F1_score: 0.9660

Eval Phase: 
Validation loss: 270.3922, Validation accuracy: 0.6400
Macro F1-score: 0.6011
Model performance on Angry speech (in validation): 
	Precision: 0.7778, Recall: 0.1400, F1_score: 0.2373
Model performance on Happy speech (in validation): 
	Precision: 0.9167, Recall: 0.4400, F1_score: 0.5946
Model performance on Neutral speech (in validation): 
	Precision: 0.4310, Recall: 1.0000, F1_score: 0.6024
Model performance on Sad speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Epoch 7/100

Training Phase:
Training loss: 368.8672, Training accuracy: 0.9125
Macro F1-score: 0.9125
Model performance on Angry speech (in training): 
	Precision: 0.8440, Recall: 0.8925, F1_score: 0.8676
Model performance on Happy speech (in training): 
	Precision: 0.8711, Recall: 0.8275, F1_score: 0.8487
Model performance on Neutral speech (in training): 
	Precision: 0.9651, Recall: 0.9675, F1_score: 0.9663
Model performance on Sad speech (in training): 
	Precision: 0.9722, Recall: 0.9625, F1_score: 0.9673

Eval Phase: 
Validation loss: 272.4466, Validation accuracy: 0.5600
Macro F1-score: 0.5077
Model performance on Angry speech (in validation): 
	Precision: 0.4737, Recall: 0.3600, F1_score: 0.4091
Model performance on Happy speech (in validation): 
	Precision: 0.8750, Recall: 0.1400, F1_score: 0.2414
Model performance on Neutral speech (in validation): 
	Precision: 0.4512, Recall: 0.7400, F1_score: 0.5606
Model performance on Sad speech (in validation): 
	Precision: 0.6944, Recall: 1.0000, F1_score: 0.8197
Epoch 8/100

Training Phase:
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|█▉        | 313/1600 [00:10<00:41, 31.30it/s]Training:  40%|███▉      | 632/1600 [00:20<00:30, 31.62it/s]Training:  59%|█████▉    | 951/1600 [00:30<00:20, 31.65it/s]Training:  80%|███████▉  | 1272/1600 [00:40<00:10, 31.80it/s]Training: 100%|█████████▉| 1596/1600 [00:50<00:00, 31.99it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|█▉        | 317/1600 [00:10<00:40, 31.65it/s]Training:  40%|███▉      | 634/1600 [00:20<00:30, 31.30it/s]Training:  59%|█████▉    | 945/1600 [00:30<00:21, 31.03it/s]Training:  79%|█████Training loss: 296.0385, Training accuracy: 0.9319
Macro F1-score: 0.9317
Model performance on Angry speech (in training): 
	Precision: 0.8835, Recall: 0.9100, F1_score: 0.8966
Model performance on Happy speech (in training): 
	Precision: 0.8912, Recall: 0.8600, F1_score: 0.8753
Model performance on Neutral speech (in training): 
	Precision: 0.9700, Recall: 0.9700, F1_score: 0.9700
Model performance on Sad speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850

Eval Phase: 
Validation loss: 194.4018, Validation accuracy: 0.7000
Macro F1-score: 0.6388
Model performance on Angry speech (in validation): 
	Precision: 0.6000, Recall: 0.7200, F1_score: 0.6545
Model performance on Happy speech (in validation): 
	Precision: 0.8333, Recall: 0.1000, F1_score: 0.1786
Model performance on Neutral speech (in validation): 
	Precision: 0.6024, Recall: 1.0000, F1_score: 0.7519
Model performance on Sad speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
New best accuracy for layer 3 on epoch 8: 0.7000. Model saved.
Epoch 9/100

Training Phase:
Training loss: 259.3827, Training accuracy: 0.9456
Macro F1-score: 0.9456
Model performance on Angry speech (in training): 
	Precision: 0.9064, Recall: 0.9200, F1_score: 0.9132
Model performance on Happy speech (in training): 
	Precision: 0.9186, Recall: 0.9025, F1_score: 0.9105
Model performance on Neutral speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Sad speech (in training): 
	Precision: 0.9825, Recall: 0.9825, F1_score: 0.9825

Eval Phase: 
Validation loss: 441.3268, Validation accuracy: 0.5300
Macro F1-score: 0.4566
Model performance on Angry speech (in validation): 
	Precision: 0.4286, Recall: 0.1800, F1_score: 0.2535
Model performance on Happy speech (in validation): 
	Precision: 0.8000, Recall: 0.0800, F1_score: 0.1455
Model performance on Neutral speech (in validation): 
	Precision: 0.3860, Recall: 0.8800, F1_score: 0.5366
Model performance on Sad speech (in validation): 
	Precision: 0.8167, Recall: 0.9800, F1_score: 0.8909
Epoch 10/100

Training Phase:
█▊  | 1257/1600 [00:40<00:11, 31.07it/s]Training:  98%|█████████▊| 1569/1600 [00:50<00:00, 31.00it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 311/1600 [00:10<00:41, 31.09it/s]Training:  39%|███▉      | 625/1600 [00:20<00:31, 31.21it/s]Training:  59%|█████▉    | 942/1600 [00:30<00:20, 31.39it/s]Training:  79%|███████▉  | 1262/1600 [00:40<00:10, 31.59it/s]Training:  99%|█████████▉| 1582/1600 [00:50<00:00, 31.69it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|██        | 321/1600 [00:10<00:39, 32.02it/s]Training:  40%|███Training loss: 250.1952, Training accuracy: 0.9487
Macro F1-score: 0.9487
Model performance on Angry speech (in training): 
	Precision: 0.9330, Recall: 0.9400, F1_score: 0.9365
Model performance on Happy speech (in training): 
	Precision: 0.9338, Recall: 0.9175, F1_score: 0.9256
Model performance on Neutral speech (in training): 
	Precision: 0.9601, Recall: 0.9625, F1_score: 0.9613
Model performance on Sad speech (in training): 
	Precision: 0.9677, Recall: 0.9750, F1_score: 0.9714

Eval Phase: 
Validation loss: 708.5126, Validation accuracy: 0.3050
Macro F1-score: 0.2101
Model performance on Angry speech (in validation): 
	Precision: 0.2143, Recall: 0.0600, F1_score: 0.0938
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.1094, Recall: 0.1400, F1_score: 0.1228
Model performance on Sad speech (in validation): 
	Precision: 0.4132, Recall: 1.0000, F1_score: 0.5848
Epoch 11/100

Training Phase:
Training loss: 198.1472, Training accuracy: 0.9569
Macro F1-score: 0.9569
Model performance on Angry speech (in training): 
	Precision: 0.9377, Recall: 0.9400, F1_score: 0.9388
Model performance on Happy speech (in training): 
	Precision: 0.9298, Recall: 0.9275, F1_score: 0.9287
Model performance on Neutral speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799

Eval Phase: 
      | 642/1600 [00:20<00:29, 32.00it/s]Training:  60%|██████    | 962/1600 [00:30<00:19, 31.92it/s]Training:  80%|████████  | 1281/1600 [00:40<00:10, 31.68it/s]Training: 100%|█████████▉| 1596/1600 [00:50<00:00, 31.61it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|█▉        | 316/1600 [00:10<00:40, 31.57it/s]Training:  40%|███▉      | 632/1600 [00:20<00:30, 31.53it/s]Training:  60%|█████▉    | 954/1600 [00:30<00:20, 31.80it/s]Training:  80%|███████▉  | 1276/1600 [00:40<00:10, 31.73it/s]Training: 100%|█████████▉| 1593/1600 [00:50<00:00, 31.63it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                              Validation loss: 359.1648, Validation accuracy: 0.5900
Macro F1-score: 0.5195
Model performance on Angry speech (in validation): 
	Precision: 0.6286, Recall: 0.4400, F1_score: 0.5176
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.4327, Recall: 0.9000, F1_score: 0.5844
Model performance on Sad speech (in validation): 
	Precision: 0.8305, Recall: 0.9800, F1_score: 0.8991
Epoch 12/100

Training Phase:
Training loss: 174.1004, Training accuracy: 0.9613
Macro F1-score: 0.9613
Model performance on Angry speech (in training): 
	Precision: 0.9505, Recall: 0.9600, F1_score: 0.9552
Model performance on Happy speech (in training): 
	Precision: 0.9348, Recall: 0.9325, F1_score: 0.9337
Model performance on Neutral speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750
Model performance on Sad speech (in training): 
	Precision: 0.9849, Recall: 0.9775, F1_score: 0.9812

Eval Phase: 
Validation loss: 460.4400, Validation accuracy: 0.5350
Macro F1-score: 0.4733
Model performance on Angry speech (in validation): 
	Precision: 0.6500, Recall: 0.7800, F1_score: 0.7091
Model performance on Happy speech (in validation): 
	Precision: 0.8000, Recall: 0.2400, F1_score: 0.3692
Model performance on Neutral speech (in validation): 
	Precision: 0.3158, Recall: 0.1200, F1_score: 0.1739
Model performance on Sad speech (in validation): 
	Precision: 0.4717, Recall: 1.0000, F1_score: 0.6410
Epoch 13/100

Training Phase:
Training loss: 152.5645, Training accuracy: 0.9675
Macro F1-score: 0.9675
Model performance on Angry speech (in training): 
	Precision: 0.9625, Recall: 0.9625, F1_score: 0.9625
Model performance on Happy speech (in training): 
	Precision: 0.9526, Recall: 0.9550, F1_score: 0.9538
Model performance on Neutral speech (in training): 
	Precision: 0.9749, Recall: 0.9700, F1_score: 0.9724
Model performance on Sad speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813

Eval Phase: 
     Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|██        | 322/1600 [00:10<00:39, 32.18it/s]Training:  40%|████      | 644/1600 [00:20<00:29, 32.06it/s]Training:  61%|██████    | 973/1600 [00:30<00:19, 32.43it/s]Training:  81%|████████▏ | 1302/1600 [00:40<00:09, 32.32it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|█▉        | 312/1600 [00:10<00:41, 31.12it/s]Training:  39%|███▉      | 624/1600 [00:20<00:31, 31.11it/s]Training:  59%|█████▉    | 946/1600 [00:30<00:20, 31.57it/s]Training:  79%|███████▉  | 1271/1600 [00:40<00:10, 31.90it/s]Training: 100%|█████████▉| 1596/1600 [00:50<00:00, 31.90it/s]                                                             Evaluating:   0%|          | 0Validation loss: 254.4179, Validation accuracy: 0.6400
Macro F1-score: 0.5690
Model performance on Angry speech (in validation): 
	Precision: 0.5797, Recall: 0.8000, F1_score: 0.6723
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.0600, F1_score: 0.1111
Model performance on Neutral speech (in validation): 
	Precision: 0.5738, Recall: 0.7000, F1_score: 0.6306
Model performance on Sad speech (in validation): 
	Precision: 0.7576, Recall: 1.0000, F1_score: 0.8621
Epoch 14/100

Training Phase:
Training loss: 175.5558, Training accuracy: 0.9613
Macro F1-score: 0.9612
Model performance on Angry speech (in training): 
	Precision: 0.9502, Recall: 0.9550, F1_score: 0.9526
Model performance on Happy speech (in training): 
	Precision: 0.9467, Recall: 0.9325, F1_score: 0.9395
Model performance on Neutral speech (in training): 
	Precision: 0.9728, Recall: 0.9825, F1_score: 0.9776
Model performance on Sad speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750

Eval Phase: 
/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 310/1600 [00:10<00:41, 30.92it/s]Training:  40%|███▉      | 636/1600 [00:20<00:30, 31.87it/s]Training:  60%|██████    | 964/1600 [00:30<00:19, 32.28it/s]Training:  81%|████████  | 1292/1600 [00:40<00:09, 31.96it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 521.8478, Validation accuracy: 0.4850
Macro F1-score: 0.4111
Model performance on Angry speech (in validation): 
	Precision: 0.6216, Recall: 0.4600, F1_score: 0.5287
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3077, Recall: 0.4800, F1_score: 0.3750
Model performance on Sad speech (in validation): 
	Precision: 0.5882, Recall: 1.0000, F1_score: 0.7407
Epoch 15/100

Training Phase:
Training loss: 151.5563, Training accuracy: 0.9681
Macro F1-score: 0.9681
Model performance on Angry speech (in training): 
	Precision: 0.9597, Recall: 0.9525, F1_score: 0.9561
Model performance on Happy speech (in training): 
	Precision: 0.9502, Recall: 0.9550, F1_score: 0.9526
Model performance on Neutral speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 308/1600 [00:10<00:41, 30.76it/s]Training:  40%|███▉      | 632/1600 [00:20<00:30, 31.72it/s]Training:  60%|█████▉    | 956/1600 [00:30<00:20, 31.96it/s]Training:  80%|████████  | 1280/1600 [00:40<00:09, 32.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 351.2628, Validation accuracy: 0.7000
Macro F1-score: 0.6086
Model performance on Angry speech (in validation): 
	Precision: 0.5753, Recall: 0.8400, F1_score: 0.6829
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.6364, Recall: 0.9800, F1_score: 0.7717
Model performance on Sad speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Epoch 16/100

Training Phase:
Training loss: 140.8864, Training accuracy: 0.9688
Macro F1-score: 0.9687
Model performance on Angry speech (in training): 
	Precision: 0.9650, Recall: 0.9650, F1_score: 0.9650
Model performance on Happy speech (in training): 
	Precision: 0.9521, Recall: 0.9450, F1_score: 0.9486
Model performance on Neutral speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Sad speech (in training): 
	Precision: 0.9802, Recall: 0.9900, F1_score: 0.9851

Eval Phase: 
Validation loss: 475.4614, Validation accuracy: 0.4900
Macro F1-score: 0.4169
Model performance on Angry speech (in validation): 
	Precision: 0.5778, Recall: 0.5200, F1_score: 0.5474
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.3333, Recall: 0.4200, F1_score: 0.3717
Model performance on Sad speech (in validation): 
	Precision: 0.5495, Recall: 1.0000, F1_score: 0.7092
Epoch 17/100

Training Phase:
Training loss: 118.1141, Training accuracy: 0.9725
Macro F1-score: 0.9725
Model performance on Angry speech (in training): 
	Precision: 0.9530, Recall: 0.9625, F1_score: 0.9577
Model performance on Happy speech (in training): 
	Precision: 0.9620, Recall: 0.9500, F1_score: 0.9560
Model performance on Neutral speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|██        | 323/1600 [00:10<00:39, 32.22it/s]Training:  40%|████      | 646/1600 [00:20<00:29, 32.11it/s]Training:  60%|██████    | 968/1600 [00:30<00:19, 32.15it/s]Training:  81%|████████  | 1290/1600 [00:40<00:09, 32.04it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|██        | 327/1600 [00:10<00:38, 32.70it/s]Training:  41%|████      | 654/1600 [00:20<00:29, 32.39it/s]Training:  61%|██████    | 976/1600 [00:30<00:19, 31.63it/s]Training:  80%|████████  | 1284/1600 [00:40<00:10, 31.25it/s]Training:  99%|█████████▉| 1591/1600 [00:50<00:00, 30.98it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 459.9447, Validation accuracy: 0.6000
Macro F1-score: 0.5213
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.4600, F1_score: 0.4792
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4673, Recall: 1.0000, F1_score: 0.6369
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Epoch 18/100

Training Phase:
Training loss: 108.8978, Training accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in training): 
	Precision: 0.9653, Recall: 0.9725, F1_score: 0.9689
Model performance on Happy speech (in training): 
	Precision: 0.9599, Recall: 0.9575, F1_score: 0.9587
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887

Eval Phase: 
Validation loss: 483.6445, Validation accuracy: 0.4800
Macro F1-score: 0.3840
Model performance on Angry speech (in validation): 
	Precision: 0.5455, Recall: 0.7200, F1_score: 0.6207
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.3214, Recall: 0.1800, F1_score: 0.2308
Model performance on Sad speech (in validation): 
	Precision: 0.4762, Recall: 1.0000, F1_score: 0.6452
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7000

Test Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.54it/s]Training:  38%|███▊      | 616/1600 [00:20<00:31, 30.75it/s]Training:  58%|█████▊    | 925/1600 [00:30<00:22, 30.52it/s]Training:  77%|███████▋  | 1228/1600 [00:40<00:12, 30.39it/s]Training:  96%|█████████▌| 1530/1600 [00:50<00:02, 30.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]                                                Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.4.attention.k_proj.bias', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.4.attention.out_proj.bias', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.4.attention.q_proj.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Test loss: 218.7235, Test accuracy: 0.6750
Macro F1-score: 0.6274
Model performance on Angry speech (in test): 
	Precision: 0.7083, Recall: 0.6800, F1_score: 0.6939
Model performance on Happy speech (in test): 
	Precision: 0.7500, Recall: 0.1200, F1_score: 0.2069
Model performance on Neutral speech (in test): 
	Precision: 0.5000, Recall: 0.9200, F1_score: 0.6479
Model performance on Sad speech (in test): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608

======================= This is fold_1 on cn =======================

Load dataset: 
Loading cn train data: fold_1...
Preprocess cn fold_1 data for cn model
Loading de eval data: fold_1...
Preprocess de fold_1 data for cn model
Loading de test data: fold_1...
Preprocess de fold_1 data for cn model
Use cn model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 904.4384, Training accuracy: 0.7850
Macro F1-score: 0.7855
Model performance on Angry speech (in training): 
	Precision: 0.7101, Recall: 0.7225, F1_score: 0.7162
Model performance on Happy speech (in training): 
	Precision: 0.6825, Recall: 0.6825, F1_score: 0.6825
Model performance on Neutral speech (in training): 
	Precision: 0.8378, Recall: 0.8525, F1_score: 0.8451
Model performance on Sad speech (in training): 
	Precision: 0.9145, Recall: 0.8825, F1_score: 0.8982

Eval Phase: 
Validation loss: 277.7380, Validation accuracy: 0.5000
Macro F1-score: 0.4232
Model performance on Angry speech (in validation): 
	Precision: 0.6333, Recall: 0.7600, F1_score: 0.6909
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3790, Recall: 0.9400, F1_score: 0.5402
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.3000, F1_score: 0.4615
New best accuracy for layer 3 on epoch 1: 0.5000. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|█         | 169/1600 [00:10<01:24, 16.85it/s]Training:  25%|██▌       | 402/1600 [00:20<00:58, 20.61it/s]Training:  41%|████      | 657/1600 [00:30<00:41, 22.83it/s]Training:  57%|█████▋    | 912/1600 [00:40<00:28, 23.82it/s]Training:  75%|███████▍  | 1196/1600 [00:50<00:15, 25.46it/s]Training:  93%|█████████▎| 1487/1600 [01:00<00:04, 26.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 310/1600 [00:10<00:41, 30.93it/s]Training:  39%|███▉      | 620/1600 [00:20<00:31, 30.87it/s]Training:  58%|█████▊    | 929/1600 [00:30<00:21, 30.66it/s]Training:  77%|███████▋  | 1234/1600 [00:40<00:12, 30.15it/s]Training:  96%|███████Training loss: 486.4517, Training accuracy: 0.8831
Macro F1-score: 0.8829
Model performance on Angry speech (in training): 
	Precision: 0.8102, Recall: 0.8325, F1_score: 0.8212
Model performance on Happy speech (in training): 
	Precision: 0.8208, Recall: 0.7900, F1_score: 0.8051
Model performance on Neutral speech (in training): 
	Precision: 0.9380, Recall: 0.9450, F1_score: 0.9415
Model performance on Sad speech (in training): 
	Precision: 0.9626, Recall: 0.9650, F1_score: 0.9638

Eval Phase: 
██▌| 1529/1600 [00:50<00:02, 29.91it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 378.7285, Validation accuracy: 0.5100
Macro F1-score: 0.4259
Model performance on Angry speech (in validation): 
	Precision: 0.6452, Recall: 0.8000, F1_score: 0.7143
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3871, Recall: 0.9600, F1_score: 0.5517
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.2800, F1_score: 0.4375
New best accuracy for layer 3 on epoch 2: 0.5100. Model saved.
Epoch 3/100

Training Phase:
Training loss: 328.5416, Training accuracy: 0.9187
Macro F1-score: 0.9187
Model performance on Angry speech (in training): 
	Precision: 0.8753, Recall: 0.8775, F1_score: 0.8764
Model performance on Happy speech (in training): 
	Precision: 0.8622, Recall: 0.8600, F1_score: 0.8611
Model performance on Neutral speech (in training): 
	Precision: 0.9625, Recall: 0.9625, F1_score: 0.9625
Model performance on Sad speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750

Eval Phase: 
Validation loss: 293.1656, Validation accuracy: 0.5850
Macro F1-score: 0.5260
Model performance on Angry speech (in validation): 
	Precision: 0.5357, Recall: 0.9000, F1_score: 0.6716
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.0600, F1_score: 0.1111
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.8200, F1_score: 0.6212
Model performance on Sad speech (in validation): 
	Precision: 0.9333, Recall: 0.5600, F1_score: 0.7000
New best accuracy for layer 3 on epoch 3: 0.5850. Model saved.
Epoch 4/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▊        | 299/1600 [00:10<00:43, 29.89it/s]Training:  38%|███▊      | 601/1600 [00:20<00:33, 30.03it/s]Training:  56%|█████▋    | 903/1600 [00:30<00:23, 29.93it/s]Training:  75%|███████▌  | 1202/1600 [00:40<00:13, 29.65it/s]Training:  94%|█████████▎| 1496/1600 [00:50<00:03, 29.55it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 310/1600 [00:10<00:41, 30.94it/s]Training:  39%|███▉      | 620/1600 [00:20<00:31, 30.91it/s]Training:  59%|█████▊    | 937/1600 [00:30<00:21, 31.25it/s]Training:  78%|███████▊  | 1254/1600 [00:40<00:11, 30.85it/s]Training:  98%|█████████▊| 1564/1600 [00:50<00:01, 30.88it/s]                   Training loss: 269.1658, Training accuracy: 0.9381
Macro F1-score: 0.9381
Model performance on Angry speech (in training): 
	Precision: 0.9100, Recall: 0.9100, F1_score: 0.9100
Model performance on Happy speech (in training): 
	Precision: 0.9043, Recall: 0.8975, F1_score: 0.9009
Model performance on Neutral speech (in training): 
	Precision: 0.9653, Recall: 0.9725, F1_score: 0.9689
Model performance on Sad speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725

Eval Phase: 
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 585.5173, Validation accuracy: 0.4300
Macro F1-score: 0.3638
Model performance on Angry speech (in validation): 
	Precision: 0.7059, Recall: 0.4800, F1_score: 0.5714
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3267, Recall: 0.9800, F1_score: 0.4900
Model performance on Sad speech (in validation): 
	Precision: 0.8125, Recall: 0.2600, F1_score: 0.3939
Epoch 5/100

Training Phase:
Training loss: 196.5686, Training accuracy: 0.9556
Macro F1-score: 0.9555
Model performance on Angry speech (in training): 
	Precision: 0.9327, Recall: 0.9350, F1_score: 0.9338
Model performance on Happy speech (in training): 
	Precision: 0.9364, Recall: 0.9200, F1_score: 0.9281
Model performance on Neutral speech (in training): 
	Precision: 0.9657, Recall: 0.9850, F1_score: 0.9752
Model performance on Sad speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.57it/s]Training:  38%|███▊      | 612/1600 [00:20<00:32, 30.04it/s]Training:  57%|█████▋    | 913/1600 [00:30<00:22, 30.05it/s]Training:  76%|███████▌  | 1217/1600 [00:40<00:12, 30.17it/s]Training:  95%|█████████▌| 1527/1600 [00:50<00:02, 30.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 393.1667, Validation accuracy: 0.5750
Macro F1-score: 0.4990
Model performance on Angry speech (in validation): 
	Precision: 0.6176, Recall: 0.8400, F1_score: 0.7119
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4516, Recall: 0.8400, F1_score: 0.5874
Model performance on Sad speech (in validation): 
	Precision: 0.7949, Recall: 0.6200, F1_score: 0.6966
Epoch 6/100

Training Phase:
Training loss: 205.0452, Training accuracy: 0.9513
Macro F1-score: 0.9512
Model performance on Angry speech (in training): 
	Precision: 0.9212, Recall: 0.9350, F1_score: 0.9280
Model performance on Happy speech (in training): 
	Precision: 0.9434, Recall: 0.9175, F1_score: 0.9303
Model performance on Neutral speech (in training): 
	Precision: 0.9656, Recall: 0.9825, F1_score: 0.9740
Model performance on Sad speech (in training): 
	Precision: 0.9749, Recall: 0.9700, F1_score: 0.9724

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 307/1600 [00:10<00:42, 30.69it/s]Training:  39%|███▊      | 618/1600 [00:20<00:31, 30.90it/s]Training:  58%|█████▊    | 930/1600 [00:30<00:21, 31.00it/s]Training:  78%|███████▊  | 1243/1600 [00:40<00:11, 31.08it/s]Training:  97%|█████████▋| 1556/1600 [00:50<00:01, 30.83it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 364.4690, Validation accuracy: 0.5850
Macro F1-score: 0.5098
Model performance on Angry speech (in validation): 
	Precision: 0.6029, Recall: 0.8200, F1_score: 0.6949
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4535, Recall: 0.7800, F1_score: 0.5735
Model performance on Sad speech (in validation): 
	Precision: 0.8043, Recall: 0.7400, F1_score: 0.7708
Epoch 7/100

Training Phase:
Training loss: 157.0558, Training accuracy: 0.9650
Macro F1-score: 0.9650
Model performance on Angry speech (in training): 
	Precision: 0.9387, Recall: 0.9575, F1_score: 0.9480
Model performance on Happy speech (in training): 
	Precision: 0.9591, Recall: 0.9375, F1_score: 0.9482
Model performance on Neutral speech (in training): 
	Precision: 0.9727, Recall: 0.9800, F1_score: 0.9763
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875

Eval Phase: 
Validation loss: 451.3801, Validation accuracy: 0.5500
Macro F1-score: 0.4894
Model performance on Angry speech (in validation): 
	Precision: 0.6792, Recall: 0.7200, F1_score: 0.6990
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.4035, Recall: 0.9200, F1_score: 0.5610
Model performance on Sad speech (in validation): 
	Precision: 0.8438, Recall: 0.5400, F1_score: 0.6585
Epoch 8/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 300/1600 [00:10<00:43, 29.91it/s]Training:  38%|███▊      | 600/1600 [00:20<00:33, 29.70it/s]Training:  56%|█████▋    | 902/1600 [00:30<00:23, 29.92it/s]Training:  75%|███████▌  | 1204/1600 [00:40<00:13, 29.96it/s]Training:  94%|█████████▍| 1505/1600 [00:50<00:03, 29.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▊        | 299/1600 [00:10<00:43, 29.81it/s]Training:  37%|███▋      | 598/1600 [00:20<00:33, 29.83it/s]Training:  56%|█████▌    | 899/1600 [00:30<00:23, 29.92it/s]Training:  75%|███████▌  | 1200/1600 [00:40<00:13, 29.91it/s]Training:  94%|█████████▍| 1500/1600 [00:50<00:03, 29.85it/s]                   Training loss: 132.0181, Training accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in training): 
	Precision: 0.9579, Recall: 0.9675, F1_score: 0.9627
Model performance on Happy speech (in training): 
	Precision: 0.9622, Recall: 0.9550, F1_score: 0.9586
Model performance on Neutral speech (in training): 
	Precision: 0.9703, Recall: 0.9800, F1_score: 0.9751
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9775, F1_score: 0.9836

Eval Phase: 
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 372.4410, Validation accuracy: 0.6200
Macro F1-score: 0.5391
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.9400, F1_score: 0.6528
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.6271, Recall: 0.7400, F1_score: 0.6789
Model performance on Sad speech (in validation): 
	Precision: 0.8511, Recall: 0.8000, F1_score: 0.8247
New best accuracy for layer 3 on epoch 8: 0.6200. Model saved.
Epoch 9/100

Training Phase:
Training loss: 141.6011, Training accuracy: 0.9650
Macro F1-score: 0.9650
Model performance on Angry speech (in training): 
	Precision: 0.9496, Recall: 0.9425, F1_score: 0.9460
Model performance on Happy speech (in training): 
	Precision: 0.9407, Recall: 0.9525, F1_score: 0.9466
Model performance on Neutral speech (in training): 
	Precision: 0.9874, Recall: 0.9775, F1_score: 0.9824
Model performance on Sad speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 303/1600 [00:10<00:42, 30.18it/s]Training:  38%|███▊      | 615/1600 [00:20<00:32, 30.75it/s]Training:  58%|█████▊    | 927/1600 [00:30<00:21, 30.91it/s]Training:  78%|███████▊  | 1242/1600 [00:40<00:11, 31.14it/s]Training:  97%|█████████▋| 1557/1600 [00:50<00:01, 31.25it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 499.0574, Validation accuracy: 0.6100
Macro F1-score: 0.5297
Model performance on Angry speech (in validation): 
	Precision: 0.5275, Recall: 0.9600, F1_score: 0.6809
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5541, Recall: 0.8200, F1_score: 0.6613
Model performance on Sad speech (in validation): 
	Precision: 0.9429, Recall: 0.6600, F1_score: 0.7765
Epoch 10/100

Training Phase:
Training loss: 133.5268, Training accuracy: 0.9738
Macro F1-score: 0.9737
Model performance on Angry speech (in training): 
	Precision: 0.9601, Recall: 0.9625, F1_score: 0.9613
Model performance on Happy speech (in training): 
	Precision: 0.9673, Recall: 0.9600, F1_score: 0.9636
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 308/1600 [00:10<00:41, 30.78it/s]Training:  39%|███▊      | 618/1600 [00:20<00:31, 30.87it/s]Training:  59%|█████▊    | 938/1600 [00:30<00:21, 31.39it/s]Training:  79%|███████▊  | 1258/1600 [00:40<00:10, 31.33it/s]Training:  98%|█████████▊| 1571/1600 [00:50<00:00, 30.65it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 698.4189, Validation accuracy: 0.4900
Macro F1-score: 0.3984
Model performance on Angry speech (in validation): 
	Precision: 0.4898, Recall: 0.9600, F1_score: 0.6486
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4157, Recall: 0.7400, F1_score: 0.5324
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.2600, F1_score: 0.4127
Epoch 11/100

Training Phase:
Training loss: 105.7147, Training accuracy: 0.9725
Macro F1-score: 0.9725
Model performance on Angry speech (in training): 
	Precision: 0.9557, Recall: 0.9700, F1_score: 0.9628
Model performance on Happy speech (in training): 
	Precision: 0.9669, Recall: 0.9500, F1_score: 0.9584
Model performance on Neutral speech (in training): 
	Precision: 0.9802, Recall: 0.9900, F1_score: 0.9851
Model performance on Sad speech (in training): 
	Precision: 0.9874, Recall: 0.9800, F1_score: 0.9837

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  18%|█▊        | 296/1600 [00:10<00:44, 29.59it/s]Training:  37%|███▋      | 592/1600 [00:20<00:34, 29.46it/s]Training:  57%|█████▋    | 905/1600 [00:30<00:22, 30.29it/s]Training:  76%|███████▌  | 1218/1600 [00:40<00:12, 30.56it/s]Training:  96%|█████████▌| 1531/1600 [00:50<00:02, 30.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 566.1852, Validation accuracy: 0.6000
Macro F1-score: 0.5202
Model performance on Angry speech (in validation): 
	Precision: 0.5542, Recall: 0.9200, F1_score: 0.6917
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5128, Recall: 0.8000, F1_score: 0.6250
Model performance on Sad speech (in validation): 
	Precision: 0.8718, Recall: 0.6800, F1_score: 0.7640
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6200

Test Phase: 
Testing:   0%|          | 0/200 [00:00<?, ?it/s]                                                /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.4.attention.k_proj.bias', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.4.attention.out_proj.bias', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.4.attention.q_proj.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Test loss: 335.6347, Test accuracy: 0.6400
Macro F1-score: 0.5570
Model performance on Angry speech (in test): 
	Precision: 0.4947, Recall: 0.9400, F1_score: 0.6483
Model performance on Happy speech (in test): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in test): 
	Precision: 0.7115, Recall: 0.7400, F1_score: 0.7255
Model performance on Sad speech (in test): 
	Precision: 0.8302, Recall: 0.8800, F1_score: 0.8544

======================= This is fold_2 on cn =======================

Load dataset: 
Loading cn train data: fold_2...
Preprocess cn fold_2 data for cn model
Loading de eval data: fold_2...
Preprocess de fold_2 data for cn model
Loading de test data: fold_2...
Preprocess de fold_2 data for cn model
Use cn model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 655.1650, Training accuracy: 0.8581
Macro F1-score: 0.8582
Model performance on Angry speech (in training): 
	Precision: 0.8225, Recall: 0.8225, F1_score: 0.8225
Model performance on Happy speech (in training): 
	Precision: 0.8040, Recall: 0.8000, F1_score: 0.8020
Model performance on Neutral speech (in training): 
	Precision: 0.8617, Recall: 0.8725, F1_score: 0.8671
Model performance on Sad speech (in training): 
	Precision: 0.9446, Recall: 0.9375, F1_score: 0.9410

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|█▏        | 183/1600 [00:10<01:17, 18.27it/s]Training:  25%|██▌       | 407/1600 [00:20<00:57, 20.69it/s]Training:  41%|████      | 657/1600 [00:30<00:41, 22.63it/s]Training:  57%|█████▋    | 919/1600 [00:40<00:28, 24.03it/s]Training:  74%|███████▍  | 1186/1600 [00:50<00:16, 24.98it/s]Training:  92%|█████████▏| 1464/1600 [01:00<00:05, 25.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 308.4591, Validation accuracy: 0.5000
Macro F1-score: 0.4442
Model performance on Angry speech (in validation): 
	Precision: 0.7143, Recall: 0.5000, F1_score: 0.5882
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3566, Recall: 0.9200, F1_score: 0.5140
Model performance on Sad speech (in validation): 
	Precision: 0.8056, Recall: 0.5800, F1_score: 0.6744
New best accuracy for layer 3 on epoch 1: 0.5000. Model saved.
Epoch 2/100

Training Phase:
Training loss: 290.5575, Training accuracy: 0.9400
Macro F1-score: 0.9400
Model performance on Angry speech (in training): 
	Precision: 0.9307, Recall: 0.9400, F1_score: 0.9353
Model performance on Happy speech (in training): 
	Precision: 0.9165, Recall: 0.9050, F1_score: 0.9107
Model performance on Neutral speech (in training): 
	Precision: 0.9428, Recall: 0.9475, F1_score: 0.9451
Model performance on Sad speech (in training): 
	Precision: 0.9699, Recall: 0.9675, F1_score: 0.9687

Eval Phase: 
Validation loss: 336.4365, Validation accuracy: 0.5600
Macro F1-score: 0.5003
Model performance on Angry speech (in validation): 
	Precision: 0.7500, Recall: 0.4200, F1_score: 0.5385
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.3860, Recall: 0.8800, F1_score: 0.5366
Model performance on Sad speech (in validation): 
	Precision: 0.8036, Recall: 0.9000, F1_score: 0.8491
New best accuracy for layer 3 on epoch 2: 0.5600. Model saved.
Epoch 3/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 307/1600 [00:10<00:42, 30.64it/s]Training:  39%|███▉      | 621/1600 [00:20<00:31, 31.07it/s]Training:  58%|█████▊    | 935/1600 [00:30<00:21, 30.98it/s]Training:  78%|███████▊  | 1245/1600 [00:40<00:11, 30.95it/s]Training:  97%|█████████▋| 1555/1600 [00:50<00:01, 30.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 310/1600 [00:10<00:41, 30.99it/s]Training:  39%|███▉      | 620/1600 [00:20<00:32, 30.39it/s]Training:  58%|█████▊    | 933/1600 [00:30<00:21, 30.77it/s]Training:  78%|███████▊  | 1246/1600 [00:40<00:11, 30.95it/s]Training:  97%|█████████▋| 1559/1600 [00:50<00:01, 31.07it/s]                   Training loss: 206.5053, Training accuracy: 0.9575
Macro F1-score: 0.9575
Model performance on Angry speech (in training): 
	Precision: 0.9455, Recall: 0.9550, F1_score: 0.9502
Model performance on Happy speech (in training): 
	Precision: 0.9490, Recall: 0.9300, F1_score: 0.9394
Model performance on Neutral speech (in training): 
	Precision: 0.9579, Recall: 0.9675, F1_score: 0.9627
Model performance on Sad speech (in training): 
	Precision: 0.9775, Recall: 0.9775, F1_score: 0.9775

Eval Phase: 
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 484.4574, Validation accuracy: 0.4400
Macro F1-score: 0.3663
Model performance on Angry speech (in validation): 
	Precision: 0.8000, Recall: 0.1600, F1_score: 0.2667
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3103, Recall: 0.9000, F1_score: 0.4615
Model performance on Sad speech (in validation): 
	Precision: 0.7778, Recall: 0.7000, F1_score: 0.7368
Epoch 4/100

Training Phase:
Training loss: 150.2199, Training accuracy: 0.9669
Macro F1-score: 0.9669
Model performance on Angry speech (in training): 
	Precision: 0.9630, Recall: 0.9750, F1_score: 0.9689
Model performance on Happy speech (in training): 
	Precision: 0.9620, Recall: 0.9500, F1_score: 0.9560
Model performance on Neutral speech (in training): 
	Precision: 0.9576, Recall: 0.9600, F1_score: 0.9588
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837

Eval Phase: 
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 307/1600 [00:10<00:42, 30.64it/s]Training:  39%|███▊      | 617/1600 [00:20<00:31, 30.79it/s]Training:  58%|█████▊    | 927/1600 [00:30<00:22, 30.59it/s]Training:  78%|███████▊  | 1240/1600 [00:40<00:11, 30.86it/s]Training:  97%|█████████▋| 1554/1600 [00:50<00:01, 31.02it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 785.7105, Validation accuracy: 0.4050
Macro F1-score: 0.2993
Model performance on Angry speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.2930, Recall: 0.9200, F1_score: 0.4444
Model performance on Sad speech (in validation): 
	Precision: 0.8140, Recall: 0.7000, F1_score: 0.7527
Epoch 5/100

Training Phase:
Training loss: 154.2957, Training accuracy: 0.9669
Macro F1-score: 0.9668
Model performance on Angry speech (in training): 
	Precision: 0.9626, Recall: 0.9650, F1_score: 0.9638
Model performance on Happy speech (in training): 
	Precision: 0.9594, Recall: 0.9450, F1_score: 0.9521
Model performance on Neutral speech (in training): 
	Precision: 0.9653, Recall: 0.9725, F1_score: 0.9689
Model performance on Sad speech (in training): 
	Precision: 0.9801, Recall: 0.9850, F1_score: 0.9825

Eval Phase: 
Validation loss: 366.2739, Validation accuracy: 0.5550
Macro F1-score: 0.4930
Model performance on Angry speech (in validation): 
	Precision: 0.6765, Recall: 0.4600, F1_score: 0.5476
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.4000, Recall: 0.9200, F1_score: 0.5576
Model performance on Sad speech (in validation): 
	Precision: 0.8367, Recall: 0.8200, F1_score: 0.8283
Epoch 6/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 308/1600 [00:10<00:41, 30.80it/s]Training:  38%|███▊      | 616/1600 [00:20<00:32, 30.09it/s]Training:  57%|█████▊    | 920/1600 [00:30<00:22, 30.19it/s]Training:  77%|███████▋  | 1228/1600 [00:40<00:12, 30.43it/s]Training:  96%|█████████▋| 1542/1600 [00:50<00:01, 30.75it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 307/1600 [00:10<00:42, 30.67it/s]Training:  38%|███▊      | 614/1600 [00:20<00:32, 30.05it/s]Training:  38%|███▊      | 614/1600 [00:30<00:32, 30.05it/s]Training:  57%|█████▋    | 906/1600 [00:30<00:23, 29.54it/s]Training:  75%|███████▍  | 1198/1600 [00:40<00:13, 29.38it/s]Training:  94%|█████Training loss: 120.9119, Training accuracy: 0.9694
Macro F1-score: 0.9694
Model performance on Angry speech (in training): 
	Precision: 0.9627, Recall: 0.9675, F1_score: 0.9651
Model performance on Happy speech (in training): 
	Precision: 0.9596, Recall: 0.9500, F1_score: 0.9548
Model performance on Neutral speech (in training): 
	Precision: 0.9726, Recall: 0.9750, F1_score: 0.9738
Model performance on Sad speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838

Eval Phase: 
Validation loss: 410.6705, Validation accuracy: 0.5800
Macro F1-score: 0.5226
Model performance on Angry speech (in validation): 
	Precision: 0.7241, Recall: 0.4200, F1_score: 0.5316
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0600, F1_score: 0.1132
Model performance on Neutral speech (in validation): 
	Precision: 0.4035, Recall: 0.9200, F1_score: 0.5610
Model performance on Sad speech (in validation): 
	Precision: 0.8519, Recall: 0.9200, F1_score: 0.8846
New best accuracy for layer 3 on epoch 6: 0.5800. Model saved.
Epoch 7/100

Training Phase:
Training loss: 127.1106, Training accuracy: 0.9744
Macro F1-score: 0.9744
Model performance on Angry speech (in training): 
	Precision: 0.9650, Recall: 0.9650, F1_score: 0.9650
Model performance on Happy speech (in training): 
	Precision: 0.9622, Recall: 0.9550, F1_score: 0.9586
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 587.9213, Validation accuracy: 0.5100
Macro F1-score: 0.4462
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.1400, F1_score: 0.2456
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1000, F1_score: 0.1818
Model performance on Neutral speech (in validation): 
	Precision: 0.3382, Recall: 0.9200, F1_score: 0.4946
Model performance on Sad speech (in validation): 
	Precision: 0.8462, Recall: 0.8800, F1_score: 0.8627
Epoch 8/100

Training Phase:
███▎| 1497/1600 [00:50<00:03, 29.55it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▊        | 297/1600 [00:10<00:43, 29.70it/s]Training:  37%|███▋      | 594/1600 [00:20<00:33, 29.67it/s]Training:  56%|█████▌    | 891/1600 [00:30<00:23, 29.65it/s]Training:  74%|███████▍  | 1188/1600 [00:40<00:13, 29.52it/s]Training:  93%|█████████▎| 1482/1600 [00:50<00:04, 29.39it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 302/1600 [00:10<00:43, 30.13it/s]Training:  38%|███▊      | 610/1600 [00:20<00:32, 30.50it/s]Training:  58%|█████▊  Training loss: 93.6220, Training accuracy: 0.9762
Macro F1-score: 0.9763
Model performance on Angry speech (in training): 
	Precision: 0.9724, Recall: 0.9675, F1_score: 0.9699
Model performance on Happy speech (in training): 
	Precision: 0.9606, Recall: 0.9750, F1_score: 0.9677
Model performance on Neutral speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875

Eval Phase: 
Validation loss: 431.7876, Validation accuracy: 0.5900
Macro F1-score: 0.5488
Model performance on Angry speech (in validation): 
	Precision: 0.9091, Recall: 0.4000, F1_score: 0.5556
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1200, F1_score: 0.2143
Model performance on Neutral speech (in validation): 
	Precision: 0.3860, Recall: 0.8800, F1_score: 0.5366
Model performance on Sad speech (in validation): 
	Precision: 0.8276, Recall: 0.9600, F1_score: 0.8889
New best accuracy for layer 3 on epoch 8: 0.5900. Model saved.
Epoch 9/100

Training Phase:
Training loss: 93.3770, Training accuracy: 0.9769
Macro F1-score: 0.9769
Model performance on Angry speech (in training): 
	Precision: 0.9704, Recall: 0.9825, F1_score: 0.9764
Model performance on Happy speech (in training): 
	Precision: 0.9822, Recall: 0.9650, F1_score: 0.9735
Model performance on Neutral speech (in training): 
	Precision: 0.9702, Recall: 0.9775, F1_score: 0.9738
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837

Eval Phase: 
  | 926/1600 [00:30<00:21, 30.95it/s]Training:  78%|███████▊  | 1241/1600 [00:40<00:11, 30.89it/s]Training:  97%|█████████▋| 1550/1600 [00:50<00:01, 30.87it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.60it/s]Training:  38%|███▊      | 615/1600 [00:20<00:32, 30.76it/s]Training:  58%|█████▊    | 924/1600 [00:30<00:22, 30.57it/s]Training:  77%|███████▋  | 1231/1600 [00:40<00:12, 30.60it/s]Training:  96%|█████████▌| 1538/1600 [00:50<00:02, 30.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 605.1318, Validation accuracy: 0.5450
Macro F1-score: 0.4618
Model performance on Angry speech (in validation): 
	Precision: 0.8235, Recall: 0.2800, F1_score: 0.4179
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.3659, Recall: 0.9000, F1_score: 0.5202
Model performance on Sad speech (in validation): 
	Precision: 0.8333, Recall: 1.0000, F1_score: 0.9091
Epoch 10/100

Training Phase:
Training loss: 81.6334, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9725, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913

Eval Phase: 
Validation loss: 451.6914, Validation accuracy: 0.6000
Macro F1-score: 0.5389
Model performance on Angry speech (in validation): 
	Precision: 0.8125, Recall: 0.5200, F1_score: 0.6341
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.4128, Recall: 0.9000, F1_score: 0.5660
Model performance on Sad speech (in validation): 
	Precision: 0.8246, Recall: 0.9400, F1_score: 0.8785
New best accuracy for layer 3 on epoch 10: 0.6000. Model saved.
Epoch 11/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.51it/s]Training:  38%|███▊      | 612/1600 [00:20<00:32, 30.49it/s]Training:  57%|█████▊    | 920/1600 [00:30<00:22, 30.62it/s]Training:  77%|███████▋  | 1228/1600 [00:40<00:12, 30.56it/s]Training:  96%|█████████▌| 1537/1600 [00:50<00:02, 30.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 301/1600 [00:10<00:43, 30.01it/s]Training:  38%|███▊      | 605/1600 [00:20<00:32, 30.24it/s]Training:  57%|█████▋    | 909/1600 [00:30<00:22, 30.09it/s]Training:  76%|███████▌  | 1216/1600 [00:40<00:12, 30.32it/s]Training:  95%|█████████▌| 1523/1600 [00:50<00:02, 30.00it/s]                   Training loss: 80.1161, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9752, Recall: 0.9850, F1_score: 0.9801
Model performance on Happy speech (in training): 
	Precision: 0.9797, Recall: 0.9675, F1_score: 0.9736
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 658.9605, Validation accuracy: 0.5000
Macro F1-score: 0.4295
Model performance on Angry speech (in validation): 
	Precision: 0.8750, Recall: 0.2800, F1_score: 0.4242
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.3153, Recall: 0.7000, F1_score: 0.4348
Model performance on Sad speech (in validation): 
	Precision: 0.6944, Recall: 1.0000, F1_score: 0.8197
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6000

Test Phase: 
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]                                                Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.4.attention.k_proj.bias', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.4.attention.out_proj.bias', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.4.attention.q_proj.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Test loss: 462.6378, Test accuracy: 0.5800
Macro F1-score: 0.5277
Model performance on Angry speech (in test): 
	Precision: 0.8519, Recall: 0.4600, F1_score: 0.5974
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in test): 
	Precision: 0.3729, Recall: 0.8800, F1_score: 0.5238
Model performance on Sad speech (in test): 
	Precision: 0.8868, Recall: 0.9400, F1_score: 0.9126

======================= This is fold_3 on cn =======================

Load dataset: 
Loading cn train data: fold_3...
Preprocess cn fold_3 data for cn model
Loading de eval data: fold_3...
Preprocess de fold_3 data for cn model
Loading de test data: fold_3...
Preprocess de fold_3 data for cn model
Use cn model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 695.8684, Training accuracy: 0.8231
Macro F1-score: 0.8231
Model performance on Angry speech (in training): 
	Precision: 0.7654, Recall: 0.7750, F1_score: 0.7702
Model performance on Happy speech (in training): 
	Precision: 0.7538, Recall: 0.7500, F1_score: 0.7519
Model performance on Neutral speech (in training): 
	Precision: 0.8622, Recall: 0.8450, F1_score: 0.8535
Model performance on Sad speech (in training): 
	Precision: 0.9111, Recall: 0.9225, F1_score: 0.9168

Eval Phase: 
Validation loss: 322.9205, Validation accuracy: 0.4900
Macro F1-score: 0.4211
Model performance on Angry speech (in validation): 
	Precision: 0.8261, Recall: 0.3800, F1_score: 0.5205
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.3373, Recall: 0.5600, F1_score: 0.4211
Model performance on Sad speech (in validation): 
	Precision: 0.5435, Recall: 1.0000, F1_score: 0.7042
New best accuracy for layer 3 on epoch 1: 0.4900. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|█         | 178/1600 [00:10<01:20, 17.67it/s]Training:  25%|██▍       | 396/1600 [00:20<00:59, 20.08it/s]Training:  40%|███▉      | 639/1600 [00:30<00:43, 21.97it/s]Training:  56%|█████▋    | 900/1600 [00:40<00:29, 23.56it/s]Training:  73%|███████▎  | 1172/1600 [00:50<00:17, 24.84it/s]Training:  90%|█████████ | 1446/1600 [01:00<00:05, 25.69it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.55it/s]Training:  39%|███▊      | 617/1600 [00:20<00:31, 30.83it/s]Training:  58%|█████▊    | 928/1600 [00:30<00:21, 30.87it/s]Training:  77%|███████▋  | 1239/1600 [00:40<00:11, 30.93it/s]Training:  97%|███████Training loss: 418.0629, Training accuracy: 0.8925
Macro F1-score: 0.8924
Model performance on Angry speech (in training): 
	Precision: 0.8529, Recall: 0.8550, F1_score: 0.8539
Model performance on Happy speech (in training): 
	Precision: 0.8422, Recall: 0.8275, F1_score: 0.8348
Model performance on Neutral speech (in training): 
	Precision: 0.9080, Recall: 0.9375, F1_score: 0.9225
Model performance on Sad speech (in training): 
	Precision: 0.9669, Recall: 0.9500, F1_score: 0.9584

Eval Phase: 
█▋| 1550/1600 [00:50<00:01, 30.66it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 308.7305, Validation accuracy: 0.5600
Macro F1-score: 0.4831
Model performance on Angry speech (in validation): 
	Precision: 0.6809, Recall: 0.6400, F1_score: 0.6598
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4000, Recall: 0.6400, F1_score: 0.4923
Model performance on Sad speech (in validation): 
	Precision: 0.6575, Recall: 0.9600, F1_score: 0.7805
New best accuracy for layer 3 on epoch 2: 0.5600. Model saved.
Epoch 3/100

Training Phase:
Training loss: 324.8681, Training accuracy: 0.9225
Macro F1-score: 0.9222
Model performance on Angry speech (in training): 
	Precision: 0.8919, Recall: 0.9075, F1_score: 0.8996
Model performance on Happy speech (in training): 
	Precision: 0.8984, Recall: 0.8625, F1_score: 0.8801
Model performance on Neutral speech (in training): 
	Precision: 0.9431, Recall: 0.9525, F1_score: 0.9478
Model performance on Sad speech (in training): 
	Precision: 0.9556, Recall: 0.9675, F1_score: 0.9615

Eval Phase: 
Validation loss: 272.5347, Validation accuracy: 0.5700
Macro F1-score: 0.5144
Model performance on Angry speech (in validation): 
	Precision: 0.6757, Recall: 0.5000, F1_score: 0.5747
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0400, F1_score: 0.0741
Model performance on Neutral speech (in validation): 
	Precision: 0.3942, Recall: 0.8200, F1_score: 0.5325
Model performance on Sad speech (in validation): 
	Precision: 0.8364, Recall: 0.9200, F1_score: 0.8762
New best accuracy for layer 3 on epoch 3: 0.5700. Model saved.
Epoch 4/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 307/1600 [00:10<00:42, 30.65it/s]Training:  38%|███▊      | 614/1600 [00:20<00:32, 30.46it/s]Training:  58%|█████▊    | 923/1600 [00:30<00:22, 30.62it/s]Training:  77%|███████▋  | 1235/1600 [00:40<00:11, 30.82it/s]Training:  97%|█████████▋| 1547/1600 [00:50<00:01, 30.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 305/1600 [00:10<00:42, 30.41it/s]Training:  38%|███▊      | 612/1600 [00:20<00:32, 30.57it/s]Training:  57%|█████▋    | 919/1600 [00:30<00:22, 30.61it/s]Training:  77%|███████▋  | 1226/1600 [00:40<00:12, 30.48it/s]Training:  96%|█████████▌| 1534/1600 [00:50<00:02, 30.59it/s]                   Training loss: 248.9573, Training accuracy: 0.9406
Macro F1-score: 0.9406
Model performance on Angry speech (in training): 
	Precision: 0.9058, Recall: 0.9375, F1_score: 0.9214
Model performance on Happy speech (in training): 
	Precision: 0.9404, Recall: 0.9075, F1_score: 0.9237
Model performance on Neutral speech (in training): 
	Precision: 0.9504, Recall: 0.9575, F1_score: 0.9539
Model performance on Sad speech (in training): 
	Precision: 0.9673, Recall: 0.9600, F1_score: 0.9636

Eval Phase: 
Validation loss: 395.1155, Validation accuracy: 0.5450
Macro F1-score: 0.4802
Model performance on Angry speech (in validation): 
	Precision: 0.7391, Recall: 0.3400, F1_score: 0.4658
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.3689, Recall: 0.9000, F1_score: 0.5233
Model performance on Sad speech (in validation): 
	Precision: 0.8679, Recall: 0.9200, F1_score: 0.8932
Epoch 5/100

Training Phase:
Training loss: 186.0830, Training accuracy: 0.9575
Macro F1-score: 0.9574
Model performance on Angry speech (in training): 
	Precision: 0.9286, Recall: 0.9425, F1_score: 0.9355
Model performance on Happy speech (in training): 
	Precision: 0.9436, Recall: 0.9200, F1_score: 0.9316
Model performance on Neutral speech (in training): 
	Precision: 0.9751, Recall: 0.9800, F1_score: 0.9776
Model performance on Sad speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850

Eval Phase: 
Validation loss: 303.4687, Validation accuracy: 0.6000
Macro F1-score: 0.5510
Model performance on Angry speech (in validation): 
	Precision: 0.8235, Recall: 0.5600, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.0600, F1_score: 0.1111
Model performance on Neutral speech (in validation): 
	Precision: 0.3905, Recall: 0.8200, F1_score: 0.5290
Model performance on Sad speech (in validation): 
	Precision: 0.8421, Recall: 0.9600, F1_score: 0.8972
New best accuracy for layer 3 on epoch 5: 0.6000. Model saved.
Epoch 6/100

Training Phase:
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 305/1600 [00:10<00:42, 30.39it/s]Training:  38%|███▊      | 611/1600 [00:20<00:32, 30.49it/s]Training:  57%|█████▊    | 920/1600 [00:30<00:22, 30.66it/s]Training:  77%|███████▋  | 1229/1600 [00:40<00:12, 30.63it/s]Training:  96%|█████████▌| 1535/1600 [00:50<00:02, 30.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.55it/s]Training:  38%|███▊      | 612/1600 [00:20<00:32, 30.56it/s]Training:  58%|█████▊    | 922/1600 [00:30<00:22, 30.75it/s]Training:  77%|█████Training loss: 175.2167, Training accuracy: 0.9587
Macro F1-score: 0.9587
Model performance on Angry speech (in training): 
	Precision: 0.9337, Recall: 0.9500, F1_score: 0.9418
Model performance on Happy speech (in training): 
	Precision: 0.9439, Recall: 0.9250, F1_score: 0.9343
Model performance on Neutral speech (in training): 
	Precision: 0.9752, Recall: 0.9825, F1_score: 0.9788
Model performance on Sad speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799

Eval Phase: 
Validation loss: 328.9550, Validation accuracy: 0.6150
Macro F1-score: 0.5521
Model performance on Angry speech (in validation): 
	Precision: 0.7234, Recall: 0.6800, F1_score: 0.7010
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0600, F1_score: 0.1132
Model performance on Neutral speech (in validation): 
	Precision: 0.4458, Recall: 0.7400, F1_score: 0.5564
Model performance on Sad speech (in validation): 
	Precision: 0.7313, Recall: 0.9800, F1_score: 0.8376
New best accuracy for layer 3 on epoch 6: 0.6150. Model saved.
Epoch 7/100

Training Phase:
Training loss: 145.5977, Training accuracy: 0.9637
Macro F1-score: 0.9638
Model performance on Angry speech (in training): 
	Precision: 0.9429, Recall: 0.9500, F1_score: 0.9465
Model performance on Happy speech (in training): 
	Precision: 0.9521, Recall: 0.9450, F1_score: 0.9486
Model performance on Neutral speech (in training): 
	Precision: 0.9701, Recall: 0.9725, F1_score: 0.9713
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887

Eval Phase: 
█▋  | 1234/1600 [00:40<00:11, 30.89it/s]Training:  97%|█████████▋| 1546/1600 [00:50<00:01, 30.81it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 311/1600 [00:10<00:41, 31.07it/s]Training:  39%|███▉      | 622/1600 [00:20<00:31, 30.80it/s]Training:  58%|█████▊    | 935/1600 [00:30<00:21, 31.02it/s]Training:  78%|███████▊  | 1248/1600 [00:40<00:11, 31.02it/s]Training:  97%|█████████▋| 1559/1600 [00:50<00:01, 30.80it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 388.0370, Validation accuracy: 0.6500
Macro F1-score: 0.5637
Model performance on Angry speech (in validation): 
	Precision: 0.6842, Recall: 0.7800, F1_score: 0.7290
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.8400, F1_score: 0.6269
Model performance on Sad speech (in validation): 
	Precision: 0.8305, Recall: 0.9800, F1_score: 0.8991
New best accuracy for layer 3 on epoch 7: 0.6500. Model saved.
Epoch 8/100

Training Phase:
Training loss: 139.8151, Training accuracy: 0.9631
Macro F1-score: 0.9631
Model performance on Angry speech (in training): 
	Precision: 0.9476, Recall: 0.9500, F1_score: 0.9488
Model performance on Happy speech (in training): 
	Precision: 0.9519, Recall: 0.9400, F1_score: 0.9459
Model performance on Neutral speech (in training): 
	Precision: 0.9704, Recall: 0.9825, F1_score: 0.9764
Model performance on Sad speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812

Eval Phase: 
Validation loss: 561.5002, Validation accuracy: 0.4450
Macro F1-score: 0.4055
Model performance on Angry speech (in validation): 
	Precision: 0.8947, Recall: 0.3400, F1_score: 0.4928
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.1600, F1_score: 0.2581
Model performance on Neutral speech (in validation): 
	Precision: 0.2857, Recall: 0.2800, F1_score: 0.2828
Model performance on Sad speech (in validation): 
	Precision: 0.4167, Recall: 1.0000, F1_score: 0.5882
Epoch 9/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.56it/s]Training:  39%|███▊      | 617/1600 [00:20<00:31, 30.82it/s]Training:  58%|█████▊    | 927/1600 [00:30<00:21, 30.79it/s]Training:  77%|███████▋  | 1235/1600 [00:40<00:11, 30.75it/s]Training:  96%|█████████▋| 1542/1600 [00:50<00:01, 30.61it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 301/1600 [00:10<00:43, 30.01it/s]Training:  38%|███▊      | 608/1600 [00:20<00:32, 30.41it/s]Training:  57%|█████▋    | 916/1600 [00:30<00:22, 30.53it/s]Training:  76%|███████▋  | 1223/1600 [00:40<00:12, 30.40it/s]Training:  95%|█████████▌| 1526/1600 [00:50<00:02, 30.36it/s]                   Training loss: 129.5126, Training accuracy: 0.9663
Macro F1-score: 0.9662
Model performance on Angry speech (in training): 
	Precision: 0.9504, Recall: 0.9575, F1_score: 0.9539
Model performance on Happy speech (in training): 
	Precision: 0.9619, Recall: 0.9475, F1_score: 0.9547
Model performance on Neutral speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763

Eval Phase: 
Validation loss: 331.7096, Validation accuracy: 0.6300
Macro F1-score: 0.5764
Model performance on Angry speech (in validation): 
	Precision: 0.6923, Recall: 0.7200, F1_score: 0.7059
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1000, F1_score: 0.1818
Model performance on Neutral speech (in validation): 
	Precision: 0.4557, Recall: 0.7200, F1_score: 0.5581
Model performance on Sad speech (in validation): 
	Precision: 0.7656, Recall: 0.9800, F1_score: 0.8596
Epoch 10/100

Training Phase:
Training loss: 99.4052, Training accuracy: 0.9769
Macro F1-score: 0.9769
Model performance on Angry speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Happy speech (in training): 
	Precision: 0.9696, Recall: 0.9575, F1_score: 0.9635
Model performance on Neutral speech (in training): 
	Precision: 0.9705, Recall: 0.9875, F1_score: 0.9789
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925

Eval Phase: 
Validation loss: 517.8566, Validation accuracy: 0.5300
Macro F1-score: 0.4813
Model performance on Angry speech (in validation): 
	Precision: 0.7692, Recall: 0.2000, F1_score: 0.3175
Model performance on Happy speech (in validation): 
	Precision: 0.8571, Recall: 0.1200, F1_score: 0.2105
Model performance on Neutral speech (in validation): 
	Precision: 0.3511, Recall: 0.9200, F1_score: 0.5083
Model performance on Sad speech (in validation): 
	Precision: 0.8980, Recall: 0.8800, F1_score: 0.8889
Epoch 11/100

Training Phase:
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 303/1600 [00:10<00:42, 30.24it/s]Training:  38%|███▊      | 606/1600 [00:20<00:32, 30.18it/s]Training:  57%|█████▋    | 908/1600 [00:30<00:23, 29.98it/s]Training:  75%|███████▌  | 1207/1600 [00:40<00:13, 29.92it/s]Training:  95%|█████████▍| 1513/1600 [00:50<00:02, 30.16it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 309/1600 [00:10<00:41, 30.89it/s]Training:  39%|███▊      | 618/1600 [00:20<00:32, 30.51it/s]Training:  58%|█████▊    | 921/1600 [00:30<00:22, 30.33it/s]Training:  76%|█████Training loss: 112.1725, Training accuracy: 0.9744
Macro F1-score: 0.9744
Model performance on Angry speech (in training): 
	Precision: 0.9627, Recall: 0.9675, F1_score: 0.9651
Model performance on Happy speech (in training): 
	Precision: 0.9651, Recall: 0.9675, F1_score: 0.9663
Model performance on Neutral speech (in training): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862

Eval Phase: 
Validation loss: 429.2518, Validation accuracy: 0.5450
Macro F1-score: 0.4990
Model performance on Angry speech (in validation): 
	Precision: 0.8333, Recall: 0.3000, F1_score: 0.4412
Model performance on Happy speech (in validation): 
	Precision: 0.8571, Recall: 0.1200, F1_score: 0.2105
Model performance on Neutral speech (in validation): 
	Precision: 0.3514, Recall: 0.7800, F1_score: 0.4845
Model performance on Sad speech (in validation): 
	Precision: 0.7656, Recall: 0.9800, F1_score: 0.8596
Epoch 12/100

Training Phase:
Training loss: 99.8198, Training accuracy: 0.9756
Macro F1-score: 0.9756
Model performance on Angry speech (in training): 
	Precision: 0.9724, Recall: 0.9700, F1_score: 0.9712
Model performance on Happy speech (in training): 
	Precision: 0.9626, Recall: 0.9650, F1_score: 0.9638
Model performance on Neutral speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 380.0302, Validation accuracy: 0.5750
Macro F1-score: 0.5359
Model performance on Angry speech (in validation): 
	Precision: 0.8095, Recall: 0.3400, F1_score: 0.4789
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.1200, F1_score: 0.2069
Model performance on Neutral speech (in validation): 
	Precision: 0.3802, Recall: 0.9200, F1_score: 0.5380
Model performance on Sad speech (in validation): 
	Precision: 0.9200, Recall: 0.9200, F1_score: 0.9200
Epoch 13/100

Training Phase:
█▋  | 1223/1600 [00:40<00:12, 30.08it/s]Training:  95%|█████████▌| 1527/1600 [00:50<00:02, 30.18it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  20%|█▉        | 312/1600 [00:10<00:41, 31.16it/s]Training:  39%|███▉      | 624/1600 [00:20<00:31, 30.84it/s]Training:  58%|█████▊    | 931/1600 [00:30<00:21, 30.76it/s]Training:  77%|███████▋  | 1238/1600 [00:40<00:11, 30.57it/s]Training:  97%|█████████▋| 1548/1600 [00:50<00:01, 30.71it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 304/1600 [00:10<00:42, 30.30it/s]Training:  38%|███Training loss: 90.3258, Training accuracy: 0.9756
Macro F1-score: 0.9756
Model performance on Angry speech (in training): 
	Precision: 0.9677, Recall: 0.9750, F1_score: 0.9714
Model performance on Happy speech (in training): 
	Precision: 0.9673, Recall: 0.9600, F1_score: 0.9636
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837

Eval Phase: 
Validation loss: 435.4102, Validation accuracy: 0.5100
Macro F1-score: 0.4849
Model performance on Angry speech (in validation): 
	Precision: 0.8235, Recall: 0.2800, F1_score: 0.4179
Model performance on Happy speech (in validation): 
	Precision: 0.6800, Recall: 0.3400, F1_score: 0.4533
Model performance on Neutral speech (in validation): 
	Precision: 0.3134, Recall: 0.4200, F1_score: 0.3590
Model performance on Sad speech (in validation): 
	Precision: 0.5495, Recall: 1.0000, F1_score: 0.7092
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6500

Test Phase: 
      | 608/1600 [00:20<00:32, 30.10it/s]Training:  57%|█████▋    | 908/1600 [00:30<00:23, 30.01it/s]Training:  76%|███████▌  | 1211/1600 [00:40<00:12, 30.12it/s]Training:  95%|█████████▍| 1519/1600 [00:50<00:02, 30.36it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]                                                Some weights of the model checkpoint at /work/tc062/tc062/zhan7721/saved_models/de were not used when initializing Wav2Vec2Model: ['encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.4.attention.k_proj.bias', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.4.attention.out_proj.bias', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.4.attention.q_proj.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.layer_norm.weight']
- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Test loss: 404.1827, Test accuracy: 0.6500
Macro F1-score: 0.5663
Model performance on Angry speech (in test): 
	Precision: 0.7170, Recall: 0.7600, F1_score: 0.7379
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in test): 
	Precision: 0.5256, Recall: 0.8200, F1_score: 0.6406
Model performance on Sad speech (in test): 
	Precision: 0.7353, Recall: 1.0000, F1_score: 0.8475

======================= This is fold_4 on cn =======================

Load dataset: 
Loading cn train data: fold_4...
Preprocess cn fold_4 data for cn model
Loading de eval data: fold_4...
Preprocess de fold_4 data for cn model
Loading de test data: fold_4...
Preprocess de fold_4 data for cn model
Use cn model to add lora
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 492.4603, Training accuracy: 0.8781
Macro F1-score: 0.8783
Model performance on Angry speech (in training): 
	Precision: 0.8164, Recall: 0.8225, F1_score: 0.8194
Model performance on Happy speech (in training): 
	Precision: 0.8263, Recall: 0.8325, F1_score: 0.8294
Model performance on Neutral speech (in training): 
	Precision: 0.9118, Recall: 0.9050, F1_score: 0.9084
Model performance on Sad speech (in training): 
	Precision: 0.9597, Recall: 0.9525, F1_score: 0.9561

Eval Phase: 
Validation loss: 247.1242, Validation accuracy: 0.6200
Macro F1-score: 0.5432
Model performance on Angry speech (in validation): 
	Precision: 0.6452, Recall: 0.8000, F1_score: 0.7143
Model performance on Happy speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.4713, Recall: 0.8200, F1_score: 0.5985
Model performance on Sad speech (in validation): 
	Precision: 0.8600, Recall: 0.8600, F1_score: 0.8600
New best accuracy for layer 3 on epoch 1: 0.6200. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|█         | 179/1600 [00:10<01:19, 17.84it/s]Training:  26%|██▌       | 412/1600 [00:20<00:56, 21.02it/s]Training:  42%|████▏     | 665/1600 [00:30<00:40, 22.94it/s]Training:  59%|█████▊    | 937/1600 [00:40<00:26, 24.60it/s]Training:  76%|███████▌  | 1211/1600 [00:50<00:15, 25.57it/s]Training:  94%|█████████▎| 1496/1600 [01:00<00:03, 26.53it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▊        | 297/1600 [00:10<00:43, 29.62it/s]Training:  38%|███▊      | 604/1600 [00:20<00:32, 30.24it/s]Training:  57%|█████▋    | 911/1600 [00:30<00:22, 30.38it/s]Training:  76%|███████▋  | 1222/1600 [00:40<00:12, 30.63it/s]Training:  96%|██████Training loss: 287.9132, Training accuracy: 0.9331
Macro F1-score: 0.9329
Model performance on Angry speech (in training): 
	Precision: 0.8973, Recall: 0.9175, F1_score: 0.9073
Model performance on Happy speech (in training): 
	Precision: 0.9086, Recall: 0.8700, F1_score: 0.8889
Model performance on Neutral speech (in training): 
	Precision: 0.9509, Recall: 0.9675, F1_score: 0.9591
Model performance on Sad speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763

Eval Phase: 
Validation loss: 184.6884, Validation accuracy: 0.6900
Macro F1-score: 0.6666
Model performance on Angry speech (in validation): 
	Precision: 0.6716, Recall: 0.9000, F1_score: 0.7692
Model performance on Happy speech (in validation): 
	Precision: 0.8235, Recall: 0.2800, F1_score: 0.4179
Model performance on Neutral speech (in validation): 
	Precision: 0.5733, Recall: 0.8600, F1_score: 0.6880
Model performance on Sad speech (in validation): 
	Precision: 0.8780, Recall: 0.7200, F1_score: 0.7912
New best accuracy for layer 3 on epoch 2: 0.6900. Model saved.
Epoch 3/100

Training Phase:
Training loss: 228.5914, Training accuracy: 0.9500
Macro F1-score: 0.9500
Model performance on Angry speech (in training): 
	Precision: 0.9330, Recall: 0.9400, F1_score: 0.9365
Model performance on Happy speech (in training): 
	Precision: 0.9365, Recall: 0.9225, F1_score: 0.9295
Model performance on Neutral speech (in training): 
	Precision: 0.9558, Recall: 0.9725, F1_score: 0.9641
Model performance on Sad speech (in training): 
	Precision: 0.9747, Recall: 0.9650, F1_score: 0.9698

Eval Phase: 
Validation loss: 224.4589, Validation accuracy: 0.6650
Macro F1-score: 0.6338
Model performance on Angry speech (in validation): 
	Precision: 0.6716, Recall: 0.9000, F1_score: 0.7692
Model performance on Happy speech (in validation): 
	Precision: 0.7857, Recall: 0.2200, F1_score: 0.3438
Model performance on Neutral speech (in validation): 
	Precision: 0.5455, Recall: 0.8400, F1_score: 0.6614
Model performance on Sad speech (in validation): 
	Precision: 0.8333, Recall: 0.7000, F1_score: 0.7609
Epoch 4/100

Training Phase:
██▌| 1533/1600 [00:50<00:02, 30.51it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 308/1600 [00:10<00:42, 30.73it/s]Training:  38%|███▊      | 616/1600 [00:20<00:32, 30.40it/s]Training:  58%|█████▊    | 922/1600 [00:30<00:22, 30.47it/s]Training:  77%|███████▋  | 1228/1600 [00:40<00:12, 30.21it/s]Training:  96%|█████████▌| 1530/1600 [00:50<00:02, 30.21it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 301/1600 [00:10<00:43, 30.08it/s]Training:  38%|███▊      | 609/1600 [00:20<00:32, 30.47it/s]Training:  57%|█████▋    Training loss: 181.6605, Training accuracy: 0.9587
Macro F1-score: 0.9587
Model performance on Angry speech (in training): 
	Precision: 0.9378, Recall: 0.9425, F1_score: 0.9401
Model performance on Happy speech (in training): 
	Precision: 0.9416, Recall: 0.9275, F1_score: 0.9345
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763

Eval Phase: 
Validation loss: 284.2173, Validation accuracy: 0.6250
Macro F1-score: 0.5673
Model performance on Angry speech (in validation): 
	Precision: 0.7255, Recall: 0.7400, F1_score: 0.7327
Model performance on Happy speech (in validation): 
	Precision: 0.7143, Recall: 0.1000, F1_score: 0.1754
Model performance on Neutral speech (in validation): 
	Precision: 0.5484, Recall: 0.6800, F1_score: 0.6071
Model performance on Sad speech (in validation): 
	Precision: 0.6125, Recall: 0.9800, F1_score: 0.7538
Epoch 5/100

Training Phase:
Training loss: 176.3216, Training accuracy: 0.9575
Macro F1-score: 0.9575
Model performance on Angry speech (in training): 
	Precision: 0.9617, Recall: 0.9425, F1_score: 0.9520
Model performance on Happy speech (in training): 
	Precision: 0.9333, Recall: 0.9450, F1_score: 0.9391
Model performance on Neutral speech (in training): 
	Precision: 0.9581, Recall: 0.9725, F1_score: 0.9653
Model performance on Sad speech (in training): 
	Precision: 0.9773, Recall: 0.9700, F1_score: 0.9737

Eval Phase: 
Validation loss: 329.4786, Validation accuracy: 0.6500
Macro F1-score: 0.5668
Model performance on Angry speech (in validation): 
	Precision: 0.5833, Recall: 0.9800, F1_score: 0.7313
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.5833, Recall: 0.9800, F1_score: 0.7313
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Epoch 6/100

Training Phase:
| 917/1600 [00:30<00:22, 30.55it/s]Training:  76%|███████▋  | 1224/1600 [00:40<00:12, 30.56it/s]Training:  96%|█████████▌| 1530/1600 [00:50<00:02, 30.43it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 301/1600 [00:10<00:43, 30.05it/s]Training:  38%|███▊      | 602/1600 [00:20<00:33, 30.06it/s]Training:  56%|█████▋    | 903/1600 [00:30<00:23, 30.05it/s]Training:  75%|███████▌  | 1206/1600 [00:40<00:13, 30.13it/s]Training:  95%|█████████▍| 1515/1600 [00:50<00:02, 30.38it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉  Training loss: 124.6573, Training accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in training): 
	Precision: 0.9727, Recall: 0.9800, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9700, F1_score: 0.9749
Model performance on Neutral speech (in training): 
	Precision: 0.9726, Recall: 0.9775, F1_score: 0.9751
Model performance on Sad speech (in training): 
	Precision: 0.9749, Recall: 0.9725, F1_score: 0.9737

Eval Phase: 
Validation loss: 368.1706, Validation accuracy: 0.5250
Macro F1-score: 0.5017
Model performance on Angry speech (in validation): 
	Precision: 0.8182, Recall: 0.3600, F1_score: 0.5000
Model performance on Happy speech (in validation): 
	Precision: 0.3750, Recall: 0.1200, F1_score: 0.1818
Model performance on Neutral speech (in validation): 
	Precision: 0.3769, Recall: 0.9800, F1_score: 0.5444
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.6400, F1_score: 0.7805
Epoch 7/100

Training Phase:
Training loss: 141.3766, Training accuracy: 0.9656
Macro F1-score: 0.9656
Model performance on Angry speech (in training): 
	Precision: 0.9525, Recall: 0.9525, F1_score: 0.9525
Model performance on Happy speech (in training): 
	Precision: 0.9449, Recall: 0.9425, F1_score: 0.9437
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9850, F1_score: 0.9825
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837

Eval Phase: 
      | 304/1600 [00:10<00:42, 30.32it/s]Training:  38%|███▊      | 608/1600 [00:20<00:32, 30.15it/s]Training:  57%|█████▋    | 916/1600 [00:30<00:22, 30.41it/s]Training:  77%|███████▋  | 1225/1600 [00:40<00:12, 30.57it/s]Training:  96%|█████████▌| 1534/1600 [00:50<00:02, 30.66it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 309/1600 [00:10<00:41, 30.84it/s]Training:  39%|███▊      | 618/1600 [00:20<00:31, 30.72it/s]Training:  58%|█████▊    | 925/1600 [00:30<00:22, 30.40it/s]Training:  77%|███████▋  | 1226/1600 [00:40<00:12, 30.19it/s]Training:  96%|█████████▌| 1531/1600 [00:50<00:02, 30.28it/s]                                                             Evaluating:   0%|          | 0/Validation loss: 298.0716, Validation accuracy: 0.6500
Macro F1-score: 0.5939
Model performance on Angry speech (in validation): 
	Precision: 0.5109, Recall: 0.9400, F1_score: 0.6620
Model performance on Happy speech (in validation): 
	Precision: 0.8000, Recall: 0.0800, F1_score: 0.1455
Model performance on Neutral speech (in validation): 
	Precision: 0.6562, Recall: 0.8400, F1_score: 0.7368
Model performance on Sad speech (in validation): 
	Precision: 0.9487, Recall: 0.7400, F1_score: 0.8315
Epoch 8/100

Training Phase:
Training loss: 106.6058, Training accuracy: 0.9769
Macro F1-score: 0.9769
Model performance on Angry speech (in training): 
	Precision: 0.9630, Recall: 0.9750, F1_score: 0.9689
Model performance on Happy speech (in training): 
	Precision: 0.9673, Recall: 0.9625, F1_score: 0.9649
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887

Eval Phase: 
Validation loss: 396.8786, Validation accuracy: 0.5800
Macro F1-score: 0.5284
Model performance on Angry speech (in validation): 
	Precision: 0.7143, Recall: 0.6000, F1_score: 0.6522
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.4118, Recall: 0.9800, F1_score: 0.5799
Model performance on Sad speech (in validation): 
	Precision: 0.9459, Recall: 0.7000, F1_score: 0.8046
Epoch 9/100

Training Phase:
200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 306/1600 [00:10<00:42, 30.58it/s]Training:  38%|███▊      | 612/1600 [00:20<00:32, 30.58it/s]Training:  57%|█████▋    | 918/1600 [00:30<00:22, 30.23it/s]Training:  76%|███████▌  | 1218/1600 [00:40<00:12, 30.12it/s]Training:  95%|█████████▍| 1519/1600 [00:50<00:02, 30.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 300/1600 [00:10<00:43, 29.94it/s]Training:  38%|███▊      | 600/1600 [00:20<00:33, 29.66it/s]Training:  56%|█████▋    | 902/1600 [00:30<00:23, 29.87it/s]Training:  76%|███████▌  | 1208/1600 [00:40<00:13, 30.12it/s]Training:  95%|████Training loss: 107.1929, Training accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in training): 
	Precision: 0.9704, Recall: 0.9825, F1_score: 0.9764
Model performance on Happy speech (in training): 
	Precision: 0.9747, Recall: 0.9625, F1_score: 0.9686
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9726, Recall: 0.9750, F1_score: 0.9738

Eval Phase: 
Validation loss: 307.6885, Validation accuracy: 0.6650
Macro F1-score: 0.6199
Model performance on Angry speech (in validation): 
	Precision: 0.5326, Recall: 0.9800, F1_score: 0.6901
Model performance on Happy speech (in validation): 
	Precision: 0.8750, Recall: 0.1400, F1_score: 0.2414
Model performance on Neutral speech (in validation): 
	Precision: 0.6667, Recall: 0.8400, F1_score: 0.7434
Model performance on Sad speech (in validation): 
	Precision: 0.9459, Recall: 0.7000, F1_score: 0.8046
Epoch 10/100

Training Phase:
Training loss: 97.1782, Training accuracy: 0.9756
Macro F1-score: 0.9756
Model performance on Angry speech (in training): 
	Precision: 0.9699, Recall: 0.9675, F1_score: 0.9687
Model performance on Happy speech (in training): 
	Precision: 0.9651, Recall: 0.9675, F1_score: 0.9663
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863

Eval Phase: 
Validation loss: 396.4833, Validation accuracy: 0.5700
Macro F1-score: 0.5084
Model performance on Angry speech (in validation): 
	Precision: 0.6949, Recall: 0.8200, F1_score: 0.7523
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.0600, F1_score: 0.1111
Model performance on Neutral speech (in validation): 
	Precision: 0.4274, Recall: 1.0000, F1_score: 0.5988
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.4000, F1_score: 0.5714
Epoch 11/100

Training Phase:
████▍| 1514/1600 [00:50<00:02, 30.21it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▊        | 298/1600 [00:10<00:43, 29.73it/s]Training:  37%|███▋      | 596/1600 [00:20<00:34, 29.42it/s]Training:  56%|█████▌    | 894/1600 [00:30<00:23, 29.58it/s]Training:  74%|███████▍  | 1192/1600 [00:40<00:13, 29.58it/s]Training:  93%|█████████▎| 1491/1600 [00:50<00:03, 29.66it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 301/1600 [00:10<00:43, 30.03it/s]Training:  38%|███▊      | 607/1600 [00:20<00:32, 30.33it/s]Training:  57%|█████Training loss: 109.9213, Training accuracy: 0.9738
Macro F1-score: 0.9738
Model performance on Angry speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9576, Recall: 0.9600, F1_score: 0.9588
Model performance on Neutral speech (in training): 
	Precision: 0.9799, Recall: 0.9775, F1_score: 0.9787
Model performance on Sad speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812

Eval Phase: 
Validation loss: 384.1576, Validation accuracy: 0.6000
Macro F1-score: 0.5253
Model performance on Angry speech (in validation): 
	Precision: 0.5844, Recall: 0.9000, F1_score: 0.7087
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.0200, F1_score: 0.0385
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.9400, F1_score: 0.6528
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Epoch 12/100

Training Phase:
Training loss: 96.7764, Training accuracy: 0.9775
Macro F1-score: 0.9775
Model performance on Angry speech (in training): 
	Precision: 0.9726, Recall: 0.9775, F1_score: 0.9751
Model performance on Happy speech (in training): 
	Precision: 0.9698, Recall: 0.9625, F1_score: 0.9661
Model performance on Neutral speech (in training): 
	Precision: 0.9849, Recall: 0.9800, F1_score: 0.9825
Model performance on Sad speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863

Eval Phase: 
Validation loss: 420.3088, Validation accuracy: 0.6250
Macro F1-score: 0.5490
Model performance on Angry speech (in validation): 
	Precision: 0.6250, Recall: 0.9000, F1_score: 0.7377
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.5054, Recall: 0.9400, F1_score: 0.6573
Model performance on Sad speech (in validation): 
	Precision: 0.9412, Recall: 0.6400, F1_score: 0.7619
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6900

Test Phase: 
    | 913/1600 [00:30<00:22, 30.37it/s]Training:  76%|███████▌  | 1218/1600 [00:40<00:12, 30.34it/s]Training:  95%|█████████▌| 1527/1600 [00:50<00:02, 30.52it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  19%|█▉        | 305/1600 [00:10<00:42, 30.42it/s]Training:  38%|███▊      | 613/1600 [00:20<00:32, 30.63it/s]Training:  58%|█████▊    | 922/1600 [00:30<00:22, 30.73it/s]Training:  77%|███████▋  | 1231/1600 [00:40<00:12, 30.53it/s]Training:  96%|█████████▌| 1536/1600 [00:50<00:02, 30.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]                   Test loss: 178.1111, Test accuracy: 0.7200
Macro F1-score: 0.6963
Model performance on Angry speech (in test): 
	Precision: 0.7273, Recall: 0.9600, F1_score: 0.8276
Model performance on Happy speech (in test): 
	Precision: 0.7143, Recall: 0.3000, F1_score: 0.4225
Model performance on Neutral speech (in test): 
	Precision: 0.6061, Recall: 0.8000, F1_score: 0.6897
Model performance on Sad speech (in test): 
	Precision: 0.8723, Recall: 0.8200, F1_score: 0.8454

cn, all folds accuracy: ['0.6750', '0.6400', '0.5800', '0.6500', '0.7200']
cn, all folds emo precision: {'Angry': ['0.7083', '0.4947', '0.8519', '0.7170', '0.7273'], 'Happy': ['0.7500', '0.0000', '1.0000', '1.0000', '0.7143'], 'Neutral': ['0.5000', '0.7115', '0.3729', '0.5256', '0.6061'], 'Sad': ['0.9423', '0.8302', '0.8868', '0.7353', '0.8723']}
cn, all folds emo recall: {'Angry': ['0.6800', '0.9400', '0.4600', '0.7600', '0.9600'], 'Happy': ['0.1200', '0.0000', '0.0400', '0.0200', '0.3000'], 'Neutral': ['0.9200', '0.7400', '0.8800', '0.8200', '0.8000'], 'Sad': ['0.9800', '0.8800', '0.9400', '1.0000', '0.8200']}
cn, all folds emo f1score: {'Angry': ['0.6939', '0.6483', '0.5974', '0.7379', '0.8276'], 'Happy': ['0.2069', '0.0000', '0.0769', '0.0392', '0.4225'], 'Neutral': ['0.6479', '0.7255', '0.5238', '0.6406', '0.6897'], 'Sad': ['0.9608', '0.8544', '0.9126', '0.8475', '0.8454']}
                             