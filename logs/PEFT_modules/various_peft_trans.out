Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
------------------NEXT SCRIPT: RUNNER_DE, current setting----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911929/matplotlib-xbwpdxi1 because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on de =======================

Load dataset: 
Loading cn train data: fold_0...
Preprocess cn fold_0 data for de model
Loading cn eval data: fold_0...
Preprocess cn fold_0 data for de model
Loading cn test data: fold_0...
Preprocess cn fold_0 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1449.8718, Training accuracy: 0.5925
Macro F1-score: 0.5707
Model performance on Angry speech (in training): 
	Precision: 0.7143, Recall: 0.1500, F1_score: 0.2479
Model performance on Happy speech (in training): 
	Precision: 0.3975, Recall: 0.8575, F1_score: 0.5432
Model performance on Neutral speech (in training): 
	Precision: 0.8221, Recall: 0.5775, F1_score: 0.6784
Model performance on Sad speech (in training): 
	Precision: 0.8441, Recall: 0.7850, F1_score: 0.8135

Eval Phase: 
Validation loss: 188.2796, Validation accuracy: 0.6150
Macro F1-score: 0.6061
Model performance on Angry speech (in validation): 
	Precision: 0.4261, Recall: 0.9800, F1_score: 0.5939
Model performance on Happy speech (in validation): 
	Precision: 0.5238, Recall: 0.2200, F1_score: 0.3099
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.5000, F1_score: 0.6667
Model performance on Sad speech (in validation): 
	Precision: 0.9744, Recall: 0.7600, F1_score: 0.8539
New best accuracy for layer 4 on epoch 1: 0.6150. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:53<23:55:34, 53.87s/it]Training:   9%|â–‰         | 140/1600 [01:03<08:19,  2.92it/s] Training:  20%|â–ˆâ–‰        | 312/1600 [01:13<03:27,  6.21it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 514/1600 [01:24<01:52,  9.65it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 729/1600 [01:34<01:08, 12.73it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 954/1600 [01:44<00:42, 15.38it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1182/1600 [01:54<00:23, 17.45it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1419/1600 [02:04<00:09, 19.23it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 243/1600 [00:10<00:56, 24.23it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 490/1600 [00:20<00:45, 24.47it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 737/1600 [00:30<00:35,Training loss: 582.4761, Training accuracy: 0.8600
Macro F1-score: 0.8597
Model performance on Angry speech (in training): 
	Precision: 0.7670, Recall: 0.7900, F1_score: 0.7783
Model performance on Happy speech (in training): 
	Precision: 0.7532, Recall: 0.7250, F1_score: 0.7389
Model performance on Neutral speech (in training): 
	Precision: 0.9383, Recall: 0.9500, F1_score: 0.9441
Model performance on Sad speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774

Eval Phase: 
Validation loss: 135.7539, Validation accuracy: 0.6950
Macro F1-score: 0.6918
Model performance on Angry speech (in validation): 
	Precision: 0.5283, Recall: 0.5600, F1_score: 0.5437
Model performance on Happy speech (in validation): 
	Precision: 0.5556, Recall: 0.5000, F1_score: 0.5263
Model performance on Neutral speech (in validation): 
	Precision: 0.9730, Recall: 0.7200, F1_score: 0.8276
Model performance on Sad speech (in validation): 
	Precision: 0.7692, Recall: 1.0000, F1_score: 0.8696
New best accuracy for layer 4 on epoch 2: 0.6950. Model saved.
Epoch 3/100

Training Phase:
Training loss: 386.4190, Training accuracy: 0.9113
Macro F1-score: 0.9107
Model performance on Angry speech (in training): 
	Precision: 0.8424, Recall: 0.8950, F1_score: 0.8679
Model performance on Happy speech (in training): 
	Precision: 0.8774, Recall: 0.8050, F1_score: 0.8396
Model performance on Neutral speech (in training): 
	Precision: 0.9436, Recall: 0.9625, F1_score: 0.9530
Model performance on Sad speech (in training): 
	Precision: 0.9825, Recall: 0.9825, F1_score: 0.9825

Eval Phase: 
 24.50it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 988/1600 [00:40<00:24, 24.71it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1239/1600 [00:50<00:14, 24.72it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1488/1600 [01:00<00:04, 24.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 247/1600 [00:10<00:54, 24.67it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 494/1600 [00:20<00:45, 24.40it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 737/1600 [00:30<00:35, 24.26it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 981/1600 [00:40<00:25, 24.29it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1226/1600 [00:50<00:15, 24.36it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1472/1600 [01:00<00:05, 24.41it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]     Validation loss: 227.9089, Validation accuracy: 0.6250
Macro F1-score: 0.6512
Model performance on Angry speech (in validation): 
	Precision: 0.5370, Recall: 0.5800, F1_score: 0.5577
Model performance on Happy speech (in validation): 
	Precision: 0.3636, Recall: 0.5600, F1_score: 0.4409
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Sad speech (in validation): 
	Precision: 0.9737, Recall: 0.7400, F1_score: 0.8409
Epoch 4/100

Training Phase:
Training loss: 216.2136, Training accuracy: 0.9500
Macro F1-score: 0.9499
Model performance on Angry speech (in training): 
	Precision: 0.8905, Recall: 0.9350, F1_score: 0.9122
Model performance on Happy speech (in training): 
	Precision: 0.9339, Recall: 0.8825, F1_score: 0.9075
Model performance on Neutral speech (in training): 
	Precision: 0.9777, Recall: 0.9850, F1_score: 0.9813
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 160.7043, Validation accuracy: 0.7550
Macro F1-score: 0.7680
Model performance on Angry speech (in validation): 
	Precision: 0.7234, Recall: 0.6800, F1_score: 0.7010
Model performance on Happy speech (in validation): 
	Precision: 0.5270, Recall: 0.7800, F1_score: 0.6290
Model performance on Neutral speech (in validation): 
	Precision: 0.9737, Recall: 0.7400, F1_score: 0.8409
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
New best accuracy for layer 4 on epoch 4: 0.7550. Model saved.
Epoch 5/100

Training Phase:
                                              Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 250/1600 [00:10<00:54, 24.95it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 507/1600 [00:20<00:43, 25.36it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 764/1600 [00:30<00:32, 25.48it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1021/1600 [00:40<00:22, 25.22it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1270/1600 [00:50<00:13, 24.99it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1516/1600 [01:00<00:03, 24.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 247/1600 [00:10<00:54, 24.68it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 494/1600 [00:20<00:44, 24.64it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 741/1600 [00:30<00:35, 24.36it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 990/1600 [00:40<00:24Training loss: 200.2861, Training accuracy: 0.9575
Macro F1-score: 0.9575
Model performance on Angry speech (in training): 
	Precision: 0.9286, Recall: 0.9425, F1_score: 0.9355
Model performance on Happy speech (in training): 
	Precision: 0.9364, Recall: 0.9200, F1_score: 0.9281
Model performance on Neutral speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862

Eval Phase: 
Validation loss: 206.3820, Validation accuracy: 0.7100
Macro F1-score: 0.7127
Model performance on Angry speech (in validation): 
	Precision: 0.5714, Recall: 0.9600, F1_score: 0.7164
Model performance on Happy speech (in validation): 
	Precision: 0.5333, Recall: 0.4800, F1_score: 0.5053
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.6000, F1_score: 0.7500
Model performance on Sad speech (in validation): 
	Precision: 0.9756, Recall: 0.8000, F1_score: 0.8791
Epoch 6/100

Training Phase:
Training loss: 165.3028, Training accuracy: 0.9681
Macro F1-score: 0.9681
Model performance on Angry speech (in training): 
	Precision: 0.9551, Recall: 0.9575, F1_score: 0.9563
Model performance on Happy speech (in training): 
	Precision: 0.9495, Recall: 0.9400, F1_score: 0.9447
Model performance on Neutral speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 158.1168, Validation accuracy: 0.7150
Macro F1-score: 0.7091
Model performance on Angry speech (in validation): 
	Precision: 0.5952, Recall: 1.0000, F1_score: 0.7463
Model performance on Happy speech (in validation): 
	Precision: 0.5227, Recall: 0.4600, F1_score: 0.4894
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.5200, F1_score: 0.6842
Model performance on Sad speech (in validation): 
	Precision: 0.9565, Recall: 0.8800, F1_score: 0.9167
Epoch 7/100

Training Phase:
, 24.55it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1239/1600 [00:50<00:14, 24.60it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1486/1600 [01:00<00:04, 24.63it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 249/1600 [00:10<00:54, 24.87it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 498/1600 [00:20<00:44, 24.69it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 744/1600 [00:30<00:34, 24.50it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 991/1600 [00:40<00:24, 24.56it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1239/1600 [00:50<00:14, 24.62it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1487/1600 [01:00<00:04, 24.51it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          |Training loss: 97.7932, Training accuracy: 0.9819
Macro F1-score: 0.9819
Model performance on Angry speech (in training): 
	Precision: 0.9751, Recall: 0.9800, F1_score: 0.9776
Model performance on Happy speech (in training): 
	Precision: 0.9724, Recall: 0.9675, F1_score: 0.9699
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 98.9195, Validation accuracy: 0.8250
Macro F1-score: 0.8279
Model performance on Angry speech (in validation): 
	Precision: 0.7586, Recall: 0.8800, F1_score: 0.8148
Model performance on Happy speech (in validation): 
	Precision: 0.6481, Recall: 0.7000, F1_score: 0.6731
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7600, F1_score: 0.8636
Model performance on Sad speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
New best accuracy for layer 4 on epoch 7: 0.8250. Model saved.
Epoch 8/100

Training Phase:
 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 246/1600 [00:10<00:55, 24.58it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 492/1600 [00:20<00:45, 24.51it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 742/1600 [00:30<00:34, 24.70it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 992/1600 [00:40<00:24, 24.61it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1237/1600 [00:50<00:14, 24.52it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1485/1600 [01:00<00:04, 24.61it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 244/1600 [00:10<00:55, 24.33it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 488/1600 [00:20<00:45, 24.28it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 735/1600 [00:30<00:35, 24.43it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 983/1600 [00:40<00:25, 24.57it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1234/1600 [00:50<00:1Training loss: 77.1949, Training accuracy: 0.9825
Macro F1-score: 0.9825
Model performance on Angry speech (in training): 
	Precision: 0.9699, Recall: 0.9675, F1_score: 0.9687
Model performance on Happy speech (in training): 
	Precision: 0.9652, Recall: 0.9700, F1_score: 0.9676
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 172.5301, Validation accuracy: 0.8050
Macro F1-score: 0.8023
Model performance on Angry speech (in validation): 
	Precision: 0.6579, Recall: 1.0000, F1_score: 0.7937
Model performance on Happy speech (in validation): 
	Precision: 0.7027, Recall: 0.5200, F1_score: 0.5977
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Sad speech (in validation): 
	Precision: 0.9565, Recall: 0.8800, F1_score: 0.9167
Epoch 9/100

Training Phase:
Training loss: 103.7660, Training accuracy: 0.9781
Macro F1-score: 0.9781
Model performance on Angry speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9748, Recall: 0.9675, F1_score: 0.9711
Model performance on Neutral speech (in training): 
	Precision: 0.9752, Recall: 0.9825, F1_score: 0.9788
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862

Eval Phase: 
Validation loss: 171.8417, Validation accuracy: 0.7950
Macro F1-score: 0.8015
Model performance on Angry speech (in validation): 
	Precision: 0.7288, Recall: 0.8600, F1_score: 0.7890
Model performance on Happy speech (in validation): 
	Precision: 0.5932, Recall: 0.7000, F1_score: 0.6422
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Sad speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Epoch 10/100

Training Phase:
4, 24.74it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1491/1600 [01:00<00:04, 25.05it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 251/1600 [00:10<00:53, 25.02it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 502/1600 [00:20<00:45, 24.34it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 744/1600 [00:30<00:35, 24.26it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 996/1600 [00:40<00:24, 24.60it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1255/1600 [00:50<00:13, 25.04it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1514/1600 [01:00<00:03, 25.21it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 240/1600 [00:10<00:Training loss: 45.0692, Training accuracy: 0.9912
Macro F1-score: 0.9913
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 160.5429, Validation accuracy: 0.8100
Macro F1-score: 0.8107
Model performance on Angry speech (in validation): 
	Precision: 0.7273, Recall: 0.9600, F1_score: 0.8276
Model performance on Happy speech (in validation): 
	Precision: 0.6531, Recall: 0.6400, F1_score: 0.6465
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Sad speech (in validation): 
	Precision: 0.9375, Recall: 0.9000, F1_score: 0.9184
Epoch 11/100

Training Phase:
56, 23.99it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 481/1600 [00:20<00:46, 24.05it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 722/1600 [00:30<00:36, 24.02it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 965/1600 [00:40<00:26, 24.09it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1208/1600 [00:50<00:16, 24.07it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1450/1600 [01:00<00:06, 24.10it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 255/1600 [00:10<00:52, 25.41it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 510/1600 [00:20<00:43, 25.29it/s]Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 763/1600 [00:30<00:33, 25.26it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1017/1600 [00:40<00:23, 25.31it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1271/1600 [00:50<00:13, 25.28it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1525/1600 [01:0Training loss: 77.8565, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Happy speech (in training): 
	Precision: 0.9748, Recall: 0.9675, F1_score: 0.9711
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 186.7185, Validation accuracy: 0.7500
Macro F1-score: 0.7525
Model performance on Angry speech (in validation): 
	Precision: 0.5882, Recall: 1.0000, F1_score: 0.7407
Model performance on Happy speech (in validation): 
	Precision: 0.6250, Recall: 0.5000, F1_score: 0.5556
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Epoch 12/100

Training Phase:
Training loss: 51.5942, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 186.5381, Validation accuracy: 0.7350
Macro F1-score: 0.7225
Model performance on Angry speech (in validation): 
	Precision: 0.6923, Recall: 0.9000, F1_score: 0.7826
Model performance on Happy speech (in validation): 
	Precision: 0.5926, Recall: 0.6400, F1_score: 0.6154
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.4400, F1_score: 0.6111
Model performance on Sad speech (in validation): 
	Precision: 0.8136, Recall: 0.9600, F1_score: 0.8807
Epoch 13/100

Training Phase:
0<00:02, 25.29it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 241/1600 [00:10<00:56, 24.08it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 482/1600 [00:20<00:46, 24.00it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 723/1600 [00:30<00:36, 24.02it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 964/1600 [00:40<00:26, 24.00it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1205/1600 [00:50<00:16, 24.02it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1600 [01:00<00:06, 24.05it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 253/1600 [00:10<00:53, 25.25it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 506/1600 [00:20<00:44, 24.61it/sTraining loss: 64.1686, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Happy speech (in training): 
	Precision: 0.9775, Recall: 0.9775, F1_score: 0.9775
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 188.1057, Validation accuracy: 0.7450
Macro F1-score: 0.7490
Model performance on Angry speech (in validation): 
	Precision: 0.6104, Recall: 0.9400, F1_score: 0.7402
Model performance on Happy speech (in validation): 
	Precision: 0.5556, Recall: 0.5000, F1_score: 0.5263
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Sad speech (in validation): 
	Precision: 0.9756, Recall: 0.8000, F1_score: 0.8791
Epoch 14/100

Training Phase:
]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 748/1600 [00:30<00:35, 24.33it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 988/1600 [00:40<00:25, 24.16it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1228/1600 [00:50<00:15, 24.07it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1477/1600 [01:00<00:05, 24.34it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 242/1600 [00:10<00:56, 24.16it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 493/1600 [00:20<00:44, 24.71it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 744/1600 [00:30<00:34, 24.79it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 993/1600 [00:40<00:24, 24.58it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1236/1600 [00:50<00:14, 24.46it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1481/1600 [01:00<00:04, 24.44it/s]                                                           Training loss: 34.1247, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 116.2679, Validation accuracy: 0.8550
Macro F1-score: 0.8556
Model performance on Angry speech (in validation): 
	Precision: 0.7500, Recall: 0.9600, F1_score: 0.8421
Model performance on Happy speech (in validation): 
	Precision: 0.7778, Recall: 0.7000, F1_score: 0.7368
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Sad speech (in validation): 
	Precision: 0.9362, Recall: 0.8800, F1_score: 0.9072
New best accuracy for layer 4 on epoch 14: 0.8550. Model saved.
Epoch 15/100

Training Phase:
Training loss: 75.3566, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9727, Recall: 0.9800, F1_score: 0.9763
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 197.6706, Validation accuracy: 0.7400
Macro F1-score: 0.7244
Model performance on Angry speech (in validation): 
	Precision: 0.6098, Recall: 1.0000, F1_score: 0.7576
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.3600, F1_score: 0.4675
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7000, F1_score: 0.8235
Model performance on Sad speech (in validation): 
	Precision: 0.8036, Recall: 0.9000, F1_score: 0.8491
Epoch 16/100

Training Phase:
  Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 248/1600 [00:10<00:54, 24.73it/s]Training:  16%|â–ˆâ–Œ        | 248/1600 [00:20<00:54, 24.73it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 496/1600 [00:20<00:45, 24.21it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 737/1600 [00:30<00:35, 24.14it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 984/1600 [00:40<00:25, 24.33it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1239/1600 [00:50<00:14, 24.72it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1494/1600 [01:00<00:04, 24.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 244/1600 [00:10<00:55, 24.35it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 488/1600 [00:20<00:45, 24.35it/s]Training: Training loss: 50.4060, Training accuracy: 0.9888
Macro F1-score: 0.9887
Model performance on Angry speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9800, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 158.6121, Validation accuracy: 0.8050
Macro F1-score: 0.8097
Model performance on Angry speech (in validation): 
	Precision: 0.7258, Recall: 0.9000, F1_score: 0.8036
Model performance on Happy speech (in validation): 
	Precision: 0.6250, Recall: 0.7000, F1_score: 0.6604
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Sad speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Epoch 17/100

Training Phase:
Training loss: 31.7215, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 732/1600 [00:30<00:35, 24.21it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 978/1600 [00:40<00:25, 24.34it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1224/1600 [00:50<00:15, 24.12it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:00<00:05, 24.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 252/1600 [00:10<00:53, 25.11it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 504/1600 [00:20<00:45, 24.20it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 747/1600 [00:30<00:35, 24.23it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 990/1600 [00:40<00:25, 24.13it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1237/1600 [00:50<00:14, 24.32it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1484/1600 [01:01<00:04, 24.13it/s]                                                             EvaluatiValidation loss: 282.3948, Validation accuracy: 0.7300
Macro F1-score: 0.7246
Model performance on Angry speech (in validation): 
	Precision: 0.5435, Recall: 1.0000, F1_score: 0.7042
Model performance on Happy speech (in validation): 
	Precision: 0.6207, Recall: 0.3600, F1_score: 0.4557
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7000, F1_score: 0.8235
Model performance on Sad speech (in validation): 
	Precision: 0.9773, Recall: 0.8600, F1_score: 0.9149
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.8550

Test Phase: 
ng:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   4%|â–         | 9/200 [00:00<00:02, 89.46it/s]Testing:  10%|â–‰         | 19/200 [00:00<00:02, 90.03it/s]Testing:  14%|â–ˆâ–        | 29/200 [00:00<00:01, 90.42it/s]Testing:  20%|â–ˆâ–‰        | 39/200 [00:00<00:01, 89.56it/s]Testing:  24%|â–ˆâ–ˆâ–       | 49/200 [00:00<00:01, 89.81it/s]Testing:  30%|â–ˆâ–ˆâ–‰       | 59/200 [00:00<00:01, 89.47it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:00<00:01, 89.67it/s]Testing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:00<00:01, 89.69it/s]Testing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:00<00:01, 89.78it/s]Testing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [00:01<00:01, 89.71it/s]Testing:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [00:01<00:01, 89.07it/s]Testing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [00:01<00:00, 90.23it/s]Testing:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [00:01<00:00, 90.55it/s]Testing:  68%|â–ˆâ–ˆâ–ˆâTest loss: 160.8803, Test accuracy: 0.8500
Macro F1-score: 0.8507
Model performance on Angry speech (in test): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Model performance on Happy speech (in test): 
	Precision: 0.7551, Recall: 0.7400, F1_score: 0.7475
Model performance on Neutral speech (in test): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Sad speech (in test): 
	Precision: 0.9130, Recall: 0.8400, F1_score: 0.8750

======================= This is fold_1 on de =======================

Load dataset: 
Loading cn train data: fold_1...
Preprocess cn fold_1 data for de model
–ˆâ–ˆâ–ˆâ–Š   | 136/200 [00:01<00:00, 91.12it/s]Testing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [00:01<00:00, 91.76it/s]Testing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [00:01<00:00, 91.65it/s]Testing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:01<00:00, 91.09it/s]Testing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [00:01<00:00, 92.23it/s]Testing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [00:02<00:00, 92.52it/s]Testing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [00:02<00:00, 90.66it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   1%|          | 19/1600 [00:00<00:08, 175.94 examples/s]Map:   2%|â–         | 39/1600 [00:00<00:08, 185.92 examples/s]Map:   4%|â–         | 65/1600 [00:00<00:08, 175.45 examples/s]Map:   6%|â–Œ         | 89/1600 [00:00<00:09, 164.46 examples/s]Map:   7%|â–‹         | 109/1600 [00:00<00:10, 141.76 examples/s]Map:   8%|â–Š         | 127/1600 [00:00<00:09, 149.59 examples/s]Map:   9%|â–‰         | 143/1600 [00:00<00:09, 150.39 examples/s]Map:  10%|â–ˆ         | 162/1600 [00:01<00:08, 159.89 examples/s]Map:  11%|â–ˆâ–        | 182/1600 [00:01<00:08, 169.70 examples/s]Map:  13%|â–ˆâ–Ž        | 207/1600 [00:01<00:08, 157.28 examples/s]Map:  14%|â–ˆâ–        | 230/1600 [00:01<00:08, 152.26 examples/s]Map:  15%|â–ˆâ–Œ        | 246/1600 [00:01<00:09, 149.21 examples/s]Map:  16%|â–ˆâ–‹        | 264/1600 [00:01<00:09, 134.82 examples/s]Map:  18%|â–ˆâ–Š        | 282/1600 [00:01<00:10, 123.15 examples/s]Map:  19%|â–ˆâ–Š        | 298/1600 [00:02<00:12, 102.89 examples/s]Map:  20%|â–ˆâ–‰        | 312/1600 [00:02<00:11, 108.92 examples/s]Map:  20%|â–ˆâ–ˆ        | 324/1600 [00:02<00:11, 110.56 examples/s]Map:  21%|â–ˆâ–ˆ        | 338/1600 [00:02<00:11, 114.25 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 352/1600 [00:02<00:10, 117.89 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 371/1600 [00:02<00:10, 117.75 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 384/1600 [00:02<00:10, 117.83 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 397/1600 [00:03<00:11, 104.24 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 413/1600 [00:03<00:10, 116.59 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 431/1600 [00:03<00:08, 130.65 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 448/1600 [00:03<00:08, 138.60 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 470/1600 [00:03<00:08, 135.30 examples/s]Map:  30%|â–ˆâ–ˆâ–ˆ       | 486/1600 [00:03<00:09, 118.31 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 499/1600 [00:03<00:09, 119.42 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 516/1600 [00:03<00:08, 130.63 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 533/1600 [00:03<00:07, 138.81 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 550/1600 [00:04<00:07, 144.82 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 568/1600 [00:04<00:06, 154.03 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 593/1600 [00:04<00:06, 147.43 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 613/1600 [00:04<00:06, 157.04 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 631/1600 [00:04<00:05, 161.95 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 650/1600 [00:04<00:05, 166.36 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 673/1600 [00:04<00:05, 156.61 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 695/1600 [00:05<00:06, 137.59 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 711/1600 [00:05<00:06, 139.53 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 728/1600 [00:05<00:06, 144.98 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 744/1600 [00:05<00:05, 146.17 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 762/1600 [00:05<00:05, 152.95 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 787/1600 [00:05<00:05, 148.35 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 808/1600 [00:05<00:04, 161.55 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 827/1600 [00:05<00:04, 166.40 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 845/1600 [00:05<00:04, 168.52 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 869/1600 [00:06<00:04, 161.95 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 887/1600 [00:06<00:05, 142.51 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 903/1600 [00:06<00:04, 144.41 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 923/1600 [00:06<00:04, 156.48 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 941/1600 [00:06<00:04, 159.87 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 960/1600 [00:06<00:03, 166.46 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 979/1600 [00:06<00:03, 155.75 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 999/1600 [00:06<00:03, 166.01 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 999/1600 [00:21<00:03, 166.01 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [00:45<08:24,  1.19 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1020/1600 [00:45<05:01,  1.92 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [00:45<03:09,  2.96 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1061/1600 [00:45<02:00,  4.48 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1079/1600 [00:45<01:22,  6.28 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1098/1600 [00:45<00:56,  8.88 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1115/1600 [00:46<00:39, 12.13 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1132/1600 [00:46<00:28, 16.55 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1154/1600 [00:46<00:19, 23.35 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1173/1600 [00:46<00:13, 31.56 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1191/1600 [00:46<00:09, 41.22 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1207/1600 [00:46<00:07, 51.39 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1229/1600 [00:46<00:05, 65.51 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1248/1600 [00:47<00:04, 72.29 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1266/1600 [00:47<00:04, 80.00 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1282/1600 [00:47<00:03, 85.23 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1294/1600 [00:47<00:03, 89.58 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1307/1600 [00:47<00:03, 94.53 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1320/1600 [00:47<00:02, 100.04 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1334/1600 [00:47<00:02, 107.70 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1600 [00:47<00:02, 113.45 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1362/1600 [00:48<00:02, 103.44 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1379/1600 [00:48<00:01, 117.48 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1393/1600 [00:48<00:01, 119.19 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1411/1600 [00:48<00:01, 132.95 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1428/1600 [00:48<00:01, 141.35 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1446/1600 [00:48<00:01, 151.00 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1462/1600 [00:48<00:00, 151.29 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1481/1600 [00:49<00:01, 90.90 examples/s] Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1496/1600 [00:49<00:01, 101.02 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1512/1600 [00:49<00:00, 113.11 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1528/1600 [00:49<00:00, 122.16 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1546/1600 [00:49<00:00, 134.09 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1563/1600 [00:49<00:00, 141.63 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1581/1600 [00:49<00:00, 150.55 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1594/1600 [01:02<00:00, 150.55 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:13<00:00,  2.42 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:13<00:00, 21.66 examples/s]
Loading cn eval data: fold_1...
Preprocess cn fold_1 data for de model
Loading cn test data: fold_1...
Preprocess cn fold_1 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 714.3942, Training accuracy: 0.8356
Macro F1-score: 0.8353
Model performance on Angry speech (in training): 
	Precision: 0.7861, Recall: 0.7900, F1_score: 0.7880
Model performance on Happy speech (in training): 
	Precision: 0.7784, Recall: 0.7550, F1_score: 0.7665
Model performance on Neutral speech (in training): 
	Precision: 0.8602, Recall: 0.8925, F1_score: 0.8761
Model performance on Sad speech (in training): 
	Precision: 0.9165, Recall: 0.9050, F1_score: 0.9107

Eval Phase: 
Validation loss: 179.7073, Validation accuracy: 0.7400
Macro F1-score: 0.7134
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Happy speech (in validation): 
	Precision: 0.6111, Recall: 0.2200, F1_score: 0.3235
Model performance on Neutral speech (in validation): 
	Precision: 0.5376, Recall: 1.0000, F1_score: 0.6993
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
New best accuracy for layer 4 on epoch 1: 0.7400. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.64it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 488/1600 [00:20<00:45, 24.47it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 739/1600 [00:30<00:34, 24.71it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 989/1600 [00:40<00:24, 24.74it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1238/1600 [00:50<00:14, 24.62it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1483/1600 [01:00<00:04, 24.53it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.60it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 485/1600 [00:20<00:45, 24.36it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 734/1600 [00:30<00:36, 24.04it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 974/1600 [00:40<00:26, 24.00it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | Training loss: 260.5366, Training accuracy: 0.9337
Macro F1-score: 0.9336
Model performance on Angry speech (in training): 
	Precision: 0.9000, Recall: 0.9000, F1_score: 0.9000
Model performance on Happy speech (in training): 
	Precision: 0.9028, Recall: 0.8825, F1_score: 0.8925
Model performance on Neutral speech (in training): 
	Precision: 0.9465, Recall: 0.9725, F1_score: 0.9593
Model performance on Sad speech (in training): 
	Precision: 0.9849, Recall: 0.9800, F1_score: 0.9825

Eval Phase: 
Validation loss: 149.5171, Validation accuracy: 0.7800
Macro F1-score: 0.7610
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Happy speech (in validation): 
	Precision: 0.7826, Recall: 0.3600, F1_score: 0.4932
Model performance on Neutral speech (in validation): 
	Precision: 0.7586, Recall: 0.8800, F1_score: 0.8148
Model performance on Sad speech (in validation): 
	Precision: 0.6667, Recall: 1.0000, F1_score: 0.8000
New best accuracy for layer 4 on epoch 2: 0.7800. Model saved.
Epoch 3/100

Training Phase:
Training loss: 172.9882, Training accuracy: 0.9644
Macro F1-score: 0.9644
Model performance on Angry speech (in training): 
	Precision: 0.9429, Recall: 0.9500, F1_score: 0.9465
Model performance on Happy speech (in training): 
	Precision: 0.9421, Recall: 0.9350, F1_score: 0.9385
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887

Eval Phase: 
974/1600 [00:50<00:26, 24.00it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1213/1600 [00:50<00:16, 23.88it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1450/1600 [01:00<00:06, 23.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.66it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 476/1600 [00:20<00:47, 23.75it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 718/1600 [00:30<00:36, 23.95it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 718/1600 [00:40<00:36, 23.95it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 943/1600 [00:40<00:28, 23.10it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1182/1600 [00:50<00:17, 23.38it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1421/1600 [01:00<00:07, 23.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, Validation loss: 285.5641, Validation accuracy: 0.6800
Macro F1-score: 0.6810
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Happy speech (in validation): 
	Precision: 0.6000, Recall: 0.3600, F1_score: 0.4500
Model performance on Neutral speech (in validation): 
	Precision: 0.4902, Recall: 1.0000, F1_score: 0.6579
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Epoch 4/100

Training Phase:
Training loss: 105.2930, Training accuracy: 0.9725
Macro F1-score: 0.9725
Model performance on Angry speech (in training): 
	Precision: 0.9527, Recall: 0.9575, F1_score: 0.9551
Model performance on Happy speech (in training): 
	Precision: 0.9545, Recall: 0.9450, F1_score: 0.9497
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 112.4772, Validation accuracy: 0.8000
Macro F1-score: 0.7961
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.5600, F1_score: 0.7179
Model performance on Happy speech (in validation): 
	Precision: 0.6071, Recall: 0.6800, F1_score: 0.6415
Model performance on Neutral speech (in validation): 
	Precision: 0.7424, Recall: 0.9800, F1_score: 0.8448
Model performance on Sad speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
New best accuracy for layer 4 on epoch 4: 0.8000. Model saved.
Epoch 5/100

Training Phase:
?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.55it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 472/1600 [00:20<00:48, 23.40it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 717/1600 [00:30<00:37, 23.86it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 717/1600 [00:40<00:37, 23.86it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 953/1600 [00:40<00:27, 23.63it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1200/1600 [00:50<00:16, 24.01it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1454/1600 [01:00<00:05, 24.46it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 239/1600 [00:10<00:57, 23.80it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 495/1600 [00:20<00:44, 24.84it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 751/1600 [00:30<00:3Training loss: 122.1732, Training accuracy: 0.9769
Macro F1-score: 0.9769
Model performance on Angry speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Happy speech (in training): 
	Precision: 0.9653, Recall: 0.9750, F1_score: 0.9701
Model performance on Neutral speech (in training): 
	Precision: 0.9799, Recall: 0.9775, F1_score: 0.9787
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862

Eval Phase: 
Validation loss: 207.1016, Validation accuracy: 0.7900
Macro F1-score: 0.7550
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9167, Recall: 0.2200, F1_score: 0.3548
Model performance on Neutral speech (in validation): 
	Precision: 0.5682, Recall: 1.0000, F1_score: 0.7246
Model performance on Sad speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Epoch 6/100

Training Phase:
Training loss: 57.7078, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
4, 24.63it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:40<00:24, 24.63it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1248/1600 [00:50<00:14, 24.76it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1499/1600 [01:00<00:04, 24.86it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 251/1600 [00:10<00:53, 25.07it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 502/1600 [00:20<00:44, 24.74it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 748/1600 [00:30<00:35, 24.11it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 990/1600 [00:40<00:25, 24.11it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1243/1600 [00:50<00:14, 24.50it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1496/1600 [01:01<00:04, 24.18it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s] Validation loss: 181.0341, Validation accuracy: 0.7950
Macro F1-score: 0.7842
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Model performance on Happy speech (in validation): 
	Precision: 0.6562, Recall: 0.4200, F1_score: 0.5122
Model performance on Neutral speech (in validation): 
	Precision: 0.6494, Recall: 1.0000, F1_score: 0.7874
Model performance on Sad speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Epoch 7/100

Training Phase:
Training loss: 49.5218, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 249.8311, Validation accuracy: 0.7800
Macro F1-score: 0.7717
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Model performance on Happy speech (in validation): 
	Precision: 0.6452, Recall: 0.4000, F1_score: 0.4938
Model performance on Neutral speech (in validation): 
	Precision: 0.6098, Recall: 1.0000, F1_score: 0.7576
Model performance on Sad speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Epoch 8/100

Training Phase:
                                                  Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.52it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 479/1600 [00:20<00:46, 23.94it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 722/1600 [00:30<00:36, 24.02it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 964/1600 [00:40<00:26, 23.86it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1203/1600 [00:50<00:16, 23.86it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1442/1600 [01:00<00:06, 23.81it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 244/1600 [00:10<00:55, 24.37it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 488/1600 [00:20<00:45, 24.22it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 735/1600 [00:30<00:35, 24.41it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 982/1600 [00:40<00:25, 2Training loss: 92.5211, Training accuracy: 0.9788
Macro F1-score: 0.9787
Model performance on Angry speech (in training): 
	Precision: 0.9749, Recall: 0.9700, F1_score: 0.9724
Model performance on Happy speech (in training): 
	Precision: 0.9701, Recall: 0.9725, F1_score: 0.9713
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862

Eval Phase: 
Validation loss: 196.8285, Validation accuracy: 0.7750
Macro F1-score: 0.7648
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Happy speech (in validation): 
	Precision: 0.7037, Recall: 0.3800, F1_score: 0.4935
Model performance on Neutral speech (in validation): 
	Precision: 0.5814, Recall: 1.0000, F1_score: 0.7353
Model performance on Sad speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Epoch 9/100

Training Phase:
Training loss: 29.4959, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
4.40it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1226/1600 [00:50<00:15, 24.18it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:00<00:05, 24.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 240/1600 [00:10<00:56, 23.92it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 484/1600 [00:20<00:46, 24.17it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 728/1600 [00:30<00:35, 24.23it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 971/1600 [00:40<00:26, 24.08it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1215/1600 [00:50<00:15, 24.16it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1215/1600 [01:00<00:15, 24.16it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1459/1600 [01:00<00:05, 24.16it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]        Validation loss: 148.9121, Validation accuracy: 0.8150
Macro F1-score: 0.8001
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.7917, Recall: 0.3800, F1_score: 0.5135
Model performance on Neutral speech (in validation): 
	Precision: 0.6250, Recall: 1.0000, F1_score: 0.7692
Model performance on Sad speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
New best accuracy for layer 4 on epoch 9: 0.8150. Model saved.
Epoch 10/100

Training Phase:
Training loss: 64.5339, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9776, Recall: 0.9800, F1_score: 0.9788
Model performance on Happy speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925

Eval Phase: 
Validation loss: 61.8488, Validation accuracy: 0.8900
Macro F1-score: 0.8878
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.8222, Recall: 0.7400, F1_score: 0.7789
Model performance on Neutral speech (in validation): 
	Precision: 0.8750, Recall: 0.9800, F1_score: 0.9245
Model performance on Sad speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
New best accuracy for layer 4 on epoch 10: 0.8900. Model saved.
Epoch 11/100

Training Phase:
                                           Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 235/1600 [00:10<00:58, 23.50it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 473/1600 [00:20<00:47, 23.65it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 711/1600 [00:30<00:37, 23.72it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 949/1600 [00:40<00:27, 23.58it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1189/1600 [00:50<00:17, 23.73it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1429/1600 [01:00<00:07, 23.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 243/1600 [00:10<00:55, 24.29it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 486/1600 [00:20<00:46, 23.99it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 724/1600 [00:30<00:37, 23.62it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 964/1600 [00:40<00:26, 23.75it/s]Training loss: 51.7620, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 210.8895, Validation accuracy: 0.7250
Macro F1-score: 0.7063
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.6111, Recall: 0.2200, F1_score: 0.3235
Model performance on Neutral speech (in validation): 
	Precision: 0.5102, Recall: 1.0000, F1_score: 0.6757
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Epoch 12/100

Training Phase:
Training loss: 33.0728, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 127.7698, Validation accuracy: 0.8100
Macro F1-score: 0.8100
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6800, F1_score: 0.8095
Model performance on Happy speech (in validation): 
	Precision: 0.6735, Recall: 0.6600, F1_score: 0.6667
Model performance on Neutral speech (in validation): 
	Precision: 0.7042, Recall: 1.0000, F1_score: 0.8264
Model performance on Sad speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Epoch 13/100

Training Phase:
Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1209/1600 [00:50<00:16, 24.00it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1454/1600 [01:00<00:06, 24.00it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 246/1600 [00:10<00:55, 24.54it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 492/1600 [00:20<00:45, 24.38it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 735/1600 [00:30<00:35, 24.13it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 977/1600 [00:40<00:25, 24.12it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1220/1600 [00:50<00:15, 24.17it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1463/1600 [01:00<00:05, 24.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?Training loss: 42.9805, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 107.2444, Validation accuracy: 0.8600
Macro F1-score: 0.8599
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.8611, Recall: 0.6200, F1_score: 0.7209
Model performance on Neutral speech (in validation): 
	Precision: 0.6849, Recall: 1.0000, F1_score: 0.8130
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Epoch 14/100

Training Phase:
, ?it/s]Training:  15%|â–ˆâ–        | 239/1600 [00:10<00:57, 23.80it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 482/1600 [00:20<00:46, 24.07it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 725/1600 [00:30<00:36, 24.05it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 966/1600 [00:40<00:26, 23.92it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1204/1600 [00:50<00:16, 23.75it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1441/1600 [01:00<00:06, 23.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 242/1600 [00:10<00:56, 24.13it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 484/1600 [00:20<00:46, 23.98it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 725/1600 [00:30<00:36, 24.03it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 971/1600 [00:40<00:25, 24.25it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1222/1600 [00:50<00:15, 24.52it/s]TrainingTraining loss: 26.5048, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 139.6044, Validation accuracy: 0.8250
Macro F1-score: 0.8291
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.8857, Recall: 0.6200, F1_score: 0.7294
Model performance on Neutral speech (in validation): 
	Precision: 0.6173, Recall: 1.0000, F1_score: 0.7634
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Epoch 15/100

Training Phase:
Training loss: 19.7279, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 201.8596, Validation accuracy: 0.7800
Macro F1-score: 0.7695
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Happy speech (in validation): 
	Precision: 0.7586, Recall: 0.4400, F1_score: 0.5570
Model performance on Neutral speech (in validation): 
	Precision: 0.6329, Recall: 1.0000, F1_score: 0.7752
Model performance on Sad speech (in validation): 
	Precision: 0.8545, Recall: 0.9400, F1_score: 0.8952
Epoch 16/100

Training Phase:
:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1472/1600 [01:00<00:05, 24.63it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.65it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 474/1600 [00:20<00:47, 23.54it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 715/1600 [00:30<00:37, 23.75it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 955/1600 [00:40<00:27, 23.67it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1194/1600 [00:50<00:17, 23.72it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1433/1600 [01:00<00:07, 23.64it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 239/1600 [00:10<00:57, 23.84it/s]Training:  30Training loss: 41.6825, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9849, Recall: 0.9800, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 202.2286, Validation accuracy: 0.7800
Macro F1-score: 0.7792
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Happy speech (in validation): 
	Precision: 0.7273, Recall: 0.4800, F1_score: 0.5783
Model performance on Neutral speech (in validation): 
	Precision: 0.5952, Recall: 1.0000, F1_score: 0.7463
Model performance on Sad speech (in validation): 
	Precision: 0.9762, Recall: 0.8200, F1_score: 0.8913
Epoch 17/100

Training Phase:
%|â–ˆâ–ˆâ–‰       | 478/1600 [00:20<00:47, 23.71it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 717/1600 [00:30<00:37, 23.76it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 971/1600 [00:40<00:25, 24.38it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1226/1600 [00:50<00:15, 24.76it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1481/1600 [01:00<00:04, 24.41it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.62it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 475/1600 [00:20<00:47, 23.67it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 713/1600 [00:30<00:37, 23.72it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 713/1600 [00:40<00:37, 23.72it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 951/1600 [00:40<00:27, 23.72it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1194/1600 [00:50<00:16, 23.90it/s]Training:  90%|â–ˆâ–ˆâTraining loss: 16.6260, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 280.0526, Validation accuracy: 0.7350
Macro F1-score: 0.7362
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Happy speech (in validation): 
	Precision: 0.5814, Recall: 0.5000, F1_score: 0.5376
Model performance on Neutral speech (in validation): 
	Precision: 0.5882, Recall: 1.0000, F1_score: 0.7407
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Epoch 18/100

Training Phase:
Training loss: 30.4413, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 374.1910, Validation accuracy: 0.6900
Macro F1-score: 0.6801
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.7949, Recall: 0.6200, F1_score: 0.6966
Model performance on Neutral speech (in validation): 
	Precision: 0.4808, Recall: 1.0000, F1_score: 0.6494
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.3000, F1_score: 0.4615
Epoch 19/100

Training Phase:
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1437/1600 [01:00<00:06, 24.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 246/1600 [00:10<00:55, 24.56it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 492/1600 [00:20<00:45, 24.26it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 736/1600 [00:30<00:35, 24.29it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 980/1600 [00:40<00:25, 24.20it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 980/1600 [00:50<00:25, 24.20it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1222/1600 [00:50<00:15, 24.17it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1464/1600 [01:00<00:05, 24.15it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|Training loss: 19.2449, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 205.3021, Validation accuracy: 0.8150
Macro F1-score: 0.8189
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Model performance on Happy speech (in validation): 
	Precision: 0.7708, Recall: 0.7400, F1_score: 0.7551
Model performance on Neutral speech (in validation): 
	Precision: 0.6579, Recall: 1.0000, F1_score: 0.7937
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Epoch 20/100

Training Phase:
â–ˆâ–        | 236/1600 [00:10<00:57, 23.55it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 477/1600 [00:20<00:47, 23.84it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 719/1600 [00:30<00:36, 23.98it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 963/1600 [00:40<00:26, 24.12it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1207/1600 [00:50<00:16, 24.10it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1448/1600 [01:00<00:06, 24.05it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.70it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 476/1600 [00:20<00:47, 23.79it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 716/1600 [00:30<00:37, 23.85it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 956/1600 [00:40<00:27, 23.51it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1192/1600 [00:50<00:17, 23.51it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Training loss: 28.5209, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 154.7676, Validation accuracy: 0.8500
Macro F1-score: 0.8524
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
Model performance on Happy speech (in validation): 
	Precision: 0.8293, Recall: 0.6800, F1_score: 0.7473
Model performance on Neutral speech (in validation): 
	Precision: 0.6849, Recall: 1.0000, F1_score: 0.8130
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.8900

Test Phase: 
ˆâ–ˆâ–ˆâ–‰ | 1428/1600 [01:00<00:07, 23.42it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:02, 90.70it/s]Testing:  10%|â–ˆ         | 21/200 [00:00<00:01, 97.70it/s]Testing:  16%|â–ˆâ–Œ        | 31/200 [00:00<00:01, 97.65it/s]Testing:  20%|â–ˆâ–ˆ        | 41/200 [00:00<00:01, 97.34it/s]Testing:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:00<00:01, 98.76it/s]Testing:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [00:00<00:01, 99.03it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [00:00<00:01, 99.06it/s]Testing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:00<00:01, 99.67it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:00<00:01, 98.91it/s]Testing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:01<00:00, 98.57it/s]Testing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:01<00:00, 99.99it/s]Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124Test loss: 95.3533, Test accuracy: 0.8450
Macro F1-score: 0.8424
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Happy speech (in test): 
	Precision: 0.7292, Recall: 0.7000, F1_score: 0.7143
Model performance on Neutral speech (in test): 
	Precision: 0.8305, Recall: 0.9800, F1_score: 0.8991
Model performance on Sad speech (in test): 
	Precision: 0.8571, Recall: 0.9600, F1_score: 0.9057

======================= This is fold_2 on de =======================

Load dataset: 
Loading cn train data: fold_2...
Preprocess cn fold_2 data for de model
/200 [00:01<00:00, 99.97it/s]Testing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [00:01<00:00, 99.84it/s]Testing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [00:01<00:00, 100.66it/s]Testing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [00:01<00:00, 98.92it/s] Testing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:01<00:00, 97.71it/s]Testing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [00:01<00:00, 97.81it/s]Testing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [00:01<00:00, 97.53it/s]Testing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [00:01<00:00, 97.45it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 4/1600 [00:00<00:44, 36.16 examples/s]Map:   1%|          | 11/1600 [00:00<00:30, 52.29 examples/s]Map:   1%|â–         | 23/1600 [00:00<00:19, 80.60 examples/s]Map:   2%|â–         | 39/1600 [00:00<00:14, 108.18 examples/s]Map:   3%|â–Ž         | 54/1600 [00:00<00:14, 110.34 examples/s]Map:   4%|â–         | 68/1600 [00:00<00:13, 116.73 examples/s]Map:   5%|â–Œ         | 80/1600 [00:00<00:13, 115.56 examples/s]Map:   6%|â–Œ         | 94/1600 [00:00<00:12, 119.05 examples/s]Map:   7%|â–‹         | 107/1600 [00:00<00:12, 120.28 examples/s]Map:   8%|â–Š         | 122/1600 [00:01<00:11, 127.10 examples/s]Map:   9%|â–Š         | 138/1600 [00:01<00:11, 132.84 examples/s]Map:  10%|â–‰         | 158/1600 [00:01<00:11, 122.22 examples/s]Map:  11%|â–ˆ         | 177/1600 [00:01<00:10, 136.79 examples/s]Map:  12%|â–ˆâ–        | 196/1600 [00:01<00:09, 148.47 examples/s]Map:  14%|â–ˆâ–Ž        | 216/1600 [00:01<00:08, 158.94 examples/s]Map:  15%|â–ˆâ–        | 235/1600 [00:01<00:08, 164.92 examples/s]Map:  16%|â–ˆâ–Œ        | 254/1600 [00:01<00:09, 146.18 examples/s]Map:  17%|â–ˆâ–‹        | 273/1600 [00:02<00:09, 134.17 examples/s]Map:  18%|â–ˆâ–Š        | 292/1600 [00:02<00:10, 128.54 examples/s]Map:  20%|â–ˆâ–‰        | 312/1600 [00:02<00:10, 126.95 examples/s]Map:  20%|â–ˆâ–ˆ        | 326/1600 [00:02<00:09, 128.16 examples/s]Map:  21%|â–ˆâ–ˆâ–       | 342/1600 [00:02<00:10, 115.70 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 358/1600 [00:02<00:10, 123.90 examples/s]Map:  24%|â–ˆâ–ˆâ–Ž       | 379/1600 [00:02<00:08, 141.98 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 397/1600 [00:03<00:08, 149.44 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 419/1600 [00:03<00:08, 147.06 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 438/1600 [00:03<00:08, 130.23 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 452/1600 [00:03<00:08, 130.11 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 467/1600 [00:03<00:09, 118.75 examples/s]Map:  30%|â–ˆâ–ˆâ–ˆ       | 483/1600 [00:03<00:09, 111.75 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 498/1600 [00:03<00:10, 106.48 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 511/1600 [00:04<00:09, 110.40 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 524/1600 [00:04<00:09, 112.37 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 541/1600 [00:04<00:10, 100.03 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–      | 555/1600 [00:04<00:09, 105.23 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 568/1600 [00:04<00:09, 107.14 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 580/1600 [00:04<00:09, 109.93 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 592/1600 [00:04<00:09, 110.39 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 608/1600 [00:04<00:08, 121.32 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1600 [00:05<00:08, 114.02 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 641/1600 [00:05<00:07, 125.79 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 655/1600 [00:05<00:07, 126.78 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1600 [00:05<00:07, 130.32 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1600 [00:05<00:07, 126.45 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 702/1600 [00:05<00:07, 125.74 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 720/1600 [00:05<00:06, 136.87 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 738/1600 [00:05<00:06, 124.79 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 755/1600 [00:06<00:06, 134.70 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 773/1600 [00:06<00:05, 145.70 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 792/1600 [00:06<00:05, 156.10 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 811/1600 [00:06<00:04, 162.18 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:06<00:09, 78.13 examples/s] Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 854/1600 [00:07<00:08, 92.95 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 875/1600 [00:07<00:07, 103.03 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 889/1600 [00:07<00:06, 109.19 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 903/1600 [00:07<00:06, 114.83 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 922/1600 [00:07<00:06, 112.49 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 938/1600 [00:07<00:05, 120.16 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 955/1600 [00:07<00:04, 130.01 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 973/1600 [00:07<00:04, 141.69 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 993/1600 [00:08<00:03, 154.86 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 993/1600 [00:21<00:03, 154.86 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [00:48<08:12,  1.22 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1021/1600 [00:48<04:56,  1.95 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1600 [00:48<02:59,  3.09 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1064/1600 [00:48<01:59,  4.48 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1087/1600 [00:48<01:16,  6.70 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1113/1600 [00:48<00:47, 10.17 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1132/1600 [00:49<00:34, 13.66 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1156/1600 [00:49<00:23, 19.28 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1177/1600 [00:49<00:16, 26.12 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1197/1600 [00:49<00:11, 34.62 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1223/1600 [00:49<00:07, 47.62 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1248/1600 [00:49<00:05, 59.49 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1266/1600 [00:49<00:04, 67.64 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1286/1600 [00:50<00:04, 77.18 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1306/1600 [00:50<00:03, 86.33 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1323/1600 [00:50<00:02, 98.21 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1339/1600 [00:50<00:02, 108.75 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1355/1600 [00:50<00:02, 117.70 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1371/1600 [00:50<00:02, 112.33 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1389/1600 [00:50<00:01, 125.97 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1406/1600 [00:50<00:01, 133.92 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1428/1600 [00:51<00:01, 133.26 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1449/1600 [00:51<00:01, 132.70 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [00:51<00:01, 123.67 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1482/1600 [00:51<00:01, 104.46 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1494/1600 [00:51<00:01, 104.88 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1507/1600 [00:51<00:00, 106.77 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1520/1600 [00:51<00:00, 108.65 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1534/1600 [00:52<00:00, 113.95 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1548/1600 [00:52<00:00, 118.53 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1564/1600 [00:52<00:00, 126.99 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1582/1600 [00:52<00:00, 138.76 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1596/1600 [01:11<00:00, 138.76 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:16<00:00,  2.28 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:16<00:00, 21.01 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   4%|â–Ž         | 7/200 [00:00<00:03, 56.24 examples/s]Map:   8%|â–Š         | 15/200 [00:00<00:03, 61.41 examples/s]Map:  18%|â–ˆâ–Š        | 35/200 [00:00<00:01, 116.11 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:00<00:01, 126.03 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [00:00<00:00, 139.33 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [00:00<00:00, 151.97 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:00<00:00, 138.41 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [00:00<00:00, 141.06 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [00:01<00:00, 143.03 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:01<00:00, 145.31 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [00:01<00:00, 148.36 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [00:01<00:00, 148.35 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:08<00:00, 23.59 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   4%|â–         | 8/200 [00:00<00:03, 56.53 examples/s]Map:  10%|â–ˆ         | 21/200 [00:00<00:01, 92.20 examples/s]Map:  20%|â–ˆâ–ˆ        | 40/200 [00:00<00:01, 128.21 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 54/200 [00:00<00:01, 125.08 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:00<00:00, 137.66 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:00<00:00, 153.25 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:00<00:00, 144.59 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:00<00:00, 147.83 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [00:01<00:00, 145.11 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [00:01<00:00, 149.89 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [00:01<00:00, 156.32 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:08<00:00, 24.11 examples/s] 
Loading cn eval data: fold_2...
Preprocess cn fold_2 data for de model
Loading cn test data: fold_2...
Preprocess cn fold_2 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 445.4853, Training accuracy: 0.9087
Macro F1-score: 0.9085
Model performance on Angry speech (in training): 
	Precision: 0.8997, Recall: 0.8750, F1_score: 0.8872
Model performance on Happy speech (in training): 
	Precision: 0.8863, Recall: 0.8575, F1_score: 0.8717
Model performance on Neutral speech (in training): 
	Precision: 0.8865, Recall: 0.9375, F1_score: 0.9113
Model performance on Sad speech (in training): 
	Precision: 0.9626, Recall: 0.9650, F1_score: 0.9638

Eval Phase: 
Validation loss: 244.6117, Validation accuracy: 0.6900
Macro F1-score: 0.6461
Model performance on Angry speech (in validation): 
	Precision: 0.5319, Recall: 1.0000, F1_score: 0.6944
Model performance on Happy speech (in validation): 
	Precision: 0.8889, Recall: 0.1600, F1_score: 0.2712
Model performance on Neutral speech (in validation): 
	Precision: 0.8750, Recall: 0.7000, F1_score: 0.7778
Model performance on Sad speech (in validation): 
	Precision: 0.7895, Recall: 0.9000, F1_score: 0.8411
New best accuracy for layer 4 on epoch 1: 0.6900. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 228/1600 [00:10<01:00, 22.75it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 471/1600 [00:20<00:47, 23.61it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 714/1600 [00:30<00:37, 23.86it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 956/1600 [00:40<00:26, 23.93it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1201/1600 [00:50<00:16, 24.12it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1446/1600 [01:00<00:06, 23.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 247/1600 [00:10<00:54, 24.61it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 494/1600 [00:20<00:45, 24.13it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 733/1600 [00:30<00:36, 24.00it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 975/1600 [00:40<00:25, 24.06it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | Training loss: 142.7666, Training accuracy: 0.9738
Macro F1-score: 0.9737
Model performance on Angry speech (in training): 
	Precision: 0.9652, Recall: 0.9700, F1_score: 0.9676
Model performance on Happy speech (in training): 
	Precision: 0.9622, Recall: 0.9550, F1_score: 0.9586
Model performance on Neutral speech (in training): 
	Precision: 0.9776, Recall: 0.9800, F1_score: 0.9788
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 183.3172, Validation accuracy: 0.7450
Macro F1-score: 0.7379
Model performance on Angry speech (in validation): 
	Precision: 0.6515, Recall: 0.8600, F1_score: 0.7414
Model performance on Happy speech (in validation): 
	Precision: 0.9630, Recall: 0.5200, F1_score: 0.6753
Model performance on Neutral speech (in validation): 
	Precision: 0.7857, Recall: 0.6600, F1_score: 0.7174
Model performance on Sad speech (in validation): 
	Precision: 0.7231, Recall: 0.9400, F1_score: 0.8174
New best accuracy for layer 4 on epoch 2: 0.7450. Model saved.
Epoch 3/100

Training Phase:
Training loss: 94.5146, Training accuracy: 0.9775
Macro F1-score: 0.9775
Model performance on Angry speech (in training): 
	Precision: 0.9799, Recall: 0.9775, F1_score: 0.9787
Model performance on Happy speech (in training): 
	Precision: 0.9673, Recall: 0.9625, F1_score: 0.9649
Model performance on Neutral speech (in training): 
	Precision: 0.9752, Recall: 0.9825, F1_score: 0.9788
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875

Eval Phase: 
Validation loss: 306.4798, Validation accuracy: 0.6800
Macro F1-score: 0.6273
Model performance on Angry speech (in validation): 
	Precision: 0.5275, Recall: 0.9600, F1_score: 0.6809
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1200, F1_score: 0.2143
Model performance on Neutral speech (in validation): 
	Precision: 0.7302, Recall: 0.9200, F1_score: 0.8142
Model performance on Sad speech (in validation): 
	Precision: 0.9000, Recall: 0.7200, F1_score: 0.8000
Epoch 4/100

Training Phase:
1217/1600 [00:50<00:15, 23.96it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1456/1600 [01:00<00:06, 23.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.62it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 477/1600 [00:20<00:47, 23.80it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 717/1600 [00:30<00:37, 23.63it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 955/1600 [00:40<00:27, 23.68it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1194/1600 [00:50<00:17, 23.76it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1435/1600 [01:00<00:06, 23.86it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 233/16Training loss: 78.5476, Training accuracy: 0.9838
Macro F1-score: 0.9837
Model performance on Angry speech (in training): 
	Precision: 0.9823, Recall: 0.9725, F1_score: 0.9774
Model performance on Happy speech (in training): 
	Precision: 0.9728, Recall: 0.9825, F1_score: 0.9776
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 230.0200, Validation accuracy: 0.7350
Macro F1-score: 0.6936
Model performance on Angry speech (in validation): 
	Precision: 0.5495, Recall: 1.0000, F1_score: 0.7092
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1800, F1_score: 0.3051
Model performance on Neutral speech (in validation): 
	Precision: 0.8654, Recall: 0.9000, F1_score: 0.8824
Model performance on Sad speech (in validation): 
	Precision: 0.8958, Recall: 0.8600, F1_score: 0.8776
Epoch 5/100

Training Phase:
00 [00:10<00:58, 23.29it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 472/1600 [00:20<00:47, 23.62it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 711/1600 [00:30<00:37, 23.62it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 953/1600 [00:40<00:27, 23.85it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1195/1600 [00:50<00:17, 23.81it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1434/1600 [01:00<00:06, 23.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 233/1600 [00:10<00:58, 23.28it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 476/1600 [00:20<00:47, 23.87it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 476/1600 [00:30<00:47, 23.87it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 718/1600 [00:30<00:37, 23.62it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 952/1600 [00:40<00:27, 23.53it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1200/1600 [00:50<00:16Training loss: 46.3566, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9825, F1_score: 0.9874
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 281.7460, Validation accuracy: 0.7150
Macro F1-score: 0.6770
Model performance on Angry speech (in validation): 
	Precision: 0.5495, Recall: 1.0000, F1_score: 0.7092
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.2000, F1_score: 0.3333
Model performance on Neutral speech (in validation): 
	Precision: 0.7500, Recall: 0.9600, F1_score: 0.8421
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.7000, F1_score: 0.8235
Epoch 6/100

Training Phase:
Training loss: 40.5806, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9800, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 262.3265, Validation accuracy: 0.7400
Macro F1-score: 0.7296
Model performance on Angry speech (in validation): 
	Precision: 0.6533, Recall: 0.9800, F1_score: 0.7840
Model performance on Happy speech (in validation): 
	Precision: 0.7105, Recall: 0.5400, F1_score: 0.6136
Model performance on Neutral speech (in validation): 
	Precision: 0.8485, Recall: 0.5600, F1_score: 0.6747
Model performance on Sad speech (in validation): 
	Precision: 0.8148, Recall: 0.8800, F1_score: 0.8462
Epoch 7/100

Training Phase:
, 23.97it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1448/1600 [01:00<00:06, 23.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 242/1600 [00:10<00:56, 24.09it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 483/1600 [00:20<00:46, 23.94it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 726/1600 [00:30<00:36, 24.09it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 969/1600 [00:40<00:26, 24.02it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1209/1600 [00:50<00:16, 23.90it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1446/1600 [01:00<00:06, 23.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 238/1600 [00:10<00:57, 23.78Training loss: 44.3078, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875

Eval Phase: 
Validation loss: 256.4875, Validation accuracy: 0.7550
Macro F1-score: 0.7342
Model performance on Angry speech (in validation): 
	Precision: 0.5833, Recall: 0.9800, F1_score: 0.7313
Model performance on Happy speech (in validation): 
	Precision: 0.9412, Recall: 0.3200, F1_score: 0.4776
Model performance on Neutral speech (in validation): 
	Precision: 0.8723, Recall: 0.8200, F1_score: 0.8454
Model performance on Sad speech (in validation): 
	Precision: 0.8654, Recall: 0.9000, F1_score: 0.8824
New best accuracy for layer 4 on epoch 7: 0.7550. Model saved.
Epoch 8/100

Training Phase:
it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 476/1600 [00:20<00:47, 23.66it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 712/1600 [00:30<00:37, 23.63it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 948/1600 [00:40<00:27, 23.51it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1189/1600 [00:50<00:17, 23.69it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1430/1600 [01:00<00:07, 23.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 234/1600 [00:10<00:58, 23.40it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 472/1600 [00:20<00:47, 23.62it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 710/1600 [00:30<00:37, 23.45it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 946/1600 [00:40<00:27, 23.48it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1196/1600 [00:50<00:16, 24.01it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1600 [01:00<00:06, 24.36itTraining loss: 46.6534, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 526.8055, Validation accuracy: 0.6550
Macro F1-score: 0.5780
Model performance on Angry speech (in validation): 
	Precision: 0.4808, Recall: 1.0000, F1_score: 0.6494
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Neutral speech (in validation): 
	Precision: 0.9688, Recall: 0.6200, F1_score: 0.7561
Model performance on Sad speech (in validation): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Epoch 9/100

Training Phase:
Training loss: 19.2187, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 629.2520, Validation accuracy: 0.5500
Macro F1-score: 0.4374
Model performance on Angry speech (in validation): 
	Precision: 0.4902, Recall: 1.0000, F1_score: 0.6579
Model performance on Happy speech (in validation): 
	Precision: 0.5882, Recall: 0.2000, F1_score: 0.2985
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.0200, F1_score: 0.0392
Model performance on Sad speech (in validation): 
	Precision: 0.6125, Recall: 0.9800, F1_score: 0.7538
Epoch 10/100

Training Phase:
/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 250/1600 [00:10<00:54, 24.94it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 501/1600 [00:20<00:44, 24.97it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 751/1600 [00:30<00:35, 24.16it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 991/1600 [00:40<00:25, 24.07it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1239/1600 [00:50<00:14, 24.31it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1487/1600 [01:00<00:04, 24.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 248/1600 [00:10<00:54, 24.70it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 495/1600 [00:20<00:44, 24.57it/s]Training: Training loss: 54.4740, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9825, F1_score: 0.9825
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 353.9527, Validation accuracy: 0.7250
Macro F1-score: 0.6852
Model performance on Angry speech (in validation): 
	Precision: 0.5376, Recall: 1.0000, F1_score: 0.6993
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.1800, F1_score: 0.3051
Model performance on Neutral speech (in validation): 
	Precision: 0.8913, Recall: 0.8200, F1_score: 0.8542
Model performance on Sad speech (in validation): 
	Precision: 0.8654, Recall: 0.9000, F1_score: 0.8824
Epoch 11/100

Training Phase:
Training loss: 12.7337, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 748/1600 [00:30<00:34, 24.90it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1002/1600 [00:40<00:23, 25.07it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1600 [00:50<00:13, 24.75it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1499/1600 [01:01<00:04, 24.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 241/1600 [00:10<00:56, 24.05it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 487/1600 [00:20<00:45, 24.35it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 739/1600 [00:30<00:34, 24.72it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 992/1600 [00:40<00:24, 24.95it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1600 [00:50<00:14, 25.11it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1502/1600 [01:00<00:03, 25.03it/s]                                                             EvaluatValidation loss: 561.9150, Validation accuracy: 0.6600
Macro F1-score: 0.5953
Model performance on Angry speech (in validation): 
	Precision: 0.4762, Recall: 1.0000, F1_score: 0.6452
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Neutral speech (in validation): 
	Precision: 0.8125, Recall: 0.7800, F1_score: 0.7959
Model performance on Sad speech (in validation): 
	Precision: 0.9111, Recall: 0.8200, F1_score: 0.8632
Epoch 12/100

Training Phase:
Training loss: 16.3364, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 259.9311, Validation accuracy: 0.8050
Macro F1-score: 0.7992
Model performance on Angry speech (in validation): 
	Precision: 0.6515, Recall: 0.8600, F1_score: 0.7414
Model performance on Happy speech (in validation): 
	Precision: 0.9630, Recall: 0.5200, F1_score: 0.6753
Model performance on Neutral speech (in validation): 
	Precision: 0.7903, Recall: 0.9800, F1_score: 0.8750
Model performance on Sad speech (in validation): 
	Precision: 0.9556, Recall: 0.8600, F1_score: 0.9053
New best accuracy for layer 4 on epoch 12: 0.8050. Model saved.
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.8050

Test Phase: 
ing:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.52it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 472/1600 [00:20<00:47, 23.54it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 714/1600 [00:30<00:37, 23.83it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 956/1600 [00:40<00:26, 23.94it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1198/1600 [00:50<00:16, 23.99it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1439/1600 [01:00<00:06, 23.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:01, 96.78it/s]Testing:  10%|â–ˆ         | 20/200 [00:00<00:01, 93.80it/s]Testing:  15%|â–ˆâ–Œ        | 30/200 [00:00<00:01, 95.67it/s]Testing:  20%|â–ˆâ–ˆ        | 40/200 [00:00<00:01, 96.04it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:00<00:01, 94.96it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:00<00:01, 95.11it/s]Testing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:00<00:01, 95.45it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:00<00:01, 92.61it/s]Testing:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:00<00:01, 92.45it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:01<00:01, 94.88it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:01<00:00, 94.62it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:01<00:00, 94.52it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:01<00:00, 95.20it/s]Testing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [00:01<00:00, 91.19it/s]Testing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [00:01<00:00, 90.98it/s]Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:01<00:00, 92.15it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:01<00:00, 92.37it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [00:01<00:00, 90.66it/s]Testing:  96%|âTest loss: 249.8501, Test accuracy: 0.8050
Macro F1-score: 0.8019
Model performance on Angry speech (in test): 
	Precision: 0.6615, Recall: 0.8600, F1_score: 0.7478
Model performance on Happy speech (in test): 
	Precision: 0.9655, Recall: 0.5600, F1_score: 0.7089
Model performance on Neutral speech (in test): 
	Precision: 0.7656, Recall: 0.9800, F1_score: 0.8596
Model performance on Sad speech (in test): 
	Precision: 0.9762, Recall: 0.8200, F1_score: 0.8913

======================= This is fold_3 on de =======================

Load dataset: 
Loading cn train data: fold_3...
Preprocess cn fold_3 data for de model
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:02<00:00, 91.44it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 8/1600 [00:00<00:27, 57.94 examples/s]Map:   1%|          | 15/1600 [00:00<00:25, 63.08 examples/s]Map:   2%|â–         | 33/1600 [00:00<00:14, 111.21 examples/s]Map:   3%|â–Ž         | 48/1600 [00:00<00:12, 123.84 examples/s]Map:   4%|â–         | 63/1600 [00:00<00:11, 131.17 examples/s]Map:   5%|â–Œ         | 82/1600 [00:00<00:12, 124.55 examples/s]Map:   6%|â–Œ         | 95/1600 [00:00<00:13, 108.57 examples/s]Map:   7%|â–‹         | 108/1600 [00:00<00:13, 111.26 examples/s]Map:   8%|â–Š         | 124/1600 [00:01<00:12, 121.30 examples/s]Map:   9%|â–Š         | 139/1600 [00:01<00:11, 127.85 examples/s]Map:  10%|â–‰         | 153/1600 [00:01<00:11, 130.82 examples/s]Map:  11%|â–ˆ         | 172/1600 [00:01<00:09, 146.28 examples/s]Map:  12%|â–ˆâ–        | 190/1600 [00:01<00:09, 154.36 examples/s]Map:  13%|â–ˆâ–Ž        | 206/1600 [00:01<00:10, 139.10 examples/s]Map:  14%|â–ˆâ–        | 225/1600 [00:01<00:09, 149.36 examples/s]Map:  15%|â–ˆâ–Œ        | 245/1600 [00:01<00:08, 160.59 examples/s]Map:  16%|â–ˆâ–‹        | 264/1600 [00:02<00:09, 142.04 examples/s]Map:  18%|â–ˆâ–Š        | 282/1600 [00:02<00:09, 132.66 examples/s]Map:  19%|â–ˆâ–‰        | 300/1600 [00:02<00:10, 123.61 examples/s]Map:  20%|â–ˆâ–‰        | 314/1600 [00:02<00:10, 126.06 examples/s]Map:  21%|â–ˆâ–ˆ        | 334/1600 [00:02<00:11, 112.91 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 348/1600 [00:02<00:10, 116.99 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 368/1600 [00:02<00:09, 133.75 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 389/1600 [00:02<00:08, 149.99 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:03<00:07, 158.72 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 427/1600 [00:03<00:07, 166.68 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 451/1600 [00:03<00:07, 152.14 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 475/1600 [00:03<00:07, 150.86 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 491/1600 [00:03<00:07, 149.35 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 514/1600 [00:03<00:07, 148.21 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 537/1600 [00:04<00:07, 137.08 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–      | 554/1600 [00:04<00:07, 142.02 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 572/1600 [00:04<00:06, 148.40 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 592/1600 [00:04<00:06, 158.85 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 610/1600 [00:04<00:06, 159.40 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 634/1600 [00:04<00:07, 136.85 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 654/1600 [00:04<00:07, 132.20 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 671/1600 [00:04<00:07, 120.87 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 686/1600 [00:05<00:08, 113.39 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 701/1600 [00:05<00:08, 106.92 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 714/1600 [00:05<00:08, 110.11 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 729/1600 [00:05<00:08, 96.96 examples/s] Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 741/1600 [00:05<00:08, 101.33 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 753/1600 [00:05<00:08, 104.55 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 765/1600 [00:05<00:07, 105.25 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 777/1600 [00:06<00:07, 106.74 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 789/1600 [00:06<00:07, 107.04 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 803/1600 [00:06<00:07, 113.69 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 819/1600 [00:06<00:06, 125.39 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1600 [00:06<00:06, 115.09 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 851/1600 [00:06<00:06, 122.61 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 865/1600 [00:06<00:05, 123.62 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 878/1600 [00:06<00:05, 123.40 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 895/1600 [00:07<00:05, 118.07 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 912/1600 [00:07<00:05, 127.59 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 926/1600 [00:07<00:05, 117.69 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 942/1600 [00:07<00:05, 125.23 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 960/1600 [00:07<00:04, 136.10 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 979/1600 [00:07<00:04, 147.41 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:07<00:03, 156.56 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:20<00:03, 156.56 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [00:48<09:26,  1.06 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1021/1600 [00:48<05:23,  1.79 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1041/1600 [00:48<03:21,  2.77 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1600 [00:48<02:14,  4.02 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1077/1600 [00:48<01:30,  5.76 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1101/1600 [00:48<00:55,  8.96 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1120/1600 [00:48<00:38, 12.47 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1137/1600 [00:49<00:28, 16.44 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1155/1600 [00:49<00:19, 22.35 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1175/1600 [00:49<00:13, 31.12 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1195/1600 [00:49<00:09, 42.14 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1218/1600 [00:49<00:06, 55.31 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1238/1600 [00:49<00:05, 66.46 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1255/1600 [00:49<00:04, 72.23 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1270/1600 [00:49<00:04, 76.94 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1287/1600 [00:50<00:03, 83.33 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1299/1600 [00:50<00:03, 87.13 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1311/1600 [00:50<00:03, 92.26 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1323/1600 [00:50<00:02, 96.99 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1337/1600 [00:50<00:02, 104.65 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1351/1600 [00:50<00:02, 110.62 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1370/1600 [00:50<00:02, 105.59 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1387/1600 [00:51<00:01, 118.12 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1403/1600 [00:51<00:01, 127.36 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1419/1600 [00:51<00:01, 134.32 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1436/1600 [00:51<00:01, 142.32 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1453/1600 [00:51<00:00, 148.49 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1476/1600 [00:51<00:00, 128.07 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1491/1600 [00:51<00:00, 131.27 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1506/1600 [00:51<00:00, 133.58 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1522/1600 [00:51<00:00, 137.01 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1539/1600 [00:52<00:00, 142.86 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1556/1600 [00:52<00:00, 146.79 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1572/1600 [00:52<00:00, 146.54 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1587/1600 [00:52<00:00, 83.67 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1596/1600 [01:10<00:00, 83.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:15<00:00,  2.14 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:15<00:00, 21.11 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   4%|â–         | 8/200 [00:00<00:03, 55.62 examples/s]Map:   7%|â–‹         | 14/200 [00:00<00:03, 55.95 examples/s]Map:  16%|â–ˆâ–‹        | 33/200 [00:00<00:01, 107.38 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [00:00<00:01, 111.61 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:00<00:01, 130.47 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [00:00<00:00, 141.93 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:00<00:00, 131.96 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:00<00:00, 138.93 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:01<00:00, 128.48 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:01<00:00, 128.80 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [00:01<00:00, 134.36 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:01<00:00, 140.89 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:09<00:00, 22.22 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   3%|â–Ž         | 6/200 [00:00<00:04, 42.18 examples/s]Map:   7%|â–‹         | 14/200 [00:00<00:03, 51.49 examples/s]Map:  16%|â–ˆâ–‹        | 33/200 [00:00<00:01, 102.56 examples/s]Map:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:00<00:01, 107.41 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [00:00<00:01, 128.93 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [00:00<00:00, 145.36 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [00:00<00:00, 161.03 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:00<00:00, 141.27 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [00:01<00:00, 132.88 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:01<00:00, 132.95 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:01<00:00, 137.86 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [00:01<00:00, 146.00 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:08<00:00, 22.45 examples/s] 
Loading cn eval data: fold_3...
Preprocess cn fold_3 data for de model
Loading cn test data: fold_3...
Preprocess cn fold_3 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 516.8487, Training accuracy: 0.8806
Macro F1-score: 0.8801
Model performance on Angry speech (in training): 
	Precision: 0.8428, Recall: 0.8575, F1_score: 0.8501
Model performance on Happy speech (in training): 
	Precision: 0.8439, Recall: 0.7975, F1_score: 0.8201
Model performance on Neutral speech (in training): 
	Precision: 0.8995, Recall: 0.9175, F1_score: 0.9084
Model performance on Sad speech (in training): 
	Precision: 0.9337, Recall: 0.9500, F1_score: 0.9418

Eval Phase: 
Validation loss: 38.9987, Validation accuracy: 0.9400
Macro F1-score: 0.9406
Model performance on Angry speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Happy speech (in validation): 
	Precision: 0.9388, Recall: 0.9200, F1_score: 0.9293
Model performance on Neutral speech (in validation): 
	Precision: 0.8596, Recall: 0.9800, F1_score: 0.9159
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
New best accuracy for layer 4 on epoch 1: 0.9400. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 239/1600 [00:10<00:57, 23.88it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 481/1600 [00:20<00:46, 24.03it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 723/1600 [00:30<00:36, 23.83it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 972/1600 [00:40<00:25, 24.22it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1221/1600 [00:50<00:15, 24.31it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:00<00:05, 24.33it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 246/1600 [00:10<00:55, 24.52it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 492/1600 [00:20<00:46, 23.67it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 724/1600 [00:30<00:37, 23.28it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 964/1600 [00:40<00:27, 23.52it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  Training loss: 237.3640, Training accuracy: 0.9437
Macro F1-score: 0.9437
Model performance on Angry speech (in training): 
	Precision: 0.9049, Recall: 0.9275, F1_score: 0.9160
Model performance on Happy speech (in training): 
	Precision: 0.9199, Recall: 0.8900, F1_score: 0.9047
Model performance on Neutral speech (in training): 
	Precision: 0.9679, Recall: 0.9800, F1_score: 0.9739
Model performance on Sad speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799

Eval Phase: 
Validation loss: 73.6629, Validation accuracy: 0.8850
Macro F1-score: 0.8772
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.8750, Recall: 0.9800, F1_score: 0.9245
Model performance on Neutral speech (in validation): 
	Precision: 0.9667, Recall: 0.5800, F1_score: 0.7250
Model performance on Sad speech (in validation): 
	Precision: 0.7692, Recall: 1.0000, F1_score: 0.8696
Epoch 3/100

Training Phase:
Training loss: 139.7941, Training accuracy: 0.9681
Macro F1-score: 0.9681
Model performance on Angry speech (in training): 
	Precision: 0.9529, Recall: 0.9600, F1_score: 0.9564
Model performance on Happy speech (in training): 
	Precision: 0.9474, Recall: 0.9450, F1_score: 0.9462
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862

Eval Phase: 
Validation loss: 23.7736, Validation accuracy: 0.9650
Macro F1-score: 0.9649
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.9074, Recall: 0.9800, F1_score: 0.9423
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
New best accuracy for layer 4 on epoch 3: 0.9650. Model saved.
Epoch 4/100

Training Phase:
| 1205/1600 [00:50<00:16, 23.70it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1446/1600 [01:01<00:06, 23.63it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 240/1600 [00:10<00:56, 23.98it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 480/1600 [00:20<00:47, 23.66it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 715/1600 [00:30<00:38, 23.07it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 956/1600 [00:40<00:27, 23.46it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1202/1600 [00:50<00:16, 23.83it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1600 [01:01<00:06, 23.87it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 229/Training loss: 91.5697, Training accuracy: 0.9781
Macro F1-score: 0.9781
Model performance on Angry speech (in training): 
	Precision: 0.9604, Recall: 0.9700, F1_score: 0.9652
Model performance on Happy speech (in training): 
	Precision: 0.9671, Recall: 0.9550, F1_score: 0.9610
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 20.9598, Validation accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
New best accuracy for layer 4 on epoch 4: 0.9700. Model saved.
Epoch 5/100

Training Phase:
1600 [00:10<00:59, 22.88it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 469/1600 [00:20<00:48, 23.48it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 709/1600 [00:30<00:37, 23.70it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 955/1600 [00:40<00:26, 24.03it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1201/1600 [00:51<00:17, 23.31it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1441/1600 [01:01<00:06, 23.53it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 235/1600 [00:10<00:58, 23.47it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 477/1600 [00:20<00:47, 23.88it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 719/1600 [00:30<00:36, 23.88it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 961/1600 [00:40<00:26, 24.00it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1203/1600 [00:50<00:16, 24.03it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1444/16Training loss: 82.1539, Training accuracy: 0.9825
Macro F1-score: 0.9825
Model performance on Angry speech (in training): 
	Precision: 0.9726, Recall: 0.9775, F1_score: 0.9751
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9700, F1_score: 0.9749
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 41.5722, Validation accuracy: 0.9450
Macro F1-score: 0.9449
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.8197, Recall: 1.0000, F1_score: 0.9009
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.8000, F1_score: 0.8889
Epoch 6/100

Training Phase:
Training loss: 63.6621, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9802, Recall: 0.9900, F1_score: 0.9851
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9775, F1_score: 0.9824
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 19.6624, Validation accuracy: 0.9600
Macro F1-score: 0.9602
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9074, Recall: 0.9800, F1_score: 0.9423
Model performance on Neutral speech (in validation): 
	Precision: 0.9583, Recall: 0.9200, F1_score: 0.9388
Model performance on Sad speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Epoch 7/100

Training Phase:
00 [01:00<00:06, 23.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 252/1600 [00:10<00:53, 25.16it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 504/1600 [00:20<00:44, 24.86it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 751/1600 [00:30<00:34, 24.41it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 993/1600 [00:40<00:24, 24.31it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1235/1600 [00:50<00:15, 24.10it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1474/1600 [01:00<00:05, 24.02it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.57it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 476/1600 [00:20<00:4Training loss: 45.4730, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 28.1974, Validation accuracy: 0.9650
Macro F1-score: 0.9649
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Happy speech (in validation): 
	Precision: 0.8909, Recall: 0.9800, F1_score: 0.9333
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 8/100

Training Phase:
7, 23.80it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 716/1600 [00:30<00:37, 23.73it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 956/1600 [00:40<00:27, 23.81it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1196/1600 [00:50<00:16, 23.87it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1441/1600 [01:00<00:06, 24.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.54it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 476/1600 [00:20<00:47, 23.81it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 723/1600 [00:30<00:36, 24.21it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 973/1600 [00:40<00:25, 24.49it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1223/1600 [00:50<00:15, 24.58it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1471/1600 [01:00<00:05, 24.62it/s]                                                     Training loss: 30.0632, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 17.9774, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
New best accuracy for layer 4 on epoch 8: 0.9750. Model saved.
Epoch 9/100

Training Phase:
Training loss: 26.1480, Training accuracy: 0.9938
Macro F1-score: 0.9938
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 66.8376, Validation accuracy: 0.9100
Macro F1-score: 0.9105
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Happy speech (in validation): 
	Precision: 0.7538, Recall: 0.9800, F1_score: 0.8522
Model performance on Neutral speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 10/100

Training Phase:
        Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 235/1600 [00:10<00:58, 23.45it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 472/1600 [00:20<00:47, 23.56it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 709/1600 [00:30<00:37, 23.52it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 953/1600 [00:40<00:27, 23.86it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1201/1600 [00:50<00:16, 24.18it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1449/1600 [01:00<00:06, 24.05it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 240/1600 [00:10<00:56, 23.97it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 484/1600 [00:20<00:46, 24.20it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 728/1600 [00:30<00:36, 24.06it/s]TrTraining loss: 50.1673, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 26.2115, Validation accuracy: 0.9400
Macro F1-score: 0.9405
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.8305, Recall: 0.9800, F1_score: 0.8991
Model performance on Neutral speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 11/100

Training Phase:
Training loss: 31.8130, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
aining:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 972/1600 [00:40<00:25, 24.18it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1216/1600 [00:50<00:15, 24.23it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1460/1600 [01:00<00:05, 24.15it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 246/1600 [00:10<00:55, 24.54it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 492/1600 [00:20<00:46, 24.00it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 732/1600 [00:30<00:36, 23.96it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 977/1600 [00:40<00:25, 24.14it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1222/1600 [00:50<00:15, 23.90it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:00<00:05, 24.05it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                       Validation loss: 12.0490, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 11: 0.9950. Model saved.
Epoch 12/100

Training Phase:
Training loss: 52.3695, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 18.7897, Validation accuracy: 0.9700
Macro F1-score: 0.9701
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Neutral speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 13/100

Training Phase:
                            Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 241/1600 [00:10<00:56, 24.06it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 485/1600 [00:20<00:45, 24.25it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 730/1600 [00:30<00:35, 24.34it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 975/1600 [00:40<00:25, 24.10it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1221/1600 [00:50<00:15, 24.26it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1470/1600 [01:00<00:05, 24.45it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 234/1600 [00:10<00:58, 23.40it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 470/1600 [00:20<00:48, 23.50it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 710/1600 [00:30<00:37, 23.69it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 951/1600 [00:40<00:27, 23.83it/s]Training:  7Training loss: 11.8657, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 18.5590, Validation accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Sad speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Epoch 14/100

Training Phase:
Training loss: 31.8438, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1192/1600 [00:50<00:17, 23.69it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1429/1600 [01:00<00:07, 23.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 240/1600 [00:10<00:56, 23.91it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 482/1600 [00:20<00:46, 24.06it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 732/1600 [00:30<00:35, 24.47it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 732/1600 [00:40<00:35, 24.47it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 971/1600 [00:40<00:26, 24.10it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1209/1600 [00:50<00:16, 23.96it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1600 [01:00<00:06, 23.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                       Validation loss: 22.0302, Validation accuracy: 0.9650
Macro F1-score: 0.9652
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.8909, Recall: 0.9800, F1_score: 0.9333
Model performance on Neutral speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 15/100

Training Phase:
Training loss: 39.7760, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 43.9595, Validation accuracy: 0.9400
Macro F1-score: 0.9410
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
Model performance on Happy speech (in validation): 
	Precision: 0.8167, Recall: 0.9800, F1_score: 0.8909
Model performance on Neutral speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 16/100

Training Phase:
            Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 249/1600 [00:10<00:54, 24.84it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 499/1600 [00:20<00:44, 24.92it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 750/1600 [00:30<00:34, 24.96it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1001/1600 [00:40<00:23, 24.96it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1251/1600 [00:50<00:14, 24.45it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1499/1600 [01:00<00:04, 24.55it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 246/1600 [00:10<00:55, 24.55it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 492/1600 [00:20<00:45, 24.50it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 737/1600 [00:30<00:35, 24.16it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 984/1600 [00:40<00:25, 24.37it/s]Training:  77%|â–ˆâ–ˆâ–Training loss: 9.4101, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 57.1980, Validation accuracy: 0.9200
Macro F1-score: 0.9212
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8000, F1_score: 0.8889
Model performance on Happy speech (in validation): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Model performance on Neutral speech (in validation): 
	Precision: 0.9583, Recall: 0.9200, F1_score: 0.9388
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 17/100

Training Phase:
Training loss: 31.5333, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 50.3668, Validation accuracy: 0.9000
Macro F1-score: 0.9004
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Happy speech (in validation): 
	Precision: 0.7424, Recall: 0.9800, F1_score: 0.8448
Model performance on Neutral speech (in validation): 
	Precision: 0.9400, Recall: 0.9400, F1_score: 0.9400
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Epoch 18/100

Training Phase:
ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1232/1600 [00:50<00:15, 24.52it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1480/1600 [01:00<00:04, 24.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 243/1600 [00:10<00:55, 24.28it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 486/1600 [00:20<00:46, 24.03it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 725/1600 [00:30<00:36, 23.94it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 965/1600 [00:40<00:26, 23.93it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1205/1600 [00:50<00:16, 23.89it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1445/1600 [01:00<00:06, 23.91it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|Training loss: 22.7999, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 123.0000, Validation accuracy: 0.8550
Macro F1-score: 0.8530
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.5600, F1_score: 0.7179
Model performance on Happy speech (in validation): 
	Precision: 0.6447, Recall: 0.9800, F1_score: 0.7778
Model performance on Neutral speech (in validation): 
	Precision: 0.9778, Recall: 0.8800, F1_score: 0.9263
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 19/100

Training Phase:
â–ˆâ–Œ        | 245/1600 [00:10<00:55, 24.42it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 490/1600 [00:20<00:45, 24.23it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 737/1600 [00:30<00:35, 24.42it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 984/1600 [00:40<00:25, 24.27it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1225/1600 [00:50<00:15, 24.05it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1462/1600 [01:00<00:05, 23.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 242/1600 [00:10<00:56, 24.11it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 484/1600 [00:20<00:46, 23.96it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 726/1600 [00:30<00:36, 24.02it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 967/1600 [00:40<00:26, 23.85it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1204/1600 [00:50<00:16, 23.79it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâTraining loss: 19.1424, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 66.8526, Validation accuracy: 0.9200
Macro F1-score: 0.9204
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Model performance on Happy speech (in validation): 
	Precision: 0.7903, Recall: 0.9800, F1_score: 0.8750
Model performance on Neutral speech (in validation): 
	Precision: 0.9412, Recall: 0.9600, F1_score: 0.9505
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Epoch 20/100

Training Phase:
Training loss: 11.4794, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 34.0154, Validation accuracy: 0.9550
Macro F1-score: 0.9556
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.8750, Recall: 0.9800, F1_score: 0.9245
Model performance on Neutral speech (in validation): 
	Precision: 0.9583, Recall: 0.9200, F1_score: 0.9388
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 21/100

Training Phase:
–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1445/1600 [01:00<00:06, 23.87it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.60it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 475/1600 [00:20<00:47, 23.66it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 714/1600 [00:30<00:37, 23.74it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 953/1600 [00:40<00:27, 23.68it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1201/1600 [00:50<00:16, 24.08it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1452/1600 [01:00<00:06, 24.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  16%|â–ˆâ–Œ        | 252/1600 [00:10<00:53, 25.16it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | Training loss: 44.4066, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 84.7920, Validation accuracy: 0.8900
Macro F1-score: 0.8899
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6800, F1_score: 0.8095
Model performance on Happy speech (in validation): 
	Precision: 0.7206, Recall: 0.9800, F1_score: 0.8305
Model performance on Neutral speech (in validation): 
	Precision: 0.9400, Recall: 0.9400, F1_score: 0.9400
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9950

Test Phase: 
504/1600 [00:20<00:44, 24.79it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 751/1600 [00:30<00:34, 24.75it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:40<00:24, 24.59it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1242/1600 [00:50<00:14, 24.15it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1477/1600 [01:00<00:05, 23.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:02, 94.98it/s]Testing:  10%|â–ˆ         | 20/200 [00:00<00:01, 93.14it/s]Testing:  15%|â–ˆâ–Œ        | 30/200 [00:00<00:01, 93.78it/s]Testing:  20%|â–ˆâ–ˆ        | 40/200 [00:00<00:01, 95.72it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:00<00:01, 94.70it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:00<00:01, 94.53it/s]Testing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:00<00:01, 94.82it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ   Test loss: 28.0869, Test accuracy: 0.9550
Macro F1-score: 0.9551
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in test): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in test): 
	Precision: 0.9388, Recall: 0.9200, F1_score: 0.9293
Model performance on Sad speech (in test): 
	Precision: 0.9231, Recall: 0.9600, F1_score: 0.9412

======================= This is fold_4 on de =======================

Load dataset: 
Loading cn train data: fold_4...
Preprocess cn fold_4 data for de model
   | 80/200 [00:00<00:01, 94.78it/s]Testing:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:00<00:01, 94.40it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:01<00:01, 96.05it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:01<00:00, 96.26it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:01<00:00, 95.16it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:01<00:00, 95.65it/s]Testing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [00:01<00:00, 96.09it/s]Testing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [00:01<00:00, 94.38it/s]Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:01<00:00, 94.62it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:01<00:00, 94.97it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [00:01<00:00, 94.22it/s]Testing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:02<00:00, 94.90it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 7/1600 [00:00<00:27, 58.17 examples/s]Map:   1%|          | 14/1600 [00:00<00:25, 61.35 examples/s]Map:   2%|â–         | 35/1600 [00:00<00:13, 120.00 examples/s]Map:   3%|â–Ž         | 52/1600 [00:00<00:14, 108.10 examples/s]Map:   4%|â–         | 68/1600 [00:00<00:12, 121.05 examples/s]Map:   5%|â–Œ         | 81/1600 [00:00<00:12, 118.65 examples/s]Map:   6%|â–Œ         | 97/1600 [00:00<00:14, 103.61 examples/s]Map:   7%|â–‹         | 111/1600 [00:01<00:13, 110.60 examples/s]Map:   8%|â–Š         | 127/1600 [00:01<00:12, 121.39 examples/s]Map:   9%|â–‰         | 142/1600 [00:01<00:11, 126.57 examples/s]Map:  10%|â–‰         | 159/1600 [00:01<00:10, 135.63 examples/s]Map:  11%|â–ˆ         | 179/1600 [00:01<00:09, 149.43 examples/s]Map:  12%|â–ˆâ–        | 198/1600 [00:01<00:08, 156.82 examples/s]Map:  14%|â–ˆâ–        | 221/1600 [00:01<00:10, 131.39 examples/s]Map:  15%|â–ˆâ–Œ        | 241/1600 [00:01<00:09, 144.57 examples/s]Map:  16%|â–ˆâ–‹        | 262/1600 [00:02<00:09, 138.32 examples/s]Map:  18%|â–ˆâ–Š        | 281/1600 [00:02<00:10, 131.13 examples/s]Map:  19%|â–ˆâ–‰        | 300/1600 [00:02<00:10, 125.27 examples/s]Map:  20%|â–ˆâ–‰        | 314/1600 [00:02<00:10, 127.61 examples/s]Map:  21%|â–ˆâ–ˆ        | 333/1600 [00:02<00:11, 114.65 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 348/1600 [00:02<00:10, 119.54 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 368/1600 [00:02<00:09, 136.73 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 389/1600 [00:03<00:07, 152.69 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:03<00:07, 161.08 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 428/1600 [00:03<00:06, 171.14 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 448/1600 [00:03<00:07, 155.77 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 473/1600 [00:03<00:07, 153.84 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 489/1600 [00:03<00:07, 150.84 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 513/1600 [00:03<00:07, 150.63 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 536/1600 [00:04<00:07, 140.36 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–      | 553/1600 [00:04<00:07, 144.84 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 572/1600 [00:04<00:06, 153.25 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 593/1600 [00:04<00:06, 164.36 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 610/1600 [00:04<00:06, 163.22 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 633/1600 [00:04<00:06, 141.48 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 654/1600 [00:04<00:06, 136.92 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1600 [00:04<00:07, 124.64 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 687/1600 [00:05<00:07, 115.97 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 703/1600 [00:05<00:07, 112.37 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 717/1600 [00:05<00:07, 115.21 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 735/1600 [00:05<00:08, 102.57 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 749/1600 [00:05<00:07, 107.36 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 766/1600 [00:05<00:07, 107.84 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 778/1600 [00:06<00:07, 109.21 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 790/1600 [00:06<00:07, 109.34 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 805/1600 [00:06<00:06, 116.98 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 819/1600 [00:06<00:07, 109.09 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 837/1600 [00:06<00:06, 123.96 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 854/1600 [00:06<00:05, 134.77 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 874/1600 [00:06<00:05, 130.61 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 894/1600 [00:06<00:05, 130.18 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 909/1600 [00:07<00:05, 131.63 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 924/1600 [00:07<00:05, 122.31 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 939/1600 [00:07<00:05, 127.75 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 956/1600 [00:07<00:04, 135.56 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 974/1600 [00:07<00:04, 144.79 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 994/1600 [00:07<00:03, 157.94 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 994/1600 [00:19<00:03, 157.94 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [00:48<08:40,  1.15 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1020/1600 [00:48<05:15,  1.84 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1039/1600 [00:48<03:23,  2.76 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1063/1600 [00:48<02:02,  4.40 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1083/1600 [00:49<01:22,  6.28 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1101/1600 [00:49<00:58,  8.59 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1118/1600 [00:49<00:41, 11.65 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1135/1600 [00:49<00:29, 15.81 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1153/1600 [00:49<00:21, 21.03 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1171/1600 [00:49<00:15, 28.51 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1188/1600 [00:49<00:11, 37.41 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1205/1600 [00:49<00:08, 47.98 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1226/1600 [00:50<00:06, 61.07 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1600 [00:50<00:04, 73.73 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1264/1600 [00:50<00:04, 74.25 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1280/1600 [00:50<00:04, 79.44 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1296/1600 [00:50<00:03, 83.88 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1308/1600 [00:50<00:03, 88.67 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1319/1600 [00:51<00:03, 91.72 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1332/1600 [00:51<00:02, 99.03 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1347/1600 [00:51<00:02, 106.78 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1362/1600 [00:51<00:02, 98.57 examples/s] Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1379/1600 [00:51<00:01, 112.78 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1396/1600 [00:51<00:01, 125.53 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1412/1600 [00:51<00:01, 132.46 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1429/1600 [00:51<00:01, 139.24 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1600 [00:51<00:01, 146.49 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1469/1600 [00:52<00:00, 143.73 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1484/1600 [00:52<00:00, 127.78 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1498/1600 [00:52<00:00, 128.96 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1514/1600 [00:52<00:00, 134.89 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1530/1600 [00:52<00:00, 139.13 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1546/1600 [00:52<00:00, 144.06 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1563/1600 [00:52<00:00, 147.62 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1581/1600 [00:52<00:00, 155.31 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1596/1600 [01:09<00:00, 155.31 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:18<00:00,  2.30 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:18<00:00, 20.50 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   4%|â–         | 8/200 [00:00<00:03, 58.96 examples/s]Map:   8%|â–Š         | 15/200 [00:00<00:02, 63.21 examples/s]Map:  17%|â–ˆâ–‹        | 34/200 [00:00<00:01, 112.41 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [00:00<00:01, 119.36 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [00:00<00:00, 144.58 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:00<00:00, 162.82 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [00:00<00:00, 149.42 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:00<00:00, 153.41 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [00:01<00:00, 151.69 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [00:01<00:00, 160.47 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [00:01<00:00, 172.02 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:08<00:00, 24.99 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   6%|â–Œ         | 12/200 [00:00<00:01, 113.04 examples/s]Map:  14%|â–ˆâ–Ž        | 27/200 [00:00<00:01, 126.32 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:00<00:01, 119.20 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:00<00:00, 136.26 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [00:00<00:00, 153.78 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:00<00:00, 144.25 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:00<00:00, 152.10 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:00<00:00, 151.82 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [00:01<00:00, 151.14 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:01<00:00, 155.83 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [00:01<00:00, 169.37 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:08<00:00, 24.10 examples/s] 
Loading cn eval data: fold_4...
Preprocess cn fold_4 data for de model
Loading cn test data: fold_4...
Preprocess cn fold_4 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 420.3869, Training accuracy: 0.9006
Macro F1-score: 0.9002
Model performance on Angry speech (in training): 
	Precision: 0.8706, Recall: 0.8750, F1_score: 0.8728
Model performance on Happy speech (in training): 
	Precision: 0.8545, Recall: 0.8225, F1_score: 0.8382
Model performance on Neutral speech (in training): 
	Precision: 0.9179, Recall: 0.9500, F1_score: 0.9337
Model performance on Sad speech (in training): 
	Precision: 0.9574, Recall: 0.9550, F1_score: 0.9562

Eval Phase: 
Validation loss: 65.0108, Validation accuracy: 0.8800
Macro F1-score: 0.8750
Model performance on Angry speech (in validation): 
	Precision: 0.9677, Recall: 0.6000, F1_score: 0.7407
Model performance on Happy speech (in validation): 
	Precision: 0.7313, Recall: 0.9800, F1_score: 0.8376
Model performance on Neutral speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
New best accuracy for layer 4 on epoch 1: 0.8800. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 242/1600 [00:10<00:56, 24.12it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 484/1600 [00:20<00:47, 23.67it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 722/1600 [00:30<00:37, 23.71it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 960/1600 [00:40<00:27, 23.55it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1198/1600 [00:50<00:17, 23.62it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1437/1600 [01:00<00:06, 23.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 245/1600 [00:10<00:55, 24.41it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 493/1600 [00:20<00:45, 24.59it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 741/1600 [00:30<00:35, 23.89it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 973/1600 [00:40<00:26, 23.62it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | Training loss: 173.7678, Training accuracy: 0.9600
Macro F1-score: 0.9599
Model performance on Angry speech (in training): 
	Precision: 0.9335, Recall: 0.9475, F1_score: 0.9404
Model performance on Happy speech (in training): 
	Precision: 0.9437, Recall: 0.9225, F1_score: 0.9330
Model performance on Neutral speech (in training): 
	Precision: 0.9751, Recall: 0.9800, F1_score: 0.9776
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888

Eval Phase: 
Validation loss: 54.9279, Validation accuracy: 0.8900
Macro F1-score: 0.8863
Model performance on Angry speech (in validation): 
	Precision: 0.9412, Recall: 0.6400, F1_score: 0.7619
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.9600, F1_score: 0.8421
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
New best accuracy for layer 4 on epoch 2: 0.8900. Model saved.
Epoch 3/100

Training Phase:
Training loss: 95.8160, Training accuracy: 0.9819
Macro F1-score: 0.9819
Model performance on Angry speech (in training): 
	Precision: 0.9775, Recall: 0.9775, F1_score: 0.9775
Model performance on Happy speech (in training): 
	Precision: 0.9700, Recall: 0.9700, F1_score: 0.9700
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900

Eval Phase: 
Validation loss: 36.7810, Validation accuracy: 0.9350
Macro F1-score: 0.9343
Model performance on Angry speech (in validation): 
	Precision: 0.9535, Recall: 0.8200, F1_score: 0.8817
Model performance on Happy speech (in validation): 
	Precision: 0.8727, Recall: 0.9600, F1_score: 0.9143
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
New best accuracy for layer 4 on epoch 3: 0.9350. Model saved.
Epoch 4/100

Training Phase:
1210/1600 [00:50<00:16, 23.64it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1600 [01:00<00:06, 23.65it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.58it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 472/1600 [00:20<00:47, 23.54it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 714/1600 [00:30<00:37, 23.81it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 956/1600 [00:40<00:26, 23.88it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1197/1600 [00:50<00:16, 23.92it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1437/1600 [01:00<00:06, 23.77it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 242/16Training loss: 81.4398, Training accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in training): 
	Precision: 0.9751, Recall: 0.9800, F1_score: 0.9776
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9725, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 64.1645, Validation accuracy: 0.8900
Macro F1-score: 0.8872
Model performance on Angry speech (in validation): 
	Precision: 0.9444, Recall: 0.6800, F1_score: 0.7907
Model performance on Happy speech (in validation): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Model performance on Neutral speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Epoch 5/100

Training Phase:
00 [00:10<00:56, 24.15it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 484/1600 [00:20<00:46, 23.86it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 721/1600 [00:30<00:36, 23.76it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 958/1600 [00:40<00:27, 23.61it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1193/1600 [00:50<00:17, 23.53it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1428/1600 [01:00<00:07, 23.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.63it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 474/1600 [00:20<00:47, 23.57it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 710/1600 [00:30<00:38, 23.37it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 944/1600 [00:40<00:28, 23.37it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1187/1600 [00:50<00:17, 23.70it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1430/1600Training loss: 75.5037, Training accuracy: 0.9862
Macro F1-score: 0.9862
Model performance on Angry speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887

Eval Phase: 
Validation loss: 85.1043, Validation accuracy: 0.8600
Macro F1-score: 0.8573
Model performance on Angry speech (in validation): 
	Precision: 0.9091, Recall: 0.6000, F1_score: 0.7229
Model performance on Happy speech (in validation): 
	Precision: 0.6957, Recall: 0.9600, F1_score: 0.8067
Model performance on Neutral speech (in validation): 
	Precision: 0.9216, Recall: 0.9400, F1_score: 0.9307
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Epoch 6/100

Training Phase:
Training loss: 64.7897, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9726, Recall: 0.9775, F1_score: 0.9751
Model performance on Happy speech (in training): 
	Precision: 0.9774, Recall: 0.9725, F1_score: 0.9749
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 48.0138, Validation accuracy: 0.9050
Macro F1-score: 0.9046
Model performance on Angry speech (in validation): 
	Precision: 0.9487, Recall: 0.7400, F1_score: 0.8315
Model performance on Happy speech (in validation): 
	Precision: 0.7742, Recall: 0.9600, F1_score: 0.8571
Model performance on Neutral speech (in validation): 
	Precision: 0.9400, Recall: 0.9400, F1_score: 0.9400
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 7/100

Training Phase:
 [01:00<00:07, 23.71it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 231/1600 [00:10<00:59, 23.04it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 468/1600 [00:20<00:48, 23.38it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 709/1600 [00:30<00:37, 23.69it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 962/1600 [00:40<00:26, 24.30it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1215/1600 [00:50<00:16, 24.03it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1451/1600 [01:00<00:06, 23.77it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 247/1600 [00:10<00:54, 24.64it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 494/1600 [00:20<00:45, 24.57Training loss: 25.0305, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 58.4454, Validation accuracy: 0.9000
Macro F1-score: 0.9008
Model performance on Angry speech (in validation): 
	Precision: 0.9302, Recall: 0.8000, F1_score: 0.8602
Model performance on Happy speech (in validation): 
	Precision: 0.7869, Recall: 0.9600, F1_score: 0.8649
Model performance on Neutral speech (in validation): 
	Precision: 0.9184, Recall: 0.9000, F1_score: 0.9091
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Epoch 8/100

Training Phase:
Training loss: 21.4017, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 740/1600 [00:30<00:35, 24.16it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 977/1600 [00:40<00:26, 23.66it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1213/1600 [00:50<00:16, 23.61it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1449/1600 [01:00<00:06, 23.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 235/1600 [00:10<00:58, 23.45it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 475/1600 [00:20<00:47, 23.73it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 715/1600 [00:30<00:37, 23.62it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 963/1600 [00:40<00:26, 24.08it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1211/1600 [00:50<00:16, 24.07it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1452/1600 [01:00<00:06, 23.93it/s]                                                             Validation loss: 67.1166, Validation accuracy: 0.8900
Macro F1-score: 0.8922
Model performance on Angry speech (in validation): 
	Precision: 0.8431, Recall: 0.8600, F1_score: 0.8515
Model performance on Happy speech (in validation): 
	Precision: 0.7679, Recall: 0.8600, F1_score: 0.8113
Model performance on Neutral speech (in validation): 
	Precision: 0.9778, Recall: 0.8800, F1_score: 0.9263
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Epoch 9/100

Training Phase:
Training loss: 37.8026, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 71.2276, Validation accuracy: 0.8850
Macro F1-score: 0.8842
Model performance on Angry speech (in validation): 
	Precision: 0.9706, Recall: 0.6600, F1_score: 0.7857
Model performance on Happy speech (in validation): 
	Precision: 0.7143, Recall: 1.0000, F1_score: 0.8333
Model performance on Neutral speech (in validation): 
	Precision: 0.9574, Recall: 0.9000, F1_score: 0.9278
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 10/100

Training Phase:
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–Œ        | 242/1600 [00:10<00:56, 24.14it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 484/1600 [00:20<00:46, 24.05it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 724/1600 [00:30<00:36, 24.01it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 964/1600 [00:40<00:26, 23.93it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1204/1600 [00:50<00:16, 23.95it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1444/1600 [01:00<00:06, 23.91it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.63it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 474/1600 [00:20<00:47, 23.59it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 713/1600 [00:30<00:37, 23.70it/s]Training:  6Training loss: 17.2437, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 81.1016, Validation accuracy: 0.8750
Macro F1-score: 0.8764
Model performance on Angry speech (in validation): 
	Precision: 0.9231, Recall: 0.7200, F1_score: 0.8090
Model performance on Happy speech (in validation): 
	Precision: 0.7164, Recall: 0.9600, F1_score: 0.8205
Model performance on Neutral speech (in validation): 
	Precision: 0.9362, Recall: 0.8800, F1_score: 0.9072
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Epoch 11/100

Training Phase:
Training loss: 24.2827, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
0%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 960/1600 [00:40<00:26, 24.06it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1208/1600 [00:50<00:16, 24.32it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1459/1600 [01:00<00:05, 24.55it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.55it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 479/1600 [00:20<00:46, 23.93it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 479/1600 [00:30<00:46, 23.93it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 717/1600 [00:30<00:37, 23.67it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 951/1600 [00:40<00:27, 23.48it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1188/1600 [00:50<00:17, 23.55it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1425/1600 [01:00<00:07, 23.54it/s]                                                             Evaluating:   0%|     Validation loss: 85.1217, Validation accuracy: 0.8950
Macro F1-score: 0.8974
Model performance on Angry speech (in validation): 
	Precision: 0.9348, Recall: 0.8600, F1_score: 0.8958
Model performance on Happy speech (in validation): 
	Precision: 0.7231, Recall: 0.9400, F1_score: 0.8174
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 12/100

Training Phase:
Training loss: 47.4450, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888

Eval Phase: 
Validation loss: 72.5782, Validation accuracy: 0.8650
Macro F1-score: 0.8672
Model performance on Angry speech (in validation): 
	Precision: 0.9091, Recall: 0.8000, F1_score: 0.8511
Model performance on Happy speech (in validation): 
	Precision: 0.6957, Recall: 0.9600, F1_score: 0.8067
Model performance on Neutral speech (in validation): 
	Precision: 0.9487, Recall: 0.7400, F1_score: 0.8315
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Epoch 13/100

Training Phase:
     | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 234/1600 [00:10<00:58, 23.34it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 481/1600 [00:20<00:46, 24.12it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 481/1600 [00:30<00:46, 24.12it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 718/1600 [00:30<00:37, 23.79it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 954/1600 [00:40<00:27, 23.70it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1190/1600 [00:50<00:17, 23.59it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1425/1600 [01:00<00:07, 23.48it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 234/1600 [00:10<00:58, 23.37it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 478/1600 [00:20<00:46, 23.96it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 7Training loss: 7.5955, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 41.2356, Validation accuracy: 0.9250
Macro F1-score: 0.9260
Model performance on Angry speech (in validation): 
	Precision: 0.9545, Recall: 0.8400, F1_score: 0.8936
Model performance on Happy speech (in validation): 
	Precision: 0.8000, Recall: 0.9600, F1_score: 0.8727
Model performance on Neutral speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9350

Test Phase: 
25/1600 [00:30<00:36, 24.26it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 725/1600 [00:40<00:36, 24.26it/s]Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 968/1600 [00:40<00:26, 24.10it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1207/1600 [00:50<00:16, 23.87it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1442/1600 [01:00<00:06, 23.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:02, 93.04it/s]Testing:  10%|â–ˆ         | 20/200 [00:00<00:01, 95.25it/s]Testing:  15%|â–ˆâ–Œ        | 30/200 [00:00<00:01, 95.63it/s]Testing:  20%|â–ˆâ–ˆ        | 40/200 [00:00<00:01, 95.15it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:00<00:01, 94.32it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:00<00:01, 94.70it/s]Testing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:00<00:01, 94.41it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | Test loss: 41.0566, Test accuracy: 0.9200
Macro F1-score: 0.9192
Model performance on Angry speech (in test): 
	Precision: 0.9524, Recall: 0.8000, F1_score: 0.8696
Model performance on Happy speech (in test): 
	Precision: 0.8364, Recall: 0.9200, F1_score: 0.8762
Model performance on Neutral speech (in test): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in test): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600

de, all folds layer accuracy: ['0.8500', '0.8450', '0.8050', '0.9550', '0.9200']
de, all emo precision: {'Angry': ['0.7778', '1.0000', '0.6615', '1.0000', '0.9524'], 'Happy': ['0.7551', '0.7292', '0.9655', '0.9608', '0.8364'], 'Neutral': ['1.0000', '0.8305', '0.7656', '0.9388', '0.9434'], 'Sad': ['0.9130', '0.8571', '0.9762', '0.9231', '0.9600']}
de, all emo recall: {'Angry': ['0.9800', '0.7400', '0.8600', '0.9600', '0.8000'], 'Happy': ['0.7400', '0.7000', '0.5600', '0.9800', '0.9200'], 'Neutral': ['0.8400', '0.9800', '0.9800', '0.9200', '1.0000'], 'Sad': ['0.8400', '0.9600', '0.8200', '0.9600', '0.9600']}
de, all emo f1score: {'Angry': ['0.8673', '0.8506', '0.7478', '0.9796', '0.8696'], 'Happy': ['0.7475', '0.7143', '0.7089', '0.9703', '0.8762'], 'Neutral': ['0.9130', '0.8991', '0.8596', '0.9293', '0.9709'], 'Sad': ['0.8750', '0.9057', '0.8913', '0.9412', '0.9600']}
80/200 [00:00<00:01, 95.48it/s]Testing:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:00<00:01, 95.37it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [00:01<00:01, 94.52it/s]Testing:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [00:01<00:00, 94.25it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:01<00:00, 94.30it/s]Testing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [00:01<00:00, 93.82it/s]Testing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:01<00:00, 94.23it/s]Testing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:01<00:00, 95.77it/s]Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [00:01<00:00, 95.22it/s]Testing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [00:01<00:00, 95.41it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [00:01<00:00, 94.56it/s]Testing:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [00:02<00:00, 93.41it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:02<00:00, 93.21it/s]                                                          ------------------NEXT SCRIPT: RUNNER_CN, current setting----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911929/matplotlib-xomuwci8 because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on cn =======================

Load dataset: 
Loading de train data: fold_0...
Preprocess de fold_0 data for cn model
Loading de eval data: fold_0...
Preprocess de fold_0 data for cn model
Loading de test data: fold_0...
Preprocess de fold_0 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 938.4458, Training accuracy: 0.7588
Macro F1-score: 0.7539
Model performance on Angry speech (in training): 
	Precision: 0.9706, Recall: 0.4950, F1_score: 0.6556
Model performance on Happy speech (in training): 
	Precision: 0.5958, Recall: 0.9325, F1_score: 0.7271
Model performance on Neutral speech (in training): 
	Precision: 0.9264, Recall: 0.6925, F1_score: 0.7926
Model performance on Sad speech (in training): 
	Precision: 0.7771, Recall: 0.9150, F1_score: 0.8404

Eval Phase: 
Validation loss: 18.2849, Validation accuracy: 0.9750
Macro F1-score: 0.9749
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 1: 0.9750. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:26<11:36:23, 26.13s/it]Training:   7%|â–‹         | 112/1600 [00:36<06:17,  3.94it/s] Training:  15%|â–ˆâ–Œ        | 242/1600 [00:46<03:15,  6.96it/s]Training:  24%|â–ˆâ–ˆâ–       | 385/1600 [00:56<02:10,  9.31it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 545/1600 [01:06<01:32, 11.42it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 717/1600 [01:16<01:06, 13.21it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 898/1600 [01:26<00:47, 14.69it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1084/1600 [01:36<00:32, 15.87it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1272/1600 [01:46<00:19, 16.75it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1470/1600 [01:56<00:07, 17.65it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10Training loss: 217.3573, Training accuracy: 0.9675
Macro F1-score: 0.9675
Model performance on Angry speech (in training): 
	Precision: 0.9797, Recall: 0.9675, F1_score: 0.9736
Model performance on Happy speech (in training): 
	Precision: 0.9500, Recall: 0.9500, F1_score: 0.9500
Model performance on Neutral speech (in training): 
	Precision: 0.9533, Recall: 0.9700, F1_score: 0.9616
Model performance on Sad speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850

Eval Phase: 
Validation loss: 4.8426, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
New best accuracy for layer 4 on epoch 2: 0.9950. Model saved.
Epoch 3/100

Training Phase:
<01:08, 20.41it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 413/1600 [00:20<00:57, 20.61it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 21.02it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 845/1600 [00:40<00:36, 20.75it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1600 [00:50<00:25, 20.94it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1272/1600 [01:01<00:15, 20.78it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1477/1600 [01:11<00:05, 20.69it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.49it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.61it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1600 [00:30<00:46, 20.86it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1600 [00:40<00:37, 20.61it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1600 [00:50<00:26, 20Training loss: 143.0804, Training accuracy: 0.9738
Macro F1-score: 0.9738
Model performance on Angry speech (in training): 
	Precision: 0.9848, Recall: 0.9725, F1_score: 0.9786
Model performance on Happy speech (in training): 
	Precision: 0.9603, Recall: 0.9675, F1_score: 0.9639
Model performance on Neutral speech (in training): 
	Precision: 0.9652, Recall: 0.9700, F1_score: 0.9676
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850

Eval Phase: 
Validation loss: 3.3641, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 4/100

Training Phase:
Training loss: 126.0473, Training accuracy: 0.9775
Macro F1-score: 0.9775
Model performance on Angry speech (in training): 
	Precision: 0.9849, Recall: 0.9800, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9676, Recall: 0.9700, F1_score: 0.9688
Model performance on Neutral speech (in training): 
	Precision: 0.9701, Recall: 0.9750, F1_score: 0.9726
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862

Eval Phase: 
.64it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1257/1600 [01:00<00:16, 20.84it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1470/1600 [01:11<00:06, 20.71it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.61it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:58, 20.41it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 621/1600 [00:30<00:47, 20.51it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1600 [00:40<00:37, 20.69it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [00:50<00:26, 20.70it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1600 [01:00<00:16, 20.82it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1464/1600 [01:10<00:06, 20.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]           Validation loss: 23.2011, Validation accuracy: 0.9650
Macro F1-score: 0.9649
Model performance on Angry speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Happy speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 5/100

Training Phase:
Training loss: 101.6068, Training accuracy: 0.9825
Macro F1-score: 0.9825
Model performance on Angry speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813
Model performance on Happy speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 3.2163, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 6/100

Training Phase:
                                        Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.09it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 422/1600 [00:20<00:56, 20.96it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 635/1600 [00:30<00:45, 21.09it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 848/1600 [00:40<00:35, 20.99it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1057/1600 [00:50<00:26, 20.80it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1262/1600 [01:00<00:16, 20.62it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1600 [01:10<00:06, 20.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 202/1600 [00:10<01:09, 20.13it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.75it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 626/1600 [00:30<00:46, 20.80it/s]TTraining loss: 59.7204, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9827, Recall: 0.9925, F1_score: 0.9876
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950

Eval Phase: 
Validation loss: 0.7966, Validation accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 6: 1.0000. Model saved.
Epoch 7/100

Training Phase:
raining:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:37, 20.58it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1039/1600 [00:50<00:27, 20.51it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1600 [01:00<00:17, 20.59it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1458/1600 [01:10<00:06, 20.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.42it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:57, 20.77it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1600 [00:30<00:47, 20.64it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 834/1600 [00:40<00:37, 20.63it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1041/1600 [00:50<00:27, 20.50it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1248/1600 [01:00<00:17, 20.53it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1455/1600 [01:10<00:07, 20Training loss: 66.6706, Training accuracy: 0.9862
Macro F1-score: 0.9862
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Neutral speech (in training): 
	Precision: 0.9802, Recall: 0.9900, F1_score: 0.9851
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 0.5871, Validation accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 8/100

Training Phase:
Training loss: 64.9620, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 0.1173, Validation accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 9/100

Training Phase:
.58it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.07it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 426/1600 [00:20<00:55, 21.28it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 641/1600 [00:30<00:45, 21.16it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 852/1600 [00:40<00:35, 20.97it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1600 [00:50<00:26, 20.79it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1270/1600 [01:00<00:15, 20.87it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1481/1600 [01:10<00:05, 20.84it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.36it/s]TrTraining loss: 56.4350, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 3.4904, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 10/100

Training Phase:
aining:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:20<00:58, 20.37it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 618/1600 [00:30<00:47, 20.64it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 830/1600 [00:40<00:36, 20.84it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [00:50<00:26, 20.85it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1251/1600 [01:00<00:16, 20.79it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1462/1600 [01:10<00:06, 20.87it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.45it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.60it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 622/1600 [00:30<00:47, 20.73it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 831/1600 [00:40<00:37, 20.65it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [00:50<00:26, 20.76it/s]Training:  7Training loss: 59.0452, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 12.9503, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 11/100

Training Phase:
Training loss: 61.7298, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9874, Recall: 0.9800, F1_score: 0.9837
Model performance on Happy speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813
Model performance on Neutral speech (in training): 
	Precision: 0.9827, Recall: 0.9925, F1_score: 0.9876
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975

Eval Phase: 
8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1600 [01:00<00:16, 20.63it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1456/1600 [01:10<00:07, 20.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.55it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 413/1600 [00:20<00:57, 20.62it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:30<00:47, 20.50it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 826/1600 [00:40<00:37, 20.48it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1033/1600 [00:50<00:27, 20.54it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1240/1600 [01:00<00:17, 20.59it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1448/1600 [01:10<00:07, 20.64it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                    Validation loss: 7.6542, Validation accuracy: 0.9850
Macro F1-score: 0.9851
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 12/100

Training Phase:
Training loss: 26.1626, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9926, Recall: 1.0000, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9949, Recall: 0.9850, F1_score: 0.9899
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 9.0650, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 13/100

Training Phase:
               Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.32it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.56it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 625/1600 [00:30<00:46, 20.87it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 838/1600 [00:40<00:36, 20.82it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1047/1600 [00:50<00:26, 20.82it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1600 [01:00<00:16, 20.82it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1465/1600 [01:10<00:06, 20.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.97it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:56, 20.82it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 628/1600 [00:30<00:47, 20.61it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–Training loss: 32.0974, Training accuracy: 0.9938
Macro F1-score: 0.9938
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 3.0227, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 14/100

Training Phase:
ˆâ–ˆâ–    | 834/1600 [00:40<00:37, 20.57it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [00:50<00:27, 20.32it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1251/1600 [01:00<00:16, 20.57it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1464/1600 [01:10<00:06, 20.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 212/1600 [00:10<01:05, 21.11it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 424/1600 [00:20<00:56, 20.73it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 635/1600 [00:30<00:46, 20.87it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1600 [00:40<00:36, 20.78it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1062/1600 [00:50<00:25, 21.07it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1278/1600 [01:00<00:15, 21.12it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1491/1600 [01:11<00:05, 20.97it/s]            Training loss: 82.2487, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875

Eval Phase: 
Validation loss: 0.8072, Validation accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 15/100

Training Phase:
Training loss: 27.3500, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 0.0266, Validation accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 16/100

Training Phase:
                                                 Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.63it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 419/1600 [00:20<00:56, 20.93it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 632/1600 [00:30<00:45, 21.08it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 845/1600 [00:40<00:35, 21.02it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1057/1600 [00:50<00:25, 21.08it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1269/1600 [01:00<00:15, 20.78it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1482/1600 [01:10<00:05, 20.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.55it/s]Training:  26%|â–ˆâ–ˆâ–Training loss: 47.7907, Training accuracy: 0.9888
Macro F1-score: 0.9887
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9827, Recall: 0.9925, F1_score: 0.9876
Model performance on Neutral speech (in training): 
	Precision: 0.9874, Recall: 0.9775, F1_score: 0.9824
Model performance on Sad speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900

Eval Phase: 
Validation loss: 1.4444, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 17/100

Training Phase:
Œ       | 412/1600 [00:20<00:58, 20.33it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 623/1600 [00:30<00:47, 20.65it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:36, 20.83it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [00:50<00:26, 20.97it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1261/1600 [01:00<00:16, 20.95it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1471/1600 [01:10<00:06, 20.91it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.57it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 419/1600 [00:20<00:56, 20.94it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 636/1600 [00:30<00:45, 21.24it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 636/1600 [00:40<00:45, 21.24it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 848/1600 [00:40<00:35, 21.18it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   |Training loss: 32.1417, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 40.1371, Validation accuracy: 0.9550
Macro F1-score: 0.9549
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9556, Recall: 0.8600, F1_score: 0.9053
Model performance on Sad speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Epoch 18/100

Training Phase:
Training loss: 58.9563, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
 1060/1600 [00:50<00:25, 20.94it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1266/1600 [01:00<00:16, 20.75it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1472/1600 [01:10<00:06, 20.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 212/1600 [00:10<01:05, 21.18it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 424/1600 [00:20<00:55, 21.09it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 635/1600 [00:30<00:45, 21.07it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1600 [00:40<00:35, 20.95it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1054/1600 [00:50<00:26, 20.89it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1262/1600 [01:00<00:16, 20.85it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1470/1600 [01:10<00:06, 20.79it/s]                                                             Evaluating:   0%|          | 0/200 [Validation loss: 0.4296, Validation accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 19/100

Training Phase:
Training loss: 30.1148, Training accuracy: 0.9938
Macro F1-score: 0.9938
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 3.3595, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 20/100

Training Phase:
00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 208/1600 [00:10<01:06, 20.80it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:57, 20.71it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 623/1600 [00:30<00:47, 20.55it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1600 [00:40<00:37, 20.66it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1045/1600 [00:50<00:26, 20.87it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1260/1600 [01:00<00:16, 21.05it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1475/1600 [01:10<00:06, 20.83it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:06, 21.04it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 422/1600 [00:20<00:56, 20.72it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600Training loss: 24.2437, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 1.7075, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 21/100

Training Phase:
 [00:30<00:46, 20.75it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 838/1600 [00:40<00:37, 20.58it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1600 [00:50<00:27, 20.55it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1600 [01:00<00:16, 20.76it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1468/1600 [01:10<00:06, 20.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.51it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.51it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 618/1600 [00:30<00:47, 20.48it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 833/1600 [00:40<00:36, 20.87it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [00:50<00:26, 20.80it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1600 [01:00<00:16, 20.78it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆTraining loss: 48.4653, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 4.4490, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 22/100

Training Phase:
Training loss: 24.3895, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 0.3016, Validation accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 23/100

Training Phase:
â–| 1467/1600 [01:10<00:06, 20.86it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.46it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 410/1600 [00:20<00:58, 20.47it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 626/1600 [00:30<00:46, 20.96it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 842/1600 [00:40<00:36, 20.95it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1056/1600 [00:50<00:25, 21.08it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1271/1600 [01:00<00:15, 21.19it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1486/1600 [01:10<00:05, 21.17it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1Training loss: 30.4500, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9949, Recall: 0.9775, F1_score: 0.9861
Model performance on Neutral speech (in training): 
	Precision: 0.9852, Recall: 0.9975, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 3.1415, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 24/100

Training Phase:
600 [00:10<01:07, 20.56it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.70it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 622/1600 [00:30<00:47, 20.42it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 830/1600 [00:40<00:37, 20.54it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [00:50<00:26, 20.89it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1262/1600 [01:00<00:16, 20.86it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1470/1600 [01:11<00:06, 20.71it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.86it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 419/1600 [00:20<00:56, 20.90it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 20.83it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 838/1600 [00:40<00:36, 20.81it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [00:50Training loss: 64.7671, Training accuracy: 0.9888
Macro F1-score: 0.9888
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950

Eval Phase: 
Validation loss: 2.0625, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 25/100

Training Phase:
Training loss: 10.8110, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
<00:26, 20.80it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1254/1600 [01:00<00:16, 20.76it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1462/1600 [01:10<00:06, 20.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.30it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 410/1600 [00:20<00:58, 20.45it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 618/1600 [00:30<00:47, 20.58it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 826/1600 [00:40<00:37, 20.61it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1033/1600 [00:50<00:27, 20.54it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1243/1600 [01:00<00:17, 20.69it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1453/1600 [01:10<00:07, 20.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]   Validation loss: 2.0435, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 1.0000

Test Phase: 
                                                Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   4%|â–         | 8/200 [00:00<00:02, 78.93it/s]Testing:   9%|â–‰         | 18/200 [00:00<00:02, 86.70it/s]Testing:  14%|â–ˆâ–        | 28/200 [00:00<00:01, 90.38it/s]Testing:  19%|â–ˆâ–‰        | 38/200 [00:00<00:01, 91.20it/s]Testing:  24%|â–ˆâ–ˆâ–       | 48/200 [00:00<00:01, 92.06it/s]Testing:  29%|â–ˆâ–ˆâ–‰       | 58/200 [00:00<00:01, 93.05it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:00<00:01, 92.09it/s]Testing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:00<00:01, 91.59it/s]Testing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:00<00:01, 92.18it/s]Testing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:01<00:01, 92.22it/s]Testing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [00:01<00:00, 93.45it/s]Testing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:01<00:00, 93.59it/s]Testing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:01<00:00, 92.94it/s]Testing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:01<00:00, 92.67it/sTest loss: 14.2569, Test accuracy: 0.9850
Macro F1-score: 0.9849
Model performance on Angry speech (in test): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in test): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

======================= This is fold_1 on cn =======================

Load dataset: 
Loading de train data: fold_1...
Preprocess de fold_1 data for cn model
Loading de eval data: fold_1...
Preprocess de fold_1 data for cn model
Loading de test data: fold_1...
Preprocess de fold_1 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
]Testing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [00:01<00:00, 92.69it/s]Testing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:01<00:00, 92.60it/s]Testing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [00:01<00:00, 94.53it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:01<00:00, 93.51it/s]Testing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [00:02<00:00, 92.05it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:02<00:00, 93.30it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.62it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:58, 20.38it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 619/1600 [00:30<00:48, 20.39it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 829/1600 [00:40<00:37, 20.61it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1039/1600 [00:50<00:27, 20.36it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1246/1600 [01:00<00:17, 20.45it/s]Training:  91%|â–ˆâ–ˆTraining loss: 330.9654, Training accuracy: 0.9337
Macro F1-score: 0.9339
Model performance on Angry speech (in training): 
	Precision: 0.9544, Recall: 0.9425, F1_score: 0.9484
Model performance on Happy speech (in training): 
	Precision: 0.8892, Recall: 0.9025, F1_score: 0.8958
Model performance on Neutral speech (in training): 
	Precision: 0.9134, Recall: 0.9225, F1_score: 0.9179
Model performance on Sad speech (in training): 
	Precision: 0.9797, Recall: 0.9675, F1_score: 0.9736

Eval Phase: 
Validation loss: 10.5567, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 1: 0.9800. Model saved.
Epoch 2/100

Training Phase:
Training loss: 123.3542, Training accuracy: 0.9738
Macro F1-score: 0.9738
Model performance on Angry speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Happy speech (in training): 
	Precision: 0.9622, Recall: 0.9550, F1_score: 0.9586
Model performance on Neutral speech (in training): 
	Precision: 0.9580, Recall: 0.9700, F1_score: 0.9640
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 11.1424, Validation accuracy: 0.9750
Macro F1-score: 0.9752
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 3/100

Training Phase:
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1453/1600 [01:11<00:07, 20.32it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–ˆâ–        | 199/1600 [00:10<01:10, 19.87it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.78it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 20.68it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 841/1600 [00:40<00:36, 20.88it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1053/1600 [00:50<00:26, 20.95it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1270/1600 [01:00<00:15, 21.17it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1486/1600 [01:11<00:05, 20.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–Training loss: 93.9731, Training accuracy: 0.9794
Macro F1-score: 0.9794
Model performance on Angry speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Happy speech (in training): 
	Precision: 0.9650, Recall: 0.9650, F1_score: 0.9650
Model performance on Neutral speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 22.3648, Validation accuracy: 0.9700
Macro F1-score: 0.9705
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Epoch 4/100

Training Phase:
ˆâ–        | 199/1600 [00:10<01:10, 19.80it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 405/1600 [00:20<00:59, 20.21it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 612/1600 [00:30<00:48, 20.43it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 819/1600 [00:40<00:38, 20.28it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1024/1600 [00:50<00:28, 20.35it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1229/1600 [01:00<00:18, 20.35it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1435/1600 [01:10<00:08, 20.42it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.98it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:57, 20.38it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 628/1600 [00:30<00:47, 20.53it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1600 [00:40<00:37, 20.59it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1Training loss: 57.0306, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 10.7855, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 4: 0.9850. Model saved.
Epoch 5/100

Training Phase:
Training loss: 85.7398, Training accuracy: 0.9812
Macro F1-score: 0.9812
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9676, Recall: 0.9700, F1_score: 0.9688
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
051/1600 [00:50<00:26, 20.87it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1265/1600 [01:00<00:16, 20.89it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1479/1600 [01:10<00:05, 21.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.68it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 419/1600 [00:20<00:56, 20.94it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 631/1600 [00:30<00:46, 20.98it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 842/1600 [00:40<00:36, 20.99it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1052/1600 [00:50<00:26, 20.98it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1268/1600 [01:00<00:15, 21.15it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1483/1600 [01:10<00:05, 21.11it/s]                                                             Evaluating:   0%|          | 0/200 [00Validation loss: 4.1082, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
New best accuracy for layer 4 on epoch 5: 0.9950. Model saved.
Epoch 6/100

Training Phase:
Training loss: 53.6078, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Neutral speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 20.4535, Validation accuracy: 0.9700
Macro F1-score: 0.9699
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 7/100

Training Phase:
:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.68it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 415/1600 [00:20<00:57, 20.71it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 625/1600 [00:30<00:46, 20.84it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:36, 20.83it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1600 [00:50<00:26, 20.64it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1600 [01:00<00:16, 20.82it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1468/1600 [01:10<00:06, 20.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 212/1600 [00:10<01:05, 21.18it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 424/1600 [00:20<00:55, 21.04it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 634/1600 [Training loss: 46.8902, Training accuracy: 0.9888
Macro F1-score: 0.9888
Model performance on Angry speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9802, Recall: 0.9900, F1_score: 0.9851
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 6.8215, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 8/100

Training Phase:
00:30<00:46, 20.98it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1600 [00:40<00:36, 20.97it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1054/1600 [00:50<00:26, 20.86it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1267/1600 [01:00<00:15, 20.99it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1480/1600 [01:10<00:05, 20.94it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.94it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 423/1600 [00:20<00:55, 21.14it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 636/1600 [00:30<00:45, 21.18it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 849/1600 [00:40<00:36, 20.80it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1600 [00:50<00:26, 20.72it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1261/1600 [01:00<00:16, 20.52it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Training loss: 42.4892, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 7.5321, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 9/100

Training Phase:
Training loss: 40.4122, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 4.6788, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 10/100

Training Phase:
| 1474/1600 [01:10<00:06, 20.77it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.33it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 410/1600 [00:20<00:58, 20.43it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 621/1600 [00:30<00:47, 20.73it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 833/1600 [00:40<00:36, 20.90it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [00:50<00:26, 21.04it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1259/1600 [01:00<00:16, 21.02it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1469/1600 [01:10<00:06, 20.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 208/160Training loss: 49.0367, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 11.4500, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 11/100

Training Phase:
0 [00:10<01:07, 20.71it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 419/1600 [00:20<00:56, 20.90it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600 [00:30<00:46, 20.99it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 841/1600 [00:40<00:36, 20.84it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1053/1600 [00:50<00:26, 20.97it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1265/1600 [01:00<00:15, 21.04it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1477/1600 [01:10<00:05, 21.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.08it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 422/1600 [00:20<00:56, 20.96it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 632/1600 [00:30<00:46, 20.96it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1600 [00:40<00:35, 21.08it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1600 [00:50<0Training loss: 15.6238, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 15.4567, Validation accuracy: 0.9750
Macro F1-score: 0.9752
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 12/100

Training Phase:
0:25, 20.87it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1600 [01:00<00:25, 20.87it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1271/1600 [01:00<00:15, 20.94it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1482/1600 [01:11<00:05, 20.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 212/1600 [00:10<01:05, 21.17it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 424/1600 [00:20<00:56, 20.97it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 633/1600 [00:30<00:46, 20.88it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 841/1600 [00:40<00:36, 20.62it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1600 [00:50<00:26, 20.75it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1261/1600 [01:00<00:16, 20.78it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1470/1600 [01:10<00:06, 20.78it/s]                                          Training loss: 46.4981, Training accuracy: 0.9881
Macro F1-score: 0.9881
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9725, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 14.7893, Validation accuracy: 0.9900
Macro F1-score: 0.9899
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 13/100

Training Phase:
Training loss: 22.8231, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 9.7933, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 14/100

Training Phase:
                   Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.32it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 411/1600 [00:20<00:57, 20.53it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 618/1600 [00:30<00:47, 20.61it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 828/1600 [00:40<00:37, 20.74it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [00:50<00:26, 20.88it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1600 [01:00<00:16, 21.02it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1468/1600 [01:10<00:06, 21.16it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 21.00it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:5Training loss: 40.3166, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 2.6381, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 15/100

Training Phase:
6, 20.99it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600 [00:30<00:46, 20.92it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 839/1600 [00:40<00:36, 20.73it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [00:50<00:26, 20.75it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1600 [01:00<00:16, 20.57it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1462/1600 [01:10<00:06, 20.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.83it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:56, 20.96it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 631/1600 [00:30<00:46, 20.92it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 840/1600 [00:40<00:36, 20.72it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1045/1600 [00:50<00:26, 20.62it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1600 [01:00<00:Training loss: 14.3008, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 5.9638, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 16/100

Training Phase:
Training loss: 51.8545, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9824, Recall: 0.9750, F1_score: 0.9787
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 4.6225, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 17/100

Training Phase:
16, 20.78it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1600 [01:10<00:06, 20.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.53it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 421/1600 [00:20<00:56, 21.04it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 638/1600 [00:30<00:45, 21.29it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 854/1600 [00:40<00:35, 21.18it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1065/1600 [00:50<00:25, 20.91it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1272/1600 [01:00<00:15, 20.83it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1479/1600 [01:10<00:05, 20.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1Training loss: 35.2715, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 7.4225, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 18/100

Training Phase:
600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:06, 20.97it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 421/1600 [00:20<00:56, 20.72it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 20.73it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 839/1600 [00:40<00:36, 20.80it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1056/1600 [00:50<00:25, 21.13it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1273/1600 [01:00<00:15, 20.91it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1481/1600 [01:10<00:05, 20.87it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 201/1600 [00:10<01:09, 20.04it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 410/1600 [00:20<00:57, 20.54it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 621/1600 [00:30<00:47, 20.78it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1600 [00:40<00:36, 20.87it/s]Training loss: 2.6570, Training accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 15.8735, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 19/100

Training Phase:
Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [00:50<00:26, 20.70it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1600 [01:00<00:17, 20.39it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1458/1600 [01:10<00:06, 20.60it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 208/1600 [00:10<01:07, 20.73it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:57, 20.58it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1600 [00:30<00:47, 20.65it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1600 [00:40<00:37, 20.54it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [00:50<00:26, 20.90it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1264/1600 [01:00<00:16, 20.91it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1474/1600 [01:10<00:06, 20.83it/s]                                                             Training loss: 53.1220, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887

Eval Phase: 
Validation loss: 32.5656, Validation accuracy: 0.9700
Macro F1-score: 0.9701
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 20/100

Training Phase:
Training loss: 5.7160, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 11.6628, Validation accuracy: 0.9900
Macro F1-score: 0.9899
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 21/100

Training Phase:
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.57it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:56, 20.82it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1600 [00:30<00:46, 20.92it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 838/1600 [00:40<00:36, 20.80it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [00:50<00:26, 20.85it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1258/1600 [01:00<00:16, 20.70it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1463/1600 [01:10<00:06, 20.58it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.79it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 418/1600 [00:20<00:56, 20.85it/s]TrainTraining loss: 30.8480, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 8.3003, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 22/100

Training Phase:
ing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1600 [00:30<00:46, 20.79it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1600 [00:40<00:36, 20.81it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1049/1600 [00:50<00:26, 20.96it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1262/1600 [01:00<00:16, 20.70it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:10<00:06, 20.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.07it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 422/1600 [00:20<00:56, 20.79it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 632/1600 [00:30<00:46, 20.86it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 842/1600 [00:40<00:36, 20.77it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1600 [00:50<00:26, 20.79it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1260/1600 [01:00<00:16, 20.68it/s]TraiTraining loss: 34.8614, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 26.0381, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Sad speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Epoch 23/100

Training Phase:
Training loss: 15.9842, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 7.4279, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 24/100

Training Phase:
ning:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1473/1600 [01:10<00:06, 20.86it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.85it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 418/1600 [00:20<00:56, 20.82it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1600 [00:30<00:46, 20.79it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:36, 20.72it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1600 [00:50<00:26, 20.77it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1600 [01:00<00:16, 20.77it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1461/1600 [01:10<00:06, 20.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/sTraining loss: 9.9134, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 19.4796, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9950

Test Phase: 
]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.62it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.59it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:30<00:57, 20.59it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 622/1600 [00:30<00:47, 20.66it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:36, 20.90it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [00:50<00:26, 20.74it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1258/1600 [01:00<00:16, 20.81it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1468/1600 [01:11<00:06, 20.48it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   4%|â–         | 8/200 [00:00<00:02, 77.34it/s]Testing:   9%|â–‰         | 18/200 [00:00<00:02, 87.32it/s]Testing:  14%|â–ˆâ–        | 28/200 [00:00<00:01, 90.63it/s]Testing:  19%|â–ˆâ–‰        | 38/200 [00:00<00:01, 91.48it/s]Testing:  24%|â–ˆâ–ˆâ–       | 48/200 [00:00<00:01, 92.40it/s]Testing:  29%|â–ˆâ–ˆâ–‰       | 58/200 [00:00<00:01, 93.03it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:00<00:01, 93.18it/s]Testing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:00<00:01, 91.99it/s]Testing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:00<00:01, 93.29it/s]Testing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:01<00:01, 93.92it/s]Testing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [00:01<00:00, 93.31it/s]Testing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:01<00:00, 92.43it/s]Testing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:01<00:00, 93.52it/s]Testing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:01<00:00, 91.72it/s]Testing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [00:01<00:00, 92.56it/s]Testing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:01<00:00, 94.64it/s]Testing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [00:01<00:00, 93.41it/s]Testing:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [00:01<00:00, 93.53it/s]Testing:  94%|â–ˆâ–ˆâTest loss: 1.8828, Test accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in test): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in test): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901

======================= This is fold_2 on cn =======================

Load dataset: 
Loading de train data: fold_2...
Preprocess de fold_2 data for cn model
Loading de eval data: fold_2...
Preprocess de fold_2 data for cn model
Loading de test data: fold_2...
Preprocess de fold_2 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 202.7626, Training accuracy: 0.9519
Macro F1-score: 0.9519
Model performance on Angry speech (in training): 
	Precision: 0.9674, Recall: 0.9650, F1_score: 0.9662
Model performance on Happy speech (in training): 
	Precision: 0.9225, Recall: 0.9225, F1_score: 0.9225
Model performance on Neutral speech (in training): 
	Precision: 0.9327, Recall: 0.9350, F1_score: 0.9338
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850

Eval Phase: 
Validation loss: 12.1813, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
New best accuracy for layer 4 on epoch 1: 0.9750. Model saved.
Epoch 2/100

Training Phase:
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:02<00:00, 95.10it/s]Testing:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [00:02<00:00, 94.74it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.39it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.62it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:30<00:47, 20.47it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 830/1600 [00:40<00:37, 20.64it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [00:50<00:27, 20.50it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1600 [01:00<00:16, 20.74it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:11<00:06, 20.71it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<Training loss: 87.9674, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9849, Recall: 0.9800, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Neutral speech (in training): 
	Precision: 0.9777, Recall: 0.9850, F1_score: 0.9813
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 12.1765, Validation accuracy: 0.9850
Macro F1-score: 0.9851
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 2: 0.9850. Model saved.
Epoch 3/100

Training Phase:
01:06, 20.90it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:56, 20.87it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 20.81it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 837/1600 [00:40<00:37, 20.49it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [00:50<00:27, 20.41it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1600 [01:00<00:16, 20.67it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1464/1600 [01:10<00:06, 20.71it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–ˆâ–        | 199/1600 [00:10<01:10, 19.87it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 400/1600 [00:20<01:00, 19.97it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 601/1600 [00:30<00:50, 19.96it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 806/1600 [00:40<00:39, 20.14it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1020/1600 [00:50<00:28, 20.59Training loss: 67.6795, Training accuracy: 0.9862
Macro F1-score: 0.9863
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Happy speech (in training): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925

Eval Phase: 
Validation loss: 21.5236, Validation accuracy: 0.9750
Macro F1-score: 0.9749
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 4/100

Training Phase:
it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1234/1600 [01:00<00:17, 20.67it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1443/1600 [01:10<00:07, 20.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 203/1600 [00:10<01:08, 20.27it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.64it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:31<00:57, 20.64it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 583/1600 [00:31<00:57, 17.60it/s]Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 729/1600 [00:42<00:53, 16.15it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 886/1600 [00:52<00:44, 15.96it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1054/1600 [01:02<00:33, 16.22it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1235/1600 [01:12<00:21, 16.79it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1416/1600 [01:22<00:10, 17.19iTraining loss: 54.1082, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750
Model performance on Neutral speech (in training): 
	Precision: 0.9799, Recall: 0.9775, F1_score: 0.9787
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 9.4429, Validation accuracy: 0.9750
Macro F1-score: 0.9749
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 5/100

Training Phase:
Training loss: 33.7859, Training accuracy: 0.9912
Macro F1-score: 0.9913
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9777, Recall: 0.9875, F1_score: 0.9826
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 18.7753, Validation accuracy: 0.9750
Macro F1-score: 0.9749
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 6/100

Training Phase:
t/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.36it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:20<00:59, 20.17it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 609/1600 [00:30<00:49, 19.89it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 810/1600 [00:40<00:39, 19.94it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1020/1600 [00:50<00:28, 20.30it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1230/1600 [01:00<00:18, 20.45it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1438/1600 [01:10<00:07, 20.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.66it/s]Training: Training loss: 50.7367, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 9.2166, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 7/100

Training Phase:
 26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.66it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 621/1600 [00:30<00:47, 20.52it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 829/1600 [00:40<00:37, 20.61it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1037/1600 [00:50<00:27, 20.61it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1244/1600 [01:00<00:17, 20.50it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1600 [01:10<00:07, 20.42it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 203/1600 [00:10<01:08, 20.26it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 407/1600 [00:20<00:58, 20.33it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 621/1600 [00:30<00:47, 20.82it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:36, 20.80it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1043/1600 [00:50<00:26, 20.73it/s]Training:  78%|â–ˆâ–ˆâTraining loss: 28.2190, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9975, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 16.7200, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 8/100

Training Phase:
Training loss: 37.0545, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1250/1600 [01:00<00:16, 20.61it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1455/1600 [01:10<00:07, 20.55it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.93it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:57, 20.64it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 625/1600 [00:30<00:47, 20.43it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:37, 20.63it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1047/1600 [00:50<00:26, 20.81it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1262/1600 [01:00<00:16, 21.00it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1481/1600 [01:10<00:05, 21.29it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                            Validation loss: 25.2058, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 9/100

Training Phase:
Training loss: 41.2381, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 3.7415, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 9: 0.9950. Model saved.
Epoch 10/100

Training Phase:
       Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:06, 20.98it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 421/1600 [00:20<00:56, 20.86it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600 [00:30<00:46, 20.85it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 839/1600 [00:40<00:36, 20.86it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [00:50<00:26, 20.79it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1261/1600 [01:00<00:16, 20.94it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1475/1600 [01:10<00:05, 21.09it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.54it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 413/1600 [00:20<00:57, 20.58it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 626/1600 [00:30<00:46, 20.89it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– Training loss: 24.3220, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 15.8585, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 11/100

Training Phase:
   | 839/1600 [00:40<00:36, 20.91it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1049/1600 [00:50<00:26, 20.72it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1258/1600 [01:00<00:16, 20.77it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1600 [01:10<00:06, 20.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.45it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 415/1600 [00:20<00:57, 20.76it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 625/1600 [00:30<00:47, 20.70it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 834/1600 [00:40<00:36, 20.76it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1600 [00:50<00:26, 20.81it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1254/1600 [01:00<00:16, 20.69it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:10<00:06, 20.84it/s]                    Training loss: 7.1502, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 12.4629, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 12/100

Training Phase:
Training loss: 64.4788, Training accuracy: 0.9888
Macro F1-score: 0.9888
Model performance on Angry speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9751, Recall: 0.9800, F1_score: 0.9776
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 19.2320, Validation accuracy: 0.9750
Macro F1-score: 0.9748
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 13/100

Training Phase:
                                         Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   9%|â–‰         | 141/1600 [00:10<01:43, 14.09it/s]Training:  18%|â–ˆâ–Š        | 288/1600 [00:20<01:30, 14.42it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 454/1600 [00:30<01:14, 15.41it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1600 [00:40<01:00, 16.14it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 810/1600 [00:50<00:46, 16.91it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1005/1600 [01:00<00:33, 17.75it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1206/1600 [01:10<00:21, 18.51it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1413/1600 [01:20<00:09, 19.20it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 216/16Training loss: 11.3149, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 9.7056, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 14/100

Training Phase:
00 [00:10<01:04, 21.60it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 439/1600 [00:20<00:52, 22.01it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 662/1600 [00:30<00:42, 21.95it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 881/1600 [00:40<00:32, 21.90it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1107/1600 [00:50<00:22, 22.15it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1333/1600 [01:00<00:12, 21.87it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1557/1600 [01:10<00:01, 22.02it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:02, 21.91it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:54, 21.32it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:30<00:54, 21.32it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 640/1600 [00:30<00:46, 20.64it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 853/1600 [00:40<00:35Training loss: 27.7697, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 6.7521, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 15/100

Training Phase:
, 20.88it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1073/1600 [00:50<00:24, 21.27it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1294/1600 [01:00<00:14, 21.54it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1516/1600 [01:10<00:03, 21.75it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 224/1600 [00:10<01:01, 22.32it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 448/1600 [00:20<00:51, 22.30it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 671/1600 [00:30<00:43, 21.51it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 881/1600 [00:40<00:33, 21.27it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1090/1600 [00:51<00:24, 20.96it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1298/1600 [01:01<00:14, 20.90it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1509/1600 [01:11<00:04, 20.94it/s]                                            Training loss: 26.0346, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 13.6279, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 16/100

Training Phase:
Training loss: 16.7522, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 53.6799, Validation accuracy: 0.9500
Macro F1-score: 0.9505
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9773, Recall: 0.8600, F1_score: 0.9149
Model performance on Sad speech (in validation): 
	Precision: 0.8475, Recall: 1.0000, F1_score: 0.9174
Epoch 17/100

Training Phase:
                 Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.08it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 422/1600 [00:20<00:56, 20.84it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 20.72it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 838/1600 [00:40<00:36, 20.79it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1052/1600 [00:50<00:26, 20.99it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1266/1600 [01:00<00:15, 21.00it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1477/1600 [01:10<00:05, 20.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.67it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 417/1600 [00:20<00:56,Training loss: 28.0348, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 12.0246, Validation accuracy: 0.9750
Macro F1-score: 0.9749
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 18/100

Training Phase:
 20.85it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600 [00:30<00:46, 21.03it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 850/1600 [00:40<00:35, 21.40it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1073/1600 [00:50<00:24, 21.72it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1296/1600 [01:00<00:13, 21.85it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1517/1600 [01:10<00:03, 21.61it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:02, 21.96it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:53, 21.83it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 658/1600 [00:30<00:43, 21.71it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 874/1600 [00:40<00:33, 21.64it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1092/1600 [00:50<00:23, 21.68it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1310/1600 [01:00<00:Training loss: 10.0638, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 8.5937, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 19/100

Training Phase:
Training loss: 23.3884, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 19.0260, Validation accuracy: 0.9750
Macro F1-score: 0.9749
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9950

Test Phase: 
13, 21.53it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1530/1600 [01:10<00:03, 21.65it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.77it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 436/1600 [00:20<00:53, 21.71it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [00:30<00:43, 21.67it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 871/1600 [00:40<00:33, 21.67it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1089/1600 [00:50<00:23, 21.70it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1307/1600 [01:00<00:13, 21.62it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1522/1600 [01:10<00:03, 21.49it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:13, 14.55it/s]Testing:   2%|â–         | 4/200 [00:00<00:12, 15.55it/s]Testing:   3%|â–Ž         | 6/200 [00:00<00:12, 15.63it/s]Testing:   4%|â–         | 9/200 [00:00<00:10, 17.53it/s]Testing:   6%|â–Œ         | 12/200 [00:00<00:09, 19.59it/s]Testing:   8%|â–Š         | 15/200 [00:00<00:08, 20.63it/s]Testing:   9%|â–‰         | 18/200 [00:00<00:08, 21.16it/s]Testing:  10%|â–ˆ         | 21/200 [00:01<00:07, 23.30it/s]Testing:  12%|â–ˆâ–        | 24/200 [00:01<00:07, 22.57it/s]Testing:  14%|â–ˆâ–Ž        | 27/200 [00:01<00:07, 22.80it/s]Testing:  15%|â–ˆâ–Œ        | 30/200 [00:01<00:07, 23.93it/s]Testing:  16%|â–ˆâ–‹        | 33/200 [00:01<00:07, 21.01it/s]Testing:  18%|â–ˆâ–Š        | 36/200 [00:01<00:08, 19.82it/s]Testing:  20%|â–ˆâ–‰        | 39/200 [00:01<00:07, 21.21it/s]Testing:  22%|â–ˆâ–ˆâ–       | 43/200 [00:02<00:06, 24.83it/s]Testing:  24%|â–ˆâ–ˆâ–       | 48/200 [00:02<00:05, 28.85it/s]Testing:  26%|â–ˆâ–ˆâ–‹       | 53/200 [00:02<00:04, 33.49it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:02<00:04, 32.82it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [00:02<00:03, 37.75it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:02<00:03, 42.47it/s]Testing:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [00:02<00:02, 43.91it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [00:02<00:02, 43.59it/s]Testing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:02<00:02, 50.30it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [00:03<00:02, 52.80it/s]Testing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:03<00:01, 54.57it/s]Testing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:03<00:01, 51.51it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:03<00:01, 49.93it/s]Testing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [00:03<00:01, 44.22it/s]Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:03<00:01, 48.38it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:03<00:01, 54.91it/s]Testing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:03<00:01, 54.27it/s]Testing:  73%|â–ˆâ–ˆâ–ˆâ–Test loss: 17.6296, Test accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in test): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in test): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

======================= This is fold_3 on cn =======================

Load dataset: 
Loading de train data: fold_3...
Preprocess de fold_3 data for cn model
Loading de eval data: fold_3...
Preprocess de fold_3 data for cn model
Loading de test data: fold_3...
Preprocess de fold_3 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [00:04<00:00, 59.71it/s]Testing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:04<00:00, 64.57it/s]Testing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:04<00:00, 70.61it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:04<00:00, 67.55it/s]Testing:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [00:04<00:00, 67.04it/s]Testing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [00:04<00:00, 64.32it/s]Testing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:04<00:00, 63.80it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:04<00:00, 65.16it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   8%|â–Š         | 130/1600 [00:10<01:53, 12.94it/s]Training:  18%|â–ˆâ–Š        | 282/1600 [00:20<01:32, 14.23it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 448/1600 [00:30<01:15, 15.28it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 626/1600 [00:40<00:59, 16.25it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 810/1600 [00:50<00:Training loss: 183.5861, Training accuracy: 0.9650
Macro F1-score: 0.9649
Model performance on Angry speech (in training): 
	Precision: 0.9677, Recall: 0.9725, F1_score: 0.9701
Model performance on Happy speech (in training): 
	Precision: 0.9587, Recall: 0.9275, F1_score: 0.9428
Model performance on Neutral speech (in training): 
	Precision: 0.9443, Recall: 0.9750, F1_score: 0.9594
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875

Eval Phase: 
Validation loss: 6.7314, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 1: 0.9950. Model saved.
Epoch 2/100

Training Phase:
46, 17.00it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1002/1600 [01:00<00:33, 17.74it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1203/1600 [01:10<00:21, 18.50it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1405/1600 [01:20<00:10, 19.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.66it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 435/1600 [00:20<00:54, 21.50it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [00:30<00:43, 21.61it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 871/1600 [00:40<00:34, 21.43it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1088/1600 [00:50<00:23, 21.51it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1305/1600 [01:00<00:13, 21.55it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1522/1600 [01:10<00:03, 21.57it/s]                                            Training loss: 66.1107, Training accuracy: 0.9862
Macro F1-score: 0.9863
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9875, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9777, Recall: 0.9875, F1_score: 0.9826
Model performance on Neutral speech (in training): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 5.2565, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 3/100

Training Phase:
Training loss: 71.6706, Training accuracy: 0.9862
Macro F1-score: 0.9862
Model performance on Angry speech (in training): 
	Precision: 0.9752, Recall: 0.9825, F1_score: 0.9788
Model performance on Happy speech (in training): 
	Precision: 0.9823, Recall: 0.9725, F1_score: 0.9774
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 3.7564, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 4/100

Training Phase:
                 Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 214/1600 [00:10<01:04, 21.38it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 430/1600 [00:20<00:54, 21.43it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 647/1600 [00:30<00:44, 21.55it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 864/1600 [00:40<00:34, 21.51it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1079/1600 [00:50<00:24, 21.32it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1289/1600 [01:00<00:14, 21.17it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1498/1600 [01:10<00:04, 20.97it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.01it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 442/1600 [00:20<00:53,Training loss: 41.9585, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 32.3482, Validation accuracy: 0.9650
Macro F1-score: 0.9651
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 5/100

Training Phase:
 21.67it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 658/1600 [00:30<00:43, 21.61it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 874/1600 [00:40<00:34, 21.31it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1089/1600 [00:50<00:23, 21.36it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1306/1600 [01:00<00:13, 21.46it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1523/1600 [01:10<00:03, 21.53it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 214/1600 [00:10<01:04, 21.35it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 431/1600 [00:20<00:54, 21.54it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 431/1600 [00:30<00:54, 21.54it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 641/1600 [00:30<00:45, 21.25it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 852/1600 [00:40<00:35, 21.19it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1063/1600 [00:50<00:25, 21.09itTraining loss: 50.1544, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925

Eval Phase: 
Validation loss: 5.0160, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 6/100

Training Phase:
Training loss: 54.6422, Training accuracy: 0.9888
Macro F1-score: 0.9888
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9776, Recall: 0.9825, F1_score: 0.9800
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1275/1600 [01:00<00:15, 21.11it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1491/1600 [01:10<00:05, 21.27it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 216/1600 [00:10<01:04, 21.52it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 432/1600 [00:20<00:54, 21.50it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 649/1600 [00:30<00:44, 21.56it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 866/1600 [00:40<00:34, 21.57it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1084/1600 [00:50<00:23, 21.62it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1302/1600 [01:00<00:13, 21.60it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1518/1600 [01:10<00:03, 21.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]              Validation loss: 4.5813, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 7/100

Training Phase:
Training loss: 26.7467, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 50.2057, Validation accuracy: 0.9500
Macro F1-score: 0.9495
Model performance on Angry speech (in validation): 
	Precision: 0.8333, Recall: 1.0000, F1_score: 0.9091
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8000, F1_score: 0.8889
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 8/100

Training Phase:
                                     Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 215/1600 [00:10<01:04, 21.46it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 430/1600 [00:20<00:54, 21.35it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 643/1600 [00:30<00:44, 21.31it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 859/1600 [00:40<00:34, 21.40it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1075/1600 [00:50<00:24, 21.35it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1297/1600 [01:00<00:14, 21.62it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1519/1600 [01:10<00:03, 21.75it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 214/1600 [00:10<01:04, 21.36it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 433/1600 [00:20<00:53, 21.67it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 652/1600 [00:30<00:43, 21.70it/s]TraiTraining loss: 21.1838, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 8.6981, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 9/100

Training Phase:
ning:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 874/1600 [00:40<00:33, 21.88it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1096/1600 [00:50<00:23, 21.87it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1319/1600 [01:00<00:12, 22.01it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1542/1600 [01:10<00:02, 22.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 223/1600 [00:10<01:01, 22.23it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 446/1600 [00:20<00:52, 22.01it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 666/1600 [00:30<00:42, 21.99it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 886/1600 [00:40<00:32, 21.96it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1106/1600 [00:50<00:22, 21.88it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1324/1600 [01:00<00:12, 21.74it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1547/1600 [01:10<00Training loss: 38.8358, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 10.0035, Validation accuracy: 0.9750
Macro F1-score: 0.9749
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 10/100

Training Phase:
Training loss: 29.0171, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 9.2781, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 11/100

Training Phase:
:02, 21.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 223/1600 [00:10<01:01, 22.26it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 446/1600 [00:20<00:51, 22.21it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 668/1600 [00:30<00:42, 21.94it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 888/1600 [00:40<00:32, 21.95it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1108/1600 [00:50<00:22, 21.82it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1327/1600 [01:00<00:12, 21.84it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1548/1600 [01:10<00:02, 21.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 224/1600 [00:10<01:01, 22Training loss: 19.7651, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 9.2799, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 12/100

Training Phase:
.28it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 447/1600 [00:20<00:52, 21.81it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 667/1600 [00:30<00:42, 21.88it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 887/1600 [00:40<00:32, 21.84it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1105/1600 [00:50<00:22, 21.79it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1324/1600 [01:00<00:12, 21.81it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1544/1600 [01:10<00:02, 21.84it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.83it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 438/1600 [00:20<00:53, 21.53it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [00:30<00:44, 21.47it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 871/1600 [00:40<00:33, 21.58it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1089/1600 [00:50<00:23, 21.45it/Training loss: 38.4210, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 10.9824, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 13/100

Training Phase:
Training loss: 23.2125, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1306/1600 [01:00<00:13, 21.50it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1524/1600 [01:10<00:03, 21.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.06it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 435/1600 [00:20<00:53, 21.79it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 659/1600 [00:30<00:43, 21.82it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 881/1600 [00:40<00:32, 21.96it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1103/1600 [00:50<00:22, 21.84it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1326/1600 [01:00<00:12, 21.97it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1549/1600 [01:10<00:02, 22.00it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]             Validation loss: 7.4715, Validation accuracy: 0.9850
Macro F1-score: 0.9849
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9950

Test Phase: 
                                      Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:02, 94.21it/s]Testing:  10%|â–ˆ         | 21/200 [00:00<00:01, 98.59it/s]Testing:  16%|â–ˆâ–Œ        | 31/200 [00:00<00:01, 96.16it/s]Testing:  20%|â–ˆâ–ˆ        | 41/200 [00:00<00:01, 97.57it/s]Testing:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:00<00:01, 98.62it/s]Testing:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [00:00<00:01, 97.66it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [00:00<00:01, 97.63it/s]Testing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:00<00:01, 98.85it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:00<00:01, 97.82it/s]Testing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:01<00:01, 96.94it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:01<00:00, 97.56it/s]Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:01<00:00, 98.86it/s]Testing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [00:01<00:00, 98.19it/s]Testing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:01<00:00, 97.37it/sTest loss: 7.0864, Test accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in test): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in test): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Sad speech (in test): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804

======================= This is fold_4 on cn =======================

Load dataset: 
Loading de train data: fold_4...
Preprocess de fold_4 data for cn model
Loading de eval data: fold_4...
Preprocess de fold_4 data for cn model
Loading de test data: fold_4...
Preprocess de fold_4 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
]Testing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:01<00:00, 97.00it/s]Testing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:01<00:00, 96.57it/s]Testing:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [00:01<00:00, 96.43it/s]Testing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [00:01<00:00, 98.17it/s]Testing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [00:02<00:00, 99.50it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.05it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 425/1600 [00:20<00:55, 21.19it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 645/1600 [00:30<00:44, 21.56it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 865/1600 [00:40<00:34, 21.51it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1084/1600 [00:50<00:23, 21.62it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1303/1600 [01:00<00:13, 21.59it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1522/1600 [01:10<00:03, 21.69it/s]            Training loss: 161.0452, Training accuracy: 0.9637
Macro F1-score: 0.9638
Model performance on Angry speech (in training): 
	Precision: 0.9696, Recall: 0.9575, F1_score: 0.9635
Model performance on Happy speech (in training): 
	Precision: 0.9472, Recall: 0.9425, F1_score: 0.9449
Model performance on Neutral speech (in training): 
	Precision: 0.9435, Recall: 0.9600, F1_score: 0.9517
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 55.2964, Validation accuracy: 0.8900
Macro F1-score: 0.8835
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6000, F1_score: 0.7500
Model performance on Neutral speech (in validation): 
	Precision: 0.7500, Recall: 0.9600, F1_score: 0.8421
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
New best accuracy for layer 4 on epoch 1: 0.8900. Model saved.
Epoch 2/100

Training Phase:
Training loss: 71.4486, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9825, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9698, Recall: 0.9625, F1_score: 0.9661
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 2.0933, Validation accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 2: 1.0000. Model saved.
Epoch 3/100

Training Phase:
                                                 Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.03it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 442/1600 [00:20<00:52, 21.87it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 665/1600 [00:30<00:42, 22.02it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 887/1600 [00:40<00:32, 21.85it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1104/1600 [00:50<00:22, 21.78it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1326/1600 [01:00<00:12, 21.92it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1548/1600 [01:10<00:02, 21.87it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 216/1600 [00:10<01:04, 21.58it/s]Training:  27%|â–ˆâTraining loss: 55.4639, Training accuracy: 0.9881
Macro F1-score: 0.9881
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9776, Recall: 0.9800, F1_score: 0.9788
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 5.9146, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 4/100

Training Phase:
–ˆâ–‹       | 436/1600 [00:20<00:53, 21.81it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 656/1600 [00:30<00:43, 21.78it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 874/1600 [00:40<00:33, 21.76it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1098/1600 [00:50<00:22, 21.96it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1322/1600 [01:00<00:12, 22.04it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1545/1600 [01:10<00:02, 21.98it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.02it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 442/1600 [00:20<00:53, 21.84it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1600 [00:30<00:42, 21.97it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 886/1600 [00:40<00:32, 21.92it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1105/1600 [00:50<00:22, 21.80it/s]Training:  83%|â–ˆâ–ˆâ–ˆâTraining loss: 73.2278, Training accuracy: 0.9838
Macro F1-score: 0.9838
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Neutral speech (in training): 
	Precision: 0.9726, Recall: 0.9775, F1_score: 0.9751
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 8.5333, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 5/100

Training Phase:
Training loss: 44.3205, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9777, Recall: 0.9850, F1_score: 0.9813
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1328/1600 [01:00<00:12, 21.96it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1551/1600 [01:10<00:02, 21.99it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.70it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 437/1600 [00:20<00:53, 21.78it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 659/1600 [00:30<00:42, 21.95it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 881/1600 [00:40<00:32, 21.88it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1104/1600 [00:50<00:22, 22.01it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1327/1600 [01:00<00:12, 21.79it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1550/1600 [01:10<00:02, 21.94it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                         Validation loss: 4.5966, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 6/100

Training Phase:
Training loss: 34.5952, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9875, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 4.4176, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 7/100

Training Phase:
          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:02, 21.95it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:52, 21.89it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 659/1600 [00:30<00:43, 21.83it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 880/1600 [00:40<00:32, 21.90it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1101/1600 [00:50<00:22, 21.88it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1320/1600 [01:00<00:12, 21.74it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1535/1600 [01:10<00:03, 21.51it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 212/1600 [00:10<01:05, 21.17it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 430/1600 [00:20<00:54, 21.52it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 652/1600 [00:30<00:43, 21.82it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–Training loss: 34.2936, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975

Eval Phase: 
Validation loss: 4.5272, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 8/100

Training Phase:
ˆâ–    | 874/1600 [00:40<00:33, 21.87it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1094/1600 [00:50<00:23, 21.89it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1314/1600 [01:00<00:13, 21.82it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1535/1600 [01:10<00:02, 21.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 216/1600 [00:10<01:04, 21.55it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 436/1600 [00:20<00:53, 21.77it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 657/1600 [00:30<00:43, 21.91it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 878/1600 [00:40<00:33, 21.74it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1100/1600 [00:50<00:22, 21.89it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1322/1600 [01:00<00:12, 21.90it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1542/1600 [01:10<00:02, 21.93it/s]           Training loss: 32.8563, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9949, Recall: 0.9850, F1_score: 0.9899
Model performance on Happy speech (in training): 
	Precision: 0.9802, Recall: 0.9925, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 12.0300, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 9/100

Training Phase:
Training loss: 13.2582, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 12.0845, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 10/100

Training Phase:
                                                  Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.07it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 442/1600 [00:20<00:53, 21.85it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 659/1600 [00:30<00:43, 21.75it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 882/1600 [00:40<00:32, 21.94it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1105/1600 [00:50<00:22, 21.94it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1325/1600 [01:00<00:12, 21.91it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1325/1600 [01:10<00:12, 21.91it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1544/1600 [01:10<00:02, 21.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]TrainTraining loss: 34.0683, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 13.7433, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 11/100

Training Phase:
ing:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 21.98it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 441/1600 [00:20<00:53, 21.80it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1600 [00:30<00:42, 22.02it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 887/1600 [00:40<00:32, 22.00it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1107/1600 [00:50<00:22, 21.98it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1327/1600 [01:00<00:12, 21.98it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1547/1600 [01:10<00:02, 21.87it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.73it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:52, 22.00it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 662/1600 [00:30<00:42, 21.98it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 882/1600 [00:40<00:32, 21.98it/s]Training:  69%|â–ˆâ–Training loss: 23.4155, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 18.8811, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 12/100

Training Phase:
Training loss: 14.0924, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1102/1600 [00:50<00:22, 21.95it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1321/1600 [01:00<00:12, 21.90it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1540/1600 [01:10<00:02, 21.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.01it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 442/1600 [00:20<00:52, 21.90it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 663/1600 [00:30<00:42, 21.97it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 884/1600 [00:40<00:32, 21.83it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1103/1600 [00:50<00:22, 21.85it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1324/1600 [01:00<00:12, 21.91it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1546/1600 [01:10<00:02, 21.98it/s]                                                             EvaluatingValidation loss: 10.2486, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 1.0000

Test Phase: 
:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:02, 94.25it/s]Testing:  10%|â–ˆ         | 20/200 [00:00<00:01, 96.97it/s]Testing:  15%|â–ˆâ–Œ        | 30/200 [00:00<00:01, 96.99it/s]Testing:  20%|â–ˆâ–ˆ        | 40/200 [00:00<00:01, 96.58it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:00<00:01, 96.99it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:00<00:01, 97.81it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:00<00:01, 97.13it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:00<00:01, 97.79it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:00<00:01, 96.18it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:01<00:01, 96.23it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:01<00:00, 94.97it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:01<00:00, 94.75it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:01<00:00, 96.31it/s]Testing:  71%|â–ˆâ–ˆâ–ˆâTest loss: 4.8920, Test accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in test): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

cn, all folds layer accuracy: ['0.9850', '0.9950', '0.9850', '0.9800', '0.9950']
cn, all emo precision: {'Angry': ['0.9804', '1.0000', '1.0000', '1.0000', '1.0000'], 'Happy': ['1.0000', '1.0000', '0.9608', '0.9800', '0.9804'], 'Neutral': ['0.9615', '1.0000', '0.9796', '0.9796', '1.0000'], 'Sad': ['1.0000', '0.9804', '1.0000', '0.9615', '1.0000']}
cn, all emo recall: {'Angry': ['1.0000', '1.0000', '1.0000', '0.9800', '0.9800'], 'Happy': ['0.9400', '1.0000', '0.9800', '0.9800', '1.0000'], 'Neutral': ['1.0000', '0.9800', '0.9600', '0.9600', '1.0000'], 'Sad': ['1.0000', '1.0000', '1.0000', '1.0000', '1.0000']}
cn, all emo f1score: {'Angry': ['0.9901', '1.0000', '1.0000', '0.9899', '0.9899'], 'Happy': ['0.9691', '1.0000', '0.9703', '0.9800', '0.9901'], 'Neutral': ['0.9804', '0.9899', '0.9697', '0.9697', '1.0000'], 'Sad': ['1.0000', '0.9901', '1.0000', '0.9804', '1.0000']}
–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [00:01<00:00, 96.32it/s]Testing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [00:01<00:00, 96.60it/s]Testing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:01<00:00, 98.36it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [00:01<00:00, 97.76it/s]Testing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [00:01<00:00, 95.99it/s]Testing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [00:02<00:00, 95.92it/s]                                                          