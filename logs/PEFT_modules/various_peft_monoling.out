Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
------------------NEXT SCRIPT: RUNNER_DE----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911927/matplotlib-ec3yyqg0 because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on de =======================

Load dataset: 
Loading de train data: fold_0...
Preprocess de fold_0 data for de model
Loading de eval data: fold_0...
Preprocess de fold_0 data for de model
Loading de test data: fold_0...
Preprocess de fold_0 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1272.9351, Training accuracy: 0.6656
Macro F1-score: 0.6715
Model performance on Angry speech (in training): 
	Precision: 0.8960, Recall: 0.4525, F1_score: 0.6013
Model performance on Happy speech (in training): 
	Precision: 0.4367, Recall: 0.7325, F1_score: 0.5472
Model performance on Neutral speech (in training): 
	Precision: 0.6916, Recall: 0.5775, F1_score: 0.6294
Model performance on Sad speech (in training): 
	Precision: 0.9160, Recall: 0.9000, F1_score: 0.9079

Eval Phase: 
Validation loss: 101.6288, Validation accuracy: 0.8450
Macro F1-score: 0.8421
Model performance on Angry speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Happy speech (in validation): 
	Precision: 0.9318, Recall: 0.8200, F1_score: 0.8723
Model performance on Neutral speech (in validation): 
	Precision: 0.7949, Recall: 0.6200, F1_score: 0.6966
Model performance on Sad speech (in validation): 
	Precision: 0.7246, Recall: 1.0000, F1_score: 0.8403
New best accuracy for layer 4 on epoch 1: 0.8450. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:53<23:55:56, 53.88s/it]Training:   7%|â–‹         | 106/1600 [01:03<11:15,  2.21it/s] Training:  14%|â–ˆâ–        | 223/1600 [01:13<05:13,  4.39it/s]Training:  22%|â–ˆâ–ˆâ–       | 351/1600 [01:23<03:12,  6.47it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 497/1600 [01:33<02:08,  8.59it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [01:43<01:30, 10.49it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 817/1600 [01:54<01:04, 12.13it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 997/1600 [02:04<00:43, 13.80it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1180/1600 [02:14<00:27, 15.09it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1377/1600 [02:24<00:13, 16.42it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1574/1600 [02:34<00:01, 17.29it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|         Training loss: 398.2102, Training accuracy: 0.9337
Macro F1-score: 0.9340
Model performance on Angry speech (in training): 
	Precision: 0.9564, Recall: 0.9325, F1_score: 0.9443
Model performance on Happy speech (in training): 
	Precision: 0.8810, Recall: 0.9250, F1_score: 0.9024
Model performance on Neutral speech (in training): 
	Precision: 0.9262, Recall: 0.9100, F1_score: 0.9180
Model performance on Sad speech (in training): 
	Precision: 0.9748, Recall: 0.9675, F1_score: 0.9711

Eval Phase: 
Validation loss: 20.3927, Validation accuracy: 0.9700
Macro F1-score: 0.9699
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Neutral speech (in validation): 
	Precision: 0.9412, Recall: 0.9600, F1_score: 0.9505
Model performance on Sad speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
New best accuracy for layer 4 on epoch 2: 0.9700. Model saved.
Epoch 3/100

Training Phase:
 | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.36it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 409/1600 [00:20<00:58, 20.44it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:30<00:47, 20.73it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 831/1600 [00:40<00:37, 20.52it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1037/1600 [00:50<00:27, 20.52it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1243/1600 [01:00<00:17, 20.27it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1447/1600 [01:10<00:07, 20.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.35it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:20<00:58, 20.27it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 613/1600 [00:30<00:48, 20.35it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 818/1600 [00:40<00:38, 20.19it/sTraining loss: 179.6773, Training accuracy: 0.9694
Macro F1-score: 0.9694
Model performance on Angry speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Happy speech (in training): 
	Precision: 0.9504, Recall: 0.9575, F1_score: 0.9539
Model performance on Neutral speech (in training): 
	Precision: 0.9697, Recall: 0.9600, F1_score: 0.9648
Model performance on Sad speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863

Eval Phase: 
Validation loss: 16.3926, Validation accuracy: 0.9700
Macro F1-score: 0.9699
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Sad speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Epoch 4/100

Training Phase:
Training loss: 119.0237, Training accuracy: 0.9788
Macro F1-score: 0.9788
Model performance on Angry speech (in training): 
	Precision: 0.9798, Recall: 0.9725, F1_score: 0.9762
Model performance on Happy speech (in training): 
	Precision: 0.9556, Recall: 0.9675, F1_score: 0.9615
Model performance on Neutral speech (in training): 
	Precision: 0.9849, Recall: 0.9775, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1020/1600 [00:50<00:28, 20.18it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1227/1600 [01:00<00:18, 20.33it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1434/1600 [01:10<00:08, 20.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 203/1600 [00:10<01:08, 20.27it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 406/1600 [00:20<00:58, 20.29it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 610/1600 [00:30<00:48, 20.31it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 814/1600 [00:40<00:38, 20.35it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1018/1600 [00:50<00:28, 20.24it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1222/1600 [01:00<00:18, 20.28it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1426/1600 [01:10<00:08, 20.30it/s]                                                             Validation loss: 17.8429, Validation accuracy: 0.9750
Macro F1-score: 0.9751
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
New best accuracy for layer 4 on epoch 4: 0.9750. Model saved.
Epoch 5/100

Training Phase:
Training loss: 121.0541, Training accuracy: 0.9769
Macro F1-score: 0.9769
Model performance on Angry speech (in training): 
	Precision: 0.9873, Recall: 0.9700, F1_score: 0.9786
Model performance on Happy speech (in training): 
	Precision: 0.9558, Recall: 0.9725, F1_score: 0.9641
Model performance on Neutral speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 18.0972, Validation accuracy: 0.9700
Macro F1-score: 0.9699
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Sad speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Epoch 6/100

Training Phase:
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.95it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:57, 20.36it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:47, 20.56it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 837/1600 [00:40<00:37, 20.59it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1600 [00:50<00:27, 20.49it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1600 [01:00<00:16, 20.59it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1464/1600 [01:10<00:06, 20.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.93it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 425/1600 [00:20<00:55, 21.20it/s]TraininTraining loss: 97.1997, Training accuracy: 0.9806
Macro F1-score: 0.9807
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9775, F1_score: 0.9849
Model performance on Happy speech (in training): 
	Precision: 0.9606, Recall: 0.9750, F1_score: 0.9677
Model performance on Neutral speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 15.5691, Validation accuracy: 0.9700
Macro F1-score: 0.9699
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Neutral speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 7/100

Training Phase:
g:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 639/1600 [00:30<00:45, 21.03it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 848/1600 [00:40<00:36, 20.89it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1600 [00:50<00:26, 20.76it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1261/1600 [01:00<00:16, 20.55it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1600 [01:10<00:06, 20.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 213/1600 [00:10<01:05, 21.23it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 426/1600 [00:20<00:55, 21.19it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 638/1600 [00:30<00:45, 21.14it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 849/1600 [00:40<00:35, 20.97it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1057/1600 [00:50<00:26, 20.48it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1254/1600 [01:01<00:17, 20.14it/s]TrainiTraining loss: 67.3532, Training accuracy: 0.9844
Macro F1-score: 0.9844
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Happy speech (in training): 
	Precision: 0.9798, Recall: 0.9725, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 32.5956, Validation accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Sad speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Epoch 8/100

Training Phase:
Training loss: 76.6924, Training accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in training): 
	Precision: 0.9898, Recall: 0.9750, F1_score: 0.9824
Model performance on Happy speech (in training): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 10.9879, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
New best accuracy for layer 4 on epoch 8: 0.9900. Model saved.
Epoch 9/100

Training Phase:
ng:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1460/1600 [01:11<00:06, 20.28it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 202/1600 [00:10<01:09, 20.12it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 404/1600 [00:20<00:59, 20.09it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 616/1600 [00:30<00:47, 20.57it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 828/1600 [00:40<00:37, 20.76it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1039/1600 [00:50<00:26, 20.79it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1251/1600 [01:00<00:16, 20.91it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1463/1600 [01:10<00:06, 20.94it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training loss: 61.3104, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 17.0429, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 10/100

Training Phase:
Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.83it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 418/1600 [00:20<00:57, 20.71it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 631/1600 [00:30<00:46, 20.95it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1600 [00:40<00:36, 20.97it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1600 [00:50<00:25, 21.00it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1267/1600 [01:00<00:15, 21.05it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1482/1600 [01:10<00:05, 21.17it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.88it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 418/1600 [00:20<00:56, 20.86it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600 [00:30<00:46, 20.99it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 842/1600 [00:40<00:36, 20.96it/s]Training:  66%|â–ˆâ–ˆTraining loss: 41.1999, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 23.8725, Validation accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Sad speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Epoch 11/100

Training Phase:
Training loss: 40.6717, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
â–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1054/1600 [00:50<00:25, 21.04it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1266/1600 [01:00<00:15, 21.06it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1478/1600 [01:10<00:05, 21.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.30it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:20<00:58, 20.30it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 612/1600 [00:30<00:48, 20.30it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 817/1600 [00:40<00:38, 20.35it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1022/1600 [00:50<00:28, 20.40it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1227/1600 [01:00<00:18, 20.36it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1434/1600 [01:10<00:08, 20.44it/s]                                                             Evaluating:   0%|    Validation loss: 17.0932, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 12/100

Training Phase:
Training loss: 7.3378, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 16.6616, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 13/100

Training Phase:
      | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.49it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 410/1600 [00:20<00:58, 20.46it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:30<00:47, 20.67it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 830/1600 [00:40<00:37, 20.70it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1038/1600 [00:50<00:27, 20.58it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1244/1600 [01:00<00:17, 20.56it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1456/1600 [01:10<00:06, 20.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 215/1600 [00:10<01:04, 21.48it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 430/1600 [00:20<00:55, 21.19it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ   Training loss: 63.9699, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9800, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9704, Recall: 0.9825, F1_score: 0.9764
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900

Eval Phase: 
Validation loss: 11.8657, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 14/100

Training Phase:
   | 641/1600 [00:30<00:45, 21.10it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 855/1600 [00:40<00:35, 21.21it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1069/1600 [00:50<00:25, 21.03it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1277/1600 [01:00<00:15, 20.94it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1490/1600 [01:10<00:05, 21.02it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 213/1600 [00:10<01:05, 21.29it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 426/1600 [00:20<00:55, 21.11it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 636/1600 [00:30<00:46, 20.91it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 843/1600 [00:40<00:36, 20.69it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1049/1600 [00:50<00:26, 20.64it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1255/1600 [01:00<00:16, 20.61it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–Training loss: 37.9270, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 9.7080, Validation accuracy: 0.9900
Macro F1-score: 0.9899
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 15/100

Training Phase:
Training loss: 34.3428, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 16.1784, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 16/100

Training Phase:
ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1461/1600 [01:10<00:06, 20.55it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.39it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 409/1600 [00:20<00:58, 20.44it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 614/1600 [00:30<00:48, 20.25it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 815/1600 [00:40<00:39, 19.95it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1020/1600 [00:50<00:28, 20.13it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1225/1600 [01:00<00:18, 20.19it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1429/1600 [01:11<00:08, 19.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž      Training loss: 54.7785, Training accuracy: 0.9888
Macro F1-score: 0.9887
Model performance on Angry speech (in training): 
	Precision: 0.9777, Recall: 0.9850, F1_score: 0.9813
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9775, F1_score: 0.9836
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938

Eval Phase: 
Validation loss: 14.8987, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Epoch 17/100

Training Phase:
  | 208/1600 [00:10<01:07, 20.77it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:57, 20.54it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 628/1600 [00:30<00:46, 20.80it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 840/1600 [00:40<00:36, 20.91it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1600 [00:50<00:26, 20.84it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1259/1600 [01:00<00:16, 20.71it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1464/1600 [01:10<00:06, 20.55it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 202/1600 [00:10<01:09, 20.12it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.73it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600 [00:30<00:45, 21.12it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1600 [00:40<00:35, 21.15it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1058/16Training loss: 34.3355, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 9.0572, Validation accuracy: 0.9900
Macro F1-score: 0.9899
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 18/100

Training Phase:
Training loss: 42.7518, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
00 [00:50<00:25, 21.02it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1266/1600 [01:00<00:16, 20.86it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1472/1600 [01:10<00:06, 20.77it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.68it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 415/1600 [00:20<00:57, 20.72it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 623/1600 [00:30<00:47, 20.68it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 830/1600 [00:40<00:37, 20.57it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1037/1600 [00:50<00:27, 20.58it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1244/1600 [01:00<00:17, 20.56it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1450/1600 [01:10<00:07, 20.52it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?Validation loss: 14.1331, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 19/100

Training Phase:
Training loss: 13.7420, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 8.6217, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 20/100

Training Phase:
it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.48it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 411/1600 [00:20<00:58, 20.48it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 616/1600 [00:30<00:48, 20.46it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 823/1600 [00:40<00:37, 20.54it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1030/1600 [00:50<00:27, 20.49it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1237/1600 [01:00<00:17, 20.55it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1444/1600 [01:10<00:07, 20.51it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.62it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 418/1600 [00:20<00:56, 20.87it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:4Training loss: 46.3854, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 16.5018, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 21/100

Training Phase:
6, 20.96it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 840/1600 [00:40<00:36, 21.01it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1600 [00:50<00:26, 20.76it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1259/1600 [01:00<00:16, 20.77it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1600 [01:10<00:06, 20.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.49it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 410/1600 [00:20<00:58, 20.31it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 614/1600 [00:30<00:48, 20.33it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 824/1600 [00:40<00:37, 20.56it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1034/1600 [00:50<00:27, 20.63it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1246/1600 [01:00<00:17, 20.82it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1458/1600Training loss: 14.3548, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 19.9935, Validation accuracy: 0.9750
Macro F1-score: 0.9748
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Happy speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 22/100

Training Phase:
Training loss: 26.0207, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 4.6187, Validation accuracy: 0.9900
Macro F1-score: 0.9899
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 23/100

Training Phase:
 [01:10<00:06, 20.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 208/1600 [00:10<01:07, 20.73it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:57, 20.62it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600 [00:30<00:46, 20.96it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1600 [00:40<00:36, 20.95it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1054/1600 [00:50<00:26, 20.81it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1260/1600 [01:01<00:16, 20.44it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:11<00:06, 20.49it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–ˆâ–        | 193/1600 [00:10<01:Training loss: 31.0621, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 11.3621, Validation accuracy: 0.9900
Macro F1-score: 0.9899
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 24/100

Training Phase:
13, 19.20it/s]Training:  25%|â–ˆâ–ˆâ–       | 393/1600 [00:20<01:01, 19.64it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 597/1600 [00:30<00:50, 19.94it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 802/1600 [00:40<00:39, 20.15it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1014/1600 [00:50<00:28, 20.50it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1226/1600 [01:00<00:18, 20.50it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1432/1600 [01:10<00:08, 20.48it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.57it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.55it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 618/1600 [00:30<00:47, 20.52it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 827/1600 [00:40<00:37, 20.65it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1036/1600 [00:50<00:27, 20.69it/s]Training loss: 36.0593, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 20.6907, Validation accuracy: 0.9700
Macro F1-score: 0.9699
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Neutral speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 25/100

Training Phase:
Training loss: 7.6854, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1246/1600 [01:00<00:17, 20.77it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1462/1600 [01:10<00:06, 21.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.92it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:56, 20.87it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 20.85it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 842/1600 [00:40<00:36, 21.00it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1600 [00:50<00:26, 20.90it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1269/1600 [01:00<00:15, 21.06it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1483/1600 [01:10<00:05, 21.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                   Validation loss: 7.0625, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 26/100

Training Phase:
Training loss: 15.8649, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 11.9227, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 27/100

Training Phase:
                                Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 208/1600 [00:10<01:06, 20.79it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:56, 20.99it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 632/1600 [00:30<00:45, 21.07it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1600 [00:40<00:35, 21.06it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1057/1600 [00:50<00:25, 21.12it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1270/1600 [01:00<00:15, 20.98it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1478/1600 [01:10<00:05, 20.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.42it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 415/1600 [00:20<00:57, 20.75it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 631/1600 [00:30<00:45, 21.12it/s]Training:Training loss: 39.8621, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 6.7515, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 27: 0.9950. Model saved.
Epoch 28/100

Training Phase:
  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 847/1600 [00:40<00:35, 21.18it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1060/1600 [00:50<00:25, 21.17it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1272/1600 [01:00<00:15, 20.91it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1482/1600 [01:10<00:05, 20.91it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 214/1600 [00:10<01:04, 21.34it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 429/1600 [00:20<00:54, 21.42it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 644/1600 [00:30<00:45, 20.87it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 847/1600 [00:40<00:36, 20.63it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1052/1600 [00:50<00:26, 20.57it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1268/1600 [01:00<00:15, 20.89it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1484/1600 [01:11<00:05, 20.84iTraining loss: 12.0149, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 4.6758, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 29/100

Training Phase:
Training loss: 0.7848, Training accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 5.2601, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 30/100

Training Phase:
t/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.81it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 418/1600 [00:20<00:56, 20.86it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 632/1600 [00:30<00:45, 21.07it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1600 [00:40<00:35, 21.13it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1600 [00:50<00:25, 21.05it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1272/1600 [01:00<00:15, 21.13it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1485/1600 [01:10<00:05, 21.10it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–ˆâ–        | 195/1600 [00:10<01:12, 19.40it/s]TrainiTraining loss: 0.1152, Training accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 7.8622, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 31/100

Training Phase:
ng:  25%|â–ˆâ–ˆâ–       | 398/1600 [00:20<01:00, 19.91it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 603/1600 [00:30<00:49, 20.14it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 808/1600 [00:40<00:39, 20.02it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1013/1600 [00:50<00:29, 20.17it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1225/1600 [01:00<00:18, 20.51it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1437/1600 [01:10<00:07, 20.72it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:06, 21.04it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 422/1600 [00:20<00:56, 20.79it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 633/1600 [00:30<00:46, 20.92it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 845/1600 [00:40<00:35, 21.02it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1600 [00:50<00:25, 21.15it/s]Training:  80%|â–ˆâ–Training loss: 37.7113, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 10.4464, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Sad speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Epoch 32/100

Training Phase:
Training loss: 10.4250, Training accuracy: 0.9988
Macro F1-score: 0.9988
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1273/1600 [01:00<00:15, 21.17it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1485/1600 [01:10<00:05, 21.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 208/1600 [00:10<01:06, 20.80it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 423/1600 [00:20<00:55, 21.20it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 638/1600 [00:30<00:45, 20.97it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1600 [00:40<00:36, 20.77it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1056/1600 [00:50<00:26, 20.84it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1270/1600 [01:00<00:15, 21.00it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1484/1600 [01:10<00:05, 20.96it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                        Validation loss: 13.2047, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9950

Test Phase: 
           Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:12, 16.11it/s]Testing:   2%|â–Ž         | 5/200 [00:00<00:10, 18.42it/s]Testing:   4%|â–Ž         | 7/200 [00:00<00:10, 18.63it/s]Testing:   6%|â–Œ         | 11/200 [00:00<00:08, 23.34it/s]Testing:   7%|â–‹         | 14/200 [00:00<00:08, 21.19it/s]Testing:   8%|â–Š         | 17/200 [00:00<00:07, 23.21it/s]Testing:  10%|â–ˆ         | 20/200 [00:00<00:07, 23.99it/s]Testing:  12%|â–ˆâ–Ž        | 25/200 [00:01<00:05, 29.47it/s]Testing:  14%|â–ˆâ–        | 29/200 [00:01<00:05, 30.60it/s]Testing:  16%|â–ˆâ–‹        | 33/200 [00:01<00:06, 27.76it/s]Testing:  18%|â–ˆâ–Š        | 36/200 [00:01<00:06, 27.23it/s]Testing:  20%|â–ˆâ–ˆ        | 41/200 [00:01<00:04, 32.33it/s]Testing:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:01<00:04, 32.98it/s]Testing:  24%|â–ˆâ–ˆâ–       | 49/200 [00:01<00:04, 33.28it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:01<00:03, 37.59it/s]Testing:  30%|â–ˆâ–ˆâ–‰       | 59/200 [00:02<00:03, 36.49it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [00:02<00:03, 42.39it/s]Testing:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [00:02<00:02, 51.54it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:02<00:02, 45.77it/s]Testing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [00:02<00:02, 42.54it/s]Testing:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:02<00:02, 44.07it/s]Testing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [00:02<00:02, 47.54it/s]Testing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [00:02<00:01, 50.37it/s]Testing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [00:02<00:01, 50.32it/s]Testing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [00:03<00:01, 49.27it/s]Testing:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:03<00:01, 53.92it/s]Testing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:03<00:01, 52.62it/s]Testing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [00:03<00:01, 47.48it/s]Testing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [00:03<00:01, 47.34it/s]Testing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [00:03<00:00, 59.59it/s]Testing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâTest loss: 23.8486, Test accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in test): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in test): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in test): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901

======================= This is fold_1 on de =======================

Load dataset: 
Loading de train data: fold_1...
Preprocess de fold_1 data for de model
Loading de eval data: fold_1...
Preprocess de fold_1 data for de model
Loading de test data: fold_1...
Preprocess de fold_1 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:03<00:00, 64.69it/s]Testing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:03<00:00, 67.06it/s]Testing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:04<00:00, 69.09it/s]Testing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [00:04<00:00, 71.06it/s]Testing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:04<00:00, 70.67it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:04<00:00, 71.69it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   9%|â–‰         | 140/1600 [00:10<01:44, 13.98it/s]Training:  19%|â–ˆâ–‰        | 306/1600 [00:20<01:23, 15.49it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 487/1600 [00:30<01:06, 16.66it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 677/1600 [00:40<00:52, 17.58it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 867/1600 [00:50<00:40, 17.93it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1060/1600 [01:00<00:29, 18.38it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1600 [01:10Training loss: 435.0171, Training accuracy: 0.8975
Macro F1-score: 0.8973
Model performance on Angry speech (in training): 
	Precision: 0.9030, Recall: 0.9075, F1_score: 0.9052
Model performance on Happy speech (in training): 
	Precision: 0.8641, Recall: 0.8425, F1_score: 0.8532
Model performance on Neutral speech (in training): 
	Precision: 0.8812, Recall: 0.8900, F1_score: 0.8856
Model performance on Sad speech (in training): 
	Precision: 0.9406, Recall: 0.9500, F1_score: 0.9453

Eval Phase: 
Validation loss: 14.5723, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 1: 0.9800. Model saved.
Epoch 2/100

Training Phase:
Training loss: 139.9090, Training accuracy: 0.9681
Macro F1-score: 0.9682
Model performance on Angry speech (in training): 
	Precision: 0.9721, Recall: 0.9575, F1_score: 0.9647
Model performance on Happy speech (in training): 
	Precision: 0.9412, Recall: 0.9600, F1_score: 0.9505
Model performance on Neutral speech (in training): 
	Precision: 0.9772, Recall: 0.9650, F1_score: 0.9711
Model performance on Sad speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863

Eval Phase: 
Validation loss: 14.3615, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 3/100

Training Phase:
<00:18, 18.49it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1448/1600 [01:20<00:08, 18.80it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 202/1600 [00:10<01:09, 20.14it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 404/1600 [00:20<00:59, 20.13it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 610/1600 [00:30<00:48, 20.34it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 822/1600 [00:40<00:37, 20.66it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1035/1600 [00:50<00:27, 20.89it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1248/1600 [01:00<00:16, 20.87it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1457/1600 [01:10<00:06, 20.75it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1Training loss: 90.0088, Training accuracy: 0.9819
Macro F1-score: 0.9819
Model performance on Angry speech (in training): 
	Precision: 0.9797, Recall: 0.9675, F1_score: 0.9736
Model performance on Happy speech (in training): 
	Precision: 0.9627, Recall: 0.9675, F1_score: 0.9651
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 44.9446, Validation accuracy: 0.9450
Macro F1-score: 0.9454
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Neutral speech (in validation): 
	Precision: 0.8679, Recall: 0.9200, F1_score: 0.8932
Model performance on Sad speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Epoch 4/100

Training Phase:
600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.34it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.57it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:30<00:47, 20.63it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 828/1600 [00:40<00:37, 20.65it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1036/1600 [00:50<00:27, 20.70it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1244/1600 [01:00<00:17, 20.69it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1456/1600 [01:10<00:06, 20.83it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.98it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:56, 20.95it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 632/1600 [00:30<00:46, 21.02it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1600 [00:40<00:35, 21.05it/s]TrTraining loss: 96.7212, Training accuracy: 0.9812
Macro F1-score: 0.9812
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9748, Recall: 0.9675, F1_score: 0.9711
Model performance on Neutral speech (in training): 
	Precision: 0.9729, Recall: 0.9875, F1_score: 0.9801
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925

Eval Phase: 
Validation loss: 21.9013, Validation accuracy: 0.9650
Macro F1-score: 0.9651
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9216, Recall: 0.9400, F1_score: 0.9307
Model performance on Sad speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Epoch 5/100

Training Phase:
Training loss: 83.9740, Training accuracy: 0.9806
Macro F1-score: 0.9806
Model performance on Angry speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9723, Recall: 0.9650, F1_score: 0.9686
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925

Eval Phase: 
aining:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1600 [00:50<00:26, 20.95it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1267/1600 [01:00<00:15, 21.00it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1479/1600 [01:11<00:05, 20.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.67it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:58, 20.29it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 615/1600 [00:30<00:49, 20.03it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 825/1600 [00:40<00:38, 20.38it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1035/1600 [00:50<00:27, 20.51it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1243/1600 [01:01<00:17, 20.28it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1446/1600 [01:11<00:07, 20.26it/s]                                                             Validation loss: 13.6911, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 6/100

Training Phase:
Training loss: 50.8643, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 33.4961, Validation accuracy: 0.9750
Macro F1-score: 0.9751
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 7/100

Training Phase:
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.66it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.67it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 621/1600 [00:30<00:48, 20.35it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 824/1600 [00:40<00:38, 20.32it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1030/1600 [00:50<00:27, 20.41it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1236/1600 [01:00<00:17, 20.46it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1442/1600 [01:10<00:07, 20.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.66it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.63it/s]Training:Training loss: 58.2064, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888

Eval Phase: 
Validation loss: 26.4725, Validation accuracy: 0.9750
Macro F1-score: 0.9751
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 8/100

Training Phase:
  39%|â–ˆâ–ˆâ–ˆâ–‰      | 621/1600 [00:30<00:47, 20.50it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 827/1600 [00:40<00:37, 20.49it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1036/1600 [00:50<00:27, 20.63it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1245/1600 [01:00<00:17, 20.65it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1452/1600 [01:10<00:07, 20.63it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.41it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 411/1600 [00:20<00:58, 20.49it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:30<00:47, 20.67it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:36, 20.99it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1600 [00:50<00:25, 21.20it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1267/1600 [01:00<00:15, 21.33it/s]Training: Training loss: 30.9337, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 69.1772, Validation accuracy: 0.9450
Macro F1-score: 0.9455
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Happy speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Model performance on Neutral speech (in validation): 
	Precision: 0.8571, Recall: 0.9600, F1_score: 0.9057
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 9/100

Training Phase:
Training loss: 37.9462, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 28.4455, Validation accuracy: 0.9700
Macro F1-score: 0.9703
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9074, Recall: 0.9800, F1_score: 0.9423
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 10/100

Training Phase:
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1483/1600 [01:10<00:05, 21.33it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 213/1600 [00:10<01:05, 21.22it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 426/1600 [00:20<00:56, 20.93it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 634/1600 [00:30<00:46, 20.74it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 845/1600 [00:40<00:36, 20.84it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1056/1600 [00:50<00:26, 20.82it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1264/1600 [01:00<00:16, 20.63it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1472/1600 [01:10<00:06, 20.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]TraiTraining loss: 36.3806, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950

Eval Phase: 
Validation loss: 31.5603, Validation accuracy: 0.9750
Macro F1-score: 0.9751
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 11/100

Training Phase:
ning:  13%|â–ˆâ–Ž        | 203/1600 [00:10<01:08, 20.26it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 406/1600 [00:20<00:59, 20.09it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 614/1600 [00:30<00:48, 20.40it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 822/1600 [00:40<00:38, 20.29it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1029/1600 [00:50<00:27, 20.42it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1236/1600 [01:00<00:17, 20.48it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1444/1600 [01:10<00:07, 20.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 203/1600 [00:10<01:08, 20.27it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 409/1600 [00:20<00:58, 20.42it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 616/1600 [00:30<00:47, 20.51it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 823/1600 [00:40<00:38, 20.40it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆTraining loss: 30.5143, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 15.3093, Validation accuracy: 0.9800
Macro F1-score: 0.9801
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 12/100

Training Phase:
Training loss: 36.5559, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
â–ˆâ–ˆâ–   | 1026/1600 [00:50<00:28, 20.31it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1229/1600 [01:00<00:18, 20.31it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1435/1600 [01:10<00:08, 20.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.31it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 409/1600 [00:20<00:58, 20.42it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 614/1600 [00:30<00:48, 20.26it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 815/1600 [00:40<00:38, 20.14it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1022/1600 [00:50<00:28, 20.31it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1229/1600 [01:00<00:18, 20.36it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1441/1600 [01:10<00:07, 20.62it/s]                                                             Evaluating:   0%|          | Validation loss: 26.2055, Validation accuracy: 0.9750
Macro F1-score: 0.9751
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 13/100

Training Phase:
Training loss: 22.9019, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 24.3071, Validation accuracy: 0.9750
Macro F1-score: 0.9751
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Neutral speech (in validation): 
	Precision: 0.9412, Recall: 0.9600, F1_score: 0.9505
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 14/100

Training Phase:
0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–ˆâ–        | 195/1600 [00:10<01:12, 19.47it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 400/1600 [00:20<00:59, 20.04it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 607/1600 [00:30<00:48, 20.32it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 814/1600 [00:40<00:38, 20.24it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1019/1600 [00:50<00:28, 20.31it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1224/1600 [01:00<00:18, 20.28it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1427/1600 [01:10<00:08, 20.15it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.81it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 419/1600 [00:20<00:56, 20.90it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1Training loss: 20.7180, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 29.6048, Validation accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 0.9400, Recall: 0.9400, F1_score: 0.9400
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 15/100

Training Phase:
600 [00:30<00:46, 20.80it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1600 [00:40<00:36, 20.71it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1045/1600 [00:50<00:26, 20.75it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1254/1600 [01:00<00:16, 20.53it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1462/1600 [01:10<00:06, 20.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 212/1600 [00:10<01:05, 21.11it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 424/1600 [00:20<00:56, 20.81it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 630/1600 [00:30<00:47, 20.63it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:37, 20.54it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1041/1600 [00:50<00:27, 20.54it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1600 [01:00<00:17, 20.39it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆTraining loss: 42.7381, Training accuracy: 0.9938
Macro F1-score: 0.9938
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 19.2371, Validation accuracy: 0.9800
Macro F1-score: 0.9801
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9800

Test Phase: 
â–ˆ | 1451/1600 [01:10<00:07, 20.39it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:14, 13.49it/s]Testing:   2%|â–         | 4/200 [00:00<00:13, 14.81it/s]Testing:   3%|â–Ž         | 6/200 [00:00<00:12, 15.66it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:08, 21.33it/s]Testing:   7%|â–‹         | 14/200 [00:00<00:07, 23.83it/s]Testing:   8%|â–Š         | 17/200 [00:00<00:07, 22.95it/s]Testing:  10%|â–ˆ         | 21/200 [00:00<00:06, 25.82it/s]Testing:  12%|â–ˆâ–        | 24/200 [00:01<00:06, 25.40it/s]Testing:  14%|â–ˆâ–        | 28/200 [00:01<00:05, 28.94it/s]Testing:  16%|â–ˆâ–Œ        | 31/200 [00:01<00:06, 27.48it/s]Testing:  18%|â–ˆâ–Š        | 36/200 [00:01<00:05, 28.71it/s]Testing:  20%|â–ˆâ–‰        | 39/200 [00:01<00:06, 24.82it/s]Testing:  21%|â–ˆâ–ˆ        | 42/200 [00:01<00:06, 24.51it/s]Testing:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:01<00:05, 27.12it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:01<00:05, 29.43it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:02<00:04, 31.23it/s]Testing:  30%|â–ˆâ–ˆâ–‰       | 59/200 [00:02<00:04, 32.60it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [00:02<00:03, 38.93it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:02<00:03, 36.90it/s]Testing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [00:02<00:03, 41.54it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:02<00:02, 45.54it/s]Testing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:02<00:02, 43.60it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:02<00:02, 43.95it/s]Testing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:03<00:02, 48.74it/s]Testing:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [00:03<00:01, 55.85it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:03<00:01, 58.18it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:03<00:01, 62.29it/s]Testing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [00:03<00:01, 68.0Test loss: 9.4353, Test accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in test): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in test): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in test): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Sad speech (in test): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709

======================= This is fold_2 on de =======================

Load dataset: 
Loading de train data: fold_2...
Preprocess de fold_2 data for de model
Loading de eval data: fold_2...
Preprocess de fold_2 data for de model
Loading de test data: fold_2...
Preprocess de fold_2 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
1it/s]Testing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:03<00:00, 69.93it/s]Testing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [00:03<00:00, 70.89it/s]Testing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:03<00:00, 69.84it/s]Testing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [00:03<00:00, 67.69it/s]Testing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [00:04<00:00, 65.50it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:04<00:00, 73.99it/s]Testing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [00:04<00:00, 75.62it/s]Testing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [00:04<00:00, 74.71it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   8%|â–Š         | 122/1600 [00:10<02:02, 12.08it/s]Training:  16%|â–ˆâ–Œ        | 253/1600 [00:20<01:46, 12.68it/s]Training:  24%|â–ˆâ–ˆâ–       | 387/1600 [00:30<01:33, 13.00it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 552/1600 [00:40<01:12, 14.37it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ  Training loss: 336.6605, Training accuracy: 0.9350
Macro F1-score: 0.9350
Model performance on Angry speech (in training): 
	Precision: 0.9380, Recall: 0.9450, F1_score: 0.9415
Model performance on Happy speech (in training): 
	Precision: 0.9109, Recall: 0.8950, F1_score: 0.9029
Model performance on Neutral speech (in training): 
	Precision: 0.9126, Recall: 0.9400, F1_score: 0.9261
Model performance on Sad speech (in training): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697

Eval Phase: 
Validation loss: 50.2178, Validation accuracy: 0.9200
Macro F1-score: 0.9161
Model performance on Angry speech (in validation): 
	Precision: 0.8909, Recall: 0.9800, F1_score: 0.9333
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.7000, F1_score: 0.8235
Model performance on Neutral speech (in validation): 
	Precision: 0.8475, Recall: 1.0000, F1_score: 0.9174
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
New best accuracy for layer 4 on epoch 1: 0.9200. Model saved.
Epoch 2/100

Training Phase:
   | 721/1600 [00:50<00:57, 15.27it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 901/1600 [01:00<00:43, 16.19it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1082/1600 [01:10<00:30, 16.80it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1273/1600 [01:20<00:18, 17.53it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1469/1600 [01:30<00:07, 18.16it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–ˆâ–        | 193/1600 [00:10<01:12, 19.30it/s]Training:  25%|â–ˆâ–ˆâ–       | 395/1600 [00:20<01:00, 19.78it/s]Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 597/1600 [00:30<00:50, 19.93it/s]Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 800/1600 [00:40<00:39, 20.05it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1006/1600 [00:50<00:29, 20.23it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1212/1600 [01:00<00:19, 20.20it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâTraining loss: 118.7068, Training accuracy: 0.9769
Macro F1-score: 0.9768
Model performance on Angry speech (in training): 
	Precision: 0.9724, Recall: 0.9675, F1_score: 0.9699
Model performance on Happy speech (in training): 
	Precision: 0.9597, Recall: 0.9525, F1_score: 0.9561
Model performance on Neutral speech (in training): 
	Precision: 0.9778, Recall: 0.9900, F1_score: 0.9839
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 18.0421, Validation accuracy: 0.9750
Macro F1-score: 0.9751
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
New best accuracy for layer 4 on epoch 2: 0.9750. Model saved.
Epoch 3/100

Training Phase:
Training loss: 108.5368, Training accuracy: 0.9781
Macro F1-score: 0.9781
Model performance on Angry speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750
Model performance on Happy speech (in training): 
	Precision: 0.9651, Recall: 0.9675, F1_score: 0.9663
Model performance on Neutral speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 33.0043, Validation accuracy: 0.9700
Macro F1-score: 0.9699
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 4/100

Training Phase:
–ˆâ–ˆâ–ˆâ–Š | 1416/1600 [01:10<00:09, 20.26it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 203/1600 [00:10<01:09, 20.23it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 409/1600 [00:20<00:58, 20.43it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 615/1600 [00:30<00:48, 20.19it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 821/1600 [00:40<00:38, 20.33it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1027/1600 [00:50<00:28, 20.16it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1235/1600 [01:00<00:17, 20.37it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1443/1600 [01:11<00:07, 20.32it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        Training loss: 61.9022, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9775, Recall: 0.9775, F1_score: 0.9775
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 15.9148, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 5/100

Training Phase:
| 204/1600 [00:10<01:08, 20.38it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 409/1600 [00:20<00:58, 20.41it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 614/1600 [00:30<00:48, 20.42it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 819/1600 [00:40<00:39, 19.77it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1022/1600 [00:50<00:29, 19.92it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1230/1600 [01:01<00:18, 20.21it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1438/1600 [01:11<00:08, 19.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 202/1600 [00:10<01:09, 20.05it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 406/1600 [00:20<00:59, 20.22it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 610/1600 [00:30<00:48, 20.21it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 813/1600 [00:40<00:39, 20.16it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1023/1600 [00:5Training loss: 59.4988, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9727, Recall: 0.9800, F1_score: 0.9763
Model performance on Neutral speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950

Eval Phase: 
Validation loss: 22.0896, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 6/100

Training Phase:
Training loss: 42.7994, Training accuracy: 0.9888
Macro F1-score: 0.9887
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862

Eval Phase: 
0<00:28, 20.45it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1233/1600 [01:00<00:17, 20.47it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1440/1600 [01:10<00:07, 20.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.44it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.57it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 619/1600 [00:30<00:47, 20.59it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 826/1600 [00:40<00:37, 20.59it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1032/1600 [00:50<00:27, 20.59it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1243/1600 [01:00<00:17, 20.75it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1454/1600 [01:10<00:07, 20.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]    Validation loss: 21.4266, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 6: 0.9800. Model saved.
Epoch 7/100

Training Phase:
Training loss: 37.1835, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 21.0832, Validation accuracy: 0.9850
Macro F1-score: 0.9851
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 7: 0.9850. Model saved.
Epoch 8/100

Training Phase:
                                               Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.58it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 415/1600 [00:20<00:57, 20.73it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1600 [00:30<00:47, 20.67it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 831/1600 [00:40<00:37, 20.65it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1038/1600 [00:50<00:27, 20.67it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1245/1600 [01:00<00:17, 20.58it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1450/1600 [01:10<00:07, 20.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.84it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 418/1600 [00:20<00:56, 20.82it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1600 [00:30<00:46, 20.77itTraining loss: 52.5343, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 25.5643, Validation accuracy: 0.9700
Macro F1-score: 0.9699
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Neutral speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 9/100

Training Phase:
/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 837/1600 [00:40<00:36, 20.86it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1047/1600 [00:50<00:26, 20.65it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1600 [01:00<00:16, 20.59it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1457/1600 [01:10<00:06, 20.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.60it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.51it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 619/1600 [00:30<00:47, 20.56it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 826/1600 [00:40<00:37, 20.46it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1030/1600 [00:50<00:27, 20.42it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1236/1600 [01:00<00:17, 20.45it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1442/1600 [01:10<00:0Training loss: 16.7451, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 14.1873, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 10/100

Training Phase:
Training loss: 24.7030, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 17.9241, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 11/100

Training Phase:
7, 20.38it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.34it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.69it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1600 [00:30<00:47, 20.68it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:36, 20.84it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [00:50<00:26, 20.72it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1600 [01:00<00:16, 20.66it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1458/1600 [01:10<00:06, 20.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.48it/s]Training loss: 32.7116, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 18.4352, Validation accuracy: 0.9850
Macro F1-score: 0.9851
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 12/100

Training Phase:
Training:  26%|â–ˆâ–ˆâ–Œ       | 410/1600 [00:20<00:58, 20.37it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 615/1600 [00:30<00:48, 20.38it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 824/1600 [00:40<00:37, 20.56it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1033/1600 [00:50<00:27, 20.66it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1242/1600 [01:00<00:17, 20.35it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1443/1600 [01:10<00:07, 20.25it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–ˆâ–        | 199/1600 [00:10<01:10, 19.88it/s]Training:  25%|â–ˆâ–ˆâ–       | 398/1600 [00:20<01:00, 19.80it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 609/1600 [00:30<00:48, 20.36it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 820/1600 [00:40<00:37, 20.55it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1029/1600 [00:50<00:27, 20.66it/s]Training:  Training loss: 36.9820, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 26.5998, Validation accuracy: 0.9800
Macro F1-score: 0.9801
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 13/100

Training Phase:
Training loss: 4.1843, Training accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1238/1600 [01:00<00:17, 20.70it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1446/1600 [01:10<00:07, 20.58it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.52it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 415/1600 [00:20<00:57, 20.68it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1600 [00:30<00:47, 20.71it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1600 [00:40<00:37, 20.70it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [00:50<00:26, 20.80it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1600 [01:00<00:16, 20.80it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1463/1600 [01:10<00:06, 20.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                 Validation loss: 29.4702, Validation accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 14/100

Training Phase:
Training loss: 26.7034, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 16.4290, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 15/100

Training Phase:
                  Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 20.97it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:57, 20.68it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1600 [00:30<00:47, 20.67it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1600 [00:40<00:36, 20.73it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [00:50<00:26, 20.80it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1256/1600 [01:00<00:16, 20.87it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1466/1600 [01:10<00:06, 20.81it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:06, 21.01it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 422/1600 [00:20<00:56, 20.99it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 632/1600 [00:30<00:46, 20.90it/s]Training:  52%|â–ˆâ–ˆâ–Training loss: 21.8591, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 29.6581, Validation accuracy: 0.9800
Macro F1-score: 0.9801
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 16/100

Training Phase:
ˆâ–ˆâ–ˆâ–Ž    | 840/1600 [00:40<00:36, 20.71it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1600 [00:50<00:26, 20.82it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1262/1600 [01:00<00:16, 20.68it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1468/1600 [01:10<00:06, 20.62it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.54it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.56it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 618/1600 [00:30<00:47, 20.52it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 828/1600 [00:40<00:37, 20.70it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1038/1600 [00:50<00:27, 20.75it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1600 [01:00<00:16, 20.79it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1456/1600 [01:10<00:06, 20.60it/s]           Training loss: 22.0820, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 40.9743, Validation accuracy: 0.9750
Macro F1-score: 0.9751
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 17/100

Training Phase:
Training loss: 24.2861, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 17.3763, Validation accuracy: 0.9800
Macro F1-score: 0.9801
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 18/100

Training Phase:
                                                  Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.52it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 412/1600 [00:20<00:57, 20.56it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:30<00:47, 20.66it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 830/1600 [00:40<00:37, 20.77it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [00:50<00:27, 20.58it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1251/1600 [01:00<00:16, 20.73it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1462/1600 [01:10<00:06, 20.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:06, 21.04it/s]Training:  26%|â–ˆâ–ˆâTraining loss: 8.5372, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 19.4129, Validation accuracy: 0.9850
Macro F1-score: 0.9851
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 19/100

Training Phase:
–‹       | 422/1600 [00:20<00:56, 20.83it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 20.73it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 838/1600 [00:40<00:36, 20.77it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1600 [00:50<00:26, 20.94it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1264/1600 [01:00<00:16, 20.94it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1474/1600 [01:10<00:06, 20.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.70it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 415/1600 [00:20<00:57, 20.73it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 623/1600 [00:30<00:47, 20.43it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 824/1600 [00:40<00:38, 20.22it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1023/1600 [00:50<00:28, 20.06it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Training loss: 0.2947, Training accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 23.6289, Validation accuracy: 0.9850
Macro F1-score: 0.9851
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9850

Test Phase: 
ˆâ–ˆâ–‹  | 1222/1600 [01:00<00:18, 19.99it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1421/1600 [01:10<00:09, 19.72it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:11, 17.96it/s]Testing:   3%|â–Ž         | 6/200 [00:00<00:07, 26.64it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:06, 29.28it/s]Testing:   7%|â–‹         | 14/200 [00:00<00:06, 30.15it/s]Testing:   8%|â–Š         | 17/200 [00:00<00:07, 25.50it/s]Testing:  10%|â–ˆ         | 20/200 [00:00<00:06, 25.91it/s]Testing:  12%|â–ˆâ–        | 23/200 [00:00<00:06, 26.09it/s]Testing:  14%|â–ˆâ–Ž        | 27/200 [00:00<00:06, 28.44it/s]Testing:  16%|â–ˆâ–Œ        | 32/200 [00:01<00:05, 31.04it/s]Testing:  18%|â–ˆâ–Š        | 36/200 [00:01<00:05, 30.18it/s]Testing:  20%|â–ˆâ–ˆ        | 40/200 [00:01<00:05, 29.61it/s]Testing:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:01<00:04, 33.92it/s]Testing:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:01<00:03, 39.73it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:01<00:03, 42.07it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:01<00:02, 48.74it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:01<00:02, 48.56it/s]Testing:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [00:02<00:03, 39.92it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [00:02<00:03, 36.37it/s]Testing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [00:02<00:02, 41.14it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [00:02<00:02, 47.67it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [00:02<00:01, 52.93it/s]Testing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [00:02<00:01, 54.42it/s]Testing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:02<00:01, 57.62it/s]Testing:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:02<00:01, 62.96it/s]Testing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [00:03<00:01, 62.17it/s]Testing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [00:03<00:01, 61.87it/s]Testing:  72%|â–ˆâ–Test loss: 19.7950, Test accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in test): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Sad speech (in test): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615

======================= This is fold_3 on de =======================

Load dataset: 
Loading de train data: fold_3...
Preprocess de fold_3 data for de model
Loading de eval data: fold_3...
Preprocess de fold_3 data for de model
Loading de test data: fold_3...
Preprocess de fold_3 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:03<00:00, 65.45it/s]Testing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [00:03<00:00, 70.06it/s]Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:03<00:00, 69.55it/s]Testing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [00:03<00:00, 74.14it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:03<00:00, 75.00it/s]Testing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:03<00:00, 77.95it/s]Testing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [00:03<00:00, 78.45it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   8%|â–Š         | 134/1600 [00:10<01:51, 13.19it/s]Training:  19%|â–ˆâ–‰        | 300/1600 [00:20<01:25, 15.16it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 470/1600 [00:30<01:10, 15.97it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [00:40<00:56, 16.85it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1600 [00:50<00:42, 17.63it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1038/1600 [01:00<Training loss: 276.3112, Training accuracy: 0.9475
Macro F1-score: 0.9475
Model performance on Angry speech (in training): 
	Precision: 0.9497, Recall: 0.9450, F1_score: 0.9474
Model performance on Happy speech (in training): 
	Precision: 0.9260, Recall: 0.9075, F1_score: 0.9167
Model performance on Neutral speech (in training): 
	Precision: 0.9229, Recall: 0.9575, F1_score: 0.9399
Model performance on Sad speech (in training): 
	Precision: 0.9924, Recall: 0.9800, F1_score: 0.9862

Eval Phase: 
Validation loss: 8.6784, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 1: 0.9950. Model saved.
Epoch 2/100

Training Phase:
Training loss: 106.4051, Training accuracy: 0.9756
Macro F1-score: 0.9756
Model performance on Angry speech (in training): 
	Precision: 0.9724, Recall: 0.9700, F1_score: 0.9712
Model performance on Happy speech (in training): 
	Precision: 0.9595, Recall: 0.9475, F1_score: 0.9535
Model performance on Neutral speech (in training): 
	Precision: 0.9729, Recall: 0.9875, F1_score: 0.9801
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
00:30, 18.21it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1236/1600 [01:10<00:19, 18.71it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1434/1600 [01:20<00:08, 19.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 208/1600 [00:10<01:07, 20.74it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:57, 20.72it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1600 [00:30<00:47, 20.70it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1600 [00:40<00:37, 20.73it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [00:50<00:26, 20.82it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1600 [01:00<00:16, 20.71it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1461/1600 [01:10<00:06, 20.77it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]    Validation loss: 11.7321, Validation accuracy: 0.9650
Macro F1-score: 0.9652
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 3/100

Training Phase:
Training loss: 66.8650, Training accuracy: 0.9875
Macro F1-score: 0.9875
Model performance on Angry speech (in training): 
	Precision: 0.9849, Recall: 0.9800, F1_score: 0.9825
Model performance on Happy speech (in training): 
	Precision: 0.9775, Recall: 0.9775, F1_score: 0.9775
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 3.9443, Validation accuracy: 0.9850
Macro F1-score: 0.9849
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 4/100

Training Phase:
                                               Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 209/1600 [00:10<01:06, 20.89it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 418/1600 [00:20<00:56, 20.86it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 627/1600 [00:30<00:46, 20.85it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1600 [00:40<00:36, 20.65it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1043/1600 [00:50<00:26, 20.66it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1600 [01:00<00:16, 20.72it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1461/1600 [01:10<00:06, 20.65it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 210/1600 [00:10<01:06, 21.00it/s]Training:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:20<00:56, 20.85it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 629/1600 [00:30<00:46, 20.83Training loss: 49.0283, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913

Eval Phase: 
Validation loss: 6.3798, Validation accuracy: 0.9850
Macro F1-score: 0.9849
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9796, Recall: 0.9600, F1_score: 0.9697
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 5/100

Training Phase:
it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 838/1600 [00:40<00:36, 20.77it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1047/1600 [00:50<00:26, 20.81it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1257/1600 [01:00<00:16, 20.86it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1600 [01:10<00:06, 20.87it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 208/1600 [00:10<01:06, 20.80it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:57, 20.74it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 626/1600 [00:30<00:46, 20.82it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 839/1600 [00:40<00:36, 20.98it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1052/1600 [00:50<00:26, 20.88it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1262/1600 [01:00<00:16, 20.91it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1472/1600 [01:1Training loss: 47.7539, Training accuracy: 0.9888
Macro F1-score: 0.9887
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 8.5662, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 6/100

Training Phase:
Training loss: 38.5688, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 25.6442, Validation accuracy: 0.9650
Macro F1-score: 0.9655
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Happy speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 7/100

Training Phase:
0<00:06, 20.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 215/1600 [00:10<01:04, 21.42it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 430/1600 [00:20<00:55, 20.96it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 638/1600 [00:30<00:46, 20.86it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1600 [00:40<00:36, 20.75it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1056/1600 [00:50<00:26, 20.83it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1274/1600 [01:00<00:15, 21.15it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1274/1600 [01:10<00:15, 21.15it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1490/1600 [01:10<00:05, 21.22it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/Training loss: 32.1332, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 5.6462, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 8/100

Training Phase:
1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.45it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 413/1600 [00:20<00:57, 20.61it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 621/1600 [00:30<00:48, 20.38it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 825/1600 [00:40<00:38, 20.38it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1029/1600 [00:50<00:28, 20.29it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1231/1600 [01:00<00:18, 20.25it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1434/1600 [01:10<00:08, 20.25it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 203/1600 [00:10<01:09, 20.22it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 406/1600 [00:20<00:58, 20.24it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:30<00:47, 20.77it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00:40<00:36, 21.05it/s]TTraining loss: 18.0957, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 11.1872, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 9/100

Training Phase:
raining:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1050/1600 [00:50<00:26, 21.14it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1050/1600 [01:00<00:26, 21.14it/s]Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1266/1600 [01:00<00:15, 21.26it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1482/1600 [01:10<00:05, 21.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 215/1600 [00:10<01:04, 21.50it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 430/1600 [00:20<00:54, 21.32it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 643/1600 [00:30<00:44, 21.31it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 857/1600 [00:40<00:34, 21.33it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1071/1600 [00:50<00:25, 21.01it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1276/1600 [01:00<00:15, 20.83it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1483/1600 [01:10<00Training loss: 34.8754, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 23.2822, Validation accuracy: 0.9650
Macro F1-score: 0.9646
Model performance on Angry speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 10/100

Training Phase:
Training loss: 22.1536, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 34.0301, Validation accuracy: 0.9600
Macro F1-score: 0.9599
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Epoch 11/100

Training Phase:
:05, 20.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  12%|â–ˆâ–        | 199/1600 [00:10<01:10, 19.88it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 407/1600 [00:20<00:58, 20.37it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 624/1600 [00:30<00:46, 20.92it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 840/1600 [00:40<00:36, 21.08it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1600 [00:50<00:25, 21.20it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1272/1600 [01:00<00:15, 21.35it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1490/1600 [01:10<00:05, 21.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.82iTraining loss: 17.8941, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 14.8010, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 12/100

Training Phase:
t/s]Training:  27%|â–ˆâ–ˆâ–‹       | 438/1600 [00:20<00:53, 21.57it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [00:30<00:43, 21.53it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 870/1600 [00:40<00:33, 21.59it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1087/1600 [00:50<00:23, 21.47it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1300/1600 [01:00<00:14, 21.35it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1513/1600 [01:10<00:04, 21.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 207/1600 [00:10<01:07, 20.62it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 414/1600 [00:20<00:57, 20.60it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 628/1600 [00:30<00:46, 20.96it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 842/1600 [00:40<00:36, 21.05it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1054/1600 [00:50<00:25, 21.07it/s]TraTraining loss: 3.8325, Training accuracy: 1.0000
Macro F1-score: 1.0000
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 8.5678, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 13/100

Training Phase:
Training loss: 26.7718, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
ining:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1265/1600 [01:00<00:15, 21.06it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1476/1600 [01:10<00:05, 20.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 205/1600 [00:10<01:08, 20.38it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 409/1600 [00:20<00:58, 20.22it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 611/1600 [00:30<00:49, 20.10it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 823/1600 [00:40<00:37, 20.52it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [00:50<00:26, 20.91it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [01:00<00:26, 20.91it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1254/1600 [01:00<00:16, 20.75it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1459/1600 [01:10<00:06, 20.60it/s]                                                             EValidation loss: 43.2911, Validation accuracy: 0.9500
Macro F1-score: 0.9496
Model performance on Angry speech (in validation): 
	Precision: 0.8475, Recall: 1.0000, F1_score: 0.9174
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9950

Test Phase: 
valuating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:16, 11.80it/s]Testing:   2%|â–         | 4/200 [00:00<00:13, 14.48it/s]Testing:   3%|â–Ž         | 6/200 [00:00<00:12, 15.72it/s]Testing:   4%|â–         | 8/200 [00:00<00:11, 16.11it/s]Testing:   6%|â–Œ         | 11/200 [00:00<00:09, 19.29it/s]Testing:   6%|â–‹         | 13/200 [00:00<00:10, 18.42it/s]Testing:   8%|â–Š         | 16/200 [00:00<00:08, 21.70it/s]Testing:  10%|â–‰         | 19/200 [00:00<00:08, 21.96it/s]Testing:  11%|â–ˆ         | 22/200 [00:01<00:08, 20.44it/s]Testing:  12%|â–ˆâ–Ž        | 25/200 [00:01<00:08, 20.65it/s]Testing:  14%|â–ˆâ–        | 28/200 [00:01<00:07, 21.81it/s]Testing:  16%|â–ˆâ–Œ        | 32/200 [00:01<00:06, 26.00it/s]Testing:  18%|â–ˆâ–Š        | 37/200 [00:01<00:05, 31.66it/s]Testing:  21%|â–ˆâ–ˆ        | 42/200 [00:01<00:04, 35.00it/s]Testing:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:01<00:04, 35.42it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:01<00:04, 36.59it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 56/200 [00:02<00:03, 40.78it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:02<00:03, 38.08it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:02<00:02, 45.42it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [00:02<00:03, 41.38it/s]Testing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:02<00:02, 42.43it/s]Testing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:02<00:02, 40.65it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:02<00:02, 50.48it/s]Testing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [00:02<00:02, 50.85it/s]Testing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:03<00:01, 58.33it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [00:03<00:01, 57.47it/s]Testing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:03<00:01, 53.74it/s]Testing:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [00:03<00:01, 54.07it/s]Testing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:03<00:01, 62.17it/s]Testing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Test loss: 20.7327, Test accuracy: 0.9750
Macro F1-score: 0.9748
Model performance on Angry speech (in test): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in test): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Neutral speech (in test): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

======================= This is fold_4 on de =======================

Load dataset: 
Loading de train data: fold_4...
Preprocess de fold_4 data for de model
Loading de eval data: fold_4...
Preprocess de fold_4 data for de model
Loading de test data: fold_4...
Preprocess de fold_4 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
ˆ   | 142/200 [00:03<00:00, 63.68it/s]Testing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [00:03<00:00, 65.08it/s]Testing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [00:03<00:00, 67.66it/s]Testing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:03<00:00, 63.90it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:04<00:00, 64.98it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:04<00:00, 67.92it/s]Testing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [00:04<00:00, 67.26it/s]Testing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [00:04<00:00, 64.10it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   7%|â–‹         | 119/1600 [00:10<02:04, 11.89it/s]Training:  16%|â–ˆâ–Œ        | 259/1600 [00:20<01:42, 13.12it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:30<01:23, 14.26it/s]Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 582/1600 [00:40<01:07, 15.17it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 751/1600 [00:50<00:53, 15.77it/s]Training loss: 239.5008, Training accuracy: 0.9487
Macro F1-score: 0.9487
Model performance on Angry speech (in training): 
	Precision: 0.9529, Recall: 0.9600, F1_score: 0.9564
Model performance on Happy speech (in training): 
	Precision: 0.9351, Recall: 0.9000, F1_score: 0.9172
Model performance on Neutral speech (in training): 
	Precision: 0.9229, Recall: 0.9575, F1_score: 0.9399
Model performance on Sad speech (in training): 
	Precision: 0.9849, Recall: 0.9775, F1_score: 0.9812

Eval Phase: 
Validation loss: 6.3279, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 4 on epoch 1: 0.9950. Model saved.
Epoch 2/100

Training Phase:
Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 932/1600 [01:00<00:40, 16.56it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1116/1600 [01:10<00:28, 17.15it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1305/1600 [01:20<00:16, 17.70it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1504/1600 [01:30<00:05, 18.37it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 206/1600 [00:10<01:07, 20.59it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 416/1600 [00:20<00:56, 20.80it/s]Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 626/1600 [00:30<00:46, 20.78it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 834/1600 [00:40<00:37, 20.55it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [00:50<00:27, 20.55it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1600 [01:00<00:17, 20.57it/s]Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1459/1600 [01:10<00:0Training loss: 110.4040, Training accuracy: 0.9738
Macro F1-score: 0.9737
Model performance on Angry speech (in training): 
	Precision: 0.9726, Recall: 0.9775, F1_score: 0.9751
Model performance on Happy speech (in training): 
	Precision: 0.9646, Recall: 0.9550, F1_score: 0.9598
Model performance on Neutral speech (in training): 
	Precision: 0.9678, Recall: 0.9775, F1_score: 0.9726
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875

Eval Phase: 
Validation loss: 17.3326, Validation accuracy: 0.9600
Macro F1-score: 0.9599
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9778, Recall: 0.8800, F1_score: 0.9263
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Epoch 3/100

Training Phase:
Training loss: 61.3267, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 12.6907, Validation accuracy: 0.9750
Macro F1-score: 0.9748
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Neutral speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 4/100

Training Phase:
6, 20.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 213/1600 [00:10<01:05, 21.20it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 428/1600 [00:20<00:54, 21.35it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 648/1600 [00:30<00:44, 21.63it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:40<00:34, 21.21it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1075/1600 [00:51<00:25, 20.84it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1277/1600 [01:01<00:15, 20.61it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1480/1600 [01:11<00:05, 20.49it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 216/1600 [00:10<01:04, 21.54it/Training loss: 38.4050, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 9.7255, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 5/100

Training Phase:
s]Training:  27%|â–ˆâ–ˆâ–‹       | 432/1600 [00:20<00:54, 21.48it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 650/1600 [00:30<00:43, 21.60it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:40<00:34, 21.46it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1082/1600 [00:50<00:24, 21.44it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1300/1600 [01:00<00:13, 21.54it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1518/1600 [01:10<00:03, 21.48it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 212/1600 [00:10<01:05, 21.14it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 431/1600 [00:20<00:54, 21.53it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 649/1600 [00:30<00:44, 21.36it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 649/1600 [00:40<00:44, 21.36it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 855/1600 [00:40<00:35, 20.79it/s]Training:  6Training loss: 33.4643, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 6.5155, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 6/100

Training Phase:
Training loss: 18.9397, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1072/1600 [00:50<00:25, 21.08it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1289/1600 [01:00<00:14, 21.25it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1505/1600 [01:10<00:04, 21.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 204/1600 [00:10<01:08, 20.39it/s]Training:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:20<00:58, 20.23it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 613/1600 [00:30<00:48, 20.34it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 819/1600 [00:40<00:38, 20.43it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1025/1600 [00:50<00:28, 20.28it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1231/1600 [01:00<00:18, 20.38it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1437/1600 [01:10<00:08, 20.32it/s]                                                             Evaluating: Validation loss: 19.6634, Validation accuracy: 0.9700
Macro F1-score: 0.9700
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Neutral speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 7/100

Training Phase:
Training loss: 28.2746, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 16.3298, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 8/100

Training Phase:
  0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 202/1600 [00:10<01:09, 20.15it/s]Training:  25%|â–ˆâ–ˆâ–Œ       | 406/1600 [00:20<00:58, 20.28it/s]Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 613/1600 [00:30<00:48, 20.44it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 820/1600 [00:40<00:38, 20.39it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1036/1600 [00:50<00:27, 20.82it/s]Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1252/1600 [01:00<00:16, 21.01it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1600 [01:10<00:06, 21.15it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 217/1600 [00:10<01:04, 21.59it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 433/1600 [00:20<00:54, 21.52it/s]Training:  40%|â–ˆâTraining loss: 33.9220, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 20.0883, Validation accuracy: 0.9650
Macro F1-score: 0.9647
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Neutral speech (in validation): 
	Precision: 0.9074, Recall: 0.9800, F1_score: 0.9423
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 9/100

Training Phase:
–ˆâ–ˆâ–ˆ      | 648/1600 [00:30<00:44, 21.36it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 863/1600 [00:40<00:34, 21.38it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1081/1600 [00:50<00:24, 21.50it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1299/1600 [01:00<00:13, 21.54it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1516/1600 [01:11<00:03, 21.04it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 214/1600 [00:10<01:04, 21.39it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 429/1600 [00:20<00:54, 21.43it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 646/1600 [00:30<00:44, 21.53it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 863/1600 [00:40<00:35, 21.05it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1069/1600 [00:50<00:25, 20.87it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1275/1600 [01:00<00:15, 20.74it/s]Training:  93%|â–ˆTraining loss: 33.7068, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 8.0168, Validation accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 10/100

Training Phase:
Training loss: 8.9498, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 13.7534, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 11/100

Training Phase:
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1485/1600 [01:10<00:05, 20.82it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.08it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 428/1600 [00:20<00:54, 21.39it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 646/1600 [00:30<00:44, 21.53it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 864/1600 [00:40<00:34, 21.40it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1082/1600 [00:50<00:24, 21.53it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1300/1600 [01:00<00:13, 21.50it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1515/1600 [01:10<00:03, 21.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training: Training loss: 20.2365, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 8.0083, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9950

Test Phase: 
 14%|â–ˆâ–Ž        | 217/1600 [00:10<01:03, 21.68it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 434/1600 [00:20<00:54, 21.45it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 648/1600 [00:30<00:44, 21.41it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 867/1600 [00:40<00:33, 21.60it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1086/1600 [00:50<00:23, 21.49it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1299/1600 [01:00<00:14, 21.42it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1513/1600 [01:10<00:04, 21.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   2%|â–         | 3/200 [00:00<00:10, 18.40it/s]Testing:   3%|â–Ž         | 6/200 [00:00<00:09, 21.38it/s]Testing:   4%|â–         | 9/200 [00:00<00:08, 21.39it/s]Testing:   6%|â–Œ         | 12/200 [00:00<00:09, 20.58it/s]Testing:   8%|â–Š         | 15/200 [00:00<00:08, 22.64it/s]Testing:   9%|â–‰         | 18/200 [00:00<00:08, 22.31it/s]Testing:  10%|â–ˆ         | 21/200 [00:00<00:07, 24.27it/s]Testing:  12%|â–ˆâ–        | 24/200 [00:01<00:07, 25.04it/s]Testing:  14%|â–ˆâ–Ž        | 27/200 [00:01<00:06, 25.98it/s]Testing:  16%|â–ˆâ–‹        | 33/200 [00:01<00:05, 31.41it/s]Testing:  18%|â–ˆâ–Š        | 37/200 [00:01<00:05, 31.63it/s]Testing:  20%|â–ˆâ–ˆ        | 41/200 [00:01<00:05, 30.73it/s]Testing:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:01<00:05, 28.83it/s]Testing:  24%|â–ˆâ–ˆâ–       | 49/200 [00:01<00:04, 30.20it/s]Testing:  26%|â–ˆâ–ˆâ–‹       | 53/200 [00:01<00:04, 30.16it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:02<00:04, 31.81it/s]Testing:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [00:02<00:03, 36.06it/s]Testing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [00:02<00:03, 36.09it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:02<00:03, 37.65it/s]Testing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:02<00:02, 45.07it/s]Testing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:02<00:02, 42.51it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:02<00:02, 50.92it/s]Testing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [00:02<00:01, 53.23it/s]Testing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:03<00:01, 50.39it/s]Testing:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [00:03<00:01, 52.31it/s]Testing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:03<00:01, 61.63it/s]Testing:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [00:03<00:01, 66.26it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [00:03<00:01, 66.03it/s]Testing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:03<00:00, 67.08it/s]Testing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [00:03<00:00, 65.36it/s]Testing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:03<00:00, 58.18it/s]Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:03<00:00, 60.21it/s]Testing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [00:04<00:00, 60.60it/s]Testing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:04<00:00, 62.48it/s]Testing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:04<00:00, 58.52it/s]Testing:  94%|â–ˆâTest loss: 8.6636, Test accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in test): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Neutral speech (in test): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

de, all folds layer accuracy: ['0.9800', '0.9750', '0.9750', '0.9750', '0.9900']
de, all emo precision: {'Angry': ['0.9804', '0.9800', '1.0000', '0.9608', '1.0000'], 'Happy': ['1.0000', '0.9800', '1.0000', '0.9787', '0.9800'], 'Neutral': ['0.9608', '1.0000', '0.9787', '0.9615', '0.9804'], 'Sad': ['0.9804', '0.9434', '0.9259', '1.0000', '1.0000']}
de, all emo recall: {'Angry': ['1.0000', '0.9800', '1.0000', '0.9800', '0.9800'], 'Happy': ['0.9400', '0.9800', '0.9800', '0.9200', '0.9800'], 'Neutral': ['0.9800', '0.9400', '0.9200', '1.0000', '1.0000'], 'Sad': ['1.0000', '1.0000', '1.0000', '1.0000', '1.0000']}
de, all emo f1score: {'Angry': ['0.9901', '0.9800', '1.0000', '0.9703', '0.9899'], 'Happy': ['0.9691', '0.9800', '0.9899', '0.9485', '0.9800'], 'Neutral': ['0.9703', '0.9691', '0.9485', '0.9804', '0.9901'], 'Sad': ['0.9901', '0.9709', '0.9615', '1.0000', '1.0000']}
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [00:04<00:00, 60.36it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:04<00:00, 70.23it/s]                                                          ------------------NEXT SCRIPT: RUNNER_CN----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911927/matplotlib-mp0ptd2f because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on cn =======================

Load dataset: 
Loading cn train data: fold_0...
Preprocess cn fold_0 data for cn model
Loading cn eval data: fold_0...
Preprocess cn fold_0 data for cn model
Loading cn test data: fold_0...
Preprocess cn fold_0 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1167.1324, Training accuracy: 0.6987
Macro F1-score: 0.6929
Model performance on Angry speech (in training): 
	Precision: 0.8596, Recall: 0.3675, F1_score: 0.5149
Model performance on Happy speech (in training): 
	Precision: 0.4957, Recall: 0.8575, F1_score: 0.6282
Model performance on Neutral speech (in training): 
	Precision: 0.8871, Recall: 0.6875, F1_score: 0.7746
Model performance on Sad speech (in training): 
	Precision: 0.8267, Recall: 0.8825, F1_score: 0.8537

Eval Phase: 
Validation loss: 121.8768, Validation accuracy: 0.8300
Macro F1-score: 0.8193
Model performance on Angry speech (in validation): 
	Precision: 0.7353, Recall: 1.0000, F1_score: 0.8475
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.5000, F1_score: 0.6667
Model performance on Sad speech (in validation): 
	Precision: 0.7576, Recall: 1.0000, F1_score: 0.8621
New best accuracy for layer 6 on epoch 1: 0.8300. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:28<12:38:35, 28.46s/it]Training:  10%|â–ˆ         | 161/1600 [00:38<04:28,  5.36it/s] Training:  22%|â–ˆâ–ˆâ–       | 351/1600 [00:48<02:08,  9.75it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 548/1600 [00:58<01:21, 12.86it/s]Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 759/1600 [01:08<00:54, 15.41it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 975/1600 [01:18<00:36, 17.29it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1192/1600 [01:28<00:21, 18.62it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1415/1600 [01:38<00:09, 19.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.84it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 438/1600 [00:20<00:53, 21.75it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 656/1600 [00:30<00:43,Training loss: 276.4722, Training accuracy: 0.9500
Macro F1-score: 0.9500
Model performance on Angry speech (in training): 
	Precision: 0.9352, Recall: 0.9375, F1_score: 0.9363
Model performance on Happy speech (in training): 
	Precision: 0.9320, Recall: 0.9250, F1_score: 0.9285
Model performance on Neutral speech (in training): 
	Precision: 0.9483, Recall: 0.9625, F1_score: 0.9553
Model performance on Sad speech (in training): 
	Precision: 0.9848, Recall: 0.9750, F1_score: 0.9799

Eval Phase: 
Validation loss: 115.1150, Validation accuracy: 0.8550
Macro F1-score: 0.8453
Model performance on Angry speech (in validation): 
	Precision: 0.8333, Recall: 1.0000, F1_score: 0.9091
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.5200, F1_score: 0.6842
Model performance on Sad speech (in validation): 
	Precision: 0.7246, Recall: 1.0000, F1_score: 0.8403
New best accuracy for layer 6 on epoch 2: 0.8550. Model saved.
Epoch 3/100

Training Phase:
 21.73it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 873/1600 [00:40<00:33, 21.71it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1091/1600 [00:50<00:23, 21.73it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1309/1600 [01:00<00:13, 21.72it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1531/1600 [01:10<00:03, 21.84it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:03, 21.90it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:52, 21.92it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 662/1600 [00:30<00:42, 22.01it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 898/1600 [00:40<00:31, 22.62it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1134/1600 [00:50<00:20, 22.88it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1134/1600 [01:00<00:20, 22.88it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1368/1600Training loss: 166.5298, Training accuracy: 0.9712
Macro F1-score: 0.9713
Model performance on Angry speech (in training): 
	Precision: 0.9580, Recall: 0.9700, F1_score: 0.9640
Model performance on Happy speech (in training): 
	Precision: 0.9646, Recall: 0.9525, F1_score: 0.9585
Model performance on Neutral speech (in training): 
	Precision: 0.9704, Recall: 0.9850, F1_score: 0.9777
Model performance on Sad speech (in training): 
	Precision: 0.9924, Recall: 0.9775, F1_score: 0.9849

Eval Phase: 
Validation loss: 95.6752, Validation accuracy: 0.8750
Macro F1-score: 0.8660
Model performance on Angry speech (in validation): 
	Precision: 0.8333, Recall: 1.0000, F1_score: 0.9091
Model performance on Happy speech (in validation): 
	Precision: 0.9200, Recall: 0.9200, F1_score: 0.9200
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.5800, F1_score: 0.7342
Model performance on Sad speech (in validation): 
	Precision: 0.8197, Recall: 1.0000, F1_score: 0.9009
New best accuracy for layer 6 on epoch 3: 0.8750. Model saved.
Epoch 4/100

Training Phase:
Training loss: 138.6979, Training accuracy: 0.9750
Macro F1-score: 0.9750
Model performance on Angry speech (in training): 
	Precision: 0.9554, Recall: 0.9650, F1_score: 0.9602
Model performance on Happy speech (in training): 
	Precision: 0.9623, Recall: 0.9575, F1_score: 0.9599
Model performance on Neutral speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 111.7276, Validation accuracy: 0.8650
Macro F1-score: 0.8634
Model performance on Angry speech (in validation): 
	Precision: 0.6944, Recall: 1.0000, F1_score: 0.8197
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.6400, F1_score: 0.7805
Model performance on Sad speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Epoch 5/100

Training Phase:
 [01:00<00:10, 22.97it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 233/1600 [00:10<00:58, 23.30it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 468/1600 [00:20<00:48, 23.40it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 703/1600 [00:30<00:39, 22.69it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 922/1600 [00:40<00:30, 22.35it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1142/1600 [00:50<00:20, 22.21it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1363/1600 [01:00<00:10, 22.14it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1584/1600 [01:10<00:00, 22.05it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:Training loss: 123.7807, Training accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Neutral speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900

Eval Phase: 
Validation loss: 124.4135, Validation accuracy: 0.8500
Macro F1-score: 0.8463
Model performance on Angry speech (in validation): 
	Precision: 0.9583, Recall: 0.9200, F1_score: 0.9388
Model performance on Happy speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Neutral speech (in validation): 
	Precision: 0.9032, Recall: 0.5600, F1_score: 0.6914
Model performance on Sad speech (in validation): 
	Precision: 0.6757, Recall: 1.0000, F1_score: 0.8065
Epoch 6/100

Training Phase:
10<01:03, 21.88it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 439/1600 [00:20<00:52, 21.91it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 659/1600 [00:30<00:43, 21.85it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 878/1600 [00:40<00:33, 21.84it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1099/1600 [00:50<00:22, 21.90it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1320/1600 [01:00<00:12, 21.82it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1545/1600 [01:10<00:02, 22.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 223/1600 [00:10<01:01, 22.22it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 448/1600 [00:20<00:51, 22.35it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 674/1600 [00:30<00:41, 22.43it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 900/1600 [00:40<00:31, 22.30it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1123/1600 [00:50<00:Training loss: 107.9276, Training accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in training): 
	Precision: 0.9727, Recall: 0.9800, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Neutral speech (in training): 
	Precision: 0.9848, Recall: 0.9750, F1_score: 0.9799
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875

Eval Phase: 
Validation loss: 75.5854, Validation accuracy: 0.9100
Macro F1-score: 0.9086
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Sad speech (in validation): 
	Precision: 0.7812, Recall: 1.0000, F1_score: 0.8772
New best accuracy for layer 6 on epoch 6: 0.9100. Model saved.
Epoch 7/100

Training Phase:
Training loss: 80.3015, Training accuracy: 0.9844
Macro F1-score: 0.9844
Model performance on Angry speech (in training): 
	Precision: 0.9776, Recall: 0.9800, F1_score: 0.9788
Model performance on Happy speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
21, 22.26it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1347/1600 [01:00<00:11, 22.31it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1572/1600 [01:10<00:01, 22.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 225/1600 [00:10<01:01, 22.42it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 450/1600 [00:20<00:51, 22.36it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 674/1600 [00:30<00:41, 22.10it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 893/1600 [00:40<00:32, 21.99it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1127/1600 [00:50<00:21, 22.47it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1361/1600 [01:01<00:10, 22.24it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1580/1600 [01:11<00:00, 22.13it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]Validation loss: 140.9487, Validation accuracy: 0.8700
Macro F1-score: 0.8633
Model performance on Angry speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.5600, F1_score: 0.7179
Model performance on Sad speech (in validation): 
	Precision: 0.7143, Recall: 1.0000, F1_score: 0.8333
Epoch 8/100

Training Phase:
Training loss: 108.0439, Training accuracy: 0.9794
Macro F1-score: 0.9794
Model performance on Angry speech (in training): 
	Precision: 0.9747, Recall: 0.9650, F1_score: 0.9698
Model performance on Happy speech (in training): 
	Precision: 0.9701, Recall: 0.9750, F1_score: 0.9726
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9850, F1_score: 0.9825
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 77.8679, Validation accuracy: 0.8650
Macro F1-score: 0.8626
Model performance on Angry speech (in validation): 
	Precision: 0.6944, Recall: 1.0000, F1_score: 0.8197
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Sad speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Epoch 9/100

Training Phase:
                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.07it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 442/1600 [00:20<00:52, 22.03it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 663/1600 [00:30<00:42, 21.97it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 883/1600 [00:40<00:32, 21.97it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 883/1600 [00:50<00:32, 21.97it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1107/1600 [00:50<00:22, 22.11it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1346/1600 [01:00<00:11, 22.69it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1585/1600 [01:10<00:00, 22.95it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.59it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 472/1600 [00:20Training loss: 64.8072, Training accuracy: 0.9856
Macro F1-score: 0.9856
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Happy speech (in training): 
	Precision: 0.9823, Recall: 0.9725, F1_score: 0.9774
Model performance on Neutral speech (in training): 
	Precision: 0.9851, Recall: 0.9950, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 49.3582, Validation accuracy: 0.9500
Macro F1-score: 0.9499
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
New best accuracy for layer 6 on epoch 9: 0.9500. Model saved.
Epoch 10/100

Training Phase:
<00:47, 23.57it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 708/1600 [00:30<00:37, 23.54it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 946/1600 [00:40<00:27, 23.61it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1184/1600 [00:50<00:17, 23.54it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1419/1600 [01:00<00:07, 23.49it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.07it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 442/1600 [00:20<00:52, 22.00it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 662/1600 [00:30<00:42, 21.90it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 881/1600 [00:40<00:32, 21.89it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1100/1600 [00:50<00:22, 21.84it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1319/1600 [01:00<00:12, 21.83it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1539/Training loss: 35.9790, Training accuracy: 0.9938
Macro F1-score: 0.9938
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 75.0515, Validation accuracy: 0.9250
Macro F1-score: 0.9228
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Happy speech (in validation): 
	Precision: 0.8545, Recall: 0.9400, F1_score: 0.8952
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7600, F1_score: 0.8636
Model performance on Sad speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Epoch 11/100

Training Phase:
Training loss: 97.0780, Training accuracy: 0.9838
Macro F1-score: 0.9837
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9850, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Neutral speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Sad speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862

Eval Phase: 
Validation loss: 87.1532, Validation accuracy: 0.8600
Macro F1-score: 0.8545
Model performance on Angry speech (in validation): 
	Precision: 0.7463, Recall: 1.0000, F1_score: 0.8547
Model performance on Happy speech (in validation): 
	Precision: 0.8723, Recall: 0.8200, F1_score: 0.8454
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Sad speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Epoch 12/100

Training Phase:
1600 [01:10<00:02, 21.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 226/1600 [00:10<01:00, 22.55it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 452/1600 [00:20<00:50, 22.57it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 680/1600 [00:30<00:40, 22.66it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 908/1600 [00:40<00:30, 22.46it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1135/1600 [00:50<00:20, 22.54it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1135/1600 [01:00<00:20, 22.54it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1361/1600 [01:00<00:10, 22.35it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1597/1600 [01:10<00:00, 22.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|  Training loss: 55.1428, Training accuracy: 0.9888
Macro F1-score: 0.9887
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9799, Recall: 0.9775, F1_score: 0.9787
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 56.7019, Validation accuracy: 0.9200
Macro F1-score: 0.9195
Model performance on Angry speech (in validation): 
	Precision: 0.8596, Recall: 0.9800, F1_score: 0.9159
Model performance on Happy speech (in validation): 
	Precision: 0.9778, Recall: 0.8800, F1_score: 0.9263
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Sad speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Epoch 13/100

Training Phase:
        | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 234/1600 [00:10<00:58, 23.30it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 471/1600 [00:20<00:48, 23.52it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 708/1600 [00:30<00:37, 23.54it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 944/1600 [00:40<00:28, 22.99it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1167/1600 [00:50<00:19, 22.61it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1387/1600 [01:00<00:09, 22.40it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 234/1600 [00:10<00:58, 23.38it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 468/1600 [00:20<00:48, 23.29it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 704/1600 [00:30<00:38, 23.39it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 940/1600 [00:40<00:28, 23.45it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1178/1600 [00:50<0Training loss: 52.5711, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 109.2870, Validation accuracy: 0.8550
Macro F1-score: 0.8511
Model performance on Angry speech (in validation): 
	Precision: 0.9362, Recall: 0.8800, F1_score: 0.9072
Model performance on Happy speech (in validation): 
	Precision: 0.8036, Recall: 0.9000, F1_score: 0.8491
Model performance on Neutral speech (in validation): 
	Precision: 0.9697, Recall: 0.6400, F1_score: 0.7711
Model performance on Sad speech (in validation): 
	Precision: 0.7812, Recall: 1.0000, F1_score: 0.8772
Epoch 14/100

Training Phase:
Training loss: 64.7434, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 86.9580, Validation accuracy: 0.8950
Macro F1-score: 0.8900
Model performance on Angry speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Happy speech (in validation): 
	Precision: 0.8679, Recall: 0.9200, F1_score: 0.8932
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.6600, F1_score: 0.7952
Model performance on Sad speech (in validation): 
	Precision: 0.8197, Recall: 1.0000, F1_score: 0.9009
Epoch 15/100

Training Phase:
0:17, 23.56it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1416/1600 [01:00<00:07, 23.63it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 235/1600 [00:10<00:58, 23.46it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 470/1600 [00:20<00:48, 23.43it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 709/1600 [00:30<00:37, 23.61it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 948/1600 [00:40<00:27, 23.59it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1184/1600 [00:50<00:17, 23.50it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1421/1600 [01:00<00:07, 23.54it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 235/1600 [00:10<00:58, 2Training loss: 36.5402, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 50.7415, Validation accuracy: 0.9450
Macro F1-score: 0.9447
Model performance on Angry speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Happy speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
Model performance on Sad speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Epoch 16/100

Training Phase:
3.47it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 470/1600 [00:20<00:48, 23.42it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 706/1600 [00:30<00:38, 23.49it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 942/1600 [00:40<00:28, 23.40it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1179/1600 [00:50<00:17, 23.49it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1417/1600 [01:00<00:07, 23.60it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 235/1600 [00:10<00:58, 23.44it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 470/1600 [00:20<00:48, 23.35it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 706/1600 [00:30<00:38, 23.43it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 942/1600 [00:40<00:28, 23.48it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1179/1600 [00:50<00:17, 23.52it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1416/1600 [01:00<00:07, 23.Training loss: 60.4933, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9824, Recall: 0.9775, F1_score: 0.9799
Model performance on Neutral speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 41.5987, Validation accuracy: 0.9550
Macro F1-score: 0.9548
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Sad speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
New best accuracy for layer 6 on epoch 16: 0.9550. Model saved.
Epoch 17/100

Training Phase:
Training loss: 33.3775, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 37.7596, Validation accuracy: 0.9650
Macro F1-score: 0.9649
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Happy speech (in validation): 
	Precision: 0.9792, Recall: 0.9400, F1_score: 0.9592
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Sad speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
New best accuracy for layer 6 on epoch 17: 0.9650. Model saved.
Epoch 18/100

Training Phase:
57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 234/1600 [00:10<00:58, 23.34it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 468/1600 [00:20<00:48, 23.32it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 701/1600 [00:30<00:38, 23.28it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 937/1600 [00:40<00:28, 23.40it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1173/1600 [00:50<00:18, 23.43it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1412/1600 [01:00<00:07, 23.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 235/1600 [00:10<00:58, 23.49it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 471/1600 [00:20<00:48, 23.52it/s]Training:  4Training loss: 53.1452, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 22.2161, Validation accuracy: 0.9750
Macro F1-score: 0.9749
Model performance on Angry speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Happy speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
New best accuracy for layer 6 on epoch 18: 0.9750. Model saved.
Epoch 19/100

Training Phase:
4%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 707/1600 [00:30<00:39, 22.73it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 927/1600 [00:40<00:30, 22.43it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1147/1600 [00:50<00:20, 22.23it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1370/1600 [01:00<00:10, 22.25it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1595/1600 [01:10<00:00, 22.33it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 225/1600 [00:10<01:01, 22.49it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 453/1600 [00:20<00:50, 22.67it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 681/1600 [00:30<00:40, 22.50it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 907/1600 [00:40<00:30, 22.51it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1133/1600 [00:50<00:20, 22.53it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1359/1600 [01:00<00:10, 22.46it/s]TTraining loss: 33.6356, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 99.1094, Validation accuracy: 0.9000
Macro F1-score: 0.8955
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Happy speech (in validation): 
	Precision: 0.8846, Recall: 0.9200, F1_score: 0.9020
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.6800, F1_score: 0.8095
Model performance on Sad speech (in validation): 
	Precision: 0.8333, Recall: 1.0000, F1_score: 0.9091
Epoch 20/100

Training Phase:
Training loss: 31.1030, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9926, Recall: 1.0000, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900

Eval Phase: 
Validation loss: 95.8753, Validation accuracy: 0.9150
Macro F1-score: 0.9143
Model performance on Angry speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Happy speech (in validation): 
	Precision: 0.9756, Recall: 0.8000, F1_score: 0.8791
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
Model performance on Sad speech (in validation): 
	Precision: 0.8197, Recall: 1.0000, F1_score: 0.9009
Epoch 21/100

Training Phase:
raining:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1588/1600 [01:10<00:00, 22.58it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 227/1600 [00:10<01:00, 22.65it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 454/1600 [00:20<00:50, 22.67it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 681/1600 [00:30<00:40, 22.67it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 909/1600 [00:40<00:30, 22.70it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1137/1600 [00:50<00:20, 22.54it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1363/1600 [01:00<00:10, 22.56it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1590/1600 [01:10<00:00, 22.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?Training loss: 43.1682, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 136.2359, Validation accuracy: 0.8400
Macro F1-score: 0.8387
Model performance on Angry speech (in validation): 
	Precision: 0.6579, Recall: 1.0000, F1_score: 0.7937
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6400, F1_score: 0.7805
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Epoch 22/100

Training Phase:
, ?it/s]Training:  14%|â–ˆâ–        | 222/1600 [00:10<01:02, 22.19it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 444/1600 [00:20<00:52, 21.95it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 666/1600 [00:30<00:42, 22.03it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 888/1600 [00:40<00:32, 22.00it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1116/1600 [00:50<00:21, 22.28it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1351/1600 [01:00<00:10, 22.68it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1588/1600 [01:10<00:00, 23.01it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 238/1600 [00:10<00:57, 23.76it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 476/1600 [00:20<00:47, 23.72it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 713/1600 [00:30<00:37, 23.65it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 951/1600 [00:40<00:27, 23.68it/s]TrainiTraining loss: 32.2159, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9875, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 129.8701, Validation accuracy: 0.8550
Macro F1-score: 0.8450
Model performance on Angry speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Model performance on Happy speech (in validation): 
	Precision: 0.9574, Recall: 0.9000, F1_score: 0.9278
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.5200, F1_score: 0.6842
Model performance on Sad speech (in validation): 
	Precision: 0.7143, Recall: 1.0000, F1_score: 0.8333
Epoch 23/100

Training Phase:
Training loss: 30.7352, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9950, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
ng:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1189/1600 [00:50<00:17, 23.61it/s]Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1426/1600 [01:00<00:07, 23.63it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 237/1600 [00:10<00:57, 23.63it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 474/1600 [00:20<00:48, 23.42it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 707/1600 [00:30<00:39, 22.79it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 930/1600 [00:40<00:29, 22.58it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1153/1600 [00:50<00:19, 22.43it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1375/1600 [01:00<00:10, 22.26it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1597/1600 [01:11<00:00, 22.22it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                      Validation loss: 82.4277, Validation accuracy: 0.9250
Macro F1-score: 0.9240
Model performance on Angry speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Model performance on Happy speech (in validation): 
	Precision: 0.9535, Recall: 0.8200, F1_score: 0.8817
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Epoch 24/100

Training Phase:
Training loss: 56.0157, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 47.8682, Validation accuracy: 0.9550
Macro F1-score: 0.9551
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Neutral speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Epoch 25/100

Training Phase:
                             Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 222/1600 [00:10<01:02, 22.12it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 444/1600 [00:20<00:52, 22.02it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1600 [00:30<00:42, 22.00it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 884/1600 [00:40<00:32, 21.98it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1104/1600 [00:50<00:22, 21.97it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1325/1600 [01:00<00:12, 21.98it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1546/1600 [01:10<00:02, 22.01it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.04it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 444/1600 [00:20<00:52, 22.16it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 667/1600 [00:30<00:42, 21.98it/s]TrainiTraining loss: 31.2839, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 66.5234, Validation accuracy: 0.9250
Macro F1-score: 0.9247
Model performance on Angry speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Sad speech (in validation): 
	Precision: 0.8475, Recall: 1.0000, F1_score: 0.9174
Epoch 26/100

Training Phase:
Training loss: 31.6017, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
ng:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 885/1600 [00:40<00:32, 21.75it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1125/1600 [00:50<00:21, 22.53it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1365/1600 [01:00<00:10, 22.75it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1597/1600 [01:12<00:00, 22.08it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 236/1600 [00:10<00:57, 23.53it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 473/1600 [00:20<00:47, 23.60it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 710/1600 [00:30<00:38, 23.02it/s]Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 940/1600 [00:40<00:28, 22.98it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1170/1600 [00:50<00:18, 22.97it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1400/1600 [01:00<00:08, 22.92it/s]                                                             EvaValidation loss: 42.6578, Validation accuracy: 0.9500
Macro F1-score: 0.9497
Model performance on Angry speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Happy speech (in validation): 
	Precision: 0.9583, Recall: 0.9200, F1_score: 0.9388
Model performance on Neutral speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Model performance on Sad speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Epoch 27/100

Training Phase:
Training loss: 16.1815, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 152.4348, Validation accuracy: 0.8700
Macro F1-score: 0.8717
Model performance on Angry speech (in validation): 
	Precision: 0.6944, Recall: 1.0000, F1_score: 0.8197
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8000, F1_score: 0.8889
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Sad speech (in validation): 
	Precision: 0.9231, Recall: 0.9600, F1_score: 0.9412
Epoch 28/100

Training Phase:
luating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 228/1600 [00:10<01:00, 22.78it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 456/1600 [00:20<00:50, 22.78it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 684/1600 [00:30<00:40, 22.73it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 912/1600 [00:40<00:30, 22.76it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1140/1600 [00:50<00:20, 22.74it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1368/1600 [01:00<00:10, 22.66it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1597/1600 [01:10<00:00, 22.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 225/1600 [00:10<01:01, 22.47it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 451/1600 [00:20<00:51, 22.52it/s]TraiTraining loss: 38.0932, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 51.5080, Validation accuracy: 0.9550
Macro F1-score: 0.9548
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Sad speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9750

Test Phase: 
ning:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 680/1600 [00:30<00:40, 22.66it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 909/1600 [00:40<00:30, 22.69it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1137/1600 [00:50<00:20, 22.49it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1359/1600 [01:00<00:10, 22.37it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1587/1600 [01:10<00:00, 22.49it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:11, 17.74it/s]Testing:   2%|â–         | 4/200 [00:00<00:10, 18.68it/s]Testing:   4%|â–Ž         | 7/200 [00:00<00:09, 21.00it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:09, 21.08it/s]Testing:   6%|â–‹         | 13/200 [00:00<00:08, 22.28it/s]Testing:   8%|â–Š         | 16/200 [00:00<00:07, 23.80it/s]Testing:  10%|â–‰         | 19/200 [00:00<00:07, 24.62it/s]Testing:  12%|â–ˆâ–        | 23/200 [00:00<00:06, 27.81it/s]Testing:  14%|â–ˆâ–        | 29/200 [00:01<00:04, 34.48it/s]Testing:  16%|â–ˆâ–‹        | 33/200 [00:01<00:06, 27.63it/s]Testing:  19%|â–ˆâ–‰        | 38/200 [00:01<00:05, 31.29it/s]Testing:  22%|â–ˆâ–ˆâ–       | 43/200 [00:01<00:04, 33.40it/s]Testing:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [00:01<00:04, 31.30it/s]Testing:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:01<00:04, 33.86it/s]Testing:  29%|â–ˆâ–ˆâ–‰       | 58/200 [00:01<00:03, 40.04it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:02<00:03, 44.23it/s]Testing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:02<00:02, 47.11it/s]Testing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:02<00:02, 49.46it/s]Testing:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [00:02<00:02, 49.81it/s]Testing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:02<00:02, 51.99it/s]Testing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [00:02<00:01, 53.93it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [00:02<00:01, 50.30it/s]Testing:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [00:02<00:01, 51.28Test loss: 24.4307, Test accuracy: 0.9600
Macro F1-score: 0.9604
Model performance on Angry speech (in test): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Happy speech (in test): 
	Precision: 0.8889, Recall: 0.9600, F1_score: 0.9231
Model performance on Neutral speech (in test): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

======================= This is fold_1 on cn =======================

Load dataset: 
Loading cn train data: fold_1...
Preprocess cn fold_1 data for cn model
Loading cn eval data: fold_1...
Preprocess cn fold_1 data for cn model
Loading cn test data: fold_1...
Preprocess cn fold_1 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:02<00:01, 54.63it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [00:03<00:01, 49.03it/s]Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [00:03<00:01, 48.61it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:03<00:01, 53.02it/s]Testing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [00:03<00:01, 59.85it/s]Testing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [00:03<00:00, 58.39it/s]Testing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [00:03<00:00, 64.12it/s]Testing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:03<00:00, 67.32it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [00:03<00:00, 67.60it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [00:03<00:00, 69.67it/s]Testing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:04<00:00, 68.90it/s]Testing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [00:04<00:00, 70.49it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]TraininTraining loss: 301.7906, Training accuracy: 0.9413
Macro F1-score: 0.9411
Model performance on Angry speech (in training): 
	Precision: 0.9177, Recall: 0.9200, F1_score: 0.9189
Model performance on Happy speech (in training): 
	Precision: 0.9304, Recall: 0.9025, F1_score: 0.9162
Model performance on Neutral speech (in training): 
	Precision: 0.9439, Recall: 0.9675, F1_score: 0.9556
Model performance on Sad speech (in training): 
	Precision: 0.9726, Recall: 0.9750, F1_score: 0.9738

Eval Phase: 
Validation loss: 57.9771, Validation accuracy: 0.8800
Macro F1-score: 0.8721
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9655, Recall: 0.5600, F1_score: 0.7089
Model performance on Neutral speech (in validation): 
	Precision: 0.7353, Recall: 1.0000, F1_score: 0.8475
Model performance on Sad speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
New best accuracy for layer 6 on epoch 1: 0.8800. Model saved.
Epoch 2/100

Training Phase:
g:   9%|â–‰         | 143/1600 [00:10<01:42, 14.26it/s]Training:  19%|â–ˆâ–‰        | 302/1600 [00:20<01:25, 15.22it/s]Training:  30%|â–ˆâ–ˆâ–‰       | 479/1600 [00:30<01:08, 16.32it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1600 [00:40<00:53, 17.40it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 870/1600 [00:50<00:39, 18.32it/s]Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1074/1600 [01:00<00:27, 19.01it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1286/1600 [01:10<00:15, 19.71it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1499/1600 [01:20<00:04, 20.21it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:02, 21.95it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 442/1600 [00:20<00:52, 22.09it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1600 [00:30<00:42, 22.10it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ Training loss: 131.6465, Training accuracy: 0.9719
Macro F1-score: 0.9718
Model performance on Angry speech (in training): 
	Precision: 0.9530, Recall: 0.9625, F1_score: 0.9577
Model performance on Happy speech (in training): 
	Precision: 0.9593, Recall: 0.9425, F1_score: 0.9508
Model performance on Neutral speech (in training): 
	Precision: 0.9803, Recall: 0.9950, F1_score: 0.9876
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9875, F1_score: 0.9912

Eval Phase: 
Validation loss: 66.5831, Validation accuracy: 0.8900
Macro F1-score: 0.8814
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9667, Recall: 0.5800, F1_score: 0.7250
Model performance on Neutral speech (in validation): 
	Precision: 0.7937, Recall: 1.0000, F1_score: 0.8850
Model performance on Sad speech (in validation): 
	Precision: 0.8621, Recall: 1.0000, F1_score: 0.9259
New best accuracy for layer 6 on epoch 2: 0.8900. Model saved.
Epoch 3/100

Training Phase:
   | 887/1600 [00:40<00:32, 22.16it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1111/1600 [00:50<00:22, 22.22it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1335/1600 [01:00<00:11, 22.25it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1559/1600 [01:10<00:01, 22.30it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 223/1600 [00:10<01:01, 22.27it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 446/1600 [00:20<00:52, 22.07it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1600 [00:30<00:41, 22.21it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 898/1600 [00:40<00:31, 22.40it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1125/1600 [00:50<00:21, 22.37it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1349/1600 [01:00<00:11, 22.26it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1570/1600 [01:10<00:01, 22.19it/s]              Training loss: 132.2422, Training accuracy: 0.9712
Macro F1-score: 0.9712
Model performance on Angry speech (in training): 
	Precision: 0.9532, Recall: 0.9675, F1_score: 0.9603
Model performance on Happy speech (in training): 
	Precision: 0.9669, Recall: 0.9500, F1_score: 0.9584
Model performance on Neutral speech (in training): 
	Precision: 0.9752, Recall: 0.9825, F1_score: 0.9788
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875

Eval Phase: 
Validation loss: 117.1662, Validation accuracy: 0.8000
Macro F1-score: 0.8090
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Happy speech (in validation): 
	Precision: 0.9714, Recall: 0.6800, F1_score: 0.8000
Model performance on Neutral speech (in validation): 
	Precision: 0.5618, Recall: 1.0000, F1_score: 0.7194
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.6400, F1_score: 0.7805
Epoch 4/100

Training Phase:
Training loss: 66.3509, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9728, Recall: 0.9825, F1_score: 0.9776
Model performance on Happy speech (in training): 
	Precision: 0.9823, Recall: 0.9725, F1_score: 0.9774
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 168.1091, Validation accuracy: 0.7750
Macro F1-score: 0.7813
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8000, F1_score: 0.8889
Model performance on Happy speech (in validation): 
	Precision: 0.9615, Recall: 0.5000, F1_score: 0.6579
Model performance on Neutral speech (in validation): 
	Precision: 0.5376, Recall: 1.0000, F1_score: 0.6993
Model performance on Sad speech (in validation): 
	Precision: 0.9756, Recall: 0.8000, F1_score: 0.8791
Epoch 5/100

Training Phase:
                                               Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 222/1600 [00:10<01:02, 22.20it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 444/1600 [00:20<00:52, 22.19it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 669/1600 [00:30<00:41, 22.31it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 894/1600 [00:40<00:31, 22.25it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1117/1600 [00:50<00:21, 22.23it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1341/1600 [01:00<00:11, 22.28it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1565/1600 [01:10<00:01, 22.28it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.86it/s]Training:  28%|â–ˆâ–ˆTraining loss: 76.4463, Training accuracy: 0.9819
Macro F1-score: 0.9819
Model performance on Angry speech (in training): 
	Precision: 0.9773, Recall: 0.9675, F1_score: 0.9724
Model performance on Happy speech (in training): 
	Precision: 0.9651, Recall: 0.9675, F1_score: 0.9663
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 249.6381, Validation accuracy: 0.7250
Macro F1-score: 0.7135
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.9333, Recall: 0.2800, F1_score: 0.4308
Model performance on Neutral speech (in validation): 
	Precision: 0.4950, Recall: 1.0000, F1_score: 0.6623
Model performance on Sad speech (in validation): 
	Precision: 0.9286, Recall: 0.7800, F1_score: 0.8478
Epoch 6/100

Training Phase:
â–Š       | 445/1600 [00:20<00:51, 22.24it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1600 [00:30<00:41, 22.20it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 892/1600 [00:40<00:31, 22.16it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1118/1600 [00:50<00:21, 22.29it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1344/1600 [01:00<00:11, 22.30it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1570/1600 [01:10<00:01, 22.38it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:02, 21.97it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 445/1600 [00:20<00:51, 22.24it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1600 [00:30<00:41, 22.33it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 895/1600 [00:40<00:31, 22.12it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1113/1600 [00:50<00:22, 22.00it/s]Training:  84%|â–ˆâ–ˆâ–ˆâTraining loss: 53.1343, Training accuracy: 0.9894
Macro F1-score: 0.9894
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 78.3964, Validation accuracy: 0.8750
Macro F1-score: 0.8791
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9744, Recall: 0.7600, F1_score: 0.8539
Model performance on Neutral speech (in validation): 
	Precision: 0.6757, Recall: 1.0000, F1_score: 0.8065
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Epoch 7/100

Training Phase:
Training loss: 53.6665, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9924, Recall: 0.9850, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 172.7069, Validation accuracy: 0.7950
Macro F1-score: 0.7958
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.4600, F1_score: 0.6301
Model performance on Neutral speech (in validation): 
	Precision: 0.5556, Recall: 1.0000, F1_score: 0.7143
Model performance on Sad speech (in validation): 
	Precision: 0.9762, Recall: 0.8200, F1_score: 0.8913
Epoch 8/100

Training Phase:
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1339/1600 [01:00<00:11, 22.19it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1569/1600 [01:10<00:01, 22.43it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 231/1600 [00:10<00:59, 23.06it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 463/1600 [00:20<00:49, 23.13it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 697/1600 [00:30<00:38, 23.23it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 931/1600 [00:40<00:28, 23.20it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1163/1600 [00:50<00:19, 22.98it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1389/1600 [01:00<00:09, 22.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|Training loss: 44.4580, Training accuracy: 0.9906
Macro F1-score: 0.9906
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9875, F1_score: 0.9912

Eval Phase: 
Validation loss: 89.8075, Validation accuracy: 0.8850
Macro F1-score: 0.8833
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 0.9714, Recall: 0.6800, F1_score: 0.8000
Model performance on Neutral speech (in validation): 
	Precision: 0.7581, Recall: 0.9400, F1_score: 0.8393
Model performance on Sad speech (in validation): 
	Precision: 0.8704, Recall: 0.9400, F1_score: 0.9038
Epoch 9/100

Training Phase:
â–ˆâ–        | 224/1600 [00:10<01:01, 22.36it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 448/1600 [00:20<00:52, 22.09it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1600 [00:30<00:42, 22.11it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 896/1600 [00:40<00:31, 22.26it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1121/1600 [00:50<00:21, 22.32it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1346/1600 [01:00<00:11, 22.28it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1568/1600 [01:10<00:01, 22.19it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 225/1600 [00:10<01:01, 22.50it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 450/1600 [00:20<00:51, 22.41it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 675/1600 [00:30<00:41, 22.44it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 900/1600 [00:40<00:31, 22.27it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆTraining loss: 57.5810, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9825, F1_score: 0.9837
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 43.2163, Validation accuracy: 0.9250
Macro F1-score: 0.9271
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9773, Recall: 0.8600, F1_score: 0.9149
Model performance on Neutral speech (in validation): 
	Precision: 0.7812, Recall: 1.0000, F1_score: 0.8772
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
New best accuracy for layer 6 on epoch 9: 0.9250. Model saved.
Epoch 10/100

Training Phase:
Training loss: 24.9189, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
â–ˆâ–ˆ   | 1124/1600 [00:50<00:21, 22.29it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1600 [01:00<00:11, 22.32it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1572/1600 [01:10<00:01, 22.27it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:02, 21.91it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:52, 21.95it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 668/1600 [00:30<00:41, 22.30it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 902/1600 [00:40<00:30, 22.70it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1136/1600 [00:50<00:20, 22.36it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1354/1600 [01:01<00:11, 21.97it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1574/1600 [01:11<00:01, 21.97it/s]                                                             Evaluating:   0%|   Validation loss: 65.4557, Validation accuracy: 0.9150
Macro F1-score: 0.9135
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.7400, F1_score: 0.8506
Model performance on Neutral speech (in validation): 
	Precision: 0.8197, Recall: 1.0000, F1_score: 0.9009
Model performance on Sad speech (in validation): 
	Precision: 0.8868, Recall: 0.9400, F1_score: 0.9126
Epoch 11/100

Training Phase:
Training loss: 49.6237, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 57.2451, Validation accuracy: 0.8800
Macro F1-score: 0.8799
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.9722, Recall: 0.7000, F1_score: 0.8140
Model performance on Neutral speech (in validation): 
	Precision: 0.7353, Recall: 1.0000, F1_score: 0.8475
Model performance on Sad speech (in validation): 
	Precision: 0.9020, Recall: 0.9200, F1_score: 0.9109
Epoch 12/100

Training Phase:
       | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.80it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 445/1600 [00:20<00:51, 22.30it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 672/1600 [00:30<00:41, 22.22it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 898/1600 [00:40<00:31, 22.35it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1124/1600 [00:50<00:21, 22.29it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1349/1600 [01:00<00:11, 22.35it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1349/1600 [01:10<00:11, 22.35it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1573/1600 [01:10<00:01, 22.32it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 226/1600 [00:10<01:01, 22.51it/s]Training: Training loss: 29.1900, Training accuracy: 0.9938
Macro F1-score: 0.9938
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 112.2537, Validation accuracy: 0.8750
Macro F1-score: 0.8716
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5800, F1_score: 0.7342
Model performance on Neutral speech (in validation): 
	Precision: 0.6849, Recall: 1.0000, F1_score: 0.8130
Model performance on Sad speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Epoch 13/100

Training Phase:
 28%|â–ˆâ–ˆâ–Š       | 452/1600 [00:20<00:51, 22.39it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 679/1600 [00:30<00:40, 22.50it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 906/1600 [00:40<00:30, 22.46it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1130/1600 [00:50<00:21, 22.31it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1353/1600 [01:00<00:11, 22.28it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1576/1600 [01:10<00:01, 22.15it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 231/1600 [00:10<00:59, 23.09it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 464/1600 [00:20<00:49, 23.17it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 698/1600 [00:30<00:38, 23.27it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 932/1600 [00:40<00:28, 23.23it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1164/1600 [00:50<00:18, 23.18it/s]Training:  8Training loss: 34.3229, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 183.9237, Validation accuracy: 0.7900
Macro F1-score: 0.7845
Model performance on Angry speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.4400, F1_score: 0.6111
Model performance on Neutral speech (in validation): 
	Precision: 0.5682, Recall: 1.0000, F1_score: 0.7246
Model performance on Sad speech (in validation): 
	Precision: 0.9250, Recall: 0.7400, F1_score: 0.8222
Epoch 14/100

Training Phase:
Training loss: 21.9749, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 56.9421, Validation accuracy: 0.9150
Macro F1-score: 0.9165
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9545, Recall: 0.8400, F1_score: 0.8936
Model performance on Neutral speech (in validation): 
	Precision: 0.7812, Recall: 1.0000, F1_score: 0.8772
Model performance on Sad speech (in validation): 
	Precision: 0.9778, Recall: 0.8800, F1_score: 0.9263
Epoch 15/100

Training Phase:
7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1397/1600 [01:00<00:08, 23.21it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:02, 21.92it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:53, 21.85it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 658/1600 [00:30<00:43, 21.77it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 877/1600 [00:40<00:33, 21.79it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1096/1600 [00:50<00:23, 21.71it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1314/1600 [01:00<00:13, 21.73it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1534/1600 [01:10<00:03, 21.81it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]TrainiTraining loss: 23.3401, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 252.1512, Validation accuracy: 0.8000
Macro F1-score: 0.8049
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Neutral speech (in validation): 
	Precision: 0.5556, Recall: 1.0000, F1_score: 0.7143
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.6000, F1_score: 0.7500
Epoch 16/100

Training Phase:
ng:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.85it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 441/1600 [00:20<00:52, 22.00it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 663/1600 [00:30<00:42, 21.90it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 881/1600 [00:40<00:32, 21.84it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1102/1600 [00:50<00:22, 21.92it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1328/1600 [01:00<00:12, 22.13it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1554/1600 [01:10<00:02, 22.09it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 225/1600 [00:10<01:01, 22.50it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 450/1600 [00:20<00:51, 22.47it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 675/1600 [00:30<00:41, 22.40it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 899/1600 [00:40<00:31, 22.36it/s]Training:  70%|â–ˆâ–ˆTraining loss: 36.9742, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Validation loss: 71.7675, Validation accuracy: 0.8850
Macro F1-score: 0.8865
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Neutral speech (in validation): 
	Precision: 0.7143, Recall: 1.0000, F1_score: 0.8333
Model performance on Sad speech (in validation): 
	Precision: 0.9348, Recall: 0.8600, F1_score: 0.8958
Epoch 17/100

Training Phase:
Training loss: 21.7732, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950

Eval Phase: 
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1122/1600 [00:50<00:21, 22.33it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1345/1600 [01:00<00:11, 22.19it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1569/1600 [01:10<00:01, 22.25it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 222/1600 [00:10<01:02, 22.17it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 446/1600 [00:20<00:51, 22.30it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 674/1600 [00:30<00:41, 22.50it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 902/1600 [00:40<00:31, 22.38it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1125/1600 [00:50<00:21, 22.35it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1352/1600 [01:00<00:11, 22.43it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1579/1600 [01:10<00:00, 22.31it/s]                                                             Evaluating:Validation loss: 240.4290, Validation accuracy: 0.7650
Macro F1-score: 0.7417
Model performance on Angry speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.3200, F1_score: 0.4848
Model performance on Neutral speech (in validation): 
	Precision: 0.6024, Recall: 1.0000, F1_score: 0.7519
Model performance on Sad speech (in validation): 
	Precision: 0.7400, Recall: 0.7400, F1_score: 0.7400
Epoch 18/100

Training Phase:
Training loss: 29.1024, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9826, Recall: 0.9875, F1_score: 0.9850
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 123.4285, Validation accuracy: 0.8500
Macro F1-score: 0.8441
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9655, Recall: 0.5600, F1_score: 0.7089
Model performance on Neutral speech (in validation): 
	Precision: 0.7206, Recall: 0.9800, F1_score: 0.8305
Model performance on Sad speech (in validation): 
	Precision: 0.8214, Recall: 0.9200, F1_score: 0.8679
Epoch 19/100

Training Phase:
   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 221/1600 [00:10<01:02, 22.04it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 447/1600 [00:20<00:51, 22.35it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 673/1600 [00:30<00:41, 22.27it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 896/1600 [00:40<00:31, 22.28it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1119/1600 [00:50<00:21, 22.19it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1340/1600 [01:00<00:11, 21.97it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1559/1600 [01:10<00:01, 21.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 216/1600 [00:10<01:04, 21.52it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:52, 22.02it/s]Training:  42%Training loss: 29.7425, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 180.0967, Validation accuracy: 0.7800
Macro F1-score: 0.7686
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.3600, F1_score: 0.5294
Model performance on Neutral speech (in validation): 
	Precision: 0.9091, Recall: 0.8000, F1_score: 0.8511
Model performance on Sad speech (in validation): 
	Precision: 0.5556, Recall: 1.0000, F1_score: 0.7143
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9250

Test Phase: 
|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1600 [00:30<00:42, 21.99it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 889/1600 [00:40<00:32, 22.16it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1116/1600 [00:50<00:21, 22.34it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1343/1600 [01:00<00:11, 22.25it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1564/1600 [01:10<00:01, 22.08it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:14, 13.41it/s]Testing:   2%|â–Ž         | 5/200 [00:00<00:09, 20.20it/s]Testing:   4%|â–         | 8/200 [00:00<00:08, 23.12it/s]Testing:   6%|â–Œ         | 11/200 [00:00<00:09, 20.38it/s]Testing:   7%|â–‹         | 14/200 [00:00<00:08, 22.68it/s]Testing:   9%|â–‰         | 18/200 [00:00<00:06, 26.71it/s]Testing:  10%|â–ˆ         | 21/200 [00:00<00:08, 22.22it/s]Testing:  12%|â–ˆâ–        | 24/200 [00:01<00:07, 23.46it/s]Testing:  14%|â–ˆâ–        | 29/200 [00:01<00:05, 28.97it/s]Testing:  16%|â–ˆâ–‹        | 33/200 [00:01<00:05, 28.06it/s]Testing:  19%|â–ˆâ–‰        | 38/200 [00:01<00:04, 33.19it/s]Testing:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:01<00:03, 40.14it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:01<00:03, 40.91it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:01<00:03, 37.40it/s]Testing:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [00:01<00:03, 45.16it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:02<00:02, 49.40it/s]Testing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:02<00:02, 54.15it/s]Testing:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [00:02<00:02, 47.08it/s]Testing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:02<00:02, 50.24it/s]Testing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [00:02<00:01, 56.69it/s]Testing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [00:02<00:01, 62.33it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [00:02<00:01, 65.86it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [00:02<00:01, 60.33it/Test loss: 64.9018, Test accuracy: 0.8900
Macro F1-score: 0.8937
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Happy speech (in test): 
	Precision: 0.9778, Recall: 0.8800, F1_score: 0.9263
Model performance on Neutral speech (in test): 
	Precision: 0.7042, Recall: 1.0000, F1_score: 0.8264
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 0.7600, F1_score: 0.8636

======================= This is fold_2 on cn =======================

Load dataset: 
Loading cn train data: fold_2...
Preprocess cn fold_2 data for cn model
Loading cn eval data: fold_2...
Preprocess cn fold_2 data for cn model
Loading cn test data: fold_2...
Preprocess cn fold_2 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
s]Testing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [00:02<00:01, 64.15it/s]Testing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:03<00:00, 67.19it/s]Testing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [00:03<00:00, 67.03it/s]Testing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:03<00:00, 65.38it/s]Testing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:03<00:00, 69.20it/s]Testing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:03<00:00, 70.53it/s]Testing:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [00:03<00:00, 72.70it/s]Testing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:03<00:00, 72.13it/s]Testing:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [00:03<00:00, 73.30it/s]Testing:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [00:03<00:00, 74.50it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   9%|â–‰         | 144/1600 [00:10<01:41, 14.33it/s]Training:  20%|â–ˆâ–‰        | 317/1600 [00:20<01:19, 16.04it/s]Training:  31%|â–ˆâ–ˆâTraining loss: 174.7791, Training accuracy: 0.9675
Macro F1-score: 0.9675
Model performance on Angry speech (in training): 
	Precision: 0.9698, Recall: 0.9650, F1_score: 0.9674
Model performance on Happy speech (in training): 
	Precision: 0.9551, Recall: 0.9575, F1_score: 0.9563
Model performance on Neutral speech (in training): 
	Precision: 0.9579, Recall: 0.9675, F1_score: 0.9627
Model performance on Sad speech (in training): 
	Precision: 0.9874, Recall: 0.9800, F1_score: 0.9837

Eval Phase: 
Validation loss: 123.3474, Validation accuracy: 0.8000
Macro F1-score: 0.7857
Model performance on Angry speech (in validation): 
	Precision: 0.6400, Recall: 0.9600, F1_score: 0.7680
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.4200, F1_score: 0.5915
Model performance on Neutral speech (in validation): 
	Precision: 0.8958, Recall: 0.8600, F1_score: 0.8776
Model performance on Sad speech (in validation): 
	Precision: 0.8571, Recall: 0.9600, F1_score: 0.9057
New best accuracy for layer 6 on epoch 1: 0.8000. Model saved.
Epoch 2/100

Training Phase:
–ˆâ–      | 503/1600 [00:30<01:03, 17.18it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 698/1600 [00:40<00:49, 18.08it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 905/1600 [00:50<00:36, 19.01it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1112/1600 [01:00<00:25, 19.51it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1323/1600 [01:10<00:13, 20.00it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1535/1600 [01:20<00:03, 20.36it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.85it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 441/1600 [00:20<00:52, 22.02it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 663/1600 [00:30<00:42, 21.99it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 885/1600 [00:40<00:32, 22.06it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1108/1600 [00:50<00:22, 22.14it/s]Training:  83%|â–ˆâ–ˆâ–ˆTraining loss: 69.8899, Training accuracy: 0.9838
Macro F1-score: 0.9837
Model performance on Angry speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Happy speech (in training): 
	Precision: 0.9822, Recall: 0.9675, F1_score: 0.9748
Model performance on Neutral speech (in training): 
	Precision: 0.9729, Recall: 0.9875, F1_score: 0.9801
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 86.6542, Validation accuracy: 0.8650
Macro F1-score: 0.8578
Model performance on Angry speech (in validation): 
	Precision: 0.6957, Recall: 0.9600, F1_score: 0.8067
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Model performance on Neutral speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
New best accuracy for layer 6 on epoch 2: 0.8650. Model saved.
Epoch 3/100

Training Phase:
Training loss: 58.6242, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1331/1600 [01:00<00:12, 22.17it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1554/1600 [01:10<00:02, 22.13it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 215/1600 [00:10<01:04, 21.42it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 432/1600 [00:20<00:54, 21.58it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 649/1600 [00:30<00:43, 21.62it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 870/1600 [00:40<00:33, 21.80it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1091/1600 [00:50<00:23, 21.90it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1091/1600 [01:00<00:23, 21.90it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1317/1600 [01:00<00:12, 22.11it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1543/1600 [01:10<00:02, 22.20it/s]                                                             Evaluating:   0%Validation loss: 108.6261, Validation accuracy: 0.8350
Macro F1-score: 0.8250
Model performance on Angry speech (in validation): 
	Precision: 0.6618, Recall: 0.9000, F1_score: 0.7627
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.4800, F1_score: 0.6486
Model performance on Neutral speech (in validation): 
	Precision: 0.8333, Recall: 1.0000, F1_score: 0.9091
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Epoch 4/100

Training Phase:
Training loss: 43.3838, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9899, Recall: 0.9825, F1_score: 0.9862
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 201.0285, Validation accuracy: 0.8050
Macro F1-score: 0.7817
Model performance on Angry speech (in validation): 
	Precision: 0.6000, Recall: 0.9600, F1_score: 0.7385
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.3200, F1_score: 0.4848
Model performance on Neutral speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Epoch 5/100

Training Phase:
|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 223/1600 [00:10<01:01, 22.23it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 446/1600 [00:20<00:51, 22.21it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 669/1600 [00:30<00:41, 22.20it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 892/1600 [00:40<00:32, 21.99it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1109/1600 [00:50<00:22, 21.82it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1338/1600 [01:00<00:11, 22.15it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1570/1600 [01:10<00:01, 22.48it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 231/1600 [00:10<00:59, 23.09it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 464/1600 [00:20<00:48, 23.19it/s]Training:  44%|â–ˆâTraining loss: 27.2055, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 176.0580, Validation accuracy: 0.8000
Macro F1-score: 0.7790
Model performance on Angry speech (in validation): 
	Precision: 0.6076, Recall: 0.9600, F1_score: 0.7442
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.3400, F1_score: 0.5075
Model performance on Neutral speech (in validation): 
	Precision: 0.8596, Recall: 0.9800, F1_score: 0.9159
Model performance on Sad speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Epoch 6/100

Training Phase:
–ˆâ–ˆâ–ˆâ–Ž     | 697/1600 [00:30<00:39, 22.75it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 927/1600 [00:40<00:29, 22.83it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 927/1600 [00:50<00:29, 22.83it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1152/1600 [00:50<00:19, 22.62it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1383/1600 [01:00<00:09, 22.78it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 215/1600 [00:10<01:04, 21.50it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 432/1600 [00:20<00:54, 21.61it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 649/1600 [00:30<00:43, 21.62it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 866/1600 [00:40<00:34, 21.48it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1095/1600 [00:50<00:22, 21.98it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1325/1600 [01:00<00:12, 22.31it/s]Training:  98%|â–ˆâTraining loss: 58.2451, Training accuracy: 0.9881
Macro F1-score: 0.9881
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9950, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9874, Recall: 0.9825, F1_score: 0.9850
Model performance on Neutral speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9875, F1_score: 0.9900

Eval Phase: 
Validation loss: 117.1158, Validation accuracy: 0.8450
Macro F1-score: 0.8375
Model performance on Angry speech (in validation): 
	Precision: 0.6812, Recall: 0.9400, F1_score: 0.7899
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5200, F1_score: 0.6842
Model performance on Neutral speech (in validation): 
	Precision: 0.8475, Recall: 1.0000, F1_score: 0.9174
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Epoch 7/100

Training Phase:
Training loss: 20.0430, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Validation loss: 175.7514, Validation accuracy: 0.8250
Macro F1-score: 0.8131
Model performance on Angry speech (in validation): 
	Precision: 0.6575, Recall: 0.9600, F1_score: 0.7805
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.4600, F1_score: 0.6301
Model performance on Neutral speech (in validation): 
	Precision: 0.9362, Recall: 0.8800, F1_score: 0.9072
Model performance on Sad speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Epoch 8/100

Training Phase:
–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1560/1600 [01:10<00:01, 22.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 227/1600 [00:10<01:00, 22.63it/s]Training:  29%|â–ˆâ–ˆâ–Š       | 459/1600 [00:20<00:49, 22.93it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 691/1600 [00:30<00:39, 23.03it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 923/1600 [00:40<00:29, 22.98it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1155/1600 [00:50<00:19, 23.04it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1387/1600 [01:00<00:09, 22.94it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 231/1600 [00:10<00:59, 23.06it/s]Training:  29%|â–ˆâ–ˆâ–‰Training loss: 17.1800, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 157.4980, Validation accuracy: 0.8400
Macro F1-score: 0.8337
Model performance on Angry speech (in validation): 
	Precision: 0.6912, Recall: 0.9400, F1_score: 0.7966
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Model performance on Neutral speech (in validation): 
	Precision: 0.8197, Recall: 1.0000, F1_score: 0.9009
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Epoch 9/100

Training Phase:
       | 462/1600 [00:20<00:49, 22.93it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 691/1600 [00:30<00:40, 22.65it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 915/1600 [00:40<00:30, 22.26it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1132/1600 [00:50<00:21, 22.00it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1600 [01:00<00:11, 21.82it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1566/1600 [01:10<00:01, 21.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.75it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 436/1600 [00:20<00:53, 21.67it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [00:30<00:43, 21.64it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 879/1600 [00:40<00:32, 22.00it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1111/1600 [00:50<00:21, 22.43it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆTraining loss: 40.0851, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 148.0774, Validation accuracy: 0.8450
Macro F1-score: 0.8393
Model performance on Angry speech (in validation): 
	Precision: 0.7121, Recall: 0.9400, F1_score: 0.8103
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5800, F1_score: 0.7342
Model performance on Neutral speech (in validation): 
	Precision: 0.9149, Recall: 0.8600, F1_score: 0.8866
Model performance on Sad speech (in validation): 
	Precision: 0.8621, Recall: 1.0000, F1_score: 0.9259
Epoch 10/100

Training Phase:
Training loss: 8.1546, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 109.1339, Validation accuracy: 0.8700
Macro F1-score: 0.8675
Model performance on Angry speech (in validation): 
	Precision: 0.7500, Recall: 0.9600, F1_score: 0.8421
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6600, F1_score: 0.7952
Model performance on Neutral speech (in validation): 
	Precision: 0.9565, Recall: 0.8800, F1_score: 0.9167
Model performance on Sad speech (in validation): 
	Precision: 0.8596, Recall: 0.9800, F1_score: 0.9159
New best accuracy for layer 6 on epoch 10: 0.8700. Model saved.
Epoch 11/100

Training Phase:
â–ˆâ–ˆâ–ˆâ– | 1343/1600 [01:00<00:11, 22.60it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1576/1600 [01:10<00:01, 22.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 232/1600 [00:10<00:59, 23.19it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 464/1600 [00:20<00:49, 23.06it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 697/1600 [00:30<00:39, 23.14it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 930/1600 [00:40<00:29, 23.02it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1161/1600 [00:50<00:19, 23.03it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1392/1600 [01:00<00:09, 23.01it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Training loss: 33.7858, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 80.2669, Validation accuracy: 0.8700
Macro F1-score: 0.8654
Model performance on Angry speech (in validation): 
	Precision: 0.7206, Recall: 0.9800, F1_score: 0.8305
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6000, F1_score: 0.7500
Model performance on Neutral speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Epoch 12/100

Training Phase:
        | 230/1600 [00:10<00:59, 22.98it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 460/1600 [00:20<00:49, 22.84it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:30<00:39, 22.88it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 925/1600 [00:40<00:29, 23.09it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1160/1600 [00:50<00:19, 23.07it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1391/1600 [01:00<00:09, 23.01it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 228/1600 [00:10<01:00, 22.78it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 456/1600 [00:20<00:51, 22.05it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 673/1600 [00:30<00:42, 21.85it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 890/1600 [00:40<00:32, 21.58it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1108/1600 [00:50<00:22, 21.66it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆTraining loss: 18.2496, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 80.6645, Validation accuracy: 0.8850
Macro F1-score: 0.8831
Model performance on Angry speech (in validation): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.7000, F1_score: 0.8235
Model performance on Neutral speech (in validation): 
	Precision: 0.9375, Recall: 0.9000, F1_score: 0.9184
Model performance on Sad speech (in validation): 
	Precision: 0.8889, Recall: 0.9600, F1_score: 0.9231
New best accuracy for layer 6 on epoch 12: 0.8850. Model saved.
Epoch 13/100

Training Phase:
Training loss: 31.4934, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 123.7679, Validation accuracy: 0.8550
Macro F1-score: 0.8502
Model performance on Angry speech (in validation): 
	Precision: 0.7273, Recall: 0.9600, F1_score: 0.8276
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6000, F1_score: 0.7500
Model performance on Neutral speech (in validation): 
	Precision: 0.8824, Recall: 0.9000, F1_score: 0.8911
Model performance on Sad speech (in validation): 
	Precision: 0.9057, Recall: 0.9600, F1_score: 0.9320
Epoch 14/100

Training Phase:
â–Ž | 1327/1600 [01:00<00:12, 21.72it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1546/1600 [01:10<00:02, 21.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 231/1600 [00:10<00:59, 23.06it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 462/1600 [00:20<00:49, 23.05it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 693/1600 [00:30<00:39, 22.97it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 922/1600 [00:40<00:30, 22.48it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1140/1600 [00:50<00:20, 22.08it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1370/1600 [01:00<00:10, 22.36it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        Training loss: 13.5486, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 146.4358, Validation accuracy: 0.8450
Macro F1-score: 0.8413
Model performance on Angry speech (in validation): 
	Precision: 0.7273, Recall: 0.9600, F1_score: 0.8276
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Neutral speech (in validation): 
	Precision: 0.9130, Recall: 0.8400, F1_score: 0.8750
Model performance on Sad speech (in validation): 
	Precision: 0.8421, Recall: 0.9600, F1_score: 0.8972
Epoch 15/100

Training Phase:
| 231/1600 [00:10<00:59, 23.05it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 463/1600 [00:20<00:49, 23.12it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 697/1600 [00:30<00:38, 23.21it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 697/1600 [00:40<00:38, 23.21it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 929/1600 [00:40<00:29, 23.05it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1160/1600 [00:50<00:19, 23.04it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1392/1600 [01:00<00:09, 23.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 234/1600 [00:10<00:58, 23.33it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 468/1600 [00:20<00:48, 23.27it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 701/1600 [00:30<00:38, 23.22it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 933/1600 [00:40<00:28, 23.16it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 933/1600 [00:5Training loss: 24.4185, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 137.0757, Validation accuracy: 0.8600
Macro F1-score: 0.8548
Model performance on Angry speech (in validation): 
	Precision: 0.7101, Recall: 0.9800, F1_score: 0.8235
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5800, F1_score: 0.7342
Model performance on Neutral speech (in validation): 
	Precision: 0.9057, Recall: 0.9600, F1_score: 0.9320
Model performance on Sad speech (in validation): 
	Precision: 0.9388, Recall: 0.9200, F1_score: 0.9293
Epoch 16/100

Training Phase:
Training loss: 14.1731, Training accuracy: 0.9962
Macro F1-score: 0.9963
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
0<00:28, 23.16it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1161/1600 [00:50<00:19, 22.94it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1387/1600 [01:00<00:09, 22.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.79it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 436/1600 [00:20<00:53, 21.72it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 655/1600 [00:30<00:43, 21.80it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 874/1600 [00:40<00:33, 21.64it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1090/1600 [00:50<00:23, 21.62it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1306/1600 [01:00<00:13, 21.60it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1522/1600 [01:10<00:03, 21.57it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]Validation loss: 132.6936, Validation accuracy: 0.8500
Macro F1-score: 0.8464
Model performance on Angry speech (in validation): 
	Precision: 0.7344, Recall: 0.9400, F1_score: 0.8246
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Neutral speech (in validation): 
	Precision: 0.8519, Recall: 0.9200, F1_score: 0.8846
Model performance on Sad speech (in validation): 
	Precision: 0.9020, Recall: 0.9200, F1_score: 0.9109
Epoch 17/100

Training Phase:
Training loss: 10.8624, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9975, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 172.8991, Validation accuracy: 0.8450
Macro F1-score: 0.8379
Model performance on Angry speech (in validation): 
	Precision: 0.6912, Recall: 0.9400, F1_score: 0.7966
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Model performance on Neutral speech (in validation): 
	Precision: 0.8868, Recall: 0.9400, F1_score: 0.9126
Model performance on Sad speech (in validation): 
	Precision: 0.9231, Recall: 0.9600, F1_score: 0.9412
Epoch 18/100

Training Phase:
                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 230/1600 [00:10<00:59, 23.00it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 461/1600 [00:20<00:49, 23.05it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 692/1600 [00:30<00:39, 23.05it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 923/1600 [00:40<00:29, 23.03it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1155/1600 [00:50<00:19, 23.06it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1387/1600 [01:00<00:09, 23.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 230/1600 [00:10<00:59, 22.92it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 461/1600 [00:20<00:49, 22.98it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 693/1600 [00:30<00:39, 23.05it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 928/1600 [00:40<00:28, 23Training loss: 29.9796, Training accuracy: 0.9931
Macro F1-score: 0.9931
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9900, F1_score: 0.9937
Model performance on Happy speech (in training): 
	Precision: 0.9851, Recall: 0.9925, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 227.5941, Validation accuracy: 0.8100
Macro F1-score: 0.8038
Model performance on Angry speech (in validation): 
	Precision: 0.6923, Recall: 0.9000, F1_score: 0.7826
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5600, F1_score: 0.7179
Model performance on Neutral speech (in validation): 
	Precision: 0.8864, Recall: 0.7800, F1_score: 0.8298
Model performance on Sad speech (in validation): 
	Precision: 0.7937, Recall: 1.0000, F1_score: 0.8850
Epoch 19/100

Training Phase:
Training loss: 28.0164, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9876, Recall: 0.9925, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
.22it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1163/1600 [00:50<00:18, 23.08it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1395/1600 [01:00<00:08, 23.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 216/1600 [00:10<01:04, 21.55it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 432/1600 [00:20<00:54, 21.55it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 648/1600 [00:30<00:44, 21.54it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 864/1600 [00:40<00:34, 21.52it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1081/1600 [00:50<00:24, 21.55it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1306/1600 [01:00<00:13, 21.86it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1535/1600 [01:10<00:02, 22.18it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]           Validation loss: 299.7250, Validation accuracy: 0.7650
Macro F1-score: 0.7533
Model performance on Angry speech (in validation): 
	Precision: 0.6857, Recall: 0.9600, F1_score: 0.8000
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5200, F1_score: 0.6842
Model performance on Neutral speech (in validation): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Sad speech (in validation): 
	Precision: 0.6849, Recall: 1.0000, F1_score: 0.8130
Epoch 20/100

Training Phase:
Training loss: 6.0932, Training accuracy: 0.9981
Macro F1-score: 0.9981
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 183.9616, Validation accuracy: 0.8450
Macro F1-score: 0.8387
Model performance on Angry speech (in validation): 
	Precision: 0.6761, Recall: 0.9600, F1_score: 0.7934
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.5400, F1_score: 0.7013
Model performance on Neutral speech (in validation): 
	Precision: 0.9565, Recall: 0.8800, F1_score: 0.9167
Model performance on Sad speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Epoch 21/100

Training Phase:
                                        Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 228/1600 [00:10<01:00, 22.74it/s]Training:  29%|â–ˆâ–ˆâ–Š       | 457/1600 [00:20<00:50, 22.79it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1600 [00:30<00:39, 22.97it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 922/1600 [00:40<00:29, 23.10it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1156/1600 [00:50<00:19, 23.17it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1390/1600 [01:00<00:09, 23.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 233/1600 [00:10<00:58, 23.21it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 466/1600 [00:20<00:49, 23.13it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 697/1600 [00:30<00:39, 23.11it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 928/1600 [00:40<00:29, 23.07it/s]TrTraining loss: 16.8530, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 170.4724, Validation accuracy: 0.8200
Macro F1-score: 0.8157
Model performance on Angry speech (in validation): 
	Precision: 0.7500, Recall: 0.9600, F1_score: 0.8421
Model performance on Happy speech (in validation): 
	Precision: 0.9714, Recall: 0.6800, F1_score: 0.8000
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.6400, F1_score: 0.7805
Model performance on Sad speech (in validation): 
	Precision: 0.7246, Recall: 1.0000, F1_score: 0.8403
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.8850

Test Phase: 
aining:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1159/1600 [00:50<00:19, 22.89it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1388/1600 [01:00<00:09, 22.88it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   2%|â–         | 3/200 [00:00<00:07, 28.08it/s]Testing:   4%|â–Ž         | 7/200 [00:00<00:06, 27.62it/s]Testing:   5%|â–Œ         | 10/200 [00:00<00:06, 27.90it/s]Testing:   6%|â–‹         | 13/200 [00:00<00:06, 28.14it/s]Testing:   8%|â–Š         | 16/200 [00:00<00:07, 25.64it/s]Testing:  10%|â–‰         | 19/200 [00:00<00:07, 25.56it/s]Testing:  12%|â–ˆâ–        | 23/200 [00:00<00:06, 27.67it/s]Testing:  14%|â–ˆâ–Ž        | 27/200 [00:00<00:05, 29.48it/s]Testing:  16%|â–ˆâ–Œ        | 31/200 [00:01<00:05, 32.07it/s]Testing:  18%|â–ˆâ–Š        | 35/200 [00:01<00:05, 31.34it/s]Testing:  20%|â–ˆâ–ˆ        | 40/200 [00:01<00:04, 36.20it/s]Testing:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:01<00:03, 40.16it/s]Testing:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [00:01<00:03, 42.74it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:01<00:03, 42.92it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [00:01<00:02, 45.87it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:01<00:02, 44.30it/s]Testing:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [00:02<00:02, 47.51it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:02<00:02, 49.72it/s]Testing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [00:02<00:02, 55.20it/s]Testing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [00:02<00:01, 61.11it/s]Testing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [00:02<00:01, 62.69it/s]Testing:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [00:02<00:01, 67.38it/s]Testing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [00:02<00:01, 64.62it/s]Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:02<00:01, 62.11it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:02<00:01, 62.15it/s]Testing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:02<00:Test loss: 110.1207, Test accuracy: 0.8700
Macro F1-score: 0.8656
Model performance on Angry speech (in test): 
	Precision: 0.6957, Recall: 0.9600, F1_score: 0.8067
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.5800, F1_score: 0.7342
Model performance on Neutral speech (in test): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691

======================= This is fold_3 on cn =======================

Load dataset: 
Loading cn train data: fold_3...
Preprocess cn fold_3 data for cn model
Loading cn eval data: fold_3...
Preprocess cn fold_3 data for cn model
Loading cn test data: fold_3...
Preprocess cn fold_3 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
00, 63.17it/s]Testing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [00:03<00:00, 59.60it/s]Testing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [00:03<00:00, 62.27it/s]Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:03<00:00, 66.19it/s]Testing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [00:03<00:00, 70.71it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:03<00:00, 74.36it/s]Testing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:03<00:00, 76.18it/s]Testing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [00:03<00:00, 77.17it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   9%|â–‰         | 143/1600 [00:10<01:42, 14.27it/s]Training:  19%|â–ˆâ–‰        | 306/1600 [00:20<01:23, 15.43it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 488/1600 [00:30<01:06, 16.69it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 677/1600 [00:40<00:52, 17.53it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:50<00:40, 18.06it/s]Training:  67%|â–ˆâ–ˆâTraining loss: 193.3525, Training accuracy: 0.9550
Macro F1-score: 0.9549
Model performance on Angry speech (in training): 
	Precision: 0.9353, Recall: 0.9400, F1_score: 0.9377
Model performance on Happy speech (in training): 
	Precision: 0.9268, Recall: 0.9175, F1_score: 0.9221
Model performance on Neutral speech (in training): 
	Precision: 0.9725, Recall: 0.9725, F1_score: 0.9725
Model performance on Sad speech (in training): 
	Precision: 0.9851, Recall: 0.9900, F1_score: 0.9875

Eval Phase: 
Validation loss: 44.5219, Validation accuracy: 0.9100
Macro F1-score: 0.9124
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.7619, Recall: 0.9600, F1_score: 0.8496
Model performance on Neutral speech (in validation): 
	Precision: 0.9592, Recall: 0.9400, F1_score: 0.9495
Model performance on Sad speech (in validation): 
	Precision: 0.9783, Recall: 0.9000, F1_score: 0.9375
New best accuracy for layer 6 on epoch 1: 0.9100. Model saved.
Epoch 2/100

Training Phase:
Training loss: 89.9800, Training accuracy: 0.9831
Macro F1-score: 0.9831
Model performance on Angry speech (in training): 
	Precision: 0.9728, Recall: 0.9850, F1_score: 0.9789
Model performance on Happy speech (in training): 
	Precision: 0.9823, Recall: 0.9700, F1_score: 0.9761
Model performance on Neutral speech (in training): 
	Precision: 0.9874, Recall: 0.9800, F1_score: 0.9837
Model performance on Sad speech (in training): 
	Precision: 0.9901, Recall: 0.9975, F1_score: 0.9938

Eval Phase: 
–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1071/1600 [01:00<00:28, 18.81it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1287/1600 [01:10<00:15, 19.71it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1508/1600 [01:20<00:04, 20.45it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  15%|â–ˆâ–        | 233/1600 [00:10<00:58, 23.23it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 466/1600 [00:20<00:49, 23.07it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 696/1600 [00:30<00:39, 22.89it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 924/1600 [00:40<00:29, 22.83it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1154/1600 [00:50<00:19, 22.86it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1384/1600 [01:00<00:09, 22.90it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                          Validation loss: 36.3840, Validation accuracy: 0.9150
Macro F1-score: 0.9182
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Model performance on Happy speech (in validation): 
	Precision: 0.7538, Recall: 0.9800, F1_score: 0.8522
Model performance on Neutral speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
New best accuracy for layer 6 on epoch 2: 0.9150. Model saved.
Epoch 3/100

Training Phase:
Training loss: 77.5501, Training accuracy: 0.9825
Macro F1-score: 0.9825
Model performance on Angry speech (in training): 
	Precision: 0.9799, Recall: 0.9775, F1_score: 0.9787
Model performance on Happy speech (in training): 
	Precision: 0.9653, Recall: 0.9750, F1_score: 0.9701
Model performance on Neutral speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950

Eval Phase: 
Validation loss: 63.7089, Validation accuracy: 0.8850
Macro F1-score: 0.8891
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7600, F1_score: 0.8636
Model performance on Happy speech (in validation): 
	Precision: 0.6957, Recall: 0.9600, F1_score: 0.8067
Model performance on Neutral speech (in validation): 
	Precision: 0.9583, Recall: 0.9200, F1_score: 0.9388
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Epoch 4/100

Training Phase:
         Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 230/1600 [00:10<00:59, 22.95it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 460/1600 [00:20<00:49, 22.86it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:30<00:39, 22.91it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 920/1600 [00:40<00:29, 22.85it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1148/1600 [00:50<00:19, 22.74it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1378/1600 [01:00<00:09, 22.79it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.75it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 436/1600 [00:20<00:53, 21.77it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 654/1600 [00:30<00:43, 21.67it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 873/1600 [00:40<00:33, 21.74it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Training loss: 69.0058, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9899, Recall: 0.9800, F1_score: 0.9849
Model performance on Happy speech (in training): 
	Precision: 0.9800, Recall: 0.9825, F1_score: 0.9813
Model performance on Neutral speech (in training): 
	Precision: 0.9826, Recall: 0.9900, F1_score: 0.9863
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950

Eval Phase: 
Validation loss: 50.8706, Validation accuracy: 0.9300
Macro F1-score: 0.9294
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Model performance on Happy speech (in validation): 
	Precision: 0.8136, Recall: 0.9600, F1_score: 0.8807
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
New best accuracy for layer 6 on epoch 4: 0.9300. Model saved.
Epoch 5/100

Training Phase:
Training loss: 52.1199, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9801, Recall: 0.9875, F1_score: 0.9838
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950

Eval Phase: 
Validation loss: 21.2229, Validation accuracy: 0.9650
Macro F1-score: 0.9650
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9412, Recall: 0.9600, F1_score: 0.9505
Model performance on Neutral speech (in validation): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
New best accuracy for layer 6 on epoch 5: 0.9650. Model saved.
Epoch 6/100

Training Phase:
Š   | 1092/1600 [00:50<00:23, 21.60it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1311/1600 [01:00<00:13, 21.67it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1530/1600 [01:10<00:03, 21.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 230/1600 [00:10<00:59, 22.97it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 460/1600 [00:20<00:49, 22.94it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:30<00:39, 22.97it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 921/1600 [00:40<00:29, 22.99it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1152/1600 [00:50<00:19, 22.90it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1380/1600 [01:00<00:09, 22.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training loss: 56.9511, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Neutral speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925

Eval Phase: 
Validation loss: 23.0158, Validation accuracy: 0.9550
Macro F1-score: 0.9549
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Happy speech (in validation): 
	Precision: 0.8889, Recall: 0.9600, F1_score: 0.9231
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 7/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.80it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 438/1600 [00:20<00:53, 21.79it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 656/1600 [00:30<00:43, 21.75it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 875/1600 [00:40<00:33, 21.81it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1094/1600 [00:50<00:23, 21.69it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1312/1600 [01:00<00:13, 21.70it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1530/1600 [01:10<00:03, 21.71it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 222/1600 [00:10<01:02, 22.13it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 451/1600 [00:20<00:50, 22.56it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 680/1600 [00:30<00:40, 22.69it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | Training loss: 28.7337, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 104.8247, Validation accuracy: 0.8950
Macro F1-score: 0.9001
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Happy speech (in validation): 
	Precision: 0.7042, Recall: 1.0000, F1_score: 0.8264
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8600, F1_score: 0.9247
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Epoch 8/100

Training Phase:
910/1600 [00:40<00:30, 22.79it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1140/1600 [00:50<00:20, 22.77it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1369/1600 [01:00<00:10, 22.81it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1598/1600 [01:10<00:00, 22.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.90it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 438/1600 [00:20<00:53, 21.81it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 659/1600 [00:30<00:42, 21.90it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 880/1600 [00:40<00:32, 21.83it/s]Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1099/1600 [00:50<00:22, 21.83it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1318/1600 [01:00<00:12, 21.76it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1539/1600 [01:10<00:02, 21.85it/s]                   Training loss: 49.6038, Training accuracy: 0.9912
Macro F1-score: 0.9913
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Happy speech (in training): 
	Precision: 0.9850, Recall: 0.9875, F1_score: 0.9863
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 12.3223, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 6 on epoch 8: 0.9800. Model saved.
Epoch 9/100

Training Phase:
Training loss: 23.4609, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 110.3599, Validation accuracy: 0.8950
Macro F1-score: 0.8940
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6600, F1_score: 0.7952
Model performance on Happy speech (in validation): 
	Precision: 0.7164, Recall: 0.9600, F1_score: 0.8205
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 10/100

Training Phase:
                                          Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 217/1600 [00:10<01:03, 21.69it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 434/1600 [00:20<00:53, 21.68it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 651/1600 [00:30<00:44, 21.27it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 872/1600 [00:40<00:33, 21.58it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1093/1600 [00:50<00:23, 21.56it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1311/1600 [01:00<00:13, 21.64it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1545/1600 [01:10<00:02, 22.20it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 225/1600 [00:10<01:01, 22.41it/s]Training:  28%|â–ˆâ–ˆâ–Š    Training loss: 29.2650, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937

Eval Phase: 
Validation loss: 85.8200, Validation accuracy: 0.9100
Macro F1-score: 0.9099
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.9600, F1_score: 0.8421
Model performance on Neutral speech (in validation): 
	Precision: 0.9608, Recall: 0.9800, F1_score: 0.9703
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 11/100

Training Phase:
   | 453/1600 [00:20<00:50, 22.63it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 685/1600 [00:30<00:40, 22.86it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 917/1600 [00:40<00:30, 22.29it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1132/1600 [00:51<00:21, 21.66it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1344/1600 [01:01<00:11, 21.48it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1560/1600 [01:11<00:01, 21.52it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 220/1600 [00:10<01:02, 21.97it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 440/1600 [00:20<00:53, 21.66it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 658/1600 [00:30<00:43, 21.72it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 876/1600 [00:40<00:33, 21.66it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1094/1600 [00:50<00:23, 21.69it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâTraining loss: 48.4728, Training accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9850, F1_score: 0.9838
Model performance on Happy speech (in training): 
	Precision: 0.9849, Recall: 0.9775, F1_score: 0.9812
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 84.0030, Validation accuracy: 0.9000
Macro F1-score: 0.9015
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7200, F1_score: 0.8372
Model performance on Happy speech (in validation): 
	Precision: 0.7206, Recall: 0.9800, F1_score: 0.8305
Model performance on Neutral speech (in validation): 
	Precision: 0.9787, Recall: 0.9200, F1_score: 0.9485
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 12/100

Training Phase:
Training loss: 13.1867, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
–ˆâ–ˆâ– | 1312/1600 [01:00<00:13, 21.67it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1532/1600 [01:10<00:03, 21.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 218/1600 [00:10<01:03, 21.73it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 437/1600 [00:20<00:53, 21.82it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 656/1600 [00:30<00:43, 21.77it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 884/1600 [00:40<00:32, 22.16it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1112/1600 [00:50<00:21, 22.39it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1342/1600 [01:00<00:11, 22.58it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1572/1600 [01:10<00:01, 22.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                  Validation loss: 41.2125, Validation accuracy: 0.9600
Macro F1-score: 0.9599
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8800, F1_score: 0.9362
Model performance on Happy speech (in validation): 
	Precision: 0.8889, Recall: 0.9600, F1_score: 0.9231
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 13/100

Training Phase:
Training loss: 33.7381, Training accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9950, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9900, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975

Eval Phase: 
Validation loss: 181.5596, Validation accuracy: 0.8500
Macro F1-score: 0.8489
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.5800, F1_score: 0.7342
Model performance on Happy speech (in validation): 
	Precision: 0.6486, Recall: 0.9600, F1_score: 0.7742
Model performance on Neutral speech (in validation): 
	Precision: 0.9362, Recall: 0.8800, F1_score: 0.9072
Model performance on Sad speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Epoch 14/100

Training Phase:
 Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 215/1600 [00:10<01:04, 21.44it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 434/1600 [00:20<00:53, 21.70it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [00:30<00:43, 21.61it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 873/1600 [00:40<00:33, 21.74it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1093/1600 [00:50<00:23, 21.72it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1310/1600 [01:00<00:13, 21.65it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1528/1600 [01:10<00:03, 21.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.87it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 438/1600 [00:20<00:53, 21.58it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 654/1600 [00:30<00:43, 21.57it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    |Training loss: 17.5691, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 0.9925, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 103.1665, Validation accuracy: 0.8850
Macro F1-score: 0.8867
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6800, F1_score: 0.8095
Model performance on Happy speech (in validation): 
	Precision: 0.6901, Recall: 0.9800, F1_score: 0.8099
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9000, F1_score: 0.9474
Epoch 15/100

Training Phase:
 872/1600 [00:40<00:33, 21.64it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1090/1600 [00:50<00:23, 21.64it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1309/1600 [01:00<00:13, 21.70it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1528/1600 [01:10<00:03, 21.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 216/1600 [00:10<01:04, 21.59it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 432/1600 [00:20<00:54, 21.57it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 649/1600 [00:30<00:43, 21.63it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:40<00:33, 21.70it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1092/1600 [00:50<00:23, 21.94it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1317/1600 [01:00<00:12, 22.11it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1546/1600 [01:10<00:02, 22.36it/s]                    Training loss: 33.7781, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 53.6595, Validation accuracy: 0.9450
Macro F1-score: 0.9451
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.8421, Recall: 0.9600, F1_score: 0.8972
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Epoch 16/100

Training Phase:
Training loss: 18.3500, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 63.5568, Validation accuracy: 0.9350
Macro F1-score: 0.9342
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7600, F1_score: 0.8636
Model performance on Happy speech (in validation): 
	Precision: 0.8033, Recall: 0.9800, F1_score: 0.8829
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 17/100

Training Phase:
                                         Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 224/1600 [00:10<01:01, 22.38it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 449/1600 [00:20<00:51, 22.42it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 674/1600 [00:30<00:41, 22.38it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 674/1600 [00:40<00:41, 22.38it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 898/1600 [00:40<00:31, 22.30it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1123/1600 [00:50<00:21, 22.35it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1600 [01:00<00:11, 22.37it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1573/1600 [01:10<00:01, 22.33it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Training loss: 17.2200, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962

Eval Phase: 
Validation loss: 87.2675, Validation accuracy: 0.9100
Macro F1-score: 0.9100
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7600, F1_score: 0.8636
Model performance on Happy speech (in validation): 
	Precision: 0.7742, Recall: 0.9600, F1_score: 0.8571
Model performance on Neutral speech (in validation): 
	Precision: 0.9583, Recall: 0.9200, F1_score: 0.9388
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Epoch 18/100

Training Phase:
        | 228/1600 [00:10<01:00, 22.74it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 456/1600 [00:20<00:51, 22.40it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 678/1600 [00:30<00:41, 22.15it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 897/1600 [00:40<00:32, 21.85it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1120/1600 [00:50<00:21, 21.98it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1345/1600 [01:00<00:11, 22.14it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1571/1600 [01:10<00:01, 22.29it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 225/1600 [00:10<01:01, 22.44it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 450/1600 [00:20<00:51, 22.38it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 675/1600 [00:30<00:41, 22.39it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 900/1600 [00:40<00:31, 22.07it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰Training loss: 26.0981, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 146.2074, Validation accuracy: 0.8700
Macro F1-score: 0.8656
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.6200, F1_score: 0.7654
Model performance on Happy speech (in validation): 
	Precision: 0.7353, Recall: 1.0000, F1_score: 0.8475
Model performance on Neutral speech (in validation): 
	Precision: 0.9773, Recall: 0.8600, F1_score: 0.9149
Model performance on Sad speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9800

Test Phase: 
   | 1118/1600 [00:50<00:21, 21.94it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1336/1600 [01:00<00:12, 21.82it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1553/1600 [01:10<00:02, 21.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   2%|â–         | 4/200 [00:00<00:05, 32.74it/s]Testing:   4%|â–         | 8/200 [00:00<00:05, 34.96it/s]Testing:   6%|â–Œ         | 12/200 [00:00<00:05, 36.03it/s]Testing:   8%|â–Š         | 16/200 [00:00<00:05, 33.41it/s]Testing:  10%|â–ˆ         | 20/200 [00:00<00:05, 34.99it/s]Testing:  12%|â–ˆâ–        | 24/200 [00:00<00:04, 36.49it/s]Testing:  14%|â–ˆâ–        | 28/200 [00:00<00:05, 34.08it/s]Testing:  17%|â–ˆâ–‹        | 34/200 [00:00<00:04, 39.45it/s]Testing:  19%|â–ˆâ–‰        | 38/200 [00:01<00:04, 37.12it/s]Testing:  22%|â–ˆâ–ˆâ–       | 44/200 [00:01<00:03, 42.64it/s]Testing:  24%|â–ˆâ–ˆâ–       | 49/200 [00:01<00:03, 38.45it/s]Testing:  27%|â–ˆâ–ˆâ–‹       | 54/200 [00:01<00:03, 40.77it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:01<00:03, 45.00it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [00:01<00:03, 44.29it/s]Testing:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:01<00:03, 42.15it/s]Testing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [00:01<00:02, 43.23it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:01<00:02, 46.29it/s]Testing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:02<00:02, 43.14it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [00:02<00:02, 44.57it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [00:02<00:01, 52.19it/s]Testing:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [00:02<00:01, 49.77it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:02<00:01, 51.49it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [00:02<00:01, 52.46it/s]Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [00:02<00:01, 52.97it/s]Testing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [00:02<00:01, 60.85it/s]Test loss: 18.8097, Test accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Happy speech (in test): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Neutral speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

======================= This is fold_4 on cn =======================

Load dataset: 
Loading cn train data: fold_4...
Preprocess cn fold_4 data for cn model
Loading cn eval data: fold_4...
Preprocess cn fold_4 data for cn model
Loading cn test data: fold_4...
Preprocess cn fold_4 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Testing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [00:03<00:00, 64.00it/s]Testing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:03<00:00, 68.18it/s]Testing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:03<00:00, 71.11it/s]Testing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:03<00:00, 69.81it/s]Testing:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [00:03<00:00, 69.29it/s]Testing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:03<00:00, 72.12it/s]Testing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:03<00:00, 74.78it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:03<00:00, 75.90it/s]                                                          Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   9%|â–‰         | 147/1600 [00:10<01:38, 14.69it/s]Training:  20%|â–ˆâ–ˆ        | 323/1600 [00:20<01:17, 16.40it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 517/1600 [00:30<01:00, 17.76it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 721/1600 [00:40<00:46, 18.77it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Training loss: 164.4297, Training accuracy: 0.9581
Macro F1-score: 0.9581
Model performance on Angry speech (in training): 
	Precision: 0.9433, Recall: 0.9575, F1_score: 0.9504
Model performance on Happy speech (in training): 
	Precision: 0.9437, Recall: 0.9225, F1_score: 0.9330
Model performance on Neutral speech (in training): 
	Precision: 0.9556, Recall: 0.9675, F1_score: 0.9615
Model performance on Sad speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875

Eval Phase: 
Validation loss: 11.0423, Validation accuracy: 0.9850
Macro F1-score: 0.9849
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 6 on epoch 1: 0.9850. Model saved.
Epoch 2/100

Training Phase:
Š    | 926/1600 [00:50<00:34, 19.36it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1136/1600 [01:00<00:23, 19.90it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1136/1600 [01:10<00:23, 19.90it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1339/1600 [01:10<00:13, 20.01it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1548/1600 [01:20<00:02, 20.28it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 214/1600 [00:10<01:05, 21.31it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 428/1600 [00:20<00:54, 21.32it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 644/1600 [00:30<00:44, 21.42it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 860/1600 [00:40<00:34, 21.49it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1082/1600 [00:50<00:23, 21.72it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1309/1600 [01:00<00:13, 22.04it/s]Training:  96%|â–ˆâ–Training loss: 95.3432, Training accuracy: 0.9788
Macro F1-score: 0.9787
Model performance on Angry speech (in training): 
	Precision: 0.9726, Recall: 0.9750, F1_score: 0.9738
Model performance on Happy speech (in training): 
	Precision: 0.9722, Recall: 0.9625, F1_score: 0.9673
Model performance on Neutral speech (in training): 
	Precision: 0.9802, Recall: 0.9900, F1_score: 0.9851
Model performance on Sad speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887

Eval Phase: 
Validation loss: 17.1762, Validation accuracy: 0.9600
Macro F1-score: 0.9594
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.8400, F1_score: 0.9130
Model performance on Happy speech (in validation): 
	Precision: 0.8772, Recall: 1.0000, F1_score: 0.9346
Model performance on Neutral speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 3/100

Training Phase:
Training loss: 68.3258, Training accuracy: 0.9869
Macro F1-score: 0.9869
Model performance on Angry speech (in training): 
	Precision: 0.9774, Recall: 0.9750, F1_score: 0.9762
Model performance on Happy speech (in training): 
	Precision: 0.9775, Recall: 0.9775, F1_score: 0.9775
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1540/1600 [01:10<00:02, 22.38it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 225/1600 [00:10<01:01, 22.43it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 453/1600 [00:20<00:50, 22.62it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 681/1600 [00:30<00:40, 22.70it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 909/1600 [00:40<00:30, 22.74it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1137/1600 [00:50<00:20, 22.72it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1367/1600 [01:00<00:10, 22.81it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1367/1600 [01:10<00:10, 22.81it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1595/1600 [01:10<00:00, 22.75it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                  Validation loss: 13.0885, Validation accuracy: 0.9800
Macro F1-score: 0.9799
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Epoch 4/100

Training Phase:
Training loss: 41.2248, Training accuracy: 0.9925
Macro F1-score: 0.9925
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9925, F1_score: 0.9937
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912

Eval Phase: 
Validation loss: 3.9961, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
New best accuracy for layer 6 on epoch 4: 0.9900. Model saved.
Epoch 5/100

Training Phase:
                 Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 230/1600 [00:10<00:59, 22.92it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 460/1600 [00:20<00:49, 22.93it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:30<00:40, 22.72it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 918/1600 [00:40<00:29, 22.75it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1147/1600 [00:50<00:19, 22.78it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1376/1600 [01:00<00:09, 22.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 228/1600 [00:10<01:00, 22.77it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 456/1600 [00:20<00:50, 22.63it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 684/1600 [00:30<00:40, 22.66it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 912/1600 [00:40<00:30, 22.68it/s]Training:  71%|â–ˆâ–ˆâ–ˆâTraining loss: 38.1631, Training accuracy: 0.9919
Macro F1-score: 0.9919
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9900, F1_score: 0.9900
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Sad speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938

Eval Phase: 
Validation loss: 36.9424, Validation accuracy: 0.9450
Macro F1-score: 0.9430
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.7800, F1_score: 0.8764
Model performance on Happy speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Neutral speech (in validation): 
	Precision: 0.8929, Recall: 1.0000, F1_score: 0.9434
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 6/100

Training Phase:
Training loss: 25.2104, Training accuracy: 0.9962
Macro F1-score: 0.9962
Model performance on Angry speech (in training): 
	Precision: 0.9975, Recall: 0.9950, F1_score: 0.9962
Model performance on Happy speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
–ˆâ–ˆâ–ˆâ–ˆâ–  | 1141/1600 [00:50<00:20, 22.74it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1370/1600 [01:00<00:10, 22.74it/s]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1598/1600 [01:10<00:00, 22.73it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 227/1600 [00:10<01:00, 22.61it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 454/1600 [00:20<00:51, 22.37it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 677/1600 [00:30<00:41, 22.07it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 895/1600 [00:40<00:32, 21.95it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1113/1600 [00:50<00:22, 21.80it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1330/1600 [01:00<00:12, 21.74it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1547/1600 [01:10<00:02, 21.71it/s]                                                             Evaluating:  Validation loss: 8.2946, Validation accuracy: 0.9900
Macro F1-score: 0.9900
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 7/100

Training Phase:
Training loss: 31.0715, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Happy speech (in training): 
	Precision: 0.9899, Recall: 0.9850, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 8.3150, Validation accuracy: 0.9800
Macro F1-score: 0.9800
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9434, Recall: 1.0000, F1_score: 0.9709
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 8/100

Training Phase:
 0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 217/1600 [00:10<01:03, 21.69it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 434/1600 [00:20<00:54, 21.51it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 651/1600 [00:30<00:43, 21.58it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:40<00:33, 21.57it/s]Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1084/1600 [00:50<00:23, 21.57it/s]Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1303/1600 [01:00<00:13, 21.66it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1522/1600 [01:10<00:03, 21.66it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 224/1600 [00:10<01:01, 22.36it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 455/1600 [00:20<00:50, 22.77it/s]Training:  43%|â–ˆTraining loss: 18.2526, Training accuracy: 0.9975
Macro F1-score: 0.9975
Model performance on Angry speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Happy speech (in training): 
	Precision: 0.9950, Recall: 0.9950, F1_score: 0.9950
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 15.4326, Validation accuracy: 0.9650
Macro F1-score: 0.9651
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Happy speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Neutral speech (in validation): 
	Precision: 0.9423, Recall: 0.9800, F1_score: 0.9608
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Epoch 9/100

Training Phase:
â–ˆâ–ˆâ–ˆâ–Ž     | 686/1600 [00:30<00:40, 22.82it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 915/1600 [00:40<00:29, 22.84it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1144/1600 [00:50<00:20, 22.79it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1374/1600 [01:00<00:09, 22.85it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–Ž        | 219/1600 [00:10<01:03, 21.85it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 446/1600 [00:20<00:51, 22.30it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 673/1600 [00:30<00:41, 22.30it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 897/1600 [00:40<00:31, 22.33it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1127/1600 [00:50<00:20, 22.56it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1357/1600 [01:00<00:10, 22.64it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1591/1600 [01:10<00:00, 22.87it/s]       Training loss: 39.2858, Training accuracy: 0.9912
Macro F1-score: 0.9912
Model performance on Angry speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9850, F1_score: 0.9862
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 0.9975, F1_score: 0.9975

Eval Phase: 
Validation loss: 5.3724, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 10/100

Training Phase:
Training loss: 20.6239, Training accuracy: 0.9956
Macro F1-score: 0.9956
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9925, F1_score: 0.9925
Model performance on Happy speech (in training): 
	Precision: 0.9900, Recall: 0.9925, F1_score: 0.9913
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 65.4267, Validation accuracy: 0.9100
Macro F1-score: 0.9060
Model performance on Angry speech (in validation): 
	Precision: 0.9231, Recall: 0.9600, F1_score: 0.9412
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.6800, F1_score: 0.8095
Model performance on Neutral speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Sad speech (in validation): 
	Precision: 0.8065, Recall: 1.0000, F1_score: 0.8929
Epoch 11/100

Training Phase:
                                                      Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 232/1600 [00:10<00:59, 23.18it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 464/1600 [00:20<00:49, 23.05it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 694/1600 [00:30<00:39, 22.90it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 924/1600 [00:40<00:29, 22.94it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1154/1600 [00:50<00:19, 22.88it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1382/1600 [01:00<00:09, 22.80it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  13%|â–ˆâ–Ž        | 211/1600 [00:10<01:05, 21.06it/s]Training:  27%|â–ˆâ–ˆâ–‹       | 428/1600 [00:20<00:54, 21.43it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆTraining loss: 24.0768, Training accuracy: 0.9944
Macro F1-score: 0.9944
Model performance on Angry speech (in training): 
	Precision: 0.9900, Recall: 0.9875, F1_score: 0.9887
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9900, F1_score: 0.9888
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000

Eval Phase: 
Validation loss: 10.1572, Validation accuracy: 0.9850
Macro F1-score: 0.9850
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Happy speech (in validation): 
	Precision: 0.9615, Recall: 1.0000, F1_score: 0.9804
Model performance on Neutral speech (in validation): 
	Precision: 0.9800, Recall: 0.9800, F1_score: 0.9800
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 12/100

Training Phase:
      | 655/1600 [00:30<00:42, 21.98it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 885/1600 [00:40<00:31, 22.35it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1119/1600 [00:50<00:21, 22.70it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1353/1600 [01:00<00:10, 22.50it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1353/1600 [01:10<00:10, 22.50it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1573/1600 [01:10<00:01, 22.22it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 231/1600 [00:10<00:59, 23.04it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 462/1600 [00:20<00:49, 22.77it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 688/1600 [00:30<00:40, 22.67it/s]Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 917/1600 [00:40<00:30, 22.76it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1146/1600 [00:50<00:19, 22.74it/s]Training:  86%|â–ˆTraining loss: 2.2038, Training accuracy: 0.9994
Macro F1-score: 0.9994
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Neutral speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987
Model performance on Sad speech (in training): 
	Precision: 0.9975, Recall: 1.0000, F1_score: 0.9988

Eval Phase: 
Validation loss: 18.3895, Validation accuracy: 0.9600
Macro F1-score: 0.9601
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9200, F1_score: 0.9583
Model performance on Happy speech (in validation): 
	Precision: 0.9259, Recall: 1.0000, F1_score: 0.9615
Model performance on Neutral speech (in validation): 
	Precision: 0.9245, Recall: 0.9800, F1_score: 0.9515
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Epoch 13/100

Training Phase:
Training loss: 19.8368, Training accuracy: 0.9969
Macro F1-score: 0.9969
Model performance on Angry speech (in training): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in training): 
	Precision: 1.0000, Recall: 0.9950, F1_score: 0.9975
Model performance on Neutral speech (in training): 
	Precision: 0.9925, Recall: 0.9950, F1_score: 0.9938
Model performance on Sad speech (in training): 
	Precision: 0.9950, Recall: 0.9975, F1_score: 0.9963

Eval Phase: 
Validation loss: 24.5324, Validation accuracy: 0.9750
Macro F1-score: 0.9753
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.9400, F1_score: 0.9691
Model performance on Happy speech (in validation): 
	Precision: 0.9091, Recall: 1.0000, F1_score: 0.9524
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.9600, F1_score: 0.9796
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Epoch 14/100

Training Phase:
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1375/1600 [01:00<00:09, 22.76it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  14%|â–ˆâ–        | 227/1600 [00:10<01:00, 22.69it/s]Training:  28%|â–ˆâ–ˆâ–Š       | 454/1600 [00:20<00:50, 22.69it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 681/1600 [00:30<00:41, 22.19it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 898/1600 [00:40<00:31, 21.94it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1114/1600 [00:50<00:22, 21.74it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1340/1600 [01:00<00:11, 22.02it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1570/1600 [01:10<00:01, 22.30it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training: Training loss: 40.8792, Training accuracy: 0.9938
Macro F1-score: 0.9937
Model performance on Angry speech (in training): 
	Precision: 0.9925, Recall: 0.9900, F1_score: 0.9912
Model performance on Happy speech (in training): 
	Precision: 0.9875, Recall: 0.9875, F1_score: 0.9875
Model performance on Neutral speech (in training): 
	Precision: 0.9950, Recall: 1.0000, F1_score: 0.9975
Model performance on Sad speech (in training): 
	Precision: 1.0000, Recall: 0.9975, F1_score: 0.9987

Eval Phase: 
Validation loss: 29.4980, Validation accuracy: 0.9350
Macro F1-score: 0.9343
Model performance on Angry speech (in validation): 
	Precision: 0.8909, Recall: 0.9800, F1_score: 0.9333
Model performance on Happy speech (in validation): 
	Precision: 0.8868, Recall: 0.9400, F1_score: 0.9126
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.8200, F1_score: 0.9011
Model performance on Sad speech (in validation): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.9900

Test Phase: 
 14%|â–ˆâ–        | 228/1600 [00:10<01:00, 22.75it/s]Training:  29%|â–ˆâ–ˆâ–‰       | 460/1600 [00:20<00:49, 22.96it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 692/1600 [00:30<00:39, 23.00it/s]Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 923/1600 [00:40<00:29, 22.98it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1153/1600 [00:50<00:19, 22.92it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1382/1600 [01:00<00:09, 22.81it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:11, 17.07it/s]Testing:   2%|â–Ž         | 5/200 [00:00<00:08, 22.65it/s]Testing:   4%|â–         | 9/200 [00:00<00:06, 27.65it/s]Testing:   6%|â–Œ         | 12/200 [00:00<00:06, 28.23it/s]Testing:   8%|â–Š         | 15/200 [00:00<00:06, 27.63it/s]Testing:  10%|â–‰         | 19/200 [00:00<00:06, 30.16it/s]Testing:  12%|â–ˆâ–        | 24/200 [00:00<00:04, 35.92it/s]Testing:  15%|â–ˆâ–Œ        | 30/200 [00:00<00:04, 39.27it/s]Testing:  18%|â–ˆâ–Š        | 36/200 [00:01<00:03, 45.04it/s]Testing:  21%|â–ˆâ–ˆ        | 42/200 [00:01<00:03, 47.69it/s]Testing:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [00:01<00:03, 48.14it/s]Testing:  27%|â–ˆâ–ˆâ–‹       | 54/200 [00:01<00:02, 53.21it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:01<00:02, 55.28it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:01<00:02, 59.92it/s]Testing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [00:01<00:02, 53.50it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:01<00:02, 53.43it/s]Testing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [00:01<00:02, 53.16it/s]Testing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [00:02<00:01, 56.50it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [00:02<00:01, 56.76it/s]Testing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [00:02<00:01, 59.78it/s]Testing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:02<00:01, 60.13it/s]Testing:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [00:02<00:01, 64.16itTest loss: 2.8977, Test accuracy: 0.9950
Macro F1-score: 0.9950
Model performance on Angry speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Happy speech (in test): 
	Precision: 1.0000, Recall: 0.9800, F1_score: 0.9899
Model performance on Neutral speech (in test): 
	Precision: 1.0000, Recall: 1.0000, F1_score: 1.0000
Model performance on Sad speech (in test): 
	Precision: 0.9804, Recall: 1.0000, F1_score: 0.9901

cn, all folds layer accuracy: ['0.9600', '0.8900', '0.8700', '0.9800', '0.9950']
cn, all emo precision: {'Angry': ['0.9592', '1.0000', '0.6957', '1.0000', '1.0000'], 'Happy': ['0.8889', '0.9778', '1.0000', '0.9259', '1.0000'], 'Neutral': ['1.0000', '0.7042', '0.9091', '1.0000', '1.0000'], 'Sad': ['1.0000', '1.0000', '1.0000', '1.0000', '0.9804']}
cn, all emo recall: {'Angry': ['0.9400', '0.9200', '0.9600', '0.9200', '1.0000'], 'Happy': ['0.9600', '0.8800', '0.5800', '1.0000', '0.9800'], 'Neutral': ['0.9400', '1.0000', '1.0000', '1.0000', '1.0000'], 'Sad': ['1.0000', '0.7600', '0.9400', '1.0000', '1.0000']}
cn, all emo f1score: {'Angry': ['0.9495', '0.9583', '0.8067', '0.9583', '1.0000'], 'Happy': ['0.9231', '0.9263', '0.7342', '0.9615', '0.9899'], 'Neutral': ['0.9691', '0.8264', '0.9524', '1.0000', '1.0000'], 'Sad': ['1.0000', '0.8636', '0.9691', '1.0000', '0.9901']}
/s]Testing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [00:02<00:01, 66.05it/s]Testing:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [00:02<00:00, 67.75it/s]Testing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [00:02<00:00, 70.55it/s]Testing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:02<00:00, 71.84it/s]Testing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [00:03<00:00, 73.70it/s]Testing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [00:03<00:00, 75.20it/s]Testing:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [00:03<00:00, 75.49it/s]Testing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [00:03<00:00, 76.33it/s]Testing:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:03<00:00, 73.68it/s]                                                          ------------------NEXT SCRIPT: RUNNER_EN----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911927/matplotlib-0ad2ouio because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on en =======================

Load dataset: 
Loading en train data: fold_0...
Preprocess en fold_0 data for en model
Loading en eval data: fold_0...
Preprocess en fold_0 data for en model
Loading en test data: fold_0...
Preprocess en fold_0 data for en model
Use en model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   0%|          | 1/1600 [00:38<17:15:56, 38.87s/it]Training:   6%|â–‹         | 103/1600 [00:48<09:02,  2.76it/s] Training:  15%|â–ˆâ–        | 237/1600 [00:58<03:59,  5.70it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 375/1600 [01:08<02:33,  7.99it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 515/1600 [01:19<01:52,  9.68it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 659/1600 [01:29<01:25, 11.03it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 814/1600 [01:39<01:03, 12.32it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 970/1600 [01:49<00:47, 13.28it/s]Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1126/1600 [01:59<00:34, 13.71it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1283/1600 [02:09<00:22, 14.27it/s]Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1283/1600 [02:20<00:22, 14.27it/s]Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1436/1600 [02:20<00:11, 14.52it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1591/1600 [02:30<00:00, 14.79it/s]                                                             /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Training loss: 2092.8974, Training accuracy: 0.3625
Macro F1-score: 0.2630
Model performance on Angry speech (in training): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in training): 
	Precision: 0.2897, Recall: 0.7975, F1_score: 0.4250
Model performance on Neutral speech (in training): 
	Precision: 0.4615, Recall: 0.0300, F1_score: 0.0563
Model performance on Sad speech (in training): 
	Precision: 0.5264, Recall: 0.6225, F1_score: 0.5704

Eval Phase: 
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   /work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 240.9834, Validation accuracy: 0.4200
Macro F1-score: 0.3220
Model performance on Angry speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in validation): 
	Precision: 0.3007, Recall: 0.9200, F1_score: 0.4532
Model performance on Neutral speech (in validation): 
	Precision: 1.0000, Recall: 0.0400, F1_score: 0.0769
Model performance on Sad speech (in validation): 
	Precision: 0.8000, Recall: 0.7200, F1_score: 0.7579
New best accuracy for layer 8 on epoch 1: 0.4200. Model saved.
Epoch 2/100

Training Phase:
Training loss: 1959.8398, Training accuracy: 0.4050
Macro F1-score: 0.3240
Model performance on Angry speech (in training): 
	Precision: 0.4615, Recall: 0.0450, F1_score: 0.0820
Model performance on Happy speech (in training): 
	Precision: 0.3149, Recall: 0.6975, F1_score: 0.4339
Model performance on Neutral speech (in training): 
	Precision: 0.2685, Recall: 0.0725, F1_score: 0.1142
Model performance on Sad speech (in training): 
	Precision: 0.5679, Recall: 0.8050, F1_score: 0.6660

Eval Phase: 
Validation loss: 228.2042, Validation accuracy: 0.4450
Macro F1-score: 0.4048
Model performance on Angry speech (in validation): 
	Precision: 0.4167, Recall: 0.3000, F1_score: 0.3488
Model performance on Happy speech (in validation): 
	Precision: 0.3304, Recall: 0.7400, F1_score: 0.4568
Model performance on Neutral speech (in validation): 
	Precision: 0.2500, Recall: 0.0400, F1_score: 0.0690
Model performance on Sad speech (in validation): 
	Precision: 0.7955, Recall: 0.7000, F1_score: 0.7447
New best accuracy for layer 8 on epoch 2: 0.4450. Model saved.
Epoch 3/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 172/1600 [00:10<01:23, 17.15it/s]Training:  22%|â–ˆâ–ˆâ–       | 344/1600 [00:20<01:13, 17.08it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 516/1600 [00:30<01:03, 17.12it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 688/1600 [00:40<00:53, 16.91it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 854/1600 [00:50<00:44, 16.79it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1030/1600 [01:00<00:33, 17.06it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1206/1600 [01:11<00:23, 16.94it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1377/1600 [01:21<00:13, 16.97it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1552/1600 [01:31<00:02, 17.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 166/1600 [00:10<01:26, 16.56it/s]Training:  21%|â–ˆâ–ˆ        | Training loss: 1858.6263, Training accuracy: 0.4731
Macro F1-score: 0.4547
Model performance on Angry speech (in training): 
	Precision: 0.4906, Recall: 0.4550, F1_score: 0.4721
Model performance on Happy speech (in training): 
	Precision: 0.3693, Recall: 0.4025, F1_score: 0.3852
Model performance on Neutral speech (in training): 
	Precision: 0.3734, Recall: 0.2250, F1_score: 0.2808
Model performance on Sad speech (in training): 
	Precision: 0.5870, Recall: 0.8100, F1_score: 0.6807

Eval Phase: 
Validation loss: 236.9157, Validation accuracy: 0.5600
Macro F1-score: 0.5582
Model performance on Angry speech (in validation): 
	Precision: 0.8049, Recall: 0.6600, F1_score: 0.7253
Model performance on Happy speech (in validation): 
	Precision: 0.5333, Recall: 0.3200, F1_score: 0.4000
Model performance on Neutral speech (in validation): 
	Precision: 0.4268, Recall: 0.7000, F1_score: 0.5303
Model performance on Sad speech (in validation): 
	Precision: 0.5957, Recall: 0.5600, F1_score: 0.5773
New best accuracy for layer 8 on epoch 3: 0.5600. Model saved.
Epoch 4/100

Training Phase:
332/1600 [00:20<01:16, 16.56it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 501/1600 [00:30<01:05, 16.71it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 676/1600 [00:40<00:54, 17.01it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 851/1600 [00:50<00:44, 16.95it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1027/1600 [01:00<00:33, 17.15it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1203/1600 [01:10<00:23, 17.15it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1375/1600 [01:20<00:13, 17.13it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1546/1600 [01:30<00:03, 17.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 168/1600 [00:10<01:25, 16.75it/s]Training:  21%|â–ˆâ–ˆâ–       | 341/1600 [00:20<01:13, 17.06it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 514/1600 [00:30<01:03, 17.10it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 686/16Training loss: 1689.4432, Training accuracy: 0.5581
Macro F1-score: 0.5453
Model performance on Angry speech (in training): 
	Precision: 0.6329, Recall: 0.6550, F1_score: 0.6437
Model performance on Happy speech (in training): 
	Precision: 0.4766, Recall: 0.4075, F1_score: 0.4394
Model performance on Neutral speech (in training): 
	Precision: 0.4416, Recall: 0.3500, F1_score: 0.3905
Model performance on Sad speech (in training): 
	Precision: 0.6224, Recall: 0.8200, F1_score: 0.7077

Eval Phase: 
Validation loss: 198.8558, Validation accuracy: 0.5700
Macro F1-score: 0.5676
Model performance on Angry speech (in validation): 
	Precision: 0.9231, Recall: 0.4800, F1_score: 0.6316
Model performance on Happy speech (in validation): 
	Precision: 0.4062, Recall: 0.7800, F1_score: 0.5342
Model performance on Neutral speech (in validation): 
	Precision: 0.4194, Recall: 0.2600, F1_score: 0.3210
Model performance on Sad speech (in validation): 
	Precision: 0.8085, Recall: 0.7600, F1_score: 0.7835
New best accuracy for layer 8 on epoch 4: 0.5700. Model saved.
Epoch 5/100

Training Phase:
00 [00:40<00:54, 16.75it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 856/1600 [00:50<00:44, 16.82it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1026/1600 [01:00<00:34, 16.86it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1198/1600 [01:10<00:23, 16.95it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1371/1600 [01:20<00:13, 17.06it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1544/1600 [01:30<00:03, 17.08it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 175/1600 [00:10<01:21, 17.44it/s]Training:  22%|â–ˆâ–ˆâ–       | 350/1600 [00:20<01:12, 17.15it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 520/1600 [00:30<01:03, 17.06it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:40<00:54, 16.78it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:50<00:42, 17.12it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/16Training loss: 1605.2979, Training accuracy: 0.5713
Macro F1-score: 0.5600
Model performance on Angry speech (in training): 
	Precision: 0.6716, Recall: 0.6800, F1_score: 0.6758
Model performance on Happy speech (in training): 
	Precision: 0.4892, Recall: 0.3950, F1_score: 0.4371
Model performance on Neutral speech (in training): 
	Precision: 0.4363, Recall: 0.3850, F1_score: 0.4090
Model performance on Sad speech (in training): 
	Precision: 0.6358, Recall: 0.8250, F1_score: 0.7182

Eval Phase: 
Validation loss: 174.2639, Validation accuracy: 0.6150
Macro F1-score: 0.6126
Model performance on Angry speech (in validation): 
	Precision: 0.9688, Recall: 0.6200, F1_score: 0.7561
Model performance on Happy speech (in validation): 
	Precision: 0.4568, Recall: 0.7400, F1_score: 0.5649
Model performance on Neutral speech (in validation): 
	Precision: 0.4118, Recall: 0.2800, F1_score: 0.3333
Model performance on Sad speech (in validation): 
	Precision: 0.7736, Recall: 0.8200, F1_score: 0.7961
New best accuracy for layer 8 on epoch 5: 0.6150. Model saved.
Epoch 6/100

Training Phase:
00 [01:00<00:42, 17.12it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1037/1600 [01:00<00:33, 16.94it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1206/1600 [01:11<00:23, 16.91it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1375/1600 [01:21<00:13, 16.86it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1543/1600 [01:31<00:03, 16.69it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 169/1600 [00:10<01:25, 16.81it/s]Training:  21%|â–ˆâ–ˆâ–       | 342/1600 [00:20<01:13, 17.08it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 515/1600 [00:30<01:03, 17.09it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1600 [00:40<00:53, 17.17it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1600 [00:50<00:53, 17.17it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 860/1600 [00:50<00:43, 17.08it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1030/1Training loss: 1507.2590, Training accuracy: 0.6088
Macro F1-score: 0.6003
Model performance on Angry speech (in training): 
	Precision: 0.7079, Recall: 0.7150, F1_score: 0.7114
Model performance on Happy speech (in training): 
	Precision: 0.5491, Recall: 0.4750, F1_score: 0.5094
Model performance on Neutral speech (in training): 
	Precision: 0.4870, Recall: 0.4200, F1_score: 0.4510
Model performance on Sad speech (in training): 
	Precision: 0.6535, Recall: 0.8250, F1_score: 0.7293

Eval Phase: 
Validation loss: 167.1957, Validation accuracy: 0.6800
Macro F1-score: 0.6826
Model performance on Angry speech (in validation): 
	Precision: 0.9429, Recall: 0.6600, F1_score: 0.7765
Model performance on Happy speech (in validation): 
	Precision: 0.6739, Recall: 0.6200, F1_score: 0.6458
Model performance on Neutral speech (in validation): 
	Precision: 0.5179, Recall: 0.5800, F1_score: 0.5472
Model performance on Sad speech (in validation): 
	Precision: 0.6825, Recall: 0.8600, F1_score: 0.7611
New best accuracy for layer 8 on epoch 6: 0.6800. Model saved.
Epoch 7/100

Training Phase:
600 [01:00<00:33, 16.97it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1200/1600 [01:10<00:23, 16.97it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1370/1600 [01:20<00:13, 16.90it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1538/1600 [01:30<00:03, 16.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 175/1600 [00:10<01:21, 17.44it/s]Training:  22%|â–ˆâ–ˆâ–       | 350/1600 [00:20<01:13, 17.09it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 523/1600 [00:30<01:02, 17.18it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 696/1600 [00:40<00:53, 16.94it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 862/1600 [00:51<00:44, 16.64it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1025/1600 [01:01<00:34, 16.53it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1197/1600 [01:11<00:24, 16.73it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆTraining loss: 1423.2383, Training accuracy: 0.6369
Macro F1-score: 0.6316
Model performance on Angry speech (in training): 
	Precision: 0.7356, Recall: 0.7025, F1_score: 0.7187
Model performance on Happy speech (in training): 
	Precision: 0.6117, Recall: 0.4725, F1_score: 0.5331
Model performance on Neutral speech (in training): 
	Precision: 0.5084, Recall: 0.5300, F1_score: 0.5190
Model performance on Sad speech (in training): 
	Precision: 0.6850, Recall: 0.8425, F1_score: 0.7556

Eval Phase: 
Validation loss: 171.3137, Validation accuracy: 0.6550
Macro F1-score: 0.6463
Model performance on Angry speech (in validation): 
	Precision: 0.5556, Recall: 0.9000, F1_score: 0.6870
Model performance on Happy speech (in validation): 
	Precision: 0.6410, Recall: 0.5000, F1_score: 0.5618
Model performance on Neutral speech (in validation): 
	Precision: 0.6471, Recall: 0.4400, F1_score: 0.5238
Model performance on Sad speech (in validation): 
	Precision: 0.8478, Recall: 0.7800, F1_score: 0.8125
Epoch 8/100

Training Phase:
â–Œ | 1369/1600 [01:21<00:13, 16.68it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1536/1600 [01:31<00:03, 16.67it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 170/1600 [00:10<01:24, 16.97it/s]Training:  22%|â–ˆâ–ˆâ–       | 344/1600 [00:20<01:13, 17.19it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 518/1600 [00:30<01:02, 17.22it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 691/1600 [00:40<00:53, 16.88it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 862/1600 [00:50<00:43, 16.95it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1033/1600 [01:00<00:33, 16.87it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1204/1600 [01:10<00:23, 16.93it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1375/1600 [01:21<00:13, 16.90it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1550/1600 [01:31<00:02, 17.06it/s]                       Training loss: 1343.7649, Training accuracy: 0.6631
Macro F1-score: 0.6579
Model performance on Angry speech (in training): 
	Precision: 0.7494, Recall: 0.7775, F1_score: 0.7632
Model performance on Happy speech (in training): 
	Precision: 0.6562, Recall: 0.4725, F1_score: 0.5494
Model performance on Neutral speech (in training): 
	Precision: 0.5369, Recall: 0.5825, F1_score: 0.5588
Model performance on Sad speech (in training): 
	Precision: 0.7084, Recall: 0.8200, F1_score: 0.7601

Eval Phase: 
Validation loss: 164.3258, Validation accuracy: 0.7200
Macro F1-score: 0.7236
Model performance on Angry speech (in validation): 
	Precision: 0.9143, Recall: 0.6400, F1_score: 0.7529
Model performance on Happy speech (in validation): 
	Precision: 0.5738, Recall: 0.7000, F1_score: 0.6306
Model performance on Neutral speech (in validation): 
	Precision: 0.6792, Recall: 0.7200, F1_score: 0.6990
Model performance on Sad speech (in validation): 
	Precision: 0.8039, Recall: 0.8200, F1_score: 0.8119
New best accuracy for layer 8 on epoch 8: 0.7200. Model saved.
Epoch 9/100

Training Phase:
Training loss: 1291.2762, Training accuracy: 0.6744
Macro F1-score: 0.6697
Model performance on Angry speech (in training): 
	Precision: 0.7920, Recall: 0.7900, F1_score: 0.7910
Model performance on Happy speech (in training): 
	Precision: 0.6709, Recall: 0.5250, F1_score: 0.5891
Model performance on Neutral speech (in training): 
	Precision: 0.5533, Recall: 0.5450, F1_score: 0.5491
Model performance on Sad speech (in training): 
	Precision: 0.6781, Recall: 0.8375, F1_score: 0.7494

Eval Phase: 
Validation loss: 211.8053, Validation accuracy: 0.5900
Macro F1-score: 0.5856
Model performance on Angry speech (in validation): 
	Precision: 0.9394, Recall: 0.6200, F1_score: 0.7470
Model performance on Happy speech (in validation): 
	Precision: 0.4272, Recall: 0.8800, F1_score: 0.5752
Model performance on Neutral speech (in validation): 
	Precision: 0.4615, Recall: 0.2400, F1_score: 0.3158
Model performance on Sad speech (in validation): 
	Precision: 0.8158, Recall: 0.6200, F1_score: 0.7045
Epoch 10/100

Training Phase:
                                      Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 172/1600 [00:10<01:23, 17.15it/s]Training:  22%|â–ˆâ–ˆâ–       | 344/1600 [00:20<01:13, 17.06it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 515/1600 [00:30<01:03, 17.05it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 686/1600 [00:40<00:54, 16.86it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 852/1600 [00:50<00:44, 16.68it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1016/1600 [01:00<00:35, 16.53it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1182/1600 [01:10<00:25, 16.55it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1600 [01:20<00:15, 16.50it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1513/1600 [01:30<00:05, 16.49it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   TTraining loss: 1195.3181, Training accuracy: 0.7031
Macro F1-score: 0.7000
Model performance on Angry speech (in training): 
	Precision: 0.8222, Recall: 0.7975, F1_score: 0.8096
Model performance on Happy speech (in training): 
	Precision: 0.6765, Recall: 0.5750, F1_score: 0.6216
Model performance on Neutral speech (in training): 
	Precision: 0.6192, Recall: 0.5975, F1_score: 0.6081
Model performance on Sad speech (in training): 
	Precision: 0.6934, Recall: 0.8425, F1_score: 0.7607

Eval Phase: 
Validation loss: 170.1717, Validation accuracy: 0.7050
Macro F1-score: 0.7041
Model performance on Angry speech (in validation): 
	Precision: 0.6935, Recall: 0.8600, F1_score: 0.7679
Model performance on Happy speech (in validation): 
	Precision: 0.7436, Recall: 0.5800, F1_score: 0.6517
Model performance on Neutral speech (in validation): 
	Precision: 0.5818, Recall: 0.6400, F1_score: 0.6095
Model performance on Sad speech (in validation): 
	Precision: 0.8409, Recall: 0.7400, F1_score: 0.7872
Epoch 11/100

Training Phase:
raining:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 166/1600 [00:10<01:26, 16.59it/s]Training:  21%|â–ˆâ–ˆâ–       | 343/1600 [00:20<01:13, 17.21it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 520/1600 [00:30<01:02, 17.22it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 693/1600 [00:40<00:53, 17.07it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 862/1600 [00:50<00:44, 16.77it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1034/1600 [01:01<00:33, 16.89it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1206/1600 [01:11<00:23, 16.78it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1372/1600 [01:21<00:13, 16.63it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1536/1600 [01:31<00:03, 16.45it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 166/1600 [00:10<01:26, 16.56it/s]Training:  21%|â–ˆâ–ˆ        | 33Training loss: 1099.3606, Training accuracy: 0.7356
Macro F1-score: 0.7332
Model performance on Angry speech (in training): 
	Precision: 0.8300, Recall: 0.8300, F1_score: 0.8300
Model performance on Happy speech (in training): 
	Precision: 0.7477, Recall: 0.6075, F1_score: 0.6703
Model performance on Neutral speech (in training): 
	Precision: 0.6388, Recall: 0.6500, F1_score: 0.6444
Model performance on Sad speech (in training): 
	Precision: 0.7308, Recall: 0.8550, F1_score: 0.7880

Eval Phase: 
Validation loss: 187.2554, Validation accuracy: 0.6450
Macro F1-score: 0.6535
Model performance on Angry speech (in validation): 
	Precision: 0.9310, Recall: 0.5400, F1_score: 0.6835
Model performance on Happy speech (in validation): 
	Precision: 0.4933, Recall: 0.7400, F1_score: 0.5920
Model performance on Neutral speech (in validation): 
	Precision: 0.5690, Recall: 0.6600, F1_score: 0.6111
Model performance on Sad speech (in validation): 
	Precision: 0.8421, Recall: 0.6400, F1_score: 0.7273
Epoch 12/100

Training Phase:
2/1600 [00:20<01:17, 16.37it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 495/1600 [00:30<01:07, 16.30it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 665/1600 [00:40<00:56, 16.56it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 840/1600 [00:50<00:45, 16.88it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 840/1600 [01:00<00:45, 16.88it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1005/1600 [01:00<00:35, 16.73it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1171/1600 [01:10<00:25, 16.64it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1336/1600 [01:20<00:16, 16.37it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1495/1600 [01:31<00:06, 16.13it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–‰         | 156/1600 [00:10<01:33, 15.52it/s]Training:  20%|â–ˆâ–ˆ        | 321/1600 [00:20<01:19, 16.08it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 498/1600 [00Training loss: 1032.2050, Training accuracy: 0.7550
Macro F1-score: 0.7519
Model performance on Angry speech (in training): 
	Precision: 0.8593, Recall: 0.8550, F1_score: 0.8571
Model performance on Happy speech (in training): 
	Precision: 0.7635, Recall: 0.6375, F1_score: 0.6948
Model performance on Neutral speech (in training): 
	Precision: 0.6882, Recall: 0.6400, F1_score: 0.6632
Model performance on Sad speech (in training): 
	Precision: 0.7157, Recall: 0.8875, F1_score: 0.7924

Eval Phase: 
Validation loss: 210.9815, Validation accuracy: 0.6150
Macro F1-score: 0.6240
Model performance on Angry speech (in validation): 
	Precision: 0.9231, Recall: 0.4800, F1_score: 0.6316
Model performance on Happy speech (in validation): 
	Precision: 0.4459, Recall: 0.6600, F1_score: 0.5323
Model performance on Neutral speech (in validation): 
	Precision: 0.5556, Recall: 0.7000, F1_score: 0.6195
Model performance on Sad speech (in validation): 
	Precision: 0.8378, Recall: 0.6200, F1_score: 0.7126
Epoch 13/100

Training Phase:
:30<01:05, 16.80it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 675/1600 [00:40<00:54, 16.83it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 855/1600 [00:50<00:43, 17.21it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1034/1600 [01:00<00:32, 17.30it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1212/1600 [01:10<00:22, 17.44it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1390/1600 [01:21<00:12, 17.49it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1566/1600 [01:31<00:01, 17.24it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 165/1600 [00:10<01:27, 16.48it/s]Training:  21%|â–ˆâ–ˆâ–       | 341/1600 [00:20<01:13, 17.10it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 517/1600 [00:30<01:03, 17.18it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:40<00:53, 17.12it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 865/1600 [00:5Training loss: 997.8933, Training accuracy: 0.7681
Macro F1-score: 0.7662
Model performance on Angry speech (in training): 
	Precision: 0.8696, Recall: 0.8500, F1_score: 0.8597
Model performance on Happy speech (in training): 
	Precision: 0.7908, Recall: 0.6425, F1_score: 0.7090
Model performance on Neutral speech (in training): 
	Precision: 0.6889, Recall: 0.6975, F1_score: 0.6932
Model performance on Sad speech (in training): 
	Precision: 0.7370, Recall: 0.8825, F1_score: 0.8032

Eval Phase: 
Validation loss: 169.4364, Validation accuracy: 0.6950
Macro F1-score: 0.6952
Model performance on Angry speech (in validation): 
	Precision: 0.7170, Recall: 0.7600, F1_score: 0.7379
Model performance on Happy speech (in validation): 
	Precision: 0.6875, Recall: 0.6600, F1_score: 0.6735
Model performance on Neutral speech (in validation): 
	Precision: 0.5882, Recall: 0.6000, F1_score: 0.5941
Model performance on Sad speech (in validation): 
	Precision: 0.7917, Recall: 0.7600, F1_score: 0.7755
Epoch 14/100

Training Phase:
0<00:42, 17.24it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [01:00<00:32, 17.25it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1213/1600 [01:10<00:22, 17.21it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1385/1600 [01:20<00:12, 17.12it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1557/1600 [01:30<00:02, 17.12it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 165/1600 [00:10<01:27, 16.48it/s]Training:  22%|â–ˆâ–ˆâ–       | 345/1600 [00:20<01:12, 17.35it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 525/1600 [00:30<01:01, 17.39it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 700/1600 [00:40<00:52, 17.10it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 873/1600 [00:50<00:42, 17.14it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1047/1600 [01:00<00:32, 17.22it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1221/16Training loss: 919.6932, Training accuracy: 0.7800
Macro F1-score: 0.7782
Model performance on Angry speech (in training): 
	Precision: 0.8985, Recall: 0.8850, F1_score: 0.8917
Model performance on Happy speech (in training): 
	Precision: 0.7845, Recall: 0.6825, F1_score: 0.7299
Model performance on Neutral speech (in training): 
	Precision: 0.7016, Recall: 0.6700, F1_score: 0.6854
Model performance on Sad speech (in training): 
	Precision: 0.7416, Recall: 0.8825, F1_score: 0.8059

Eval Phase: 
Validation loss: 191.6390, Validation accuracy: 0.6700
Macro F1-score: 0.6573
Model performance on Angry speech (in validation): 
	Precision: 0.7000, Recall: 0.8400, F1_score: 0.7636
Model performance on Happy speech (in validation): 
	Precision: 0.6000, Recall: 0.6600, F1_score: 0.6286
Model performance on Neutral speech (in validation): 
	Precision: 0.6333, Recall: 0.3800, F1_score: 0.4750
Model performance on Sad speech (in validation): 
	Precision: 0.7273, Recall: 0.8000, F1_score: 0.7619
Epoch 15/100

Training Phase:
00 [01:11<00:22, 17.07it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1394/1600 [01:21<00:12, 17.13it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1567/1600 [01:31<00:01, 17.10it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 173/1600 [00:10<01:22, 17.28it/s]Training:  11%|â–ˆ         | 173/1600 [00:20<01:22, 17.28it/s]Training:  21%|â–ˆâ–ˆâ–       | 340/1600 [00:20<01:15, 16.72it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 504/1600 [00:30<01:06, 16.56it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 674/1600 [00:40<00:55, 16.70it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1600 [00:50<00:45, 16.50it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1011/1600 [01:00<00:35, 16.55it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1178/1600 [01:11<00:25, 16.34it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1600 Training loss: 847.2408, Training accuracy: 0.7987
Macro F1-score: 0.7979
Model performance on Angry speech (in training): 
	Precision: 0.8895, Recall: 0.8650, F1_score: 0.8771
Model performance on Happy speech (in training): 
	Precision: 0.8260, Recall: 0.7000, F1_score: 0.7578
Model performance on Neutral speech (in training): 
	Precision: 0.7279, Recall: 0.7425, F1_score: 0.7351
Model performance on Sad speech (in training): 
	Precision: 0.7651, Recall: 0.8875, F1_score: 0.8218

Eval Phase: 
Validation loss: 200.2614, Validation accuracy: 0.6600
Macro F1-score: 0.6503
Model performance on Angry speech (in validation): 
	Precision: 0.6885, Recall: 0.8400, F1_score: 0.7568
Model performance on Happy speech (in validation): 
	Precision: 0.7143, Recall: 0.5000, F1_score: 0.5882
Model performance on Neutral speech (in validation): 
	Precision: 0.5455, Recall: 0.4800, F1_score: 0.5106
Model performance on Sad speech (in validation): 
	Precision: 0.6833, Recall: 0.8200, F1_score: 0.7455
Epoch 16/100

Training Phase:
[01:21<00:15, 16.53it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1518/1600 [01:32<00:05, 16.29it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 162/1600 [00:10<01:28, 16.17it/s]Training:  10%|â–ˆ         | 162/1600 [00:20<01:28, 16.17it/s]Training:  20%|â–ˆâ–ˆ        | 327/1600 [00:20<01:18, 16.30it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 491/1600 [00:30<01:08, 16.22it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 670/1600 [00:40<00:55, 16.88it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 849/1600 [00:50<00:44, 16.96it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1023/1600 [01:00<00:33, 17.10it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1197/1600 [01:10<00:23, 17.13it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1372/1600 [01:20<00:13, 17.24it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1547/1600 [01:3Training loss: 797.3570, Training accuracy: 0.8100
Macro F1-score: 0.8083
Model performance on Angry speech (in training): 
	Precision: 0.9130, Recall: 0.8925, F1_score: 0.9027
Model performance on Happy speech (in training): 
	Precision: 0.8210, Recall: 0.7225, F1_score: 0.7686
Model performance on Neutral speech (in training): 
	Precision: 0.7473, Recall: 0.7025, F1_score: 0.7242
Model performance on Sad speech (in training): 
	Precision: 0.7672, Recall: 0.9225, F1_score: 0.8377

Eval Phase: 
Validation loss: 220.3262, Validation accuracy: 0.6650
Macro F1-score: 0.6641
Model performance on Angry speech (in validation): 
	Precision: 0.7593, Recall: 0.8200, F1_score: 0.7885
Model performance on Happy speech (in validation): 
	Precision: 0.5484, Recall: 0.6800, F1_score: 0.6071
Model performance on Neutral speech (in validation): 
	Precision: 0.6190, Recall: 0.5200, F1_score: 0.5652
Model performance on Sad speech (in validation): 
	Precision: 0.7619, Recall: 0.6400, F1_score: 0.6957
Epoch 17/100

Training Phase:
1<00:03, 17.16it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 176/1600 [00:10<01:21, 17.58it/s]Training:  22%|â–ˆâ–ˆâ–       | 352/1600 [00:20<01:11, 17.45it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 526/1600 [00:30<01:02, 17.31it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 699/1600 [00:40<00:52, 17.30it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 872/1600 [00:50<00:42, 17.20it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [01:00<00:31, 17.32it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [01:10<00:31, 17.32it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1221/1600 [01:10<00:21, 17.23it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1392/1600 [01:20<00:12, 17.03it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1559/1600 [01:31<00:02, 16.71it/s]                                                  Training loss: 749.6382, Training accuracy: 0.8213
Macro F1-score: 0.8201
Model performance on Angry speech (in training): 
	Precision: 0.9167, Recall: 0.9075, F1_score: 0.9121
Model performance on Happy speech (in training): 
	Precision: 0.8430, Recall: 0.7250, F1_score: 0.7796
Model performance on Neutral speech (in training): 
	Precision: 0.7655, Recall: 0.7425, F1_score: 0.7538
Model performance on Sad speech (in training): 
	Precision: 0.7712, Recall: 0.9100, F1_score: 0.8349

Eval Phase: 
Validation loss: 198.8242, Validation accuracy: 0.6850
Macro F1-score: 0.6876
Model performance on Angry speech (in validation): 
	Precision: 0.9032, Recall: 0.5600, F1_score: 0.6914
Model performance on Happy speech (in validation): 
	Precision: 0.5211, Recall: 0.7400, F1_score: 0.6116
Model performance on Neutral speech (in validation): 
	Precision: 0.6304, Recall: 0.5800, F1_score: 0.6042
Model performance on Sad speech (in validation): 
	Precision: 0.8269, Recall: 0.8600, F1_score: 0.8431
Epoch 18/100

Training Phase:
Training loss: 769.7623, Training accuracy: 0.8200
Macro F1-score: 0.8189
Model performance on Angry speech (in training): 
	Precision: 0.9015, Recall: 0.8925, F1_score: 0.8970
Model performance on Happy speech (in training): 
	Precision: 0.8475, Recall: 0.7225, F1_score: 0.7800
Model performance on Neutral speech (in training): 
	Precision: 0.7556, Recall: 0.7575, F1_score: 0.7566
Model performance on Sad speech (in training): 
	Precision: 0.7857, Recall: 0.9075, F1_score: 0.8422

Eval Phase: 
Validation loss: 212.7936, Validation accuracy: 0.7000
Macro F1-score: 0.6997
Model performance on Angry speech (in validation): 
	Precision: 0.6833, Recall: 0.8200, F1_score: 0.7455
Model performance on Happy speech (in validation): 
	Precision: 0.7021, Recall: 0.6600, F1_score: 0.6804
Model performance on Neutral speech (in validation): 
	Precision: 0.6275, Recall: 0.6400, F1_score: 0.6337
Model performance on Sad speech (in validation): 
	Precision: 0.8095, Recall: 0.6800, F1_score: 0.7391
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7200

Test Phase: 
           Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 176/1600 [00:10<01:21, 17.56it/s]Training:  22%|â–ˆâ–ˆâ–       | 352/1600 [00:20<01:11, 17.47it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 527/1600 [00:30<01:01, 17.31it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 701/1600 [00:40<00:51, 17.35it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 875/1600 [00:50<00:42, 17.16it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1050/1600 [01:00<00:31, 17.27it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1225/1600 [01:11<00:22, 17.00it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1390/1600 [01:21<00:12, 16.73it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1552/1600 [01:31<00:02, 16.56it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   1%|          | 2/200 [00:00<00:11, 17.06it/s]Testing:   2%|â–Ž         | 5/200 [00:00<00:09, 20.33it/s]Testing:   4%|â–         | 8/200 [00:00<00:09, 21.03it/s]Testing:   6%|â–Œ         | 11/200 [00:00<00:08, 21.80it/s]Testing:   8%|â–Š         | 15/200 [00:00<00:06, 26.47it/s]Testing:   9%|â–‰         | 18/200 [00:00<00:06, 27.26it/s]Testing:  10%|â–ˆ         | 21/200 [00:00<00:06, 27.31it/s]Testing:  13%|â–ˆâ–Ž        | 26/200 [00:00<00:05, 32.14it/s]Testing:  16%|â–ˆâ–Œ        | 32/200 [00:01<00:04, 38.64it/s]Testing:  18%|â–ˆâ–Š        | 36/200 [00:01<00:04, 36.56it/s]Testing:  20%|â–ˆâ–ˆ        | 40/200 [00:01<00:04, 32.88it/s]Testing:  22%|â–ˆâ–ˆâ–       | 44/200 [00:01<00:04, 33.54it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:01<00:03, 38.63it/s]Testing:  27%|â–ˆâ–ˆâ–‹       | 54/200 [00:01<00:04, 35.64it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [00:01<00:03, 40.32it/s]Testing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [00:01<00:03, 44.41it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:02<00:02, 43.45it/s]Testing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:02<00:02, 44.56it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:02<00:02, 42.33it/s]Testing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:02<00:02, 44.02it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:02<00:02, 42.43it/s]Testing:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [00:02<00:02, 41.04it/s]Testing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [00:02<00:02, 44.98it/s]Testing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [00:02<00:02, 46.29it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [00:02<00:01, 45.17it/s]Testing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:03<00:01, 47.36it/s]Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [00:03<00:01, 51.41it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [00:03<00:01, 52.04it/s]Testing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [00:03<00:01, 54.08it/s]Testing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [00:03<00:01, 48.02it/s]Testing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [00:03<00:01, Test loss: 168.2351, Test accuracy: 0.6600
Macro F1-score: 0.6601
Model performance on Angry speech (in test): 
	Precision: 0.8409, Recall: 0.7400, F1_score: 0.7872
Model performance on Happy speech (in test): 
	Precision: 0.5769, Recall: 0.6000, F1_score: 0.5882
Model performance on Neutral speech (in test): 
	Precision: 0.5532, Recall: 0.5200, F1_score: 0.5361
Model performance on Sad speech (in test): 
	Precision: 0.6842, Recall: 0.7800, F1_score: 0.7290

======================= This is fold_1 on en =======================

Load dataset: 
Loading en train data: fold_1...
Preprocess en fold_1 data for en model
49.44it/s]Testing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [00:03<00:00, 50.91it/s]Testing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [00:03<00:00, 54.20it/s]Testing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [00:04<00:00, 54.22it/s]Testing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:04<00:00, 55.97it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [00:04<00:00, 56.55it/s]Testing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [00:04<00:00, 56.72it/s]Testing:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:04<00:00, 59.58it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:04<00:00, 58.59it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 2/1600 [00:00<01:48, 14.72 examples/s]Map:   1%|          | 9/1600 [00:00<00:47, 33.39 examples/s]Map:   1%|â–         | 22/1600 [00:00<00:25, 62.63 examples/s]Map:   2%|â–         | 31/1600 [00:00<00:24, 64.39 examples/s]Map:   3%|â–Ž         | 42/1600 [00:00<00:20, 76.15 examples/s]Map:   3%|â–Ž         | 52/1600 [00:00<00:20, 76.63 examples/s]Map:   4%|â–         | 60/1600 [00:00<00:20, 75.86 examples/s]Map:   4%|â–         | 72/1600 [00:01<00:20, 72.92 examples/s]Map:   5%|â–Œ         | 82/1600 [00:01<00:19, 77.30 examples/s]Map:   6%|â–Œ         | 94/1600 [00:01<00:19, 76.43 examples/s]Map:   6%|â–‹         | 102/1600 [00:01<00:19, 76.14 examples/s]Map:   7%|â–‹         | 113/1600 [00:01<00:17, 83.84 examples/s]Map:   8%|â–Š         | 127/1600 [00:01<00:15, 97.91 examples/s]Map:   9%|â–‰         | 141/1600 [00:01<00:15, 94.58 examples/s]Map:   9%|â–‰         | 151/1600 [00:01<00:16, 89.83 examples/s]Map:  10%|â–ˆ         | 162/1600 [00:02<00:15, 93.46 examples/s]Map:  11%|â–ˆ         | 173/1600 [00:02<00:17, 82.08 examples/s]Map:  12%|â–ˆâ–        | 187/1600 [00:02<00:15, 92.83 examples/s]Map:  13%|â–ˆâ–Ž        | 201/1600 [00:02<00:13, 103.67 examples/s]Map:  13%|â–ˆâ–Ž        | 214/1600 [00:02<00:13, 106.14 examples/s]Map:  14%|â–ˆâ–        | 226/1600 [00:02<00:12, 106.32 examples/s]Map:  15%|â–ˆâ–        | 239/1600 [00:02<00:12, 109.78 examples/s]Map:  16%|â–ˆâ–Œ        | 253/1600 [00:02<00:11, 117.32 examples/s]Map:  17%|â–ˆâ–‹        | 265/1600 [00:03<00:12, 103.83 examples/s]Map:  17%|â–ˆâ–‹        | 279/1600 [00:03<00:12, 109.77 examples/s]Map:  18%|â–ˆâ–Š        | 294/1600 [00:03<00:11, 112.56 examples/s]Map:  19%|â–ˆâ–‰        | 309/1600 [00:03<00:10, 120.31 examples/s]Map:  20%|â–ˆâ–ˆ        | 325/1600 [00:03<00:10, 126.75 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 344/1600 [00:03<00:10, 124.06 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 357/1600 [00:03<00:10, 124.22 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 373/1600 [00:04<00:16, 74.93 examples/s] Map:  24%|â–ˆâ–ˆâ–       | 391/1600 [00:04<00:13, 92.25 examples/s]Map:  25%|â–ˆâ–ˆâ–Œ       | 407/1600 [00:04<00:13, 90.65 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 419/1600 [00:04<00:12, 96.03 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 431/1600 [00:04<00:11, 100.80 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 443/1600 [00:04<00:11, 103.89 examples/s]Map:  29%|â–ˆâ–ˆâ–Š       | 458/1600 [00:04<00:11, 100.78 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 470/1600 [00:05<00:11, 95.41 examples/s] Map:  30%|â–ˆâ–ˆâ–ˆ       | 485/1600 [00:05<00:10, 102.88 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 498/1600 [00:05<00:10, 106.03 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 514/1600 [00:05<00:09, 118.88 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 528/1600 [00:05<00:09, 116.09 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 540/1600 [00:05<00:09, 112.55 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–      | 553/1600 [00:05<00:09, 114.34 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 572/1600 [00:05<00:10, 97.47 examples/s] Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 585/1600 [00:06<00:10, 100.76 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 599/1600 [00:06<00:10, 96.02 examples/s] Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 611/1600 [00:06<00:10, 98.55 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 622/1600 [00:06<00:09, 98.00 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 633/1600 [00:06<00:09, 99.58 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 650/1600 [00:06<00:09, 100.72 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1600 [00:06<00:10, 93.52 examples/s] Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 676/1600 [00:07<00:09, 98.66 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:07<00:10, 89.64 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 703/1600 [00:07<00:09, 93.97 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 714/1600 [00:07<00:09, 96.62 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 726/1600 [00:07<00:08, 99.85 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 740/1600 [00:07<00:09, 92.15 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 751/1600 [00:07<00:10, 78.18 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 766/1600 [00:08<00:09, 90.43 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 781/1600 [00:08<00:08, 101.49 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 796/1600 [00:08<00:08, 99.19 examples/s] Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 810/1600 [00:08<00:08, 94.70 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 823/1600 [00:08<00:08, 96.34 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 834/1600 [00:08<00:08, 94.91 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 846/1600 [00:08<00:07, 99.60 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 861/1600 [00:09<00:08, 89.80 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 871/1600 [00:09<00:08, 85.37 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 883/1600 [00:09<00:07, 92.90 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 895/1600 [00:09<00:07, 96.81 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 907/1600 [00:09<00:07, 98.54 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 922/1600 [00:09<00:06, 108.34 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 938/1600 [00:09<00:05, 115.98 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 954/1600 [00:09<00:06, 97.27 examples/s] Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 967/1600 [00:10<00:07, 87.54 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 977/1600 [00:10<00:07, 85.83 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 986/1600 [00:10<00:07, 82.32 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 996/1600 [00:10<00:07, 82.66 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 996/1600 [00:22<00:07, 82.66 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [01:10<19:52,  1.99s/ examples]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1017/1600 [01:10<10:43,  1.10s/ examples]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1034/1600 [01:10<06:22,  1.48 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1048/1600 [01:10<04:17,  2.15 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1061/1600 [01:10<02:59,  3.01 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1078/1600 [01:10<01:53,  4.58 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1092/1600 [01:10<01:20,  6.34 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1104/1600 [01:10<00:58,  8.45 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1118/1600 [01:11<00:40, 11.82 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1133/1600 [01:11<00:28, 16.60 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1146/1600 [01:11<00:21, 20.84 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1159/1600 [01:11<00:16, 27.23 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1176/1600 [01:11<00:11, 36.83 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1190/1600 [01:11<00:08, 46.22 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1204/1600 [01:11<00:06, 56.76 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1216/1600 [01:12<00:06, 61.04 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1229/1600 [01:12<00:05, 70.30 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1241/1600 [01:12<00:05, 71.32 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1600 [01:12<00:04, 70.20 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1267/1600 [01:12<00:04, 75.38 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1281/1600 [01:12<00:04, 77.27 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1290/1600 [01:12<00:03, 77.59 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1304/1600 [01:13<00:03, 81.00 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1318/1600 [01:13<00:03, 90.17 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1335/1600 [01:13<00:02, 106.92 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1351/1600 [01:13<00:02, 118.37 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1367/1600 [01:13<00:02, 103.48 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1384/1600 [01:13<00:02, 103.96 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1397/1600 [01:13<00:01, 107.78 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1415/1600 [01:14<00:01, 123.16 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1429/1600 [01:14<00:01, 125.73 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1444/1600 [01:14<00:01, 129.43 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1459/1600 [01:14<00:01, 133.12 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1478/1600 [01:14<00:01, 121.88 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1495/1600 [01:14<00:00, 113.78 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1507/1600 [01:14<00:00, 111.75 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1522/1600 [01:14<00:00, 116.53 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1537/1600 [01:15<00:00, 120.25 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1550/1600 [01:15<00:00, 121.32 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1569/1600 [01:15<00:00, 135.20 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1585/1600 [01:15<00:00, 125.82 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1593/1600 [01:31<00:00, 125.82 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:48<00:00,  1.56 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:48<00:00, 14.74 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|â–         | 4/200 [00:00<00:05, 33.39 examples/s]Map:   5%|â–Œ         | 10/200 [00:00<00:04, 42.30 examples/s]Map:  12%|â–ˆâ–        | 24/200 [00:00<00:02, 76.77 examples/s]Map:  20%|â–ˆâ–‰        | 39/200 [00:00<00:01, 101.96 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:00<00:01, 118.52 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:00<00:01, 95.07 examples/s] Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [00:00<00:01, 105.27 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:01<00:00, 104.19 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [00:01<00:00, 109.25 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [00:01<00:00, 114.81 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [00:01<00:00, 106.98 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:01<00:00, 115.33 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [00:01<00:00, 119.24 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:01<00:00, 110.89 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:12<00:00, 110.89 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:13<00:00,  3.67 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:13<00:00, 15.29 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|â–         | 4/200 [00:00<00:05, 33.80 examples/s]Map:   7%|â–‹         | 14/200 [00:00<00:02, 65.46 examples/s]Map:  16%|â–ˆâ–Œ        | 31/200 [00:00<00:01, 105.93 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 48/200 [00:00<00:01, 125.32 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [00:00<00:01, 110.55 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:00<00:00, 123.96 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [00:00<00:00, 126.96 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:01<00:00, 107.26 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [00:01<00:00, 110.69 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [00:01<00:00, 105.35 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:01<00:00, 100.74 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [00:01<00:00, 105.24 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [00:01<00:00, 114.17 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:01<00:00, 99.60 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:13<00:00, 15.30 examples/s]
Loading en eval data: fold_1...
Preprocess en fold_1 data for en model
Loading en test data: fold_1...
Preprocess en fold_1 data for en model
Use en model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1796.5953, Training accuracy: 0.5088
Macro F1-score: 0.4939
Model performance on Angry speech (in training): 
	Precision: 0.5538, Recall: 0.6825, F1_score: 0.6114
Model performance on Happy speech (in training): 
	Precision: 0.4582, Recall: 0.3425, F1_score: 0.3920
Model performance on Neutral speech (in training): 
	Precision: 0.4139, Recall: 0.3125, F1_score: 0.3561
Model performance on Sad speech (in training): 
	Precision: 0.5514, Recall: 0.6975, F1_score: 0.6159

Eval Phase: 
Validation loss: 197.9169, Validation accuracy: 0.5550
Macro F1-score: 0.5005
Model performance on Angry speech (in validation): 
	Precision: 0.4700, Recall: 0.9400, F1_score: 0.6267
Model performance on Happy speech (in validation): 
	Precision: 0.4516, Recall: 0.2800, F1_score: 0.3457
Model performance on Neutral speech (in validation): 
	Precision: 0.5833, Recall: 0.1400, F1_score: 0.2258
Model performance on Sad speech (in validation): 
	Precision: 0.7544, Recall: 0.8600, F1_score: 0.8037
New best accuracy for layer 8 on epoch 1: 0.5550. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 172/1600 [00:10<01:23, 17.17it/s]Training:  22%|â–ˆâ–ˆâ–       | 344/1600 [00:20<01:14, 16.95it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 517/1600 [00:30<01:03, 17.07it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:40<00:53, 17.03it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:50<00:42, 17.30it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [01:00<00:32, 17.30it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1224/1600 [01:10<00:21, 17.44it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1402/1600 [01:21<00:11, 17.35it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1579/1600 [01:31<00:01, 17.43it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆâ–        | 181/1600 [00:10<01:18, 18.05it/s]Training:  23%|â–ˆâ–ˆâ–Ž     Training loss: 1593.8908, Training accuracy: 0.5669
Macro F1-score: 0.5559
Model performance on Angry speech (in training): 
	Precision: 0.6455, Recall: 0.7100, F1_score: 0.6762
Model performance on Happy speech (in training): 
	Precision: 0.5275, Recall: 0.4075, F1_score: 0.4598
Model performance on Neutral speech (in training): 
	Precision: 0.4641, Recall: 0.3875, F1_score: 0.4223
Model performance on Sad speech (in training): 
	Precision: 0.5899, Recall: 0.7625, F1_score: 0.6652

Eval Phase: 
Validation loss: 203.5700, Validation accuracy: 0.5850
Macro F1-score: 0.5663
Model performance on Angry speech (in validation): 
	Precision: 0.5217, Recall: 0.9600, F1_score: 0.6761
Model performance on Happy speech (in validation): 
	Precision: 0.4762, Recall: 0.4000, F1_score: 0.4348
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.2800, F1_score: 0.3590
Model performance on Sad speech (in validation): 
	Precision: 0.9211, Recall: 0.7000, F1_score: 0.7955
New best accuracy for layer 8 on epoch 2: 0.5850. Model saved.
Epoch 3/100

Training Phase:
  | 362/1600 [00:20<01:09, 17.73it/s]Training:  23%|â–ˆâ–ˆâ–Ž       | 362/1600 [00:30<01:09, 17.73it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 538/1600 [00:30<01:00, 17.64it/s]Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 714/1600 [00:40<00:51, 17.35it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 887/1600 [00:50<00:41, 17.32it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1060/1600 [01:01<00:31, 17.02it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1238/1600 [01:11<00:20, 17.25it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1416/1600 [01:21<00:10, 17.19it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1590/1600 [01:31<00:00, 17.25it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 172/1600 [00:10<01:23, 17.12it/s]Training:  22%|â–ˆâ–ˆâ–       | 347/1600 [00:20<01:12, 17.31it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 522/16Training loss: 1462.0837, Training accuracy: 0.6181
Macro F1-score: 0.6137
Model performance on Angry speech (in training): 
	Precision: 0.7210, Recall: 0.7300, F1_score: 0.7255
Model performance on Happy speech (in training): 
	Precision: 0.5687, Recall: 0.4550, F1_score: 0.5056
Model performance on Neutral speech (in training): 
	Precision: 0.5258, Recall: 0.5350, F1_score: 0.5304
Model performance on Sad speech (in training): 
	Precision: 0.6432, Recall: 0.7525, F1_score: 0.6935

Eval Phase: 
Validation loss: 254.0836, Validation accuracy: 0.4950
Macro F1-score: 0.4679
Model performance on Angry speech (in validation): 
	Precision: 0.4324, Recall: 0.9600, F1_score: 0.5963
Model performance on Happy speech (in validation): 
	Precision: 0.4737, Recall: 0.1800, F1_score: 0.2609
Model performance on Neutral speech (in validation): 
	Precision: 0.3810, Recall: 0.3200, F1_score: 0.3478
Model performance on Sad speech (in validation): 
	Precision: 0.9286, Recall: 0.5200, F1_score: 0.6667
Epoch 4/100

Training Phase:
00 [00:30<01:02, 17.31it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 695/1600 [00:40<00:52, 17.09it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 864/1600 [00:50<00:43, 17.02it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1033/1600 [01:00<00:33, 16.80it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1200/1600 [01:10<00:23, 16.76it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1369/1600 [01:20<00:13, 16.79it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1538/1600 [01:31<00:03, 16.70it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 167/1600 [00:10<01:26, 16.61it/s]Training:  21%|â–ˆâ–ˆ        | 334/1600 [00:20<01:16, 16.48it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 499/1600 [00:30<01:06, 16.49it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 667/1600 [00:40<00:56, 16.60it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 835/1600 [00Training loss: 1346.7216, Training accuracy: 0.6512
Macro F1-score: 0.6463
Model performance on Angry speech (in training): 
	Precision: 0.7347, Recall: 0.7825, F1_score: 0.7579
Model performance on Happy speech (in training): 
	Precision: 0.6414, Recall: 0.4875, F1_score: 0.5540
Model performance on Neutral speech (in training): 
	Precision: 0.5608, Recall: 0.5650, F1_score: 0.5629
Model performance on Sad speech (in training): 
	Precision: 0.6595, Recall: 0.7700, F1_score: 0.7105

Eval Phase: 
Validation loss: 190.8108, Validation accuracy: 0.6250
Macro F1-score: 0.6064
Model performance on Angry speech (in validation): 
	Precision: 0.5393, Recall: 0.9600, F1_score: 0.6906
Model performance on Happy speech (in validation): 
	Precision: 0.5517, Recall: 0.3200, F1_score: 0.4051
Model performance on Neutral speech (in validation): 
	Precision: 0.5789, Recall: 0.4400, F1_score: 0.5000
Model performance on Sad speech (in validation): 
	Precision: 0.8864, Recall: 0.7800, F1_score: 0.8298
New best accuracy for layer 8 on epoch 4: 0.6250. Model saved.
Epoch 5/100

Training Phase:
:50<00:46, 16.49it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1009/1600 [01:00<00:35, 16.78it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1183/1600 [01:10<00:24, 16.90it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1362/1600 [01:20<00:13, 17.18it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1541/1600 [01:31<00:03, 17.22it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 178/1600 [00:10<01:20, 17.75it/s]Training:  22%|â–ˆâ–ˆâ–       | 356/1600 [00:20<01:11, 17.48it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 529/1600 [00:30<01:02, 17.15it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 697/1600 [00:40<00:53, 16.96it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 864/1600 [00:51<00:44, 16.50it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1033/1600 [01:01<00:34, 16.63it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1202/Training loss: 1265.7114, Training accuracy: 0.6875
Macro F1-score: 0.6861
Model performance on Angry speech (in training): 
	Precision: 0.8041, Recall: 0.7800, F1_score: 0.7919
Model performance on Happy speech (in training): 
	Precision: 0.6676, Recall: 0.5825, F1_score: 0.6222
Model performance on Neutral speech (in training): 
	Precision: 0.5995, Recall: 0.6025, F1_score: 0.6010
Model performance on Sad speech (in training): 
	Precision: 0.6811, Recall: 0.7850, F1_score: 0.7294

Eval Phase: 
Validation loss: 196.9347, Validation accuracy: 0.6250
Macro F1-score: 0.6085
Model performance on Angry speech (in validation): 
	Precision: 0.5222, Recall: 0.9400, F1_score: 0.6714
Model performance on Happy speech (in validation): 
	Precision: 0.6296, Recall: 0.3400, F1_score: 0.4416
Model performance on Neutral speech (in validation): 
	Precision: 0.5789, Recall: 0.4400, F1_score: 0.5000
Model performance on Sad speech (in validation): 
	Precision: 0.8667, Recall: 0.7800, F1_score: 0.8211
Epoch 6/100

Training Phase:
1600 [01:11<00:24, 16.57it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1367/1600 [01:21<00:14, 16.46it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1543/1600 [01:31<00:03, 16.80it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 170/1600 [00:10<01:24, 16.95it/s]Training:  22%|â–ˆâ–ˆâ–       | 351/1600 [00:20<01:11, 17.58it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 534/1600 [00:30<00:59, 17.90it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 534/1600 [00:40<00:59, 17.90it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 707/1600 [00:40<00:51, 17.49it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 876/1600 [00:50<00:42, 17.21it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1054/1600 [01:00<00:31, 17.38it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1231/1600 [01:11<00:21, 17.14it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14Training loss: 1197.3325, Training accuracy: 0.6994
Macro F1-score: 0.6962
Model performance on Angry speech (in training): 
	Precision: 0.8046, Recall: 0.7925, F1_score: 0.7985
Model performance on Happy speech (in training): 
	Precision: 0.6769, Recall: 0.5500, F1_score: 0.6069
Model performance on Neutral speech (in training): 
	Precision: 0.6341, Recall: 0.6325, F1_score: 0.6333
Model performance on Sad speech (in training): 
	Precision: 0.6826, Recall: 0.8225, F1_score: 0.7460

Eval Phase: 
Validation loss: 322.9445, Validation accuracy: 0.5400
Macro F1-score: 0.5099
Model performance on Angry speech (in validation): 
	Precision: 0.4068, Recall: 0.9600, F1_score: 0.5714
Model performance on Happy speech (in validation): 
	Precision: 0.5500, Recall: 0.2200, F1_score: 0.3143
Model performance on Neutral speech (in validation): 
	Precision: 0.6500, Recall: 0.2600, F1_score: 0.3714
Model performance on Sad speech (in validation): 
	Precision: 0.8571, Recall: 0.7200, F1_score: 0.7826
Epoch 7/100

Training Phase:
06/1600 [01:21<00:11, 17.25it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1585/1600 [01:31<00:00, 17.45it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 178/1600 [00:10<01:20, 17.74it/s]Training:  22%|â–ˆâ–ˆâ–       | 356/1600 [00:20<01:10, 17.70it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 533/1600 [00:30<01:02, 17.20it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 709/1600 [00:40<00:51, 17.33it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 885/1600 [00:50<00:41, 17.32it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1059/1600 [01:01<00:31, 17.29it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1232/1600 [01:11<00:21, 16.97it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1396/1600 [01:21<00:12, 16.71it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1561/1600 [01:31<00:02, 16.62it/s]                               Training loss: 1080.4705, Training accuracy: 0.7438
Macro F1-score: 0.7442
Model performance on Angry speech (in training): 
	Precision: 0.8656, Recall: 0.8375, F1_score: 0.8513
Model performance on Happy speech (in training): 
	Precision: 0.7703, Recall: 0.6625, F1_score: 0.7124
Model performance on Neutral speech (in training): 
	Precision: 0.6362, Recall: 0.6775, F1_score: 0.6562
Model performance on Sad speech (in training): 
	Precision: 0.7201, Recall: 0.7975, F1_score: 0.7568

Eval Phase: 
Validation loss: 272.9662, Validation accuracy: 0.5650
Macro F1-score: 0.5184
Model performance on Angry speech (in validation): 
	Precision: 0.4660, Recall: 0.9600, F1_score: 0.6275
Model performance on Happy speech (in validation): 
	Precision: 0.3750, Recall: 0.1200, F1_score: 0.1818
Model performance on Neutral speech (in validation): 
	Precision: 0.5806, Recall: 0.3600, F1_score: 0.4444
Model performance on Sad speech (in validation): 
	Precision: 0.8200, Recall: 0.8200, F1_score: 0.8200
Epoch 8/100

Training Phase:
Training loss: 1027.8556, Training accuracy: 0.7525
Macro F1-score: 0.7517
Model performance on Angry speech (in training): 
	Precision: 0.8496, Recall: 0.8475, F1_score: 0.8486
Model performance on Happy speech (in training): 
	Precision: 0.7778, Recall: 0.6475, F1_score: 0.7067
Model performance on Neutral speech (in training): 
	Precision: 0.6611, Recall: 0.6925, F1_score: 0.6764
Model performance on Sad speech (in training): 
	Precision: 0.7327, Recall: 0.8225, F1_score: 0.7750

Eval Phase: 
Validation loss: 239.4690, Validation accuracy: 0.5700
Macro F1-score: 0.5400
Model performance on Angry speech (in validation): 
	Precision: 0.5109, Recall: 0.9400, F1_score: 0.6620
Model performance on Happy speech (in validation): 
	Precision: 0.4500, Recall: 0.1800, F1_score: 0.2571
Model performance on Neutral speech (in validation): 
	Precision: 0.4783, Recall: 0.4400, F1_score: 0.4583
Model performance on Sad speech (in validation): 
	Precision: 0.8571, Recall: 0.7200, F1_score: 0.7826
Epoch 9/100

Training Phase:
                              Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 165/1600 [00:10<01:27, 16.47it/s]Training:  21%|â–ˆâ–ˆ        | 335/1600 [00:20<01:15, 16.77it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 505/1600 [00:30<01:05, 16.84it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 675/1600 [00:40<00:55, 16.78it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 843/1600 [00:50<00:45, 16.79it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1011/1600 [01:00<00:35, 16.68it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1176/1600 [01:10<00:25, 16.62it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1345/1600 [01:20<00:15, 16.69it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1514/1600 [01:30<00:05, 16.74it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:  Training loss: 936.8394, Training accuracy: 0.7756
Macro F1-score: 0.7750
Model performance on Angry speech (in training): 
	Precision: 0.8657, Recall: 0.8700, F1_score: 0.8678
Model performance on Happy speech (in training): 
	Precision: 0.8293, Recall: 0.6925, F1_score: 0.7548
Model performance on Neutral speech (in training): 
	Precision: 0.7010, Recall: 0.6975, F1_score: 0.6992
Model performance on Sad speech (in training): 
	Precision: 0.7232, Recall: 0.8425, F1_score: 0.7783

Eval Phase: 
Validation loss: 240.3747, Validation accuracy: 0.5950
Macro F1-score: 0.5799
Model performance on Angry speech (in validation): 
	Precision: 0.5679, Recall: 0.9200, F1_score: 0.7023
Model performance on Happy speech (in validation): 
	Precision: 0.5385, Recall: 0.2800, F1_score: 0.3684
Model performance on Neutral speech (in validation): 
	Precision: 0.4737, Recall: 0.5400, F1_score: 0.5047
Model performance on Sad speech (in validation): 
	Precision: 0.8889, Recall: 0.6400, F1_score: 0.7442
Epoch 10/100

Training Phase:
 0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 173/1600 [00:10<01:22, 17.22it/s]Training:  22%|â–ˆâ–ˆâ–       | 346/1600 [00:20<01:15, 16.58it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 513/1600 [00:30<01:05, 16.61it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 680/1600 [00:41<00:56, 16.40it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 848/1600 [00:51<00:45, 16.53it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1016/1600 [01:01<00:35, 16.40it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1178/1600 [01:11<00:25, 16.28it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1600 [01:21<00:15, 16.49it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1518/1600 [01:31<00:04, 16.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 170/1600 [00:10<01:24, 16.92it/s]Training:  21%|â–ˆâ–ˆâ–       | 340/1600 [Training loss: 877.3697, Training accuracy: 0.7844
Macro F1-score: 0.7838
Model performance on Angry speech (in training): 
	Precision: 0.8804, Recall: 0.8650, F1_score: 0.8726
Model performance on Happy speech (in training): 
	Precision: 0.8076, Recall: 0.6925, F1_score: 0.7456
Model performance on Neutral speech (in training): 
	Precision: 0.7150, Recall: 0.7275, F1_score: 0.7212
Model performance on Sad speech (in training): 
	Precision: 0.7462, Recall: 0.8525, F1_score: 0.7958

Eval Phase: 
Validation loss: 274.0124, Validation accuracy: 0.5700
Macro F1-score: 0.5638
Model performance on Angry speech (in validation): 
	Precision: 0.5111, Recall: 0.9200, F1_score: 0.6571
Model performance on Happy speech (in validation): 
	Precision: 0.5429, Recall: 0.3800, F1_score: 0.4471
Model performance on Neutral speech (in validation): 
	Precision: 0.4348, Recall: 0.4000, F1_score: 0.4167
Model performance on Sad speech (in validation): 
	Precision: 1.0000, Recall: 0.5800, F1_score: 0.7342
Epoch 11/100

Training Phase:
00:20<01:15, 16.63it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 510/1600 [00:30<01:04, 16.78it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 681/1600 [00:40<00:54, 16.88it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 852/1600 [00:50<00:44, 16.85it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1022/1600 [01:00<00:34, 16.88it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1192/1600 [01:11<00:24, 16.54it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1355/1600 [01:21<00:14, 16.46it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1520/1600 [01:31<00:04, 16.44it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 162/1600 [00:10<01:29, 16.13it/s]Training:  21%|â–ˆâ–ˆ        | 332/1600 [00:20<01:16, 16.60it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 502/1600 [00:30<01:05, 16.74it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 502/1600 [00:40<01:0Training loss: 808.0179, Training accuracy: 0.8087
Macro F1-score: 0.8082
Model performance on Angry speech (in training): 
	Precision: 0.9057, Recall: 0.9125, F1_score: 0.9091
Model performance on Happy speech (in training): 
	Precision: 0.8410, Recall: 0.7275, F1_score: 0.7802
Model performance on Neutral speech (in training): 
	Precision: 0.7424, Recall: 0.7350, F1_score: 0.7387
Model performance on Sad speech (in training): 
	Precision: 0.7560, Recall: 0.8600, F1_score: 0.8047

Eval Phase: 
Validation loss: 296.8414, Validation accuracy: 0.5650
Macro F1-score: 0.5242
Model performance on Angry speech (in validation): 
	Precision: 0.5106, Recall: 0.9600, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.4211, Recall: 0.1600, F1_score: 0.2319
Model performance on Neutral speech (in validation): 
	Precision: 0.4359, Recall: 0.3400, F1_score: 0.3820
Model performance on Sad speech (in validation): 
	Precision: 0.8333, Recall: 0.8000, F1_score: 0.8163
Epoch 12/100

Training Phase:
5, 16.74it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 672/1600 [00:40<00:55, 16.72it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 842/1600 [00:50<00:45, 16.79it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1012/1600 [01:00<00:34, 16.86it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1182/1600 [01:10<00:24, 16.75it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1348/1600 [01:20<00:15, 16.60it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1516/1600 [01:30<00:05, 16.61it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 169/1600 [00:10<01:24, 16.89it/s]Training:  21%|â–ˆâ–ˆ        | 338/1600 [00:20<01:15, 16.66it/s]Training:  21%|â–ˆâ–ˆ        | 338/1600 [00:31<01:15, 16.66it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 502/1600 [00:31<01:09, 15.83it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 652/1600 [00:41<01:01, 15.50it/s]Training loss: 736.4777, Training accuracy: 0.8231
Macro F1-score: 0.8234
Model performance on Angry speech (in training): 
	Precision: 0.9188, Recall: 0.9050, F1_score: 0.9118
Model performance on Happy speech (in training): 
	Precision: 0.8683, Recall: 0.7750, F1_score: 0.8190
Model performance on Neutral speech (in training): 
	Precision: 0.7732, Recall: 0.7500, F1_score: 0.7614
Model performance on Sad speech (in training): 
	Precision: 0.7484, Recall: 0.8625, F1_score: 0.8014

Eval Phase: 
Validation loss: 354.5866, Validation accuracy: 0.5350
Macro F1-score: 0.5109
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 0.9600, F1_score: 0.6575
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.2400, F1_score: 0.3243
Model performance on Neutral speech (in validation): 
	Precision: 0.4490, Recall: 0.4400, F1_score: 0.4444
Model performance on Sad speech (in validation): 
	Precision: 0.8065, Recall: 0.5000, F1_score: 0.6173
Epoch 13/100

Training Phase:
Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 814/1600 [00:51<00:50, 15.71it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 977/1600 [01:01<00:39, 15.89it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1143/1600 [01:11<00:28, 16.09it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1309/1600 [01:21<00:18, 16.08it/s]Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1480/1600 [01:31<00:07, 16.39it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 160/1600 [00:10<01:30, 15.91it/s]Training:  20%|â–ˆâ–ˆ        | 326/1600 [00:20<01:18, 16.30it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 494/1600 [00:30<01:06, 16.51it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1600 [00:40<00:56, 16.68it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 834/1600 [00:50<00:46, 16.64it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1005/1600 [01:00<00:35, 16.79it/s]TrainiTraining loss: 734.5726, Training accuracy: 0.8325
Macro F1-score: 0.8325
Model performance on Angry speech (in training): 
	Precision: 0.9124, Recall: 0.8850, F1_score: 0.8985
Model performance on Happy speech (in training): 
	Precision: 0.8786, Recall: 0.7600, F1_score: 0.8150
Model performance on Neutral speech (in training): 
	Precision: 0.7726, Recall: 0.7900, F1_score: 0.7812
Model performance on Sad speech (in training): 
	Precision: 0.7834, Recall: 0.8950, F1_score: 0.8355

Eval Phase: 
Validation loss: 423.6255, Validation accuracy: 0.4950
Macro F1-score: 0.4737
Model performance on Angry speech (in validation): 
	Precision: 0.4159, Recall: 0.9400, F1_score: 0.5767
Model performance on Happy speech (in validation): 
	Precision: 0.5500, Recall: 0.2200, F1_score: 0.3143
Model performance on Neutral speech (in validation): 
	Precision: 0.3846, Recall: 0.3000, F1_score: 0.3371
Model performance on Sad speech (in validation): 
	Precision: 0.9286, Recall: 0.5200, F1_score: 0.6667
Epoch 14/100

Training Phase:
ng:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1182/1600 [01:10<00:24, 17.07it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1361/1600 [01:20<00:13, 17.32it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1540/1600 [01:30<00:03, 17.29it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 168/1600 [00:10<01:25, 16.75it/s]Training:  21%|â–ˆâ–ˆ        | 336/1600 [00:20<01:16, 16.53it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 515/1600 [00:30<01:03, 17.12it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 694/1600 [00:40<00:53, 17.09it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:50<00:42, 17.19it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [01:00<00:32, 17.22it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1216/1600 [01:10<00:22, 17.24it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1391/1600 [01:20<00:12, 17.32itTraining loss: 621.7683, Training accuracy: 0.8562
Macro F1-score: 0.8565
Model performance on Angry speech (in training): 
	Precision: 0.9492, Recall: 0.9350, F1_score: 0.9421
Model performance on Happy speech (in training): 
	Precision: 0.8774, Recall: 0.8050, F1_score: 0.8396
Model performance on Neutral speech (in training): 
	Precision: 0.8161, Recall: 0.8100, F1_score: 0.8130
Model performance on Sad speech (in training): 
	Precision: 0.7919, Recall: 0.8750, F1_score: 0.8314

Eval Phase: 
Validation loss: 426.2550, Validation accuracy: 0.5400
Macro F1-score: 0.5061
Model performance on Angry speech (in validation): 
	Precision: 0.4706, Recall: 0.9600, F1_score: 0.6316
Model performance on Happy speech (in validation): 
	Precision: 0.4474, Recall: 0.3400, F1_score: 0.3864
Model performance on Neutral speech (in validation): 
	Precision: 0.4000, Recall: 0.1600, F1_score: 0.2286
Model performance on Sad speech (in validation): 
	Precision: 0.8750, Recall: 0.7000, F1_score: 0.7778
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6250

Test Phase: 
/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1566/1600 [01:31<00:01, 17.33it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   4%|â–Ž         | 7/200 [00:00<00:03, 62.67it/s]Testing:   8%|â–Š         | 15/200 [00:00<00:02, 69.11it/s]Testing:  11%|â–ˆ         | 22/200 [00:00<00:02, 66.36it/s]Testing:  14%|â–ˆâ–        | 29/200 [00:00<00:02, 65.18it/s]Testing:  18%|â–ˆâ–Š        | 36/200 [00:00<00:02, 66.00it/s]Testing:  22%|â–ˆâ–ˆâ–       | 44/200 [00:00<00:02, 67.54it/s]Testing:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [00:00<00:02, 68.25it/s]Testing:  29%|â–ˆâ–ˆâ–‰       | 58/200 [00:00<00:02, 68.30it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [00:00<00:02, 66.55it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [00:01<00:01, 67.42it/s]Testing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [00:01<00:01, 67.63it/s]Testing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [00:01<00:01, 65.82it/s]Testing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [00:01<00:01, 65.85it/s]Testing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [00:01<00:01, 66.82it/s]Testing:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [00:01<00:01, 68.08it/s]Testing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [00:01<00:01, 66.50it/s]Testing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [00:01<00:01, 67.00it/s]Testing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [00:01<00:00, 68.12it/s]Testing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [00:02<00:00, 66.69it/s]Testing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [00:02<00:00, 66.25it/s]Testing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [00:02<00:00, 66.35it/s]Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [00:02<00:00, 64.46it/s]Testing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [00:02<00:00, 65.46it/s]Testing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:02<00:00, 67.71it/s]Testing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:02<00:00, 64.52it/s]Testing:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâTest loss: 191.0989, Test accuracy: 0.6000
Macro F1-score: 0.5799
Model performance on Angry speech (in test): 
	Precision: 0.5641, Recall: 0.8800, F1_score: 0.6875
Model performance on Happy speech (in test): 
	Precision: 0.6842, Recall: 0.2600, F1_score: 0.3768
Model performance on Neutral speech (in test): 
	Precision: 0.5079, Recall: 0.6400, F1_score: 0.5664
Model performance on Sad speech (in test): 
	Precision: 0.7750, Recall: 0.6200, F1_score: 0.6889

======================= This is fold_2 on en =======================

Load dataset: 
Loading en train data: fold_2...
Preprocess en fold_2 data for en model
–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [00:02<00:00, 66.61it/s]Testing:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [00:02<00:00, 65.36it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 4/1600 [00:00<00:47, 33.91 examples/s]Map:   1%|â–         | 20/1600 [00:00<00:16, 96.23 examples/s]Map:   2%|â–         | 31/1600 [00:00<00:17, 91.44 examples/s]Map:   3%|â–Ž         | 45/1600 [00:00<00:15, 99.34 examples/s]Map:   3%|â–Ž         | 55/1600 [00:00<00:17, 90.59 examples/s]Map:   4%|â–         | 71/1600 [00:00<00:17, 89.38 examples/s]Map:   5%|â–Œ         | 86/1600 [00:00<00:17, 85.31 examples/s]Map:   6%|â–Œ         | 95/1600 [00:01<00:18, 82.48 examples/s]Map:   7%|â–‹         | 105/1600 [00:01<00:18, 82.19 examples/s]Map:   7%|â–‹         | 114/1600 [00:01<00:19, 75.16 examples/s]Map:   8%|â–Š         | 129/1600 [00:01<00:16, 89.21 examples/s]Map:   9%|â–‰         | 140/1600 [00:01<00:15, 92.08 examples/s]Map:  10%|â–‰         | 156/1600 [00:01<00:15, 94.10 examples/s]Map:  11%|â–ˆ         | 169/1600 [00:01<00:14, 99.78 examples/s]Map:  11%|â–ˆâ–        | 180/1600 [00:01<00:13, 101.45 examples/s]Map:  12%|â–ˆâ–        | 196/1600 [00:02<00:12, 112.89 examples/s]Map:  13%|â–ˆâ–Ž        | 210/1600 [00:02<00:11, 118.03 examples/s]Map:  14%|â–ˆâ–        | 222/1600 [00:02<00:16, 81.18 examples/s] Map:  15%|â–ˆâ–        | 235/1600 [00:02<00:15, 88.89 examples/s]Map:  16%|â–ˆâ–Œ        | 249/1600 [00:02<00:13, 97.86 examples/s]Map:  16%|â–ˆâ–‹        | 261/1600 [00:02<00:13, 101.25 examples/s]Map:  17%|â–ˆâ–‹        | 276/1600 [00:02<00:11, 111.23 examples/s]Map:  18%|â–ˆâ–Š        | 290/1600 [00:03<00:11, 113.51 examples/s]Map:  19%|â–ˆâ–‰        | 303/1600 [00:03<00:11, 116.92 examples/s]Map:  20%|â–ˆâ–ˆ        | 321/1600 [00:03<00:10, 127.62 examples/s]Map:  21%|â–ˆâ–ˆ        | 335/1600 [00:03<00:10, 124.57 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 350/1600 [00:03<00:11, 109.25 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 363/1600 [00:03<00:11, 110.66 examples/s]Map:  24%|â–ˆâ–ˆâ–Ž       | 378/1600 [00:03<00:10, 115.00 examples/s]Map:  25%|â–ˆâ–ˆâ–       | 393/1600 [00:03<00:09, 123.02 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:04<00:10, 113.68 examples/s]Map:  26%|â–ˆâ–ˆâ–‹       | 424/1600 [00:04<00:10, 109.15 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 438/1600 [00:04<00:10, 115.08 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 456/1600 [00:04<00:10, 111.85 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 473/1600 [00:04<00:09, 124.16 examples/s]Map:  30%|â–ˆâ–ˆâ–ˆ       | 488/1600 [00:04<00:08, 129.65 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 506/1600 [00:04<00:07, 137.31 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 524/1600 [00:05<00:09, 107.96 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 541/1600 [00:05<00:10, 104.06 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–      | 557/1600 [00:05<00:09, 104.38 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 572/1600 [00:05<00:09, 112.49 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 590/1600 [00:05<00:08, 121.76 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 606/1600 [00:05<00:07, 129.70 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 628/1600 [00:05<00:07, 131.90 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 643/1600 [00:06<00:08, 115.61 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 660/1600 [00:06<00:08, 111.35 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 672/1600 [00:06<00:08, 110.71 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 684/1600 [00:06<00:08, 106.09 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 695/1600 [00:06<00:08, 102.11 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 708/1600 [00:06<00:08, 101.79 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 727/1600 [00:06<00:07, 118.50 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 740/1600 [00:06<00:08, 101.71 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 751/1600 [00:07<00:08, 102.66 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 766/1600 [00:07<00:08, 99.95 examples/s] Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 780/1600 [00:07<00:09, 90.93 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 791/1600 [00:07<00:08, 91.08 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 801/1600 [00:07<00:08, 89.06 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 816/1600 [00:07<00:08, 88.02 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 829/1600 [00:07<00:08, 94.94 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 845/1600 [00:08<00:09, 83.11 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 859/1600 [00:08<00:08, 92.39 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 870/1600 [00:08<00:08, 91.00 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 881/1600 [00:08<00:07, 93.29 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 895/1600 [00:08<00:06, 101.92 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 907/1600 [00:08<00:06, 104.24 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 922/1600 [00:08<00:05, 115.30 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 940/1600 [00:09<00:06, 107.32 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 954/1600 [00:09<00:05, 109.53 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 967/1600 [00:09<00:06, 97.94 examples/s] Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 984/1600 [00:09<00:06, 97.01 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:09<00:06, 92.66 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:21<00:06, 92.66 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [01:09<16:31,  1.65s/ examples]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1019/1600 [01:09<09:10,  1.06 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1036/1600 [01:09<05:46,  1.63 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1051/1600 [01:09<03:54,  2.34 examples/s]Map:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1065/1600 [01:09<02:43,  3.27 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1081/1600 [01:09<01:49,  4.74 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1097/1600 [01:09<01:14,  6.77 examples/s]Map:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1108/1600 [01:10<00:56,  8.69 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1121/1600 [01:10<00:40, 11.81 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1136/1600 [01:10<00:28, 16.18 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1147/1600 [01:10<00:22, 20.53 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1159/1600 [01:10<00:16, 26.63 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1176/1600 [01:10<00:11, 36.62 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1190/1600 [01:10<00:08, 46.20 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1205/1600 [01:11<00:06, 57.84 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1219/1600 [01:11<00:06, 63.50 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1230/1600 [01:11<00:05, 69.13 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1246/1600 [01:11<00:04, 76.23 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1260/1600 [01:11<00:04, 71.43 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1269/1600 [01:11<00:04, 69.56 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1281/1600 [01:11<00:04, 78.02 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1293/1600 [01:12<00:04, 75.26 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1304/1600 [01:12<00:03, 81.63 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1317/1600 [01:12<00:03, 91.68 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1333/1600 [01:12<00:02, 107.11 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1349/1600 [01:12<00:02, 117.44 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1367/1600 [01:12<00:02, 102.93 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1384/1600 [01:12<00:02, 104.13 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1397/1600 [01:13<00:01, 107.84 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1415/1600 [01:13<00:01, 123.57 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1429/1600 [01:13<00:01, 126.30 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1444/1600 [01:13<00:01, 131.03 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1460/1600 [01:13<00:01, 136.57 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1478/1600 [01:13<00:00, 122.68 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1495/1600 [01:13<00:00, 114.34 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1507/1600 [01:13<00:00, 113.00 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1522/1600 [01:14<00:00, 117.49 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1537/1600 [01:14<00:00, 120.80 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1552/1600 [01:14<00:00, 123.14 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1570/1600 [01:14<00:00, 136.99 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1593/1600 [01:14<00:00, 126.80 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1593/1600 [01:32<00:00, 126.80 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:47<00:00,  1.47 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:47<00:00, 14.87 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|â–Ž         | 5/200 [00:00<00:04, 45.64 examples/s]Map:   8%|â–Š         | 15/200 [00:00<00:02, 71.64 examples/s]Map:  14%|â–ˆâ–        | 29/200 [00:00<00:01, 91.36 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:00<00:01, 116.87 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:00<00:01, 115.61 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:00<00:01, 115.06 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [00:00<00:00, 112.97 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [00:00<00:00, 108.27 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:01<00:00, 98.17 examples/s] Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [00:01<00:00, 102.67 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:01<00:00, 102.56 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [00:01<00:00, 103.27 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [00:01<00:00, 107.94 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [00:01<00:00, 120.00 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:01<00:00, 117.83 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:12<00:00, 15.84 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|â–Ž         | 5/200 [00:00<00:04, 39.91 examples/s]Map:   5%|â–Œ         | 10/200 [00:00<00:04, 42.46 examples/s]Map:  12%|â–ˆâ–Ž        | 25/200 [00:00<00:02, 86.52 examples/s]Map:  18%|â–ˆâ–Š        | 37/200 [00:00<00:01, 97.33 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 55/200 [00:00<00:01, 121.34 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [00:00<00:01, 119.82 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [00:00<00:00, 120.34 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:00<00:00, 116.69 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:01<00:00, 101.85 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [00:01<00:00, 100.02 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:01<00:00, 107.67 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [00:01<00:00, 93.12 examples/s] Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [00:01<00:00, 97.49 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:01<00:00, 103.76 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:01<00:00, 105.71 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:12<00:00, 105.71 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:12<00:00,  3.20 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:12<00:00, 15.48 examples/s]
Loading en eval data: fold_2...
Preprocess en fold_2 data for en model
Loading en test data: fold_2...
Preprocess en fold_2 data for en model
Use en model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1665.3070, Training accuracy: 0.5544
Macro F1-score: 0.5475
Model performance on Angry speech (in training): 
	Precision: 0.6023, Recall: 0.6550, F1_score: 0.6275
Model performance on Happy speech (in training): 
	Precision: 0.4680, Recall: 0.4025, F1_score: 0.4328
Model performance on Neutral speech (in training): 
	Precision: 0.4873, Recall: 0.4300, F1_score: 0.4568
Model performance on Sad speech (in training): 
	Precision: 0.6239, Recall: 0.7300, F1_score: 0.6728

Eval Phase: 
Validation loss: 198.9811, Validation accuracy: 0.5850
Macro F1-score: 0.5750
Model performance on Angry speech (in validation): 
	Precision: 0.6029, Recall: 0.8200, F1_score: 0.6949
Model performance on Happy speech (in validation): 
	Precision: 0.6286, Recall: 0.4400, F1_score: 0.5176
Model performance on Neutral speech (in validation): 
	Precision: 0.4773, Recall: 0.4200, F1_score: 0.4468
Model performance on Sad speech (in validation): 
	Precision: 0.6226, Recall: 0.6600, F1_score: 0.6408
New best accuracy for layer 8 on epoch 1: 0.5850. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 165/1600 [00:10<01:27, 16.44it/s]Training:  21%|â–ˆâ–ˆ        | 330/1600 [00:20<01:17, 16.44it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 495/1600 [00:30<01:07, 16.46it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 660/1600 [00:40<00:57, 16.37it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 827/1600 [00:50<00:46, 16.46it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 995/1600 [01:00<00:36, 16.57it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1164/1600 [01:10<00:26, 16.65it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1333/1600 [01:20<00:16, 16.63it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1499/1600 [01:30<00:06, 16.41it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 171/1600 [00:10<01:23, 17.04it/s]Training:  21%|â–ˆâ–ˆâ–       | 342Training loss: 1473.4529, Training accuracy: 0.6188
Macro F1-score: 0.6118
Model performance on Angry speech (in training): 
	Precision: 0.6874, Recall: 0.7200, F1_score: 0.7033
Model performance on Happy speech (in training): 
	Precision: 0.5706, Recall: 0.4850, F1_score: 0.5243
Model performance on Neutral speech (in training): 
	Precision: 0.5225, Recall: 0.4650, F1_score: 0.4921
Model performance on Sad speech (in training): 
	Precision: 0.6639, Recall: 0.8050, F1_score: 0.7277

Eval Phase: 
Validation loss: 198.9104, Validation accuracy: 0.5700
Macro F1-score: 0.5680
Model performance on Angry speech (in validation): 
	Precision: 0.7568, Recall: 0.5600, F1_score: 0.6437
Model performance on Happy speech (in validation): 
	Precision: 0.5294, Recall: 0.3600, F1_score: 0.4286
Model performance on Neutral speech (in validation): 
	Precision: 0.4306, Recall: 0.6200, F1_score: 0.5082
Model performance on Sad speech (in validation): 
	Precision: 0.6491, Recall: 0.7400, F1_score: 0.6916
Epoch 3/100

Training Phase:
/1600 [00:20<01:16, 16.44it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 510/1600 [00:30<01:05, 16.60it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 681/1600 [00:40<00:54, 16.76it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 852/1600 [00:51<00:44, 16.63it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1017/1600 [01:01<00:35, 16.24it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1182/1600 [01:11<00:25, 16.29it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1349/1600 [01:21<00:15, 16.41it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1516/1600 [01:32<00:05, 16.26it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 161/1600 [00:10<01:29, 16.10it/s]Training:  21%|â–ˆâ–ˆ        | 329/1600 [00:20<01:17, 16.47it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 501/1600 [00:30<01:05, 16.76it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 673/1600 [0Training loss: 1334.8709, Training accuracy: 0.6581
Macro F1-score: 0.6526
Model performance on Angry speech (in training): 
	Precision: 0.7368, Recall: 0.7700, F1_score: 0.7531
Model performance on Happy speech (in training): 
	Precision: 0.6147, Recall: 0.5025, F1_score: 0.5530
Model performance on Neutral speech (in training): 
	Precision: 0.5722, Recall: 0.5450, F1_score: 0.5583
Model performance on Sad speech (in training): 
	Precision: 0.6878, Recall: 0.8150, F1_score: 0.7460

Eval Phase: 
Validation loss: 204.0276, Validation accuracy: 0.5800
Macro F1-score: 0.5802
Model performance on Angry speech (in validation): 
	Precision: 0.5763, Recall: 0.6800, F1_score: 0.6239
Model performance on Happy speech (in validation): 
	Precision: 0.5714, Recall: 0.5600, F1_score: 0.5657
Model performance on Neutral speech (in validation): 
	Precision: 0.4694, Recall: 0.4600, F1_score: 0.4646
Model performance on Sad speech (in validation): 
	Precision: 0.7209, Recall: 0.6200, F1_score: 0.6667
Epoch 4/100

Training Phase:
0:40<00:55, 16.57it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 836/1600 [00:51<00:47, 16.03it/s]Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 995/1600 [01:01<00:37, 15.99it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1158/1600 [01:11<00:27, 16.06it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1321/1600 [01:21<00:17, 16.11it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1484/1600 [01:31<00:07, 16.14it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 167/1600 [00:10<01:25, 16.68it/s]Training:  21%|â–ˆâ–ˆ        | 334/1600 [00:20<01:17, 16.30it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 495/1600 [00:30<01:08, 16.10it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 658/1600 [00:40<00:58, 16.16it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 834/1600 [00:50<00:45, 16.66it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1010/1600 [01:01Training loss: 1250.6039, Training accuracy: 0.6781
Macro F1-score: 0.6742
Model performance on Angry speech (in training): 
	Precision: 0.7786, Recall: 0.7825, F1_score: 0.7805
Model performance on Happy speech (in training): 
	Precision: 0.6393, Recall: 0.5450, F1_score: 0.5884
Model performance on Neutral speech (in training): 
	Precision: 0.5901, Recall: 0.5650, F1_score: 0.5773
Model performance on Sad speech (in training): 
	Precision: 0.6920, Recall: 0.8200, F1_score: 0.7506

Eval Phase: 
Validation loss: 215.7408, Validation accuracy: 0.5650
Macro F1-score: 0.5273
Model performance on Angry speech (in validation): 
	Precision: 0.5811, Recall: 0.8600, F1_score: 0.6935
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.1600, F1_score: 0.2581
Model performance on Neutral speech (in validation): 
	Precision: 0.4821, Recall: 0.5400, F1_score: 0.5094
Model performance on Sad speech (in validation): 
	Precision: 0.6034, Recall: 0.7000, F1_score: 0.6481
Epoch 5/100

Training Phase:
<00:35, 16.75it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1181/1600 [01:11<00:24, 16.85it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1352/1600 [01:21<00:14, 16.71it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1517/1600 [01:31<00:05, 16.51it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:   9%|â–‰         | 149/1600 [00:10<01:37, 14.89it/s]Training:  20%|â–ˆâ–‰        | 315/1600 [00:20<01:20, 15.87it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 482/1600 [00:30<01:08, 16.23it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 649/1600 [00:40<00:58, 16.16it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 810/1600 [00:52<00:53, 14.81it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 974/1600 [01:02<00:40, 15.31it/s]Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1139/1600 [01:13<00:29, 15.66it/s]Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1312/1600 [01:23<Training loss: 1131.3058, Training accuracy: 0.7094
Macro F1-score: 0.7051
Model performance on Angry speech (in training): 
	Precision: 0.7941, Recall: 0.8100, F1_score: 0.8020
Model performance on Happy speech (in training): 
	Precision: 0.6687, Recall: 0.5550, F1_score: 0.6066
Model performance on Neutral speech (in training): 
	Precision: 0.6343, Recall: 0.6200, F1_score: 0.6271
Model performance on Sad speech (in training): 
	Precision: 0.7271, Recall: 0.8525, F1_score: 0.7848

Eval Phase: 
Validation loss: 204.2641, Validation accuracy: 0.6300
Macro F1-score: 0.6232
Model performance on Angry speech (in validation): 
	Precision: 0.7778, Recall: 0.7000, F1_score: 0.7368
Model performance on Happy speech (in validation): 
	Precision: 0.6061, Recall: 0.4000, F1_score: 0.4819
Model performance on Neutral speech (in validation): 
	Precision: 0.5849, Recall: 0.6200, F1_score: 0.6019
Model performance on Sad speech (in validation): 
	Precision: 0.5797, Recall: 0.8000, F1_score: 0.6723
New best accuracy for layer 8 on epoch 5: 0.6300. Model saved.
Epoch 6/100

Training Phase:
00:17, 16.16it/s]Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1485/1600 [01:33<00:07, 16.39it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 163/1600 [00:10<01:28, 16.21it/s]Training:  21%|â–ˆâ–ˆ        | 339/1600 [00:20<01:14, 16.99it/s]Training:  21%|â–ˆâ–ˆ        | 339/1600 [00:30<01:14, 16.99it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 515/1600 [00:30<01:04, 16.79it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 684/1600 [00:40<00:54, 16.79it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 860/1600 [00:50<00:43, 17.05it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1036/1600 [01:01<00:33, 16.84it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1204/1600 [01:11<00:23, 16.83it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1378/1600 [01:21<00:13, 16.99it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1552/1600 [01:32<0Training loss: 1021.2585, Training accuracy: 0.7481
Macro F1-score: 0.7456
Model performance on Angry speech (in training): 
	Precision: 0.8337, Recall: 0.8400, F1_score: 0.8369
Model performance on Happy speech (in training): 
	Precision: 0.7326, Recall: 0.6300, F1_score: 0.6774
Model performance on Neutral speech (in training): 
	Precision: 0.6692, Recall: 0.6575, F1_score: 0.6633
Model performance on Sad speech (in training): 
	Precision: 0.7522, Recall: 0.8650, F1_score: 0.8047

Eval Phase: 
Validation loss: 236.0378, Validation accuracy: 0.5800
Macro F1-score: 0.5697
Model performance on Angry speech (in validation): 
	Precision: 0.5333, Recall: 0.8000, F1_score: 0.6400
Model performance on Happy speech (in validation): 
	Precision: 0.5946, Recall: 0.4400, F1_score: 0.5057
Model performance on Neutral speech (in validation): 
	Precision: 0.5405, Recall: 0.4000, F1_score: 0.4598
Model performance on Sad speech (in validation): 
	Precision: 0.6667, Recall: 0.6800, F1_score: 0.6733
Epoch 7/100

Training Phase:
Training loss: 995.7289, Training accuracy: 0.7500
Macro F1-score: 0.7487
Model performance on Angry speech (in training): 
	Precision: 0.8463, Recall: 0.8400, F1_score: 0.8432
Model performance on Happy speech (in training): 
	Precision: 0.7429, Recall: 0.6500, F1_score: 0.6933
Model performance on Neutral speech (in training): 
	Precision: 0.6692, Recall: 0.6725, F1_score: 0.6708
Model performance on Sad speech (in training): 
	Precision: 0.7428, Recall: 0.8375, F1_score: 0.7873

Eval Phase: 
0:02, 16.69it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 167/1600 [00:10<01:25, 16.67it/s]Training:  21%|â–ˆâ–ˆâ–       | 341/1600 [00:20<01:13, 17.09it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 515/1600 [00:30<01:04, 16.81it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1600 [00:40<00:53, 17.00it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 862/1600 [00:51<00:44, 16.63it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1032/1600 [01:01<00:33, 16.72it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1202/1600 [01:11<00:23, 16.64it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1369/1600 [01:21<00:13, 16.64it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1536/1600 [01:32<00:03, 16.51it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]              Validation loss: 248.3631, Validation accuracy: 0.5850
Macro F1-score: 0.5554
Model performance on Angry speech (in validation): 
	Precision: 0.5857, Recall: 0.8200, F1_score: 0.6833
Model performance on Happy speech (in validation): 
	Precision: 0.5455, Recall: 0.3600, F1_score: 0.4337
Model performance on Neutral speech (in validation): 
	Precision: 0.6250, Recall: 0.3000, F1_score: 0.4054
Model performance on Sad speech (in validation): 
	Precision: 0.5890, Recall: 0.8600, F1_score: 0.6992
Epoch 8/100

Training Phase:
Training loss: 886.1946, Training accuracy: 0.7931
Macro F1-score: 0.7920
Model performance on Angry speech (in training): 
	Precision: 0.8722, Recall: 0.8700, F1_score: 0.8711
Model performance on Happy speech (in training): 
	Precision: 0.8056, Recall: 0.7250, F1_score: 0.7632
Model performance on Neutral speech (in training): 
	Precision: 0.7198, Recall: 0.7000, F1_score: 0.7098
Model performance on Sad speech (in training): 
	Precision: 0.7765, Recall: 0.8775, F1_score: 0.8239

Eval Phase: 
Validation loss: 241.5051, Validation accuracy: 0.5900
Macro F1-score: 0.5829
Model performance on Angry speech (in validation): 
	Precision: 0.6604, Recall: 0.7000, F1_score: 0.6796
Model performance on Happy speech (in validation): 
	Precision: 0.4902, Recall: 0.5000, F1_score: 0.4950
Model performance on Neutral speech (in validation): 
	Precision: 0.5405, Recall: 0.4000, F1_score: 0.4598
Model performance on Sad speech (in validation): 
	Precision: 0.6441, Recall: 0.7600, F1_score: 0.6972
Epoch 9/100

Training Phase:
                                     Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 176/1600 [00:10<01:21, 17.58it/s]Training:  22%|â–ˆâ–ˆâ–       | 352/1600 [00:20<01:12, 17.28it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 526/1600 [00:30<01:02, 17.32it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 700/1600 [00:40<00:52, 17.26it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 872/1600 [00:50<00:42, 17.06it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1040/1600 [01:00<00:33, 16.91it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1207/1600 [01:10<00:23, 16.80it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1373/1600 [01:21<00:13, 16.71it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1539/1600 [01:31<00:03, 16.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 165/1600 [00:10<01:27, 16.4Training loss: 843.9644, Training accuracy: 0.7950
Macro F1-score: 0.7939
Model performance on Angry speech (in training): 
	Precision: 0.8897, Recall: 0.8875, F1_score: 0.8886
Model performance on Happy speech (in training): 
	Precision: 0.7978, Recall: 0.7300, F1_score: 0.7624
Model performance on Neutral speech (in training): 
	Precision: 0.7258, Recall: 0.6950, F1_score: 0.7101
Model performance on Sad speech (in training): 
	Precision: 0.7677, Recall: 0.8675, F1_score: 0.8146

Eval Phase: 
Validation loss: 286.0614, Validation accuracy: 0.5550
Macro F1-score: 0.5336
Model performance on Angry speech (in validation): 
	Precision: 0.5606, Recall: 0.7400, F1_score: 0.6379
Model performance on Happy speech (in validation): 
	Precision: 0.5152, Recall: 0.3400, F1_score: 0.4096
Model performance on Neutral speech (in validation): 
	Precision: 0.5312, Recall: 0.3400, F1_score: 0.4146
Model performance on Sad speech (in validation): 
	Precision: 0.5797, Recall: 0.8000, F1_score: 0.6723
Epoch 10/100

Training Phase:
7it/s]Training:  21%|â–ˆâ–ˆ        | 330/1600 [00:20<01:17, 16.31it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 503/1600 [00:30<01:05, 16.73it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 676/1600 [00:40<00:54, 16.91it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 848/1600 [00:50<00:44, 16.97it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1019/1600 [01:01<00:35, 16.55it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1183/1600 [01:11<00:25, 16.50it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1347/1600 [01:21<00:15, 16.37it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1513/1600 [01:31<00:05, 16.43it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 172/1600 [00:10<01:23, 17.18it/s]Training:  22%|â–ˆâ–ˆâ–       | 344/1600 [00:20<01:16, 16.38it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 506/1600 [00:30<01:07, 16.27it/s]TrainTraining loss: 736.6066, Training accuracy: 0.8256
Macro F1-score: 0.8249
Model performance on Angry speech (in training): 
	Precision: 0.8938, Recall: 0.9050, F1_score: 0.8994
Model performance on Happy speech (in training): 
	Precision: 0.8453, Recall: 0.7650, F1_score: 0.8031
Model performance on Neutral speech (in training): 
	Precision: 0.7798, Recall: 0.7525, F1_score: 0.7659
Model performance on Sad speech (in training): 
	Precision: 0.7875, Recall: 0.8800, F1_score: 0.8312

Eval Phase: 
Validation loss: 294.6306, Validation accuracy: 0.5400
Macro F1-score: 0.5264
Model performance on Angry speech (in validation): 
	Precision: 0.5325, Recall: 0.8200, F1_score: 0.6457
Model performance on Happy speech (in validation): 
	Precision: 0.5806, Recall: 0.3600, F1_score: 0.4444
Model performance on Neutral speech (in validation): 
	Precision: 0.4091, Recall: 0.3600, F1_score: 0.3830
Model performance on Sad speech (in validation): 
	Precision: 0.6458, Recall: 0.6200, F1_score: 0.6327
Epoch 11/100

Training Phase:
ing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 675/1600 [00:40<00:56, 16.52it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 844/1600 [00:51<00:45, 16.47it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1009/1600 [01:01<00:35, 16.43it/s]Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1173/1600 [01:11<00:26, 16.41it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1339/1600 [01:21<00:15, 16.45it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1505/1600 [01:31<00:05, 16.41it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 167/1600 [00:10<01:25, 16.70it/s]Training:  21%|â–ˆâ–ˆ        | 334/1600 [00:20<01:16, 16.44it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 504/1600 [00:30<01:05, 16.69it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 674/1600 [00:40<00:55, 16.67it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 841/1600 [00:50<00:46, 16.47it/s]Training:Training loss: 674.8790, Training accuracy: 0.8337
Macro F1-score: 0.8335
Model performance on Angry speech (in training): 
	Precision: 0.9223, Recall: 0.9200, F1_score: 0.9212
Model performance on Happy speech (in training): 
	Precision: 0.8658, Recall: 0.7900, F1_score: 0.8261
Model performance on Neutral speech (in training): 
	Precision: 0.7661, Recall: 0.7450, F1_score: 0.7554
Model performance on Sad speech (in training): 
	Precision: 0.7875, Recall: 0.8800, F1_score: 0.8312

Eval Phase: 
Validation loss: 344.2727, Validation accuracy: 0.5350
Macro F1-score: 0.5069
Model performance on Angry speech (in validation): 
	Precision: 0.4819, Recall: 0.8000, F1_score: 0.6015
Model performance on Happy speech (in validation): 
	Precision: 0.6522, Recall: 0.3000, F1_score: 0.4110
Model performance on Neutral speech (in validation): 
	Precision: 0.4828, Recall: 0.2800, F1_score: 0.3544
Model performance on Sad speech (in validation): 
	Precision: 0.5846, Recall: 0.7600, F1_score: 0.6609
Epoch 12/100

Training Phase:
  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1011/1600 [01:01<00:35, 16.56it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1179/1600 [01:11<00:25, 16.33it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1343/1600 [01:21<00:15, 16.32it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1507/1600 [01:31<00:05, 16.31it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 166/1600 [00:10<01:26, 16.52it/s]Training:  21%|â–ˆâ–ˆ        | 333/1600 [00:20<01:16, 16.60it/s]Training:  31%|â–ˆâ–ˆâ–ˆâ–      | 500/1600 [00:30<01:06, 16.57it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 666/1600 [00:40<00:57, 16.35it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 833/1600 [00:50<00:46, 16.45it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1006/1600 [01:00<00:35, 16.73it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1179/1600 [01:10<00:24, 16.87it/s]TraTraining loss: 631.8373, Training accuracy: 0.8494
Macro F1-score: 0.8489
Model performance on Angry speech (in training): 
	Precision: 0.9340, Recall: 0.9200, F1_score: 0.9270
Model performance on Happy speech (in training): 
	Precision: 0.8325, Recall: 0.8075, F1_score: 0.8198
Model performance on Neutral speech (in training): 
	Precision: 0.8084, Recall: 0.7700, F1_score: 0.7887
Model performance on Sad speech (in training): 
	Precision: 0.8238, Recall: 0.9000, F1_score: 0.8602

Eval Phase: 
Validation loss: 314.5341, Validation accuracy: 0.5750
Macro F1-score: 0.5596
Model performance on Angry speech (in validation): 
	Precision: 0.6296, Recall: 0.6800, F1_score: 0.6538
Model performance on Happy speech (in validation): 
	Precision: 0.4884, Recall: 0.4200, F1_score: 0.4516
Model performance on Neutral speech (in validation): 
	Precision: 0.5806, Recall: 0.3600, F1_score: 0.4444
Model performance on Sad speech (in validation): 
	Precision: 0.5833, Recall: 0.8400, F1_score: 0.6885
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6300

Test Phase: 
ining:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1351/1600 [01:20<00:14, 16.96it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1523/1600 [01:30<00:04, 17.03it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   4%|â–Ž         | 7/200 [00:00<00:03, 63.40it/s]Testing:   8%|â–Š         | 15/200 [00:00<00:02, 69.06it/s]Testing:  11%|â–ˆ         | 22/200 [00:00<00:02, 69.10it/s]Testing:  14%|â–ˆâ–        | 29/200 [00:00<00:02, 67.63it/s]Testing:  18%|â–ˆâ–Š        | 37/200 [00:00<00:02, 68.94it/s]Testing:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [00:00<00:02, 69.93it/s]Testing:  26%|â–ˆâ–ˆâ–‹       | 53/200 [00:00<00:02, 70.37it/s]Testing:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:00<00:01, 69.59it/s]Testing:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:00<00:01, 69.18it/s]Testing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:01<00:01, 69.75it/s]Testing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [00:01<00:01, 67.20it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [00:01<00:01, 69.46it/s]Testing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:01<00:01, 66.41it/s]Testing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:01<00:01, 65.76it/s]Testing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:01<00:01, 67.20it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:01<00:01, 67.63it/s]Testing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:01<00:01, 67.98it/s]Testing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:01<00:00, 68.31it/s]Testing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [00:02<00:00, 67.04it/s]Testing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [00:02<00:00, 66.15it/s]Testing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [00:02<00:00, 67.75it/s]Testing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:02<00:00, 66.79it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:02<00:00, 66.65it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [00:02<00:00, 67.93it/s]Testing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–Test loss: 210.9659, Test accuracy: 0.6450
Macro F1-score: 0.6391
Model performance on Angry speech (in test): 
	Precision: 0.6415, Recall: 0.6800, F1_score: 0.6602
Model performance on Happy speech (in test): 
	Precision: 0.7667, Recall: 0.4600, F1_score: 0.5750
Model performance on Neutral speech (in test): 
	Precision: 0.6327, Recall: 0.6200, F1_score: 0.6263
Model performance on Sad speech (in test): 
	Precision: 0.6029, Recall: 0.8200, F1_score: 0.6949

======================= This is fold_3 on en =======================

Load dataset: 
Loading en train data: fold_3...
Preprocess en fold_3 data for en model
ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [00:02<00:00, 65.59it/s]Testing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [00:02<00:00, 66.31it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:02<00:00, 66.14it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 4/1600 [00:00<00:58, 27.46 examples/s]Map:   1%|          | 13/1600 [00:00<00:27, 57.71 examples/s]Map:   2%|â–         | 24/1600 [00:00<00:20, 75.66 examples/s]Map:   2%|â–         | 33/1600 [00:00<00:20, 77.45 examples/s]Map:   3%|â–Ž         | 45/1600 [00:00<00:17, 90.09 examples/s]Map:   3%|â–Ž         | 55/1600 [00:00<00:18, 84.38 examples/s]Map:   4%|â–         | 64/1600 [00:00<00:18, 82.77 examples/s]Map:   5%|â–         | 77/1600 [00:00<00:16, 91.85 examples/s]Map:   6%|â–Œ         | 88/1600 [00:01<00:17, 88.82 examples/s]Map:   6%|â–Œ         | 97/1600 [00:01<00:17, 88.21 examples/s]Map:   7%|â–‹         | 108/1600 [00:01<00:20, 71.07 examples/s]Map:   8%|â–Š         | 121/1600 [00:01<00:17, 82.40 examples/s]Map:   8%|â–Š         | 133/1600 [00:01<00:16, 88.47 examples/s]Map:   9%|â–‰         | 145/1600 [00:01<00:15, 93.11 examples/s]Map:  10%|â–‰         | 156/1600 [00:01<00:15, 94.72 examples/s]Map:  11%|â–ˆ         | 169/1600 [00:01<00:14, 101.19 examples/s]Map:  12%|â–ˆâ–        | 185/1600 [00:02<00:13, 101.23 examples/s]Map:  13%|â–ˆâ–Ž        | 201/1600 [00:02<00:12, 113.97 examples/s]Map:  14%|â–ˆâ–Ž        | 216/1600 [00:02<00:11, 116.29 examples/s]Map:  14%|â–ˆâ–        | 228/1600 [00:02<00:14, 96.48 examples/s] Map:  15%|â–ˆâ–Œ        | 243/1600 [00:02<00:12, 107.13 examples/s]Map:  16%|â–ˆâ–Œ        | 256/1600 [00:02<00:12, 111.91 examples/s]Map:  17%|â–ˆâ–‹        | 268/1600 [00:02<00:11, 111.50 examples/s]Map:  18%|â–ˆâ–Š        | 281/1600 [00:02<00:11, 114.79 examples/s]Map:  18%|â–ˆâ–Š        | 294/1600 [00:03<00:11, 117.59 examples/s]Map:  19%|â–ˆâ–‰        | 309/1600 [00:03<00:10, 124.28 examples/s]Map:  20%|â–ˆâ–ˆ        | 325/1600 [00:03<00:09, 131.62 examples/s]Map:  21%|â–ˆâ–ˆâ–       | 343/1600 [00:03<00:11, 109.77 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 357/1600 [00:03<00:10, 115.02 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 375/1600 [00:03<00:10, 113.23 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 392/1600 [00:03<00:09, 124.70 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 408/1600 [00:04<00:10, 112.18 examples/s]Map:  26%|â–ˆâ–ˆâ–‹       | 420/1600 [00:04<00:10, 110.94 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 439/1600 [00:04<00:10, 113.23 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 455/1600 [00:04<00:10, 107.43 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 473/1600 [00:04<00:09, 121.45 examples/s]Map:  30%|â–ˆâ–ˆâ–ˆ       | 488/1600 [00:04<00:08, 127.69 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 505/1600 [00:04<00:08, 135.29 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 521/1600 [00:05<00:09, 109.22 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 536/1600 [00:05<00:10, 101.83 examples/s]Map:  35%|â–ˆâ–ˆâ–ˆâ–      | 557/1600 [00:05<00:10, 102.76 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 572/1600 [00:05<00:09, 111.77 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 590/1600 [00:05<00:08, 120.87 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 606/1600 [00:05<00:07, 128.60 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:05<00:07, 125.60 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 636/1600 [00:05<00:07, 129.31 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 655/1600 [00:06<00:08, 113.03 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 672/1600 [00:06<00:08, 108.00 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 686/1600 [00:06<00:08, 102.09 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 698/1600 [00:06<00:08, 102.15 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 712/1600 [00:06<00:08, 110.62 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 728/1600 [00:06<00:07, 117.37 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 747/1600 [00:07<00:07, 108.59 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 762/1600 [00:07<00:08, 103.05 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 774/1600 [00:07<00:08, 93.44 examples/s] Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 785/1600 [00:07<00:08, 94.46 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 798/1600 [00:07<00:08, 89.99 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 811/1600 [00:07<00:08, 96.07 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 821/1600 [00:07<00:08, 95.04 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1600 [00:07<00:08, 95.67 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 842/1600 [00:08<00:18, 41.89 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 857/1600 [00:08<00:13, 56.24 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 874/1600 [00:08<00:10, 72.31 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 887/1600 [00:08<00:08, 81.95 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 902/1600 [00:09<00:07, 92.88 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 914/1600 [00:09<00:06, 98.23 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 926/1600 [00:09<00:06, 101.56 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 941/1600 [00:09<00:06, 98.11 examples/s] Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 954/1600 [00:09<00:06, 103.47 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 966/1600 [00:09<00:06, 103.04 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 980/1600 [00:09<00:05, 106.49 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:09<00:05, 108.93 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:21<00:05, 108.93 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [01:03<14:58,  1.50s/ examples]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1013/1600 [01:04<09:42,  1.01 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1029/1600 [01:04<05:56,  1.60 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [01:04<03:42,  2.49 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1064/1600 [01:04<02:21,  3.80 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1081/1600 [01:04<01:34,  5.52 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1094/1600 [01:04<01:08,  7.35 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1112/1600 [01:04<00:45, 10.75 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1125/1600 [01:04<00:33, 14.10 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1143/1600 [01:05<00:24, 18.95 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1160/1600 [01:05<00:16, 26.24 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1174/1600 [01:05<00:12, 33.14 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1186/1600 [01:05<00:10, 40.02 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1201/1600 [01:05<00:07, 51.51 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1216/1600 [01:05<00:06, 58.87 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1229/1600 [01:06<00:05, 67.72 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1242/1600 [01:06<00:05, 70.89 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1253/1600 [01:06<00:04, 69.49 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1267/1600 [01:06<00:04, 74.68 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1281/1600 [01:06<00:04, 76.97 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1290/1600 [01:06<00:04, 77.37 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1305/1600 [01:06<00:03, 81.28 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1318/1600 [01:07<00:03, 89.91 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1336/1600 [01:07<00:02, 106.67 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1353/1600 [01:07<00:02, 119.20 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1367/1600 [01:07<00:02, 103.98 examples/s]Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1384/1600 [01:07<00:02, 105.06 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1397/1600 [01:07<00:01, 109.34 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1415/1600 [01:07<00:01, 125.42 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1429/1600 [01:07<00:01, 127.97 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1444/1600 [01:08<00:01, 132.09 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1460/1600 [01:08<00:01, 138.53 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1475/1600 [01:08<00:00, 125.28 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1494/1600 [01:08<00:00, 121.13 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1511/1600 [01:08<00:00, 115.26 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1524/1600 [01:08<00:00, 116.94 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1538/1600 [01:08<00:00, 121.46 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1558/1600 [01:09<00:00, 117.85 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1578/1600 [01:09<00:00, 135.47 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1593/1600 [01:09<00:00, 119.98 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1593/1600 [01:22<00:00, 119.98 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:43<00:00,  1.35 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:43<00:00, 15.50 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|â–Ž         | 5/200 [00:00<00:06, 29.42 examples/s]Map:   6%|â–Œ         | 11/200 [00:00<00:04, 40.91 examples/s]Map:  11%|â–ˆ         | 22/200 [00:00<00:02, 63.17 examples/s]Map:  16%|â–ˆâ–‹        | 33/200 [00:00<00:02, 78.25 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [00:00<00:01, 93.57 examples/s]Map:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:00<00:01, 99.10 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [00:00<00:01, 111.85 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [00:01<00:01, 97.35 examples/s] Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [00:01<00:01, 92.58 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [00:01<00:01, 83.84 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [00:01<00:00, 94.30 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [00:01<00:00, 96.92 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [00:01<00:00, 89.27 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [00:01<00:00, 95.05 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [00:01<00:00, 94.79 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [00:02<00:00, 98.71 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:02<00:00, 101.25 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:14<00:00, 13.81 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|â–         | 4/200 [00:00<00:06, 29.07 examples/s]Map:   8%|â–Š         | 15/200 [00:00<00:02, 67.04 examples/s]Map:  12%|â–ˆâ–Ž        | 25/200 [00:00<00:02, 79.79 examples/s]Map:  19%|â–ˆâ–‰        | 38/200 [00:00<00:01, 95.22 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 49/200 [00:00<00:01, 97.17 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:00<00:01, 112.33 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:00<00:01, 118.80 examples/s]Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:00<00:00, 112.89 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [00:01<00:01, 92.63 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:01<00:00, 105.03 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:01<00:00, 98.16 examples/s] Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [00:01<00:00, 102.75 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [00:01<00:00, 99.96 examples/s] Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [00:01<00:00, 103.31 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [00:01<00:00, 112.43 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:13<00:00,  3.83 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:13<00:00, 15.16 examples/s]
Loading en eval data: fold_3...
Preprocess en fold_3 data for en model
Loading en test data: fold_3...
Preprocess en fold_3 data for en model
Use en model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1700.6146, Training accuracy: 0.5387
Macro F1-score: 0.5314
Model performance on Angry speech (in training): 
	Precision: 0.5843, Recall: 0.6150, F1_score: 0.5993
Model performance on Happy speech (in training): 
	Precision: 0.4625, Recall: 0.3550, F1_score: 0.4017
Model performance on Neutral speech (in training): 
	Precision: 0.4721, Recall: 0.4650, F1_score: 0.4685
Model performance on Sad speech (in training): 
	Precision: 0.6025, Recall: 0.7200, F1_score: 0.6560

Eval Phase: 
Validation loss: 187.9296, Validation accuracy: 0.5850
Macro F1-score: 0.5703
Model performance on Angry speech (in validation): 
	Precision: 0.7907, Recall: 0.6800, F1_score: 0.7312
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.2600, F1_score: 0.4127
Model performance on Neutral speech (in validation): 
	Precision: 0.3881, Recall: 0.5200, F1_score: 0.4444
Model performance on Sad speech (in validation): 
	Precision: 0.5714, Recall: 0.8800, F1_score: 0.6929
New best accuracy for layer 8 on epoch 1: 0.5850. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 160/1600 [00:10<01:30, 15.95it/s]Training:  20%|â–ˆâ–ˆ        | 325/1600 [00:20<01:18, 16.23it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 504/1600 [00:30<01:04, 16.98it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 683/1600 [00:40<00:54, 16.79it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 849/1600 [00:51<00:45, 16.52it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1032/1600 [01:01<00:33, 17.09it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1215/1600 [01:11<00:22, 17.28it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1392/1600 [01:21<00:12, 17.27it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1565/1600 [01:32<00:02, 17.13it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆâ–        | 180/1600 [00:10<01:19, 17.95it/s]Training:  22%|â–ˆâ–ˆâ–Ž       Training loss: 1488.2428, Training accuracy: 0.6200
Macro F1-score: 0.6130
Model performance on Angry speech (in training): 
	Precision: 0.6831, Recall: 0.7275, F1_score: 0.7046
Model performance on Happy speech (in training): 
	Precision: 0.5466, Recall: 0.4400, F1_score: 0.4875
Model performance on Neutral speech (in training): 
	Precision: 0.5491, Recall: 0.5175, F1_score: 0.5328
Model performance on Sad speech (in training): 
	Precision: 0.6695, Recall: 0.7950, F1_score: 0.7269

Eval Phase: 
Validation loss: 163.1881, Validation accuracy: 0.6650
Macro F1-score: 0.6621
Model performance on Angry speech (in validation): 
	Precision: 0.8636, Recall: 0.7600, F1_score: 0.8085
Model performance on Happy speech (in validation): 
	Precision: 0.9200, Recall: 0.4600, F1_score: 0.6133
Model performance on Neutral speech (in validation): 
	Precision: 0.4643, Recall: 0.5200, F1_score: 0.4906
Model performance on Sad speech (in validation): 
	Precision: 0.6133, Recall: 0.9200, F1_score: 0.7360
New best accuracy for layer 8 on epoch 2: 0.6650. Model saved.
Epoch 3/100

Training Phase:
| 360/1600 [00:20<01:11, 17.44it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 534/1600 [00:30<01:01, 17.41it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 710/1600 [00:40<00:51, 17.45it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 889/1600 [00:50<00:40, 17.59it/s]Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 889/1600 [01:01<00:40, 17.59it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1062/1600 [01:01<00:31, 17.24it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1237/1600 [01:11<00:20, 17.31it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1412/1600 [01:21<00:10, 17.35it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1590/1600 [01:31<00:00, 17.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 173/1600 [00:10<01:22, 17.27it/s]Training:  22%|â–ˆâ–ˆâ–       | 347/1600 [00:20<01:12, 17.34it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 52Training loss: 1342.7460, Training accuracy: 0.6619
Macro F1-score: 0.6577
Model performance on Angry speech (in training): 
	Precision: 0.7389, Recall: 0.7500, F1_score: 0.7444
Model performance on Happy speech (in training): 
	Precision: 0.6257, Recall: 0.5225, F1_score: 0.5695
Model performance on Neutral speech (in training): 
	Precision: 0.5964, Recall: 0.5725, F1_score: 0.5842
Model performance on Sad speech (in training): 
	Precision: 0.6744, Recall: 0.8025, F1_score: 0.7329

Eval Phase: 
Validation loss: 176.0214, Validation accuracy: 0.6300
Macro F1-score: 0.6280
Model performance on Angry speech (in validation): 
	Precision: 0.8780, Recall: 0.7200, F1_score: 0.7912
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.3200, F1_score: 0.4848
Model performance on Neutral speech (in validation): 
	Precision: 0.4074, Recall: 0.6600, F1_score: 0.5038
Model performance on Sad speech (in validation): 
	Precision: 0.6613, Recall: 0.8200, F1_score: 0.7321
Epoch 4/100

Training Phase:
8/1600 [00:30<01:00, 17.65it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 709/1600 [00:40<00:50, 17.54it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 883/1600 [00:50<00:41, 17.35it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1058/1600 [01:00<00:31, 17.38it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1233/1600 [01:10<00:21, 17.37it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1412/1600 [01:20<00:10, 17.53it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1591/1600 [01:31<00:00, 17.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 166/1600 [00:10<01:26, 16.60it/s]Training:  21%|â–ˆâ–ˆ        | 339/1600 [00:20<01:14, 16.99it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 516/1600 [00:30<01:02, 17.28it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 516/1600 [00:40<01:02, 17.28it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [Training loss: 1274.5547, Training accuracy: 0.6813
Macro F1-score: 0.6786
Model performance on Angry speech (in training): 
	Precision: 0.7488, Recall: 0.7675, F1_score: 0.7580
Model performance on Happy speech (in training): 
	Precision: 0.6647, Recall: 0.5650, F1_score: 0.6108
Model performance on Neutral speech (in training): 
	Precision: 0.6117, Recall: 0.6025, F1_score: 0.6071
Model performance on Sad speech (in training): 
	Precision: 0.6930, Recall: 0.7900, F1_score: 0.7383

Eval Phase: 
Validation loss: 202.5830, Validation accuracy: 0.5950
Macro F1-score: 0.5850
Model performance on Angry speech (in validation): 
	Precision: 0.9429, Recall: 0.6600, F1_score: 0.7765
Model performance on Happy speech (in validation): 
	Precision: 0.8889, Recall: 0.3200, F1_score: 0.4706
Model performance on Neutral speech (in validation): 
	Precision: 0.4400, Recall: 0.4400, F1_score: 0.4400
Model performance on Sad speech (in validation): 
	Precision: 0.4948, Recall: 0.9600, F1_score: 0.6531
Epoch 5/100

Training Phase:
00:40<00:53, 16.92it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 857/1600 [00:50<00:44, 16.84it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1024/1600 [01:00<00:34, 16.66it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1195/1600 [01:10<00:24, 16.79it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1366/1600 [01:21<00:14, 16.65it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1538/1600 [01:31<00:03, 16.81it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 178/1600 [00:10<01:20, 17.74it/s]Training:  22%|â–ˆâ–ˆâ–       | 358/1600 [00:20<01:09, 17.84it/s]Training:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 538/1600 [00:30<01:01, 17.28it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 705/1600 [00:41<00:53, 16.86it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:51<00:44, 16.50it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1037/160Training loss: 1161.2373, Training accuracy: 0.7100
Macro F1-score: 0.7070
Model performance on Angry speech (in training): 
	Precision: 0.7955, Recall: 0.7875, F1_score: 0.7915
Model performance on Happy speech (in training): 
	Precision: 0.6850, Recall: 0.5925, F1_score: 0.6354
Model performance on Neutral speech (in training): 
	Precision: 0.6406, Recall: 0.6150, F1_score: 0.6276
Model performance on Sad speech (in training): 
	Precision: 0.7131, Recall: 0.8450, F1_score: 0.7735

Eval Phase: 
Validation loss: 167.7728, Validation accuracy: 0.6700
Macro F1-score: 0.6682
Model performance on Angry speech (in validation): 
	Precision: 0.8333, Recall: 0.8000, F1_score: 0.8163
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.4200, F1_score: 0.5915
Model performance on Neutral speech (in validation): 
	Precision: 0.4697, Recall: 0.6200, F1_score: 0.5345
Model performance on Sad speech (in validation): 
	Precision: 0.6462, Recall: 0.8400, F1_score: 0.7304
New best accuracy for layer 8 on epoch 5: 0.6700. Model saved.
Epoch 6/100

Training Phase:
0 [01:01<00:33, 16.60it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1205/1600 [01:11<00:23, 16.58it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1371/1600 [01:21<00:13, 16.53it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1539/1600 [01:31<00:03, 16.59it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 179/1600 [00:10<01:19, 17.83it/s]Training:  22%|â–ˆâ–ˆâ–       | 358/1600 [00:20<01:10, 17.68it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 534/1600 [00:30<01:01, 17.39it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 710/1600 [00:40<00:50, 17.46it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 886/1600 [00:51<00:41, 17.15it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1061/1600 [01:01<00:31, 17.23it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1235/1600 [01:12<00:21, 16.66it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Training loss: 1067.2999, Training accuracy: 0.7306
Macro F1-score: 0.7278
Model performance on Angry speech (in training): 
	Precision: 0.7935, Recall: 0.7975, F1_score: 0.7955
Model performance on Happy speech (in training): 
	Precision: 0.7251, Recall: 0.6000, F1_score: 0.6566
Model performance on Neutral speech (in training): 
	Precision: 0.6923, Recall: 0.6750, F1_score: 0.6835
Model performance on Sad speech (in training): 
	Precision: 0.7128, Recall: 0.8500, F1_score: 0.7754

Eval Phase: 
Validation loss: 156.2181, Validation accuracy: 0.7050
Macro F1-score: 0.7078
Model performance on Angry speech (in validation): 
	Precision: 0.8511, Recall: 0.8000, F1_score: 0.8247
Model performance on Happy speech (in validation): 
	Precision: 0.9259, Recall: 0.5000, F1_score: 0.6494
Model performance on Neutral speech (in validation): 
	Precision: 0.5195, Recall: 0.8000, F1_score: 0.6299
Model performance on Sad speech (in validation): 
	Precision: 0.7347, Recall: 0.7200, F1_score: 0.7273
New best accuracy for layer 8 on epoch 6: 0.7050. Model saved.
Epoch 7/100

Training Phase:
‹ | 1399/1600 [01:22<00:12, 16.58it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1564/1600 [01:32<00:02, 16.53it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–‰         | 152/1600 [00:10<01:35, 15.14it/s]Training:  20%|â–ˆâ–‰        | 318/1600 [00:20<01:20, 15.99it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 484/1600 [00:30<01:08, 16.24it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 651/1600 [00:40<00:57, 16.40it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 818/1600 [00:50<00:47, 16.36it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 982/1600 [01:00<00:37, 16.36it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1154/1600 [01:10<00:26, 16.62it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1329/1600 [01:20<00:16, 16.89it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1504/1600 [01:30<00:05, 16.99it/s]                                  Training loss: 969.7551, Training accuracy: 0.7681
Macro F1-score: 0.7659
Model performance on Angry speech (in training): 
	Precision: 0.8433, Recall: 0.8475, F1_score: 0.8454
Model performance on Happy speech (in training): 
	Precision: 0.7573, Recall: 0.6475, F1_score: 0.6981
Model performance on Neutral speech (in training): 
	Precision: 0.7212, Recall: 0.7050, F1_score: 0.7130
Model performance on Sad speech (in training): 
	Precision: 0.7505, Recall: 0.8725, F1_score: 0.8069

Eval Phase: 
Validation loss: 179.4125, Validation accuracy: 0.6550
Macro F1-score: 0.6607
Model performance on Angry speech (in validation): 
	Precision: 0.9167, Recall: 0.6600, F1_score: 0.7674
Model performance on Happy speech (in validation): 
	Precision: 0.6170, Recall: 0.5800, F1_score: 0.5979
Model performance on Neutral speech (in validation): 
	Precision: 0.5205, Recall: 0.7600, F1_score: 0.6179
Model performance on Sad speech (in validation): 
	Precision: 0.7045, Recall: 0.6200, F1_score: 0.6596
Epoch 8/100

Training Phase:
Training loss: 910.6864, Training accuracy: 0.7756
Macro F1-score: 0.7743
Model performance on Angry speech (in training): 
	Precision: 0.8618, Recall: 0.8575, F1_score: 0.8596
Model performance on Happy speech (in training): 
	Precision: 0.7772, Recall: 0.6975, F1_score: 0.7352
Model performance on Neutral speech (in training): 
	Precision: 0.7237, Recall: 0.6875, F1_score: 0.7051
Model performance on Sad speech (in training): 
	Precision: 0.7430, Recall: 0.8600, F1_score: 0.7972

Eval Phase: 
Validation loss: 167.4042, Validation accuracy: 0.6750
Macro F1-score: 0.6702
Model performance on Angry speech (in validation): 
	Precision: 0.7963, Recall: 0.8600, F1_score: 0.8269
Model performance on Happy speech (in validation): 
	Precision: 0.8065, Recall: 0.5000, F1_score: 0.6173
Model performance on Neutral speech (in validation): 
	Precision: 0.5294, Recall: 0.5400, F1_score: 0.5347
Model performance on Sad speech (in validation): 
	Precision: 0.6250, Recall: 0.8000, F1_score: 0.7018
Epoch 9/100

Training Phase:
                           Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 173/1600 [00:10<01:22, 17.30it/s]Training:  22%|â–ˆâ–ˆâ–       | 346/1600 [00:20<01:13, 17.07it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 516/1600 [00:30<01:04, 16.82it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 683/1600 [00:40<00:54, 16.74it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 852/1600 [00:50<00:44, 16.77it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1021/1600 [01:00<00:34, 16.69it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1195/1600 [01:10<00:23, 16.92it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1369/1600 [01:21<00:13, 16.78it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1547/1600 [01:31<00:03, 17.08it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   Training loss: 796.9012, Training accuracy: 0.8113
Macro F1-score: 0.8105
Model performance on Angry speech (in training): 
	Precision: 0.8769, Recall: 0.8725, F1_score: 0.8747
Model performance on Happy speech (in training): 
	Precision: 0.8174, Recall: 0.7275, F1_score: 0.7698
Model performance on Neutral speech (in training): 
	Precision: 0.7892, Recall: 0.7675, F1_score: 0.7782
Model performance on Sad speech (in training): 
	Precision: 0.7681, Recall: 0.8775, F1_score: 0.8191

Eval Phase: 
Validation loss: 185.4057, Validation accuracy: 0.6650
Macro F1-score: 0.6638
Model performance on Angry speech (in validation): 
	Precision: 0.8810, Recall: 0.7400, F1_score: 0.8043
Model performance on Happy speech (in validation): 
	Precision: 0.8400, Recall: 0.4200, F1_score: 0.5600
Model performance on Neutral speech (in validation): 
	Precision: 0.4928, Recall: 0.6800, F1_score: 0.5714
Model performance on Sad speech (in validation): 
	Precision: 0.6406, Recall: 0.8200, F1_score: 0.7193
Epoch 10/100

Training Phase:
0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 166/1600 [00:10<01:27, 16.47it/s]Training:  21%|â–ˆâ–ˆ        | 338/1600 [00:20<01:14, 16.88it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 510/1600 [00:30<01:05, 16.73it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 679/1600 [00:40<00:54, 16.78it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 848/1600 [00:50<00:45, 16.70it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1014/1600 [01:00<00:35, 16.54it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1178/1600 [01:10<00:25, 16.47it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1343/1600 [01:21<00:15, 16.45it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1508/1600 [01:31<00:05, 16.24it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 164/1600 [00:10<01:27, 16.37it/s]Training:  21%|â–ˆâ–ˆ        | 331/1600 [00:20Training loss: 703.4078, Training accuracy: 0.8438
Macro F1-score: 0.8436
Model performance on Angry speech (in training): 
	Precision: 0.9059, Recall: 0.8900, F1_score: 0.8979
Model performance on Happy speech (in training): 
	Precision: 0.8626, Recall: 0.7850, F1_score: 0.8220
Model performance on Neutral speech (in training): 
	Precision: 0.8075, Recall: 0.8075, F1_score: 0.8075
Model performance on Sad speech (in training): 
	Precision: 0.8059, Recall: 0.8925, F1_score: 0.8470

Eval Phase: 
Validation loss: 207.5472, Validation accuracy: 0.6650
Macro F1-score: 0.6627
Model performance on Angry speech (in validation): 
	Precision: 0.9722, Recall: 0.7000, F1_score: 0.8140
Model performance on Happy speech (in validation): 
	Precision: 0.8056, Recall: 0.5800, F1_score: 0.6744
Model performance on Neutral speech (in validation): 
	Precision: 0.5122, Recall: 0.4200, F1_score: 0.4615
Model performance on Sad speech (in validation): 
	Precision: 0.5517, Recall: 0.9600, F1_score: 0.7007
Epoch 11/100

Training Phase:
<01:16, 16.55it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 498/1600 [00:30<01:06, 16.50it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 665/1600 [00:40<00:56, 16.56it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 833/1600 [00:50<00:46, 16.63it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1001/1600 [01:00<00:36, 16.44it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1178/1600 [01:10<00:25, 16.84it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1355/1600 [01:20<00:14, 17.04it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1530/1600 [01:30<00:04, 17.13it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 172/1600 [00:10<01:23, 17.18it/s]Training:  22%|â–ˆâ–ˆâ–       | 347/1600 [00:20<01:12, 17.32it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 527/1600 [00:30<01:00, 17.60it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 707/1600 [00:40<00:51, Training loss: 697.9776, Training accuracy: 0.8325
Macro F1-score: 0.8321
Model performance on Angry speech (in training): 
	Precision: 0.8934, Recall: 0.8800, F1_score: 0.8866
Model performance on Happy speech (in training): 
	Precision: 0.8500, Recall: 0.7650, F1_score: 0.8053
Model performance on Neutral speech (in training): 
	Precision: 0.7945, Recall: 0.7925, F1_score: 0.7935
Model performance on Sad speech (in training): 
	Precision: 0.7987, Recall: 0.8925, F1_score: 0.8430

Eval Phase: 
Validation loss: 185.5486, Validation accuracy: 0.6750
Macro F1-score: 0.6650
Model performance on Angry speech (in validation): 
	Precision: 0.8367, Recall: 0.8200, F1_score: 0.8283
Model performance on Happy speech (in validation): 
	Precision: 0.9231, Recall: 0.4800, F1_score: 0.6316
Model performance on Neutral speech (in validation): 
	Precision: 0.5238, Recall: 0.4400, F1_score: 0.4783
Model performance on Sad speech (in validation): 
	Precision: 0.5783, Recall: 0.9600, F1_score: 0.7218
Epoch 12/100

Training Phase:
17.35it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 881/1600 [00:50<00:41, 17.34it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1600 [01:01<00:32, 16.70it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1231/1600 [01:11<00:21, 16.97it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1407/1600 [01:22<00:11, 17.08it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1581/1600 [01:33<00:01, 16.52it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 170/1600 [00:10<01:24, 16.91it/s]Training:  22%|â–ˆâ–ˆâ–       | 347/1600 [00:20<01:12, 17.36it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 524/1600 [00:30<01:02, 17.30it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 698/1600 [00:40<00:52, 17.34it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 877/1600 [00:50<00:41, 17.50it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1055/1600 [01:00<00:3Training loss: 619.4147, Training accuracy: 0.8606
Macro F1-score: 0.8602
Model performance on Angry speech (in training): 
	Precision: 0.9165, Recall: 0.9050, F1_score: 0.9107
Model performance on Happy speech (in training): 
	Precision: 0.8709, Recall: 0.7925, F1_score: 0.8298
Model performance on Neutral speech (in training): 
	Precision: 0.8440, Recall: 0.8250, F1_score: 0.8344
Model performance on Sad speech (in training): 
	Precision: 0.8178, Recall: 0.9200, F1_score: 0.8659

Eval Phase: 
Validation loss: 196.2667, Validation accuracy: 0.6650
Macro F1-score: 0.6655
Model performance on Angry speech (in validation): 
	Precision: 0.8511, Recall: 0.8000, F1_score: 0.8247
Model performance on Happy speech (in validation): 
	Precision: 0.7742, Recall: 0.4800, F1_score: 0.5926
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.6200, F1_score: 0.5536
Model performance on Sad speech (in validation): 
	Precision: 0.6333, Recall: 0.7600, F1_score: 0.6909
Epoch 13/100

Training Phase:
1, 17.33it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1232/1600 [01:10<00:21, 17.45it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1409/1600 [01:21<00:10, 17.36it/s]Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1583/1600 [01:31<00:00, 17.35it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 171/1600 [00:10<01:24, 17.00it/s]Training:  21%|â–ˆâ–ˆâ–       | 342/1600 [00:20<01:13, 17.01it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 516/1600 [00:30<01:03, 17.15it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 691/1600 [00:40<00:52, 17.26it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 867/1600 [00:50<00:42, 17.36it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1047/1600 [01:00<00:31, 17.55it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1227/1600 [01:10<00:21, 17.65it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1406/1600Training loss: 578.3825, Training accuracy: 0.8581
Macro F1-score: 0.8578
Model performance on Angry speech (in training): 
	Precision: 0.9370, Recall: 0.9300, F1_score: 0.9335
Model performance on Happy speech (in training): 
	Precision: 0.8672, Recall: 0.8000, F1_score: 0.8322
Model performance on Neutral speech (in training): 
	Precision: 0.8159, Recall: 0.7975, F1_score: 0.8066
Model performance on Sad speech (in training): 
	Precision: 0.8172, Recall: 0.9050, F1_score: 0.8588

Eval Phase: 
Validation loss: 208.6235, Validation accuracy: 0.6350
Macro F1-score: 0.6288
Model performance on Angry speech (in validation): 
	Precision: 0.8039, Recall: 0.8200, F1_score: 0.8119
Model performance on Happy speech (in validation): 
	Precision: 0.8333, Recall: 0.4000, F1_score: 0.5405
Model performance on Neutral speech (in validation): 
	Precision: 0.4643, Recall: 0.5200, F1_score: 0.4906
Model performance on Sad speech (in validation): 
	Precision: 0.5797, Recall: 0.8000, F1_score: 0.6723
Epoch 14/100

Training Phase:
 [01:21<00:11, 17.36it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1574/1600 [01:31<00:01, 17.13it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 164/1600 [00:10<01:27, 16.38it/s]Training:  21%|â–ˆâ–ˆ        | 330/1600 [00:20<01:17, 16.47it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 496/1600 [00:30<01:06, 16.49it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 662/1600 [00:40<00:56, 16.48it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 837/1600 [00:50<00:45, 16.83it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1012/1600 [01:00<00:34, 17.05it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1187/1600 [01:10<00:24, 17.07it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1187/1600 [01:20<00:24, 17.07it/s]Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1358/1600 [01:20<00:14, 16.95it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ|Training loss: 553.4297, Training accuracy: 0.8688
Macro F1-score: 0.8682
Model performance on Angry speech (in training): 
	Precision: 0.9373, Recall: 0.9350, F1_score: 0.9362
Model performance on Happy speech (in training): 
	Precision: 0.8764, Recall: 0.7975, F1_score: 0.8351
Model performance on Neutral speech (in training): 
	Precision: 0.8346, Recall: 0.8200, F1_score: 0.8272
Model performance on Sad speech (in training): 
	Precision: 0.8311, Recall: 0.9225, F1_score: 0.8744

Eval Phase: 
Validation loss: 190.1314, Validation accuracy: 0.6550
Macro F1-score: 0.6511
Model performance on Angry speech (in validation): 
	Precision: 0.7692, Recall: 0.8000, F1_score: 0.7843
Model performance on Happy speech (in validation): 
	Precision: 0.6364, Recall: 0.5600, F1_score: 0.5957
Model performance on Neutral speech (in validation): 
	Precision: 0.5435, Recall: 0.5000, F1_score: 0.5208
Model performance on Sad speech (in validation): 
	Precision: 0.6552, Recall: 0.7600, F1_score: 0.7037
Epoch 15/100

Training Phase:
Training loss: 493.7699, Training accuracy: 0.8800
Macro F1-score: 0.8799
Model performance on Angry speech (in training): 
	Precision: 0.9424, Recall: 0.9400, F1_score: 0.9412
Model performance on Happy speech (in training): 
	Precision: 0.8859, Recall: 0.8350, F1_score: 0.8597
Model performance on Neutral speech (in training): 
	Precision: 0.8460, Recall: 0.8375, F1_score: 0.8417
Model performance on Sad speech (in training): 
	Precision: 0.8481, Recall: 0.9075, F1_score: 0.8768

Eval Phase: 
 1535/1600 [01:30<00:03, 17.17it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 175/1600 [00:10<01:21, 17.49it/s]Training:  22%|â–ˆâ–ˆâ–       | 350/1600 [00:20<01:12, 17.21it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 522/1600 [00:30<01:02, 17.20it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 694/1600 [00:40<00:52, 17.18it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:50<00:42, 17.25it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [01:00<00:32, 17.29it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1216/1600 [01:10<00:22, 17.31it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1392/1600 [01:20<00:11, 17.39it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1570/1600 [01:30<00:01, 17.50it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?iValidation loss: 233.7471, Validation accuracy: 0.6200
Macro F1-score: 0.6206
Model performance on Angry speech (in validation): 
	Precision: 0.8182, Recall: 0.7200, F1_score: 0.7660
Model performance on Happy speech (in validation): 
	Precision: 0.6757, Recall: 0.5000, F1_score: 0.5747
Model performance on Neutral speech (in validation): 
	Precision: 0.4630, Recall: 0.5000, F1_score: 0.4808
Model performance on Sad speech (in validation): 
	Precision: 0.5846, Recall: 0.7600, F1_score: 0.6609
Epoch 16/100

Training Phase:
Training loss: 490.2823, Training accuracy: 0.8831
Macro F1-score: 0.8831
Model performance on Angry speech (in training): 
	Precision: 0.9496, Recall: 0.9425, F1_score: 0.9460
Model performance on Happy speech (in training): 
	Precision: 0.9066, Recall: 0.8250, F1_score: 0.8639
Model performance on Neutral speech (in training): 
	Precision: 0.8521, Recall: 0.8500, F1_score: 0.8511
Model performance on Sad speech (in training): 
	Precision: 0.8318, Recall: 0.9150, F1_score: 0.8714

Eval Phase: 
Validation loss: 211.7270, Validation accuracy: 0.6650
Macro F1-score: 0.6650
Model performance on Angry speech (in validation): 
	Precision: 0.8723, Recall: 0.8200, F1_score: 0.8454
Model performance on Happy speech (in validation): 
	Precision: 0.6170, Recall: 0.5800, F1_score: 0.5979
Model performance on Neutral speech (in validation): 
	Precision: 0.5306, Recall: 0.5200, F1_score: 0.5253
Model performance on Sad speech (in validation): 
	Precision: 0.6491, Recall: 0.7400, F1_score: 0.6916
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7050

Test Phase: 
t/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 173/1600 [00:10<01:22, 17.27it/s]Training:  22%|â–ˆâ–ˆâ–       | 346/1600 [00:20<01:13, 17.03it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 518/1600 [00:30<01:03, 17.08it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 693/1600 [00:40<00:52, 17.21it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 868/1600 [00:51<00:44, 16.50it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [01:01<00:32, 16.91it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1224/1600 [01:12<00:22, 17.03it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1397/1600 [01:22<00:12, 16.69it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1572/1600 [01:32<00:01, 16.92it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   4%|â–Ž         | 7/200 [00:00<00:02, 69.58it/s]Testing:   7%|â–‹         | 14/200 [00:00<00:02, 66.08it/s]Testing:  10%|â–ˆ         | 21/200 [00:00<00:02, 66.24it/s]Testing:  14%|â–ˆâ–        | 28/200 [00:00<00:02, 66.83it/s]Testing:  18%|â–ˆâ–Š        | 35/200 [00:00<00:02, 61.06it/s]Testing:  21%|â–ˆâ–ˆ        | 42/200 [00:00<00:02, 63.32it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:00<00:02, 66.24it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:00<00:02, 66.66it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:00<00:02, 65.58it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:01<00:01, 66.21it/s]Testing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:01<00:01, 65.87it/s]Testing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:01<00:01, 67.94it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:01<00:01, 65.98it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:01<00:01, 67.49it/s]Testing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [00:01<00:01, 65.23it/s]Testing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [00:01<00:01, 66.41it/s]Testing:  61%|â–ˆâ–ˆâ–ˆâ–Test loss: 156.7869, Test accuracy: 0.6750
Macro F1-score: 0.6776
Model performance on Angry speech (in test): 
	Precision: 0.7500, Recall: 0.7200, F1_score: 0.7347
Model performance on Happy speech (in test): 
	Precision: 0.7632, Recall: 0.5800, F1_score: 0.6591
Model performance on Neutral speech (in test): 
	Precision: 0.5373, Recall: 0.7200, F1_score: 0.6154
Model performance on Sad speech (in test): 
	Precision: 0.7234, Recall: 0.6800, F1_score: 0.7010

======================= This is fold_4 on en =======================

Load dataset: 
Loading en train data: fold_4...
Preprocess en fold_4 data for en model
ˆâ–ˆâ–ˆ    | 122/200 [00:01<00:01, 62.94it/s]Testing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [00:01<00:01, 61.27it/s]Testing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [00:02<00:01, 61.16it/s]Testing:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [00:02<00:00, 63.65it/s]Testing:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [00:02<00:00, 66.24it/s]Testing:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [00:02<00:00, 65.89it/s]Testing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [00:02<00:00, 66.87it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [00:02<00:00, 66.42it/s]Testing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [00:02<00:00, 65.55it/s]Testing:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [00:02<00:00, 64.26it/s]Testing:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [00:02<00:00, 64.90it/s]                                                          Map:   0%|          | 0/1600 [00:00<?, ? examples/s]Map:   0%|          | 2/1600 [00:00<02:03, 12.99 examples/s]Map:   1%|          | 9/1600 [00:00<00:47, 33.80 examples/s]Map:   1%|â–         | 23/1600 [00:00<00:22, 70.05 examples/s]Map:   2%|â–         | 33/1600 [00:00<00:20, 77.88 examples/s]Map:   3%|â–Ž         | 47/1600 [00:00<00:16, 95.02 examples/s]Map:   4%|â–         | 60/1600 [00:00<00:18, 84.93 examples/s]Map:   4%|â–         | 71/1600 [00:00<00:17, 87.66 examples/s]Map:   5%|â–Œ         | 82/1600 [00:01<00:16, 90.83 examples/s]Map:   6%|â–Œ         | 92/1600 [00:01<00:17, 86.89 examples/s]Map:   6%|â–‹         | 101/1600 [00:01<00:17, 85.43 examples/s]Map:   7%|â–‹         | 114/1600 [00:01<00:15, 93.72 examples/s]Map:   8%|â–Š         | 129/1600 [00:01<00:14, 101.78 examples/s]Map:   9%|â–‰         | 140/1600 [00:01<00:18, 80.52 examples/s] Map:   9%|â–‰         | 149/1600 [00:01<00:17, 81.82 examples/s]Map:  10%|â–ˆ         | 161/1600 [00:01<00:16, 89.46 examples/s]Map:  11%|â–ˆ         | 174/1600 [00:02<00:14, 95.31 examples/s]Map:  12%|â–ˆâ–        | 191/1600 [00:02<00:12, 111.09 examples/s]Map:  13%|â–ˆâ–Ž        | 207/1600 [00:02<00:11, 119.35 examples/s]Map:  14%|â–ˆâ–        | 226/1600 [00:02<00:12, 114.29 examples/s]Map:  15%|â–ˆâ–Œ        | 246/1600 [00:02<00:12, 105.94 examples/s]Map:  16%|â–ˆâ–Œ        | 259/1600 [00:02<00:12, 110.10 examples/s]Map:  17%|â–ˆâ–‹        | 272/1600 [00:02<00:11, 112.32 examples/s]Map:  18%|â–ˆâ–Š        | 286/1600 [00:02<00:11, 116.08 examples/s]Map:  19%|â–ˆâ–Š        | 298/1600 [00:03<00:11, 116.21 examples/s]Map:  20%|â–ˆâ–‰        | 313/1600 [00:03<00:10, 122.36 examples/s]Map:  20%|â–ˆâ–ˆ        | 328/1600 [00:03<00:10, 126.00 examples/s]Map:  21%|â–ˆâ–ˆâ–       | 341/1600 [00:03<00:11, 108.05 examples/s]Map:  22%|â–ˆâ–ˆâ–       | 356/1600 [00:03<00:10, 116.17 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 373/1600 [00:03<00:11, 111.51 examples/s]Map:  24%|â–ˆâ–ˆâ–       | 390/1600 [00:03<00:09, 122.25 examples/s]Map:  25%|â–ˆâ–ˆâ–Œ       | 405/1600 [00:04<00:11, 108.53 examples/s]Map:  26%|â–ˆâ–ˆâ–Œ       | 417/1600 [00:04<00:10, 110.47 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 429/1600 [00:04<00:19, 61.61 examples/s] Map:  28%|â–ˆâ–ˆâ–Š       | 446/1600 [00:04<00:14, 77.56 examples/s]Map:  29%|â–ˆâ–ˆâ–‰       | 464/1600 [00:04<00:11, 95.80 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 478/1600 [00:04<00:10, 104.26 examples/s]Map:  31%|â–ˆâ–ˆâ–ˆ       | 495/1600 [00:04<00:09, 118.36 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 511/1600 [00:05<00:09, 109.26 examples/s]Map:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 526/1600 [00:05<00:10, 102.89 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 538/1600 [00:05<00:12, 88.07 examples/s] Map:  35%|â–ˆâ–ˆâ–ˆâ–      | 555/1600 [00:05<00:10, 102.99 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 572/1600 [00:05<00:08, 114.42 examples/s]Map:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 590/1600 [00:05<00:08, 123.14 examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 606/1600 [00:05<00:07, 129.65 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 620/1600 [00:06<00:07, 126.14 examples/s]Map:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 638/1600 [00:06<00:08, 112.37 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 651/1600 [00:06<00:08, 115.92 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 666/1600 [00:06<00:08, 108.05 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 684/1600 [00:06<00:08, 106.48 examples/s]Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 700/1600 [00:06<00:09, 99.46 examples/s] Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 718/1600 [00:07<00:07, 114.98 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 736/1600 [00:07<00:08, 104.75 examples/s]Map:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 748/1600 [00:07<00:07, 107.33 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 763/1600 [00:07<00:08, 99.09 examples/s] Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 779/1600 [00:07<00:08, 93.38 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 789/1600 [00:07<00:08, 91.29 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 804/1600 [00:07<00:08, 91.55 examples/s]Map:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 817/1600 [00:08<00:07, 98.38 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 832/1600 [00:08<00:08, 87.53 examples/s]Map:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 845/1600 [00:08<00:07, 94.80 examples/s]Map:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 860/1600 [00:08<00:06, 106.59 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 875/1600 [00:08<00:06, 115.26 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 889/1600 [00:08<00:06, 117.59 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 903/1600 [00:08<00:05, 118.14 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 916/1600 [00:09<00:06, 105.37 examples/s]Map:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 932/1600 [00:09<00:05, 113.71 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 945/1600 [00:09<00:05, 115.01 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 959/1600 [00:09<00:05, 115.51 examples/s]Map:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 974/1600 [00:09<00:05, 113.81 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 990/1600 [00:09<00:05, 108.17 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 998/1600 [00:23<00:05, 108.17 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1000/1600 [01:05<12:40,  1.27s/ examples]Map:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1014/1600 [01:05<08:33,  1.14 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1030/1600 [01:06<05:33,  1.71 examples/s]Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1046/1600 [01:06<03:40,  2.52 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1064/1600 [01:06<02:21,  3.78 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1081/1600 [01:06<01:35,  5.44 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1095/1600 [01:06<01:08,  7.34 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1112/1600 [01:06<00:46, 10.43 examples/s]Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1125/1600 [01:06<00:34, 13.67 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1143/1600 [01:07<00:24, 18.36 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1161/1600 [01:07<00:17, 25.77 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 1174/1600 [01:07<00:13, 32.06 examples/s]Map:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1192/1600 [01:07<00:09, 42.27 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1207/1600 [01:07<00:07, 52.79 examples/s]Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1222/1600 [01:07<00:06, 59.02 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1235/1600 [01:07<00:05, 67.06 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1247/1600 [01:08<00:05, 66.05 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1260/1600 [01:08<00:04, 76.19 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1271/1600 [01:08<00:04, 80.56 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1282/1600 [01:08<00:03, 86.09 examples/s]Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1294/1600 [01:08<00:03, 90.88 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1306/1600 [01:08<00:03, 95.19 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1319/1600 [01:08<00:02, 100.77 examples/s]Map:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1335/1600 [01:08<00:02, 109.75 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1353/1600 [01:09<00:02, 123.32 examples/s]Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1367/1600 [01:09<00:02, 85.61 examples/s] Map:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1379/1600 [01:09<00:02, 89.06 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1390/1600 [01:09<00:02, 91.66 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1406/1600 [01:09<00:02, 94.59 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1420/1600 [01:09<00:01, 104.22 examples/s]Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1438/1600 [01:09<00:01, 118.71 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1452/1600 [01:10<00:01, 119.54 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1467/1600 [01:10<00:01, 109.45 examples/s]Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1479/1600 [01:10<00:01, 85.22 examples/s] Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1490/1600 [01:10<00:01, 86.88 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1502/1600 [01:10<00:01, 89.98 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1516/1600 [01:10<00:00, 97.26 examples/s]Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1527/1600 [01:10<00:00, 99.11 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1540/1600 [01:11<00:00, 103.98 examples/s]Map:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1552/1600 [01:11<00:00, 102.22 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1565/1600 [01:11<00:00, 102.75 examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1579/1600 [01:11<00:00, 110.81 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1592/1600 [01:11<00:00, 93.49 examples/s] Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1598/1600 [01:23<00:00, 93.49 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:47<00:00,  1.07 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [01:47<00:00, 14.94 examples/s]
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|â–         | 3/200 [00:00<00:08, 22.08 examples/s]Map:   6%|â–Œ         | 12/200 [00:00<00:04, 46.43 examples/s]Map:  11%|â–ˆ         | 22/200 [00:00<00:02, 64.14 examples/s]Map:  18%|â–ˆâ–Š        | 37/200 [00:00<00:01, 82.28 examples/s]Map:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [00:00<00:01, 81.21 examples/s]Map:  28%|â–ˆâ–ˆâ–Š       | 56/200 [00:00<00:01, 80.49 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:00<00:01, 98.29 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [00:00<00:01, 100.73 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:01<00:00, 110.14 examples/s]Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [00:01<00:00, 98.32 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [00:01<00:00, 109.46 examples/s]Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [00:01<00:00, 119.90 examples/s]Map:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [00:01<00:00, 109.75 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [00:01<00:00, 119.74 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:01<00:00, 130.82 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:12<00:00, 15.67 examples/s] 
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:   2%|â–         | 3/200 [00:00<00:08, 22.04 examples/s]Map:   4%|â–         | 8/200 [00:00<00:06, 31.52 examples/s]Map:  10%|â–‰         | 19/200 [00:00<00:02, 60.43 examples/s]Map:  15%|â–ˆâ–Œ        | 30/200 [00:00<00:02, 76.47 examples/s]Map:  21%|â–ˆâ–ˆ        | 42/200 [00:00<00:01, 90.13 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 54/200 [00:00<00:01, 89.29 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [00:00<00:01, 102.53 examples/s]Map:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [00:00<00:01, 115.43 examples/s]Map:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [00:01<00:00, 119.93 examples/s]Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [00:01<00:00, 109.12 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:01<00:00, 120.02 examples/s]Map:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [00:01<00:00, 120.67 examples/s]Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [00:01<00:00, 119.47 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [00:01<00:00, 133.94 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:11<00:00, 16.70 examples/s] 
Loading en eval data: fold_4...
Preprocess en fold_4 data for en model
Loading en test data: fold_4...
Preprocess en fold_4 data for en model
Use en model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.6.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.6.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.6.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.7.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.7.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.7.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.8.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.8.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.8.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1711.8517, Training accuracy: 0.5475
Macro F1-score: 0.5404
Model performance on Angry speech (in training): 
	Precision: 0.5866, Recall: 0.5925, F1_score: 0.5896
Model performance on Happy speech (in training): 
	Precision: 0.5047, Recall: 0.4050, F1_score: 0.4494
Model performance on Neutral speech (in training): 
	Precision: 0.4577, Recall: 0.4325, F1_score: 0.4447
Model performance on Sad speech (in training): 
	Precision: 0.6117, Recall: 0.7600, F1_score: 0.6778

Eval Phase: 
Validation loss: 205.1887, Validation accuracy: 0.5850
Macro F1-score: 0.5812
Model performance on Angry speech (in validation): 
	Precision: 0.6721, Recall: 0.8200, F1_score: 0.7387
Model performance on Happy speech (in validation): 
	Precision: 0.4306, Recall: 0.6200, F1_score: 0.5082
Model performance on Neutral speech (in validation): 
	Precision: 0.6970, Recall: 0.4600, F1_score: 0.5542
Model performance on Sad speech (in validation): 
	Precision: 0.6471, Recall: 0.4400, F1_score: 0.5238
New best accuracy for layer 8 on epoch 1: 0.5850. Model saved.
Epoch 2/100

Training Phase:
Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 169/1600 [00:10<01:24, 16.89it/s]Training:  21%|â–ˆâ–ˆâ–       | 342/1600 [00:20<01:13, 17.10it/s]Training:  21%|â–ˆâ–ˆâ–       | 342/1600 [00:30<01:13, 17.10it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 512/1600 [00:30<01:04, 16.97it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1600 [00:40<00:52, 17.24it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 866/1600 [00:50<00:42, 17.07it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1036/1600 [01:00<00:33, 17.02it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1206/1600 [01:10<00:23, 17.01it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1376/1600 [01:20<00:13, 16.95it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1545/1600 [01:30<00:03, 16.93it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         Training loss: 1462.3277, Training accuracy: 0.6225
Macro F1-score: 0.6163
Model performance on Angry speech (in training): 
	Precision: 0.6946, Recall: 0.7050, F1_score: 0.6998
Model performance on Happy speech (in training): 
	Precision: 0.5606, Recall: 0.4625, F1_score: 0.5068
Model performance on Neutral speech (in training): 
	Precision: 0.5479, Recall: 0.5150, F1_score: 0.5309
Model performance on Sad speech (in training): 
	Precision: 0.6619, Recall: 0.8075, F1_score: 0.7275

Eval Phase: 
Validation loss: 181.1265, Validation accuracy: 0.6200
Macro F1-score: 0.6145
Model performance on Angry speech (in validation): 
	Precision: 0.5942, Recall: 0.8200, F1_score: 0.6891
Model performance on Happy speech (in validation): 
	Precision: 0.5854, Recall: 0.4800, F1_score: 0.5275
Model performance on Neutral speech (in validation): 
	Precision: 0.6512, Recall: 0.5600, F1_score: 0.6022
Model performance on Sad speech (in validation): 
	Precision: 0.6596, Recall: 0.6200, F1_score: 0.6392
New best accuracy for layer 8 on epoch 2: 0.6200. Model saved.
Epoch 3/100

Training Phase:
| 175/1600 [00:10<01:21, 17.43it/s]Training:  22%|â–ˆâ–ˆâ–       | 350/1600 [00:20<01:13, 17.04it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 520/1600 [00:30<01:03, 16.99it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 690/1600 [00:40<00:53, 16.97it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 863/1600 [00:50<00:43, 17.07it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1041/1600 [01:00<00:32, 17.29it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1219/1600 [01:11<00:22, 17.14it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1390/1600 [01:21<00:12, 17.10it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1561/1600 [01:31<00:02, 17.05it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 175/1600 [00:10<01:21, 17.42it/s]Training:  22%|â–ˆâ–ˆâ–       | 350/1600 [00:20<01:11, 17.40it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 526/1600Training loss: 1366.9655, Training accuracy: 0.6506
Macro F1-score: 0.6451
Model performance on Angry speech (in training): 
	Precision: 0.7136, Recall: 0.7350, F1_score: 0.7241
Model performance on Happy speech (in training): 
	Precision: 0.6246, Recall: 0.5075, F1_score: 0.5600
Model performance on Neutral speech (in training): 
	Precision: 0.5703, Recall: 0.5375, F1_score: 0.5534
Model performance on Sad speech (in training): 
	Precision: 0.6770, Recall: 0.8225, F1_score: 0.7427

Eval Phase: 
Validation loss: 168.5558, Validation accuracy: 0.6650
Macro F1-score: 0.6529
Model performance on Angry speech (in validation): 
	Precision: 0.7368, Recall: 0.8400, F1_score: 0.7850
Model performance on Happy speech (in validation): 
	Precision: 0.7917, Recall: 0.3800, F1_score: 0.5135
Model performance on Neutral speech (in validation): 
	Precision: 0.6111, Recall: 0.6600, F1_score: 0.6346
Model performance on Sad speech (in validation): 
	Precision: 0.6000, Recall: 0.7800, F1_score: 0.6783
New best accuracy for layer 8 on epoch 3: 0.6650. Model saved.
Epoch 4/100

Training Phase:
 [00:30<01:01, 17.48it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 702/1600 [00:40<00:51, 17.43it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 876/1600 [00:50<00:42, 17.14it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1044/1600 [01:00<00:32, 17.00it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1217/1600 [01:10<00:22, 17.08it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1390/1600 [01:20<00:12, 17.09it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1562/1600 [01:32<00:02, 16.47it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 164/1600 [00:10<01:27, 16.33it/s]Training:  20%|â–ˆâ–ˆ        | 328/1600 [00:20<01:20, 15.89it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 485/1600 [00:30<01:11, 15.69it/s]Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 644/1600 [00:40<01:00, 15.74it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 811/1600 [00:50<00Training loss: 1232.5533, Training accuracy: 0.6894
Macro F1-score: 0.6852
Model performance on Angry speech (in training): 
	Precision: 0.7694, Recall: 0.7675, F1_score: 0.7685
Model performance on Happy speech (in training): 
	Precision: 0.6725, Recall: 0.5800, F1_score: 0.6228
Model performance on Neutral speech (in training): 
	Precision: 0.6114, Recall: 0.5625, F1_score: 0.5859
Model performance on Sad speech (in training): 
	Precision: 0.6947, Recall: 0.8475, F1_score: 0.7635

Eval Phase: 
Validation loss: 209.0970, Validation accuracy: 0.5850
Macro F1-score: 0.5623
Model performance on Angry speech (in validation): 
	Precision: 0.6724, Recall: 0.7800, F1_score: 0.7222
Model performance on Happy speech (in validation): 
	Precision: 0.5517, Recall: 0.3200, F1_score: 0.4051
Model performance on Neutral speech (in validation): 
	Precision: 0.4889, Recall: 0.8800, F1_score: 0.6286
Model performance on Sad speech (in validation): 
	Precision: 0.7826, Recall: 0.3600, F1_score: 0.4932
Epoch 5/100

Training Phase:
:49, 16.03it/s]Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 977/1600 [01:00<00:38, 16.19it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1149/1600 [01:10<00:27, 16.51it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1327/1600 [01:20<00:16, 16.91it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1505/1600 [01:31<00:05, 17.07it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 170/1600 [00:10<01:24, 16.94it/s]Training:  21%|â–ˆâ–ˆâ–       | 340/1600 [00:20<01:14, 16.82it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 511/1600 [00:30<01:04, 16.93it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 684/1600 [00:40<00:53, 17.05it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 857/1600 [00:50<00:44, 16.80it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1031/1600 [01:00<00:33, 16.98it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1206/1600 [01:Training loss: 1135.1162, Training accuracy: 0.7238
Macro F1-score: 0.7211
Model performance on Angry speech (in training): 
	Precision: 0.7880, Recall: 0.7900, F1_score: 0.7890
Model performance on Happy speech (in training): 
	Precision: 0.7284, Recall: 0.6100, F1_score: 0.6639
Model performance on Neutral speech (in training): 
	Precision: 0.6506, Recall: 0.6425, F1_score: 0.6465
Model performance on Sad speech (in training): 
	Precision: 0.7271, Recall: 0.8525, F1_score: 0.7848

Eval Phase: 
Validation loss: 193.0376, Validation accuracy: 0.6200
Macro F1-score: 0.6006
Model performance on Angry speech (in validation): 
	Precision: 0.6875, Recall: 0.8800, F1_score: 0.7719
Model performance on Happy speech (in validation): 
	Precision: 0.7619, Recall: 0.3200, F1_score: 0.4507
Model performance on Neutral speech (in validation): 
	Precision: 0.4881, Recall: 0.8200, F1_score: 0.6119
Model performance on Sad speech (in validation): 
	Precision: 0.7419, Recall: 0.4600, F1_score: 0.5679
Epoch 6/100

Training Phase:
10<00:23, 17.13it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1384/1600 [01:20<00:12, 17.31it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1562/1600 [01:31<00:02, 17.14it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 169/1600 [00:10<01:25, 16.80it/s]Training:  21%|â–ˆâ–ˆâ–       | 341/1600 [00:20<01:13, 17.02it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 516/1600 [00:30<01:02, 17.23it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 516/1600 [00:40<01:02, 17.23it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1600 [00:40<00:53, 17.02it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 859/1600 [00:50<00:43, 16.97it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1035/1600 [01:00<00:32, 17.16it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1211/1600 [01:10<00:22, 17.05it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1392/1600 [Training loss: 1070.2870, Training accuracy: 0.7444
Macro F1-score: 0.7419
Model performance on Angry speech (in training): 
	Precision: 0.8207, Recall: 0.8125, F1_score: 0.8166
Model performance on Happy speech (in training): 
	Precision: 0.7584, Recall: 0.6200, F1_score: 0.6823
Model performance on Neutral speech (in training): 
	Precision: 0.6758, Recall: 0.6775, F1_score: 0.6767
Model performance on Sad speech (in training): 
	Precision: 0.7290, Recall: 0.8675, F1_score: 0.7922

Eval Phase: 
Validation loss: 186.0734, Validation accuracy: 0.6550
Macro F1-score: 0.6409
Model performance on Angry speech (in validation): 
	Precision: 0.6301, Recall: 0.9200, F1_score: 0.7480
Model performance on Happy speech (in validation): 
	Precision: 0.6471, Recall: 0.4400, F1_score: 0.5238
Model performance on Neutral speech (in validation): 
	Precision: 0.6552, Recall: 0.7600, F1_score: 0.7037
Model performance on Sad speech (in validation): 
	Precision: 0.7143, Recall: 0.5000, F1_score: 0.5882
Epoch 7/100

Training Phase:
01:21<00:11, 17.37it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1573/1600 [01:31<00:01, 17.10it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 170/1600 [00:10<01:24, 16.89it/s]Training:  21%|â–ˆâ–ˆâ–       | 342/1600 [00:20<01:13, 17.05it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 521/1600 [00:30<01:01, 17.41it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 700/1600 [00:40<00:51, 17.53it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 878/1600 [00:51<00:42, 16.98it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1049/1600 [01:01<00:32, 16.99it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1226/1600 [01:11<00:21, 17.22it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1403/1600 [01:21<00:11, 17.19it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1575/1600 [01:31<00:01, 17.09it/s]                                        Training loss: 964.8825, Training accuracy: 0.7731
Macro F1-score: 0.7707
Model performance on Angry speech (in training): 
	Precision: 0.8243, Recall: 0.8325, F1_score: 0.8284
Model performance on Happy speech (in training): 
	Precision: 0.7834, Recall: 0.6600, F1_score: 0.7164
Model performance on Neutral speech (in training): 
	Precision: 0.7176, Recall: 0.7050, F1_score: 0.7112
Model performance on Sad speech (in training): 
	Precision: 0.7682, Recall: 0.8950, F1_score: 0.8268

Eval Phase: 
Validation loss: 232.0998, Validation accuracy: 0.6000
Macro F1-score: 0.5692
Model performance on Angry speech (in validation): 
	Precision: 0.6197, Recall: 0.8800, F1_score: 0.7273
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.2400, F1_score: 0.3529
Model performance on Neutral speech (in validation): 
	Precision: 0.5190, Recall: 0.8200, F1_score: 0.6357
Model performance on Sad speech (in validation): 
	Precision: 0.7188, Recall: 0.4600, F1_score: 0.5610
Epoch 8/100

Training Phase:
Training loss: 834.3136, Training accuracy: 0.8000
Macro F1-score: 0.7991
Model performance on Angry speech (in training): 
	Precision: 0.8696, Recall: 0.8500, F1_score: 0.8597
Model performance on Happy speech (in training): 
	Precision: 0.7989, Recall: 0.7150, F1_score: 0.7546
Model performance on Neutral speech (in training): 
	Precision: 0.7383, Recall: 0.7475, F1_score: 0.7429
Model performance on Sad speech (in training): 
	Precision: 0.7960, Recall: 0.8875, F1_score: 0.8392

Eval Phase: 
Validation loss: 242.4413, Validation accuracy: 0.5800
Macro F1-score: 0.5716
Model performance on Angry speech (in validation): 
	Precision: 0.7561, Recall: 0.6200, F1_score: 0.6813
Model performance on Happy speech (in validation): 
	Precision: 0.6296, Recall: 0.3400, F1_score: 0.4416
Model performance on Neutral speech (in validation): 
	Precision: 0.4536, Recall: 0.8800, F1_score: 0.5986
Model performance on Sad speech (in validation): 
	Precision: 0.6857, Recall: 0.4800, F1_score: 0.5647
Epoch 9/100

Training Phase:
                     Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 163/1600 [00:10<01:28, 16.23it/s]Training:  20%|â–ˆâ–ˆ        | 326/1600 [00:20<01:19, 16.10it/s]Training:  31%|â–ˆâ–ˆâ–ˆ       | 495/1600 [00:30<01:07, 16.43it/s]Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 664/1600 [00:40<00:56, 16.45it/s]Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 839/1600 [00:50<00:45, 16.81it/s]Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1014/1600 [01:00<00:34, 16.89it/s]Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1185/1600 [01:11<00:25, 16.48it/s]Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1352/1600 [01:21<00:14, 16.54it/s]Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1519/1600 [01:32<00:05, 16.16it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|       Training loss: 766.9708, Training accuracy: 0.8181
Macro F1-score: 0.8172
Model performance on Angry speech (in training): 
	Precision: 0.8886, Recall: 0.8775, F1_score: 0.8830
Model performance on Happy speech (in training): 
	Precision: 0.8362, Recall: 0.7275, F1_score: 0.7781
Model performance on Neutral speech (in training): 
	Precision: 0.7714, Recall: 0.7675, F1_score: 0.7694
Model performance on Sad speech (in training): 
	Precision: 0.7843, Recall: 0.9000, F1_score: 0.8382

Eval Phase: 
Validation loss: 252.1176, Validation accuracy: 0.6350
Macro F1-score: 0.6267
Model performance on Angry speech (in validation): 
	Precision: 0.5972, Recall: 0.8600, F1_score: 0.7049
Model performance on Happy speech (in validation): 
	Precision: 0.7188, Recall: 0.4600, F1_score: 0.5610
Model performance on Neutral speech (in validation): 
	Precision: 0.5833, Recall: 0.7000, F1_score: 0.6364
Model performance on Sad speech (in validation): 
	Precision: 0.7222, Recall: 0.5200, F1_score: 0.6047
Epoch 10/100

Training Phase:
   | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 174/1600 [00:10<01:22, 17.36it/s]Training:  22%|â–ˆâ–ˆâ–       | 350/1600 [00:20<01:11, 17.47it/s]Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 530/1600 [00:30<01:00, 17.65it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 709/1600 [00:40<00:51, 17.34it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 885/1600 [00:50<00:41, 17.42it/s]Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1061/1600 [01:01<00:31, 17.18it/s]Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1233/1600 [01:11<00:21, 17.18it/s]Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1405/1600 [01:21<00:11, 17.04it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1576/1600 [01:31<00:01, 17.04it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 174/1600 [00:10<01:22, 17.31it/s]Training:  11%|â–ˆ         | 174/1600 [00:20<01:22, 17Training loss: 703.7758, Training accuracy: 0.8213
Macro F1-score: 0.8205
Model performance on Angry speech (in training): 
	Precision: 0.8835, Recall: 0.8725, F1_score: 0.8780
Model performance on Happy speech (in training): 
	Precision: 0.8408, Recall: 0.7525, F1_score: 0.7942
Model performance on Neutral speech (in training): 
	Precision: 0.7696, Recall: 0.7600, F1_score: 0.7648
Model performance on Sad speech (in training): 
	Precision: 0.7965, Recall: 0.9000, F1_score: 0.8451

Eval Phase: 
Validation loss: 217.8950, Validation accuracy: 0.5900
Macro F1-score: 0.5877
Model performance on Angry speech (in validation): 
	Precision: 0.6250, Recall: 0.7000, F1_score: 0.6604
Model performance on Happy speech (in validation): 
	Precision: 0.6154, Recall: 0.4800, F1_score: 0.5393
Model performance on Neutral speech (in validation): 
	Precision: 0.5660, Recall: 0.6000, F1_score: 0.5825
Model performance on Sad speech (in validation): 
	Precision: 0.5577, Recall: 0.5800, F1_score: 0.5686
Epoch 11/100

Training Phase:
.31it/s]Training:  22%|â–ˆâ–ˆâ–       | 347/1600 [00:20<01:14, 16.93it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 518/1600 [00:30<01:03, 16.96it/s]Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 689/1600 [00:41<00:54, 16.60it/s]Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 850/1600 [00:51<00:45, 16.41it/s]Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1020/1600 [01:01<00:34, 16.59it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1195/1600 [01:11<00:24, 16.87it/s]Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1372/1600 [01:21<00:13, 17.09it/s]Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1548/1600 [01:31<00:03, 17.06it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 171/1600 [00:10<01:23, 17.07it/s]Training:  21%|â–ˆâ–ˆâ–       | 342/1600 [00:20<01:13, 17.08it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 513/1600 [00:30<01:03, 17.08it/s]TTraining loss: 650.8595, Training accuracy: 0.8512
Macro F1-score: 0.8510
Model performance on Angry speech (in training): 
	Precision: 0.9215, Recall: 0.9100, F1_score: 0.9157
Model performance on Happy speech (in training): 
	Precision: 0.8685, Recall: 0.7925, F1_score: 0.8288
Model performance on Neutral speech (in training): 
	Precision: 0.7965, Recall: 0.8025, F1_score: 0.7995
Model performance on Sad speech (in training): 
	Precision: 0.8238, Recall: 0.9000, F1_score: 0.8602

Eval Phase: 
Validation loss: 253.6663, Validation accuracy: 0.5750
Macro F1-score: 0.5673
Model performance on Angry speech (in validation): 
	Precision: 0.5781, Recall: 0.7400, F1_score: 0.6491
Model performance on Happy speech (in validation): 
	Precision: 0.5588, Recall: 0.3800, F1_score: 0.4524
Model performance on Neutral speech (in validation): 
	Precision: 0.5152, Recall: 0.6800, F1_score: 0.5862
Model performance on Sad speech (in validation): 
	Precision: 0.6944, Recall: 0.5000, F1_score: 0.5814
Epoch 12/100

Training Phase:
raining:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 686/1600 [00:40<00:53, 17.16it/s]Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 859/1600 [00:50<00:43, 16.94it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1033/1600 [01:00<00:33, 17.08it/s]Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1213/1600 [01:10<00:22, 17.36it/s]Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1393/1600 [01:20<00:11, 17.48it/s]Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1571/1600 [01:31<00:01, 17.26it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  10%|â–ˆ         | 162/1600 [00:10<01:29, 16.14it/s]Training:  20%|â–ˆâ–ˆ        | 324/1600 [00:20<01:19, 16.05it/s]Training:  30%|â–ˆâ–ˆâ–ˆ       | 486/1600 [00:30<01:09, 16.11it/s]Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 653/1600 [00:40<00:58, 16.30it/s]Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 819/1600 [00:50<00:47, 16.33it/s]Training:  Training loss: 597.7125, Training accuracy: 0.8594
Macro F1-score: 0.8592
Model performance on Angry speech (in training): 
	Precision: 0.9241, Recall: 0.9125, F1_score: 0.9182
Model performance on Happy speech (in training): 
	Precision: 0.8956, Recall: 0.8150, F1_score: 0.8534
Model performance on Neutral speech (in training): 
	Precision: 0.7990, Recall: 0.7950, F1_score: 0.7970
Model performance on Sad speech (in training): 
	Precision: 0.8262, Recall: 0.9150, F1_score: 0.8683

Eval Phase: 
Validation loss: 313.7123, Validation accuracy: 0.5700
Macro F1-score: 0.5511
Model performance on Angry speech (in validation): 
	Precision: 0.5309, Recall: 0.8600, F1_score: 0.6565
Model performance on Happy speech (in validation): 
	Precision: 0.6154, Recall: 0.3200, F1_score: 0.4211
Model performance on Neutral speech (in validation): 
	Precision: 0.5397, Recall: 0.6800, F1_score: 0.6018
Model performance on Sad speech (in validation): 
	Precision: 0.7000, Recall: 0.4200, F1_score: 0.5250
Epoch 13/100

Training Phase:
61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 983/1600 [01:00<00:37, 16.34it/s]Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1158/1600 [01:10<00:26, 16.71it/s]Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1333/1600 [01:20<00:15, 16.95it/s]Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1508/1600 [01:30<00:05, 17.11it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Training:   0%|          | 0/1600 [00:00<?, ?it/s]Training:  11%|â–ˆ         | 173/1600 [00:10<01:22, 17.25it/s]Training:  22%|â–ˆâ–ˆâ–       | 346/1600 [00:20<01:12, 17.20it/s]Training:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 520/1600 [00:30<01:02, 17.28it/s]Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 696/1600 [00:40<00:51, 17.41it/s]Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 872/1600 [00:50<00:42, 17.24it/s]Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1042/1600 [01:01<00:33, 16.69it/s]Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1200/1600 [01:11<00:24, 16.39it/s]TraiTraining loss: 571.5074, Training accuracy: 0.8612
Macro F1-score: 0.8611
Model performance on Angry speech (in training): 
	Precision: 0.9342, Recall: 0.9225, F1_score: 0.9283
Model performance on Happy speech (in training): 
	Precision: 0.8693, Recall: 0.8150, F1_score: 0.8413
Model performance on Neutral speech (in training): 
	Precision: 0.8131, Recall: 0.8050, F1_score: 0.8090
Model performance on Sad speech (in training): 
	Precision: 0.8318, Recall: 0.9025, F1_score: 0.8657

Eval Phase: 
Validation loss: 287.4564, Validation accuracy: 0.6200
Macro F1-score: 0.6142
Model performance on Angry speech (in validation): 
	Precision: 0.7255, Recall: 0.7400, F1_score: 0.7327
Model performance on Happy speech (in validation): 
	Precision: 0.5946, Recall: 0.4400, F1_score: 0.5057
Model performance on Neutral speech (in validation): 
	Precision: 0.5342, Recall: 0.7800, F1_score: 0.6341
Model performance on Sad speech (in validation): 
	Precision: 0.6667, Recall: 0.5200, F1_score: 0.5843
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6650

Test Phase: 
ning:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1364/1600 [01:21<00:14, 16.38it/s]Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1528/1600 [01:32<00:04, 15.68it/s]                                                             Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]                                                   Testing:   0%|          | 0/200 [00:00<?, ?it/s]Testing:   4%|â–Ž         | 7/200 [00:00<00:03, 60.54it/s]Testing:   7%|â–‹         | 14/200 [00:00<00:02, 62.77it/s]Testing:  10%|â–ˆ         | 21/200 [00:00<00:02, 63.11it/s]Testing:  14%|â–ˆâ–        | 28/200 [00:00<00:02, 63.51it/s]Testing:  18%|â–ˆâ–Š        | 35/200 [00:00<00:02, 64.35it/s]Testing:  21%|â–ˆâ–ˆ        | 42/200 [00:00<00:02, 63.39it/s]Testing:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [00:00<00:02, 65.72it/s]Testing:  28%|â–ˆâ–ˆâ–Š       | 57/200 [00:00<00:02, 64.92it/s]Testing:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [00:00<00:02, 65.75it/s]Testing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [00:01<00:01, 66.62it/s]Testing:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [00:01<00:01, 64.66it/s]Testing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [00:01<00:01, 65.46it/s]Testing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [00:01<00:01, 65.43it/s]Testing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [00:01<00:01, 65.25it/s]Testing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [00:01<00:01, 65.90it/s]Testing:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [00:01<00:01, 62.93it/s]Testing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [00:01<00:01, 59.59it/s]Testing:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [00:02<00:01, 62.24it/s]Testing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [00:02<00:01, 63.23it/s]Testing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [00:02<00:00, 63.48it/s]Testing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:02<00:00, 65.40it/s]Testing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [00:02<00:00, 64.88it/s]Testing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [00:02<00:00, 66.10it/s]Testing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:02<00:00, 65.93it/s]Testing:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰Test loss: 166.5146, Test accuracy: 0.6500
Macro F1-score: 0.6314
Model performance on Angry speech (in test): 
	Precision: 0.6119, Recall: 0.8200, F1_score: 0.7009
Model performance on Happy speech (in test): 
	Precision: 0.7619, Recall: 0.3200, F1_score: 0.4507
Model performance on Neutral speech (in test): 
	Precision: 0.5741, Recall: 0.6200, F1_score: 0.5962
Model performance on Sad speech (in test): 
	Precision: 0.7241, Recall: 0.8400, F1_score: 0.7778

en, all folds layer accuracy: ['0.6600', '0.6000', '0.6450', '0.6750', '0.6500']
en, all emo precision: {'Angry': ['0.8409', '0.5641', '0.6415', '0.7500', '0.6119'], 'Happy': ['0.5769', '0.6842', '0.7667', '0.7632', '0.7619'], 'Neutral': ['0.5532', '0.5079', '0.6327', '0.5373', '0.5741'], 'Sad': ['0.6842', '0.7750', '0.6029', '0.7234', '0.7241']}
en, all emo recall: {'Angry': ['0.7400', '0.8800', '0.6800', '0.7200', '0.8200'], 'Happy': ['0.6000', '0.2600', '0.4600', '0.5800', '0.3200'], 'Neutral': ['0.5200', '0.6400', '0.6200', '0.7200', '0.6200'], 'Sad': ['0.7800', '0.6200', '0.8200', '0.6800', '0.8400']}
en, all emo f1score: {'Angry': ['0.7872', '0.6875', '0.6602', '0.7347', '0.7009'], 'Happy': ['0.5882', '0.3768', '0.5750', '0.6591', '0.4507'], 'Neutral': ['0.5361', '0.5664', '0.6263', '0.6154', '0.5962'], 'Sad': ['0.7290', '0.6889', '0.6949', '0.7010', '0.7778']}
 | 178/200 [00:02<00:00, 66.04it/s]Testing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [00:02<00:00, 66.71it/s]Testing:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [00:02<00:00, 67.62it/s]Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [00:03<00:00, 68.11it/s]                                                          