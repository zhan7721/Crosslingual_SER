Loading python/3.10.8-gpu
  Loading requirement: nvidia/nvhpc-nompi/22.2 gcc/10.2.0
    openmpi/4.1.6-cuda-11.6
Loading pytorch/1.13.1-gpu
  Loading requirement: nvidia/cudnn/8.6.0-cuda-11.6 nvidia/tensorrt/8.4.3.1-u2
    libsndfile/1.0.28
------------------NEXT SCRIPT: RUNNER_DE, TL on EN----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911930/matplotlib-54473k3a because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on de =======================

Load dataset: 
Loading en train data: fold_0...
Preprocess en fold_0 data for de model
Loading en eval data: fold_0...
Preprocess en fold_0 data for de model
Loading en test data: fold_0...
Preprocess en fold_0 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:   0%|          | 1/1600 [00:53<23:50:33, 53.68s/it]
Training:   9%|▊         | 137/1600 [01:03<08:29,  2.87it/s] 
Training:  18%|█▊        | 291/1600 [01:13<03:47,  5.76it/s]
Training:  28%|██▊       | 454/1600 [01:23<02:17,  8.36it/s]
Training:  39%|███▉      | 626/1600 [01:33<01:31, 10.67it/s]
Training:  50%|█████     | 800/1600 [01:43<01:04, 12.49it/s]
Training:  61%|██████▏   | 981/1600 [01:53<00:44, 14.04it/s]
Training:  73%|███████▎  | 1174/1600 [02:03<00:27, 15.53it/s]
Training:  85%|████████▌ | 1367/1600 [02:13<00:14, 16.57it/s]
Training:  98%|█████████▊| 1564/1600 [02:24<00:02, 17.48it/s]
                                                             
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Training loss: 1990.8287, Training accuracy: 0.4031
Macro F1-score: 0.3253
Model performance on Angry speech (in training): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in training): 
	Precision: 0.3105, Recall: 0.7150, F1_score: 0.4330
Model performance on Neutral speech (in training): 
	Precision: 0.4038, Recall: 0.1575, F1_score: 0.2266
Model performance on Sad speech (in training): 
	Precision: 0.5660, Recall: 0.7400, F1_score: 0.6414

Eval Phase: 

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Validation loss: 228.0161, Validation accuracy: 0.4300
Macro F1-score: 0.3583
Model performance on Angry speech (in validation): 
	Precision: 0.0000, Recall: 0.0000, F1_score: 0.0000
Model performance on Happy speech (in validation): 
	Precision: 0.3205, Recall: 0.5000, F1_score: 0.3906
Model performance on Neutral speech (in validation): 
	Precision: 0.4595, Recall: 0.3400, F1_score: 0.3908
Model performance on Sad speech (in validation): 
	Precision: 0.5176, Recall: 0.8800, F1_score: 0.6519
New best accuracy for layer 5 on epoch 1: 0.4300. Model saved.
Epoch 2/100

Training Phase:
Training loss: 1714.9920, Training accuracy: 0.5500
Macro F1-score: 0.5407
Model performance on Angry speech (in training): 
	Precision: 0.5799, Recall: 0.5900, F1_score: 0.5849
Model performance on Happy speech (in training): 
	Precision: 0.4500, Recall: 0.3375, F1_score: 0.3857
Model performance on Neutral speech (in training): 
	Precision: 0.4576, Recall: 0.4725, F1_score: 0.4649
Model performance on Sad speech (in training): 
	Precision: 0.6667, Recall: 0.8000, F1_score: 0.7273

Eval Phase: 
Validation loss: 206.0956, Validation accuracy: 0.5500
Macro F1-score: 0.5338
Model performance on Angry speech (in validation): 
	Precision: 0.9259, Recall: 0.5000, F1_score: 0.6494
Model performance on Happy speech (in validation): 
	Precision: 0.4194, Recall: 0.2600, F1_score: 0.3210
Model performance on Neutral speech (in validation): 
	Precision: 0.5435, Recall: 0.5000, F1_score: 0.5208
Model performance on Sad speech (in validation): 
	Precision: 0.4896, Recall: 0.9400, F1_score: 0.6438
New best accuracy for layer 5 on epoch 2: 0.5500. Model saved.
Epoch 3/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 209/1600 [00:10<01:06, 20.85it/s]
Training:  26%|██▋       | 424/1600 [00:20<00:55, 21.18it/s]
Training:  40%|███▉      | 639/1600 [00:30<00:46, 20.84it/s]
Training:  53%|█████▎    | 847/1600 [00:40<00:36, 20.75it/s]
Training:  66%|██████▌   | 1054/1600 [00:51<00:27, 20.14it/s]
Training:  78%|███████▊  | 1256/1600 [01:01<00:17, 20.14it/s]
Training:  91%|█████████ | 1458/1600 [01:11<00:07, 20.10it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.39it/s]
Training:  25%|██▍       | 393/1600 [00:20<01:01, 19.67it/s]
Training:  37%|███▋      | 592/1600 [00:30<00:51, 19.59it/s]
Training:  49%|████▉     | 788/160Training loss: 1521.9587, Training accuracy: 0.6000
Macro F1-score: 0.5928
Model performance on Angry speech (in training): 
	Precision: 0.7056, Recall: 0.6950, F1_score: 0.7003
Model performance on Happy speech (in training): 
	Precision: 0.5258, Recall: 0.3825, F1_score: 0.4428
Model performance on Neutral speech (in training): 
	Precision: 0.4874, Recall: 0.5300, F1_score: 0.5078
Model performance on Sad speech (in training): 
	Precision: 0.6604, Recall: 0.7925, F1_score: 0.7205

Eval Phase: 
Validation loss: 180.5823, Validation accuracy: 0.6800
Macro F1-score: 0.6765
Model performance on Angry speech (in validation): 
	Precision: 0.5972, Recall: 0.8600, F1_score: 0.7049
Model performance on Happy speech (in validation): 
	Precision: 0.7143, Recall: 0.5000, F1_score: 0.5882
Model performance on Neutral speech (in validation): 
	Precision: 0.6042, Recall: 0.5800, F1_score: 0.5918
Model performance on Sad speech (in validation): 
	Precision: 0.8667, Recall: 0.7800, F1_score: 0.8211
New best accuracy for layer 5 on epoch 3: 0.6800. Model saved.
Epoch 4/100

Training Phase:
0 [00:40<00:41, 19.43it/s]
Training:  62%|██████▏   | 987/1600 [00:50<00:31, 19.57it/s]
Training:  74%|███████▍  | 1186/1600 [01:01<00:21, 19.28it/s]
Training:  86%|████████▌ | 1376/1600 [01:11<00:11, 19.18it/s]
Training:  99%|█████████▊| 1578/1600 [01:21<00:01, 19.46it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.46it/s]
Training:  24%|██▍       | 390/1600 [00:20<01:02, 19.40it/s]
Training:  36%|███▋      | 584/1600 [00:30<00:52, 19.33it/s]
Training:  49%|████▊     | 778/1600 [00:40<00:42, 19.34it/s]
Training:  61%|██████    | 973/1600 [00:50<00:32, 19.37it/s]
Training:  73%|███████▎  | 1170/1600 [01:00<00:22, 19.47it/s]
Training:  85%|████████�Training loss: 1394.3169, Training accuracy: 0.6550
Macro F1-score: 0.6495
Model performance on Angry speech (in training): 
	Precision: 0.7417, Recall: 0.7250, F1_score: 0.7332
Model performance on Happy speech (in training): 
	Precision: 0.5960, Recall: 0.4500, F1_score: 0.5128
Model performance on Neutral speech (in training): 
	Precision: 0.5601, Recall: 0.6175, F1_score: 0.5874
Model performance on Sad speech (in training): 
	Precision: 0.7103, Recall: 0.8275, F1_score: 0.7644

Eval Phase: 
Validation loss: 191.3804, Validation accuracy: 0.6400
Macro F1-score: 0.6286
Model performance on Angry speech (in validation): 
	Precision: 0.8163, Recall: 0.8000, F1_score: 0.8081
Model performance on Happy speech (in validation): 
	Precision: 0.8462, Recall: 0.4400, F1_score: 0.5789
Model performance on Neutral speech (in validation): 
	Precision: 0.8182, Recall: 0.3600, F1_score: 0.5000
Model performance on Sad speech (in validation): 
	Precision: 0.4660, Recall: 0.9600, F1_score: 0.6275
Epoch 5/100

Training Phase:
Training loss: 1253.1851, Training accuracy: 0.6913
Macro F1-score: 0.6888
Model performance on Angry speech (in training): 
	Precision: 0.8005, Recall: 0.7525, F1_score: 0.7758
Model performance on Happy speech (in training): 
	Precision: 0.6557, Recall: 0.5475, F1_score: 0.5967
Model performance on Neutral speech (in training): 
	Precision: 0.6033, Recall: 0.6350, F1_score: 0.6188
Model performance on Sad speech (in training): 
	Precision: 0.7079, Recall: 0.8300, F1_score: 0.7641

Eval Phase: 
�� | 1367/1600 [01:10<00:12, 19.40it/s]
Training:  98%|█████████▊| 1564/1600 [01:20<00:01, 19.49it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 198/1600 [00:10<01:11, 19.62it/s]
Training:  25%|██▍       | 395/1600 [00:20<01:02, 19.28it/s]
Training:  37%|███▋      | 595/1600 [00:30<00:51, 19.57it/s]
Training:  50%|████▉     | 795/1600 [00:40<00:40, 19.66it/s]
Training:  62%|██████▏   | 994/1600 [00:51<00:31, 19.44it/s]
Training:  74%|███████▍  | 1188/1600 [01:01<00:21, 19.40it/s]
Training:  87%|████████▋ | 1394/1600 [01:11<00:10, 19.77it/s]
Training: 100%|██████████| 1600/1600 [01:21<00:00, 19.91it/s]
                                                             

Evaluating:   0%|          | 0/20Validation loss: 174.0924, Validation accuracy: 0.6700
Macro F1-score: 0.6505
Model performance on Angry speech (in validation): 
	Precision: 0.8667, Recall: 0.7800, F1_score: 0.8211
Model performance on Happy speech (in validation): 
	Precision: 0.6000, Recall: 0.6600, F1_score: 0.6286
Model performance on Neutral speech (in validation): 
	Precision: 0.7895, Recall: 0.3000, F1_score: 0.4348
Model performance on Sad speech (in validation): 
	Precision: 0.5802, Recall: 0.9400, F1_score: 0.7176
Epoch 6/100

Training Phase:
Training loss: 1109.9605, Training accuracy: 0.7338
Macro F1-score: 0.7300
Model performance on Angry speech (in training): 
	Precision: 0.8150, Recall: 0.8150, F1_score: 0.8150
Model performance on Happy speech (in training): 
	Precision: 0.7121, Recall: 0.5750, F1_score: 0.6362
Model performance on Neutral speech (in training): 
	Precision: 0.6601, Recall: 0.6700, F1_score: 0.6650
Model performance on Sad speech (in training): 
	Precision: 0.7431, Recall: 0.8750, F1_score: 0.8037

Eval Phase: 
Validation loss: 156.4426, Validation accuracy: 0.6750
Macro F1-score: 0.6724
Model performance on Angry speech (in validation): 
	Precision: 0.9444, Recall: 0.6800, F1_score: 0.7907
Model performance on Happy speech (in validation): 
	Precision: 0.5439, Recall: 0.6200, F1_score: 0.5794
Model performance on Neutral speech (in validation): 
	Precision: 0.7059, Recall: 0.4800, F1_score: 0.5714
Model performance on Sad speech (in validation): 
	Precision: 0.6301, Recall: 0.9200, F1_score: 0.7480
Epoch 7/100

Training Phase:
0 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 214/1600 [00:10<01:05, 21.31it/s]
Training:  27%|██▋       | 428/1600 [00:20<00:55, 21.04it/s]
Training:  40%|███▉      | 637/1600 [00:30<00:47, 20.37it/s]
Training:  52%|█████▏    | 838/1600 [00:40<00:37, 20.26it/s]
Training:  66%|██████▌   | 1050/1600 [00:50<00:26, 20.59it/s]
Training:  79%|███████▉  | 1262/1600 [01:01<00:16, 20.75it/s]
Training:  92%|█████████▏| 1479/1600 [01:11<00:05, 21.05it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 214/1600 [00:10<01:04, 21.36it/s]
Training:  27%|██▋       | 430/1600 [00:20<00:54, 21.48it/s]
Training:  40%|████      | 646/1Training loss: 969.9106, Training accuracy: 0.7806
Macro F1-score: 0.7789
Model performance on Angry speech (in training): 
	Precision: 0.8626, Recall: 0.8475, F1_score: 0.8550
Model performance on Happy speech (in training): 
	Precision: 0.7758, Recall: 0.6575, F1_score: 0.7118
Model performance on Neutral speech (in training): 
	Precision: 0.7382, Recall: 0.7400, F1_score: 0.7391
Model performance on Sad speech (in training): 
	Precision: 0.7516, Recall: 0.8775, F1_score: 0.8097

Eval Phase: 
Validation loss: 152.2911, Validation accuracy: 0.7100
Macro F1-score: 0.7102
Model performance on Angry speech (in validation): 
	Precision: 0.9024, Recall: 0.7400, F1_score: 0.8132
Model performance on Happy speech (in validation): 
	Precision: 0.6889, Recall: 0.6200, F1_score: 0.6526
Model performance on Neutral speech (in validation): 
	Precision: 0.6078, Recall: 0.6200, F1_score: 0.6139
Model performance on Sad speech (in validation): 
	Precision: 0.6825, Recall: 0.8600, F1_score: 0.7611
New best accuracy for layer 5 on epoch 7: 0.7100. Model saved.
Epoch 8/100

Training Phase:
600 [00:30<00:44, 21.41it/s]
Training:  54%|█████▍    | 860/1600 [00:40<00:35, 21.11it/s]
Training:  67%|██████▋   | 1067/1600 [00:50<00:25, 20.92it/s]
Training:  80%|███████▉  | 1273/1600 [01:01<00:15, 20.52it/s]
Training:  93%|█████████▎| 1483/1600 [01:11<00:05, 20.66it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 209/1600 [00:10<01:06, 20.80it/s]
Training:  26%|██▌       | 418/1600 [00:20<00:57, 20.65it/s]
Training:  39%|███▉      | 624/1600 [00:31<00:49, 19.65it/s]
Training:  52%|█████▏    | 834/1600 [00:41<00:38, 20.14it/s]
Training:  65%|██████▌   | 1044/1600 [00:51<00:27, 20.16it/s]
Training:  78%|███████▊  | 1246/1600 [01:01<00:17, 20.09it/s]
Training:  90%|████████Training loss: 811.3139, Training accuracy: 0.8131
Macro F1-score: 0.8121
Model performance on Angry speech (in training): 
	Precision: 0.8883, Recall: 0.8750, F1_score: 0.8816
Model performance on Happy speech (in training): 
	Precision: 0.8260, Recall: 0.7000, F1_score: 0.7578
Model performance on Neutral speech (in training): 
	Precision: 0.7578, Recall: 0.7900, F1_score: 0.7736
Model performance on Sad speech (in training): 
	Precision: 0.7889, Recall: 0.8875, F1_score: 0.8353

Eval Phase: 
Validation loss: 161.8035, Validation accuracy: 0.7300
Macro F1-score: 0.7274
Model performance on Angry speech (in validation): 
	Precision: 0.7885, Recall: 0.8200, F1_score: 0.8039
Model performance on Happy speech (in validation): 
	Precision: 0.6610, Recall: 0.7800, F1_score: 0.7156
Model performance on Neutral speech (in validation): 
	Precision: 0.7000, Recall: 0.5600, F1_score: 0.6222
Model performance on Sad speech (in validation): 
	Precision: 0.7755, Recall: 0.7600, F1_score: 0.7677
New best accuracy for layer 5 on epoch 8: 0.7300. Model saved.
Epoch 9/100

Training Phase:
Training loss: 724.1981, Training accuracy: 0.8294
Macro F1-score: 0.8280
Model performance on Angry speech (in training): 
	Precision: 0.9013, Recall: 0.8900, F1_score: 0.8956
Model performance on Happy speech (in training): 
	Precision: 0.8407, Recall: 0.7125, F1_score: 0.7713
Model performance on Neutral speech (in training): 
	Precision: 0.7786, Recall: 0.8000, F1_score: 0.7891
Model performance on Sad speech (in training): 
	Precision: 0.8044, Recall: 0.9150, F1_score: 0.8561

Eval Phase: 
Validation loss: 204.1738, Validation accuracy: 0.6400
Macro F1-score: 0.6468
Model performance on Angry speech (in validation): 
	Precision: 0.8605, Recall: 0.7400, F1_score: 0.7957
Model performance on Happy speech (in validation): 
	Precision: 0.7097, Recall: 0.4400, F1_score: 0.5432
Model performance on Neutral speech (in validation): 
	Precision: 0.4409, Recall: 0.8200, F1_score: 0.5734
Model performance on Sad speech (in validation): 
	Precision: 0.8485, Recall: 0.5600, F1_score: 0.6747
Epoch 10/100

Training Phase:
█ | 1446/1600 [01:11<00:07, 20.04it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 208/1600 [00:10<01:07, 20.75it/s]
Training:  26%|██▌       | 416/1600 [00:20<00:58, 20.11it/s]
Training:  38%|███▊      | 613/1600 [00:30<00:49, 19.75it/s]
Training:  51%|█████     | 816/1600 [00:40<00:39, 19.95it/s]
Training:  64%|██████▍   | 1020/1600 [00:50<00:28, 20.09it/s]
Training:  77%|███████▋  | 1234/1600 [01:00<00:17, 20.52it/s]
Training:  90%|█████████ | 1448/1600 [01:11<00:07, 20.58it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▎        | 216/1600Training loss: 588.9664, Training accuracy: 0.8638
Macro F1-score: 0.8629
Model performance on Angry speech (in training): 
	Precision: 0.9244, Recall: 0.9175, F1_score: 0.9210
Model performance on Happy speech (in training): 
	Precision: 0.8711, Recall: 0.7600, F1_score: 0.8117
Model performance on Neutral speech (in training): 
	Precision: 0.8225, Recall: 0.8575, F1_score: 0.8397
Model performance on Sad speech (in training): 
	Precision: 0.8421, Recall: 0.9200, F1_score: 0.8793

Eval Phase: 
Validation loss: 189.9117, Validation accuracy: 0.6700
Macro F1-score: 0.6696
Model performance on Angry speech (in validation): 
	Precision: 0.8788, Recall: 0.5800, F1_score: 0.6988
Model performance on Happy speech (in validation): 
	Precision: 0.5588, Recall: 0.7600, F1_score: 0.6441
Model performance on Neutral speech (in validation): 
	Precision: 0.6279, Recall: 0.5400, F1_score: 0.5806
Model performance on Sad speech (in validation): 
	Precision: 0.7143, Recall: 0.8000, F1_score: 0.7547
Epoch 11/100

Training Phase:
 [00:10<01:04, 21.59it/s]
Training:  27%|██▋       | 432/1600 [00:20<00:54, 21.27it/s]
Training:  40%|████      | 643/1600 [00:30<00:45, 20.97it/s]
Training:  53%|█████▎    | 850/1600 [00:40<00:35, 20.85it/s]
Training:  66%|██████▋   | 1062/1600 [00:50<00:25, 20.95it/s]
Training:  80%|███████▉  | 1274/1600 [01:00<00:15, 20.94it/s]
Training:  93%|█████████▎| 1484/1600 [01:10<00:05, 20.91it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 211/1600 [00:10<01:05, 21.05it/s]
Training:  26%|██▋       | 422/1600 [00:20<00:56, 20.95it/s]
Training:  39%|███▉      | 631/1600 [00:30<00:47, 20.47it/s]
Training:  52%|█████▏    | 837/1600 [00:40<00:37, 20.50it/s]
Training:  66%|██████▌   | 1055/1600 [00:50<00Training loss: 527.7399, Training accuracy: 0.8812
Macro F1-score: 0.8809
Model performance on Angry speech (in training): 
	Precision: 0.9391, Recall: 0.9250, F1_score: 0.9320
Model performance on Happy speech (in training): 
	Precision: 0.8886, Recall: 0.7975, F1_score: 0.8406
Model performance on Neutral speech (in training): 
	Precision: 0.8627, Recall: 0.8800, F1_score: 0.8713
Model performance on Sad speech (in training): 
	Precision: 0.8405, Recall: 0.9225, F1_score: 0.8796

Eval Phase: 
Validation loss: 171.0306, Validation accuracy: 0.7150
Macro F1-score: 0.7140
Model performance on Angry speech (in validation): 
	Precision: 0.8750, Recall: 0.7000, F1_score: 0.7778
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.5400, F1_score: 0.6279
Model performance on Neutral speech (in validation): 
	Precision: 0.5909, Recall: 0.7800, F1_score: 0.6724
Model performance on Sad speech (in validation): 
	Precision: 0.7241, Recall: 0.8400, F1_score: 0.7778
Epoch 12/100

Training Phase:
Training loss: 440.3796, Training accuracy: 0.9106
Macro F1-score: 0.9102
Model performance on Angry speech (in training): 
	Precision: 0.9599, Recall: 0.9575, F1_score: 0.9587
Model performance on Happy speech (in training): 
	Precision: 0.9302, Recall: 0.8325, F1_score: 0.8786
Model performance on Neutral speech (in training): 
	Precision: 0.8856, Recall: 0.8900, F1_score: 0.8878
Model performance on Sad speech (in training): 
	Precision: 0.8730, Recall: 0.9625, F1_score: 0.9156

Eval Phase: 
:26, 20.93it/s]
Training:  80%|███████▉  | 1273/1600 [01:01<00:16, 20.38it/s]
Training:  92%|█████████▏| 1472/1600 [01:11<00:06, 20.22it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 205/1600 [00:10<01:08, 20.48it/s]
Training:  26%|██▌       | 417/1600 [00:20<00:56, 20.88it/s]
Training:  39%|███▉      | 629/1600 [00:30<00:46, 20.70it/s]
Training:  52%|█████▏    | 834/1600 [00:40<00:37, 20.58it/s]
Training:  65%|██████▌   | 1047/1600 [00:50<00:26, 20.81it/s]
Training:  79%|███████▉  | 1260/1600 [01:01<00:16, 20.23it/s]
Training:  91%|█████████ | 1452/1600 [01:12<00:07, 19.61it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
      Validation loss: 222.1074, Validation accuracy: 0.6750
Macro F1-score: 0.6743
Model performance on Angry speech (in validation): 
	Precision: 0.8621, Recall: 0.5000, F1_score: 0.6329
Model performance on Happy speech (in validation): 
	Precision: 0.6739, Recall: 0.6200, F1_score: 0.6458
Model performance on Neutral speech (in validation): 
	Precision: 0.5342, Recall: 0.7800, F1_score: 0.6341
Model performance on Sad speech (in validation): 
	Precision: 0.7692, Recall: 0.8000, F1_score: 0.7843
Epoch 13/100

Training Phase:
Training loss: 377.1187, Training accuracy: 0.9150
Macro F1-score: 0.9148
Model performance on Angry speech (in training): 
	Precision: 0.9550, Recall: 0.9550, F1_score: 0.9550
Model performance on Happy speech (in training): 
	Precision: 0.9245, Recall: 0.8575, F1_score: 0.8898
Model performance on Neutral speech (in training): 
	Precision: 0.8958, Recall: 0.9025, F1_score: 0.8991
Model performance on Sad speech (in training): 
	Precision: 0.8873, Recall: 0.9450, F1_score: 0.9153

Eval Phase: 
Validation loss: 221.0036, Validation accuracy: 0.7150
Macro F1-score: 0.7127
Model performance on Angry speech (in validation): 
	Precision: 0.8824, Recall: 0.6000, F1_score: 0.7143
Model performance on Happy speech (in validation): 
	Precision: 0.6852, Recall: 0.7400, F1_score: 0.7115
Model performance on Neutral speech (in validation): 
	Precision: 0.6667, Recall: 0.6400, F1_score: 0.6531
Model performance on Sad speech (in validation): 
	Precision: 0.6875, Recall: 0.8800, F1_score: 0.7719
Epoch 14/100

Training Phase:
                                             

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 206/1600 [00:10<01:07, 20.53it/s]
Training:  26%|██▌       | 415/1600 [00:20<00:57, 20.74it/s]
Training:  39%|███▉      | 624/1600 [00:30<00:47, 20.64it/s]
Training:  52%|█████▏    | 831/1600 [00:40<00:37, 20.66it/s]
Training:  65%|██████▌   | 1040/1600 [00:50<00:26, 20.75it/s]
Training:  78%|███████▊  | 1250/1600 [01:00<00:16, 20.83it/s]
Training:  92%|█████████▏| 1467/1600 [01:10<00:06, 21.09it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 190/1600 [00:10<01:14, 18.99it/s]
Training:  24%|██▍       | 380/1600 [00:20<01:06, 18.36it/s]
Training:  36%|███▋      | 584/1600 [00:30<00:52, 19.25itTraining loss: 336.0012, Training accuracy: 0.9206
Macro F1-score: 0.9204
Model performance on Angry speech (in training): 
	Precision: 0.9605, Recall: 0.9725, F1_score: 0.9665
Model performance on Happy speech (in training): 
	Precision: 0.9208, Recall: 0.8725, F1_score: 0.8960
Model performance on Neutral speech (in training): 
	Precision: 0.9025, Recall: 0.9025, F1_score: 0.9025
Model performance on Sad speech (in training): 
	Precision: 0.8990, Recall: 0.9350, F1_score: 0.9167

Eval Phase: 
Validation loss: 252.9430, Validation accuracy: 0.6650
Macro F1-score: 0.6693
Model performance on Angry speech (in validation): 
	Precision: 0.9143, Recall: 0.6400, F1_score: 0.7529
Model performance on Happy speech (in validation): 
	Precision: 0.5634, Recall: 0.8000, F1_score: 0.6612
Model performance on Neutral speech (in validation): 
	Precision: 0.5574, Recall: 0.6800, F1_score: 0.6126
Model performance on Sad speech (in validation): 
	Precision: 0.8182, Recall: 0.5400, F1_score: 0.6506
Epoch 15/100

Training Phase:
/s]
Training:  50%|████▉     | 796/1600 [00:40<00:40, 19.98it/s]
Training:  63%|██████▎   | 1008/1600 [00:50<00:29, 20.22it/s]
Training:  77%|███████▋  | 1225/1600 [01:00<00:18, 20.70it/s]
Training:  77%|███████▋  | 1225/1600 [01:11<00:18, 20.70it/s]
Training:  90%|█████████ | 1440/1600 [01:11<00:07, 20.85it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▎        | 217/1600 [00:10<01:03, 21.62it/s]
Training:  27%|██▋       | 434/1600 [00:20<00:55, 20.96it/s]
Training:  40%|████      | 643/1600 [00:30<00:45, 20.90it/s]
Training:  53%|█████▎    | 854/1600 [00:40<00:35, 20.97it/s]
Training:  67%|██████▋   | 1070/1600 [00:50<00:25, 21.16it/s]
Training:  80%|████████  | 1286/1600 [01:01<00:15, 2Training loss: 274.5977, Training accuracy: 0.9475
Macro F1-score: 0.9475
Model performance on Angry speech (in training): 
	Precision: 0.9675, Recall: 0.9675, F1_score: 0.9675
Model performance on Happy speech (in training): 
	Precision: 0.9608, Recall: 0.9200, F1_score: 0.9400
Model performance on Neutral speech (in training): 
	Precision: 0.9395, Recall: 0.9325, F1_score: 0.9360
Model performance on Sad speech (in training): 
	Precision: 0.9238, Recall: 0.9700, F1_score: 0.9463

Eval Phase: 
Validation loss: 227.8824, Validation accuracy: 0.6950
Macro F1-score: 0.6939
Model performance on Angry speech (in validation): 
	Precision: 0.8438, Recall: 0.5400, F1_score: 0.6585
Model performance on Happy speech (in validation): 
	Precision: 0.6038, Recall: 0.6400, F1_score: 0.6214
Model performance on Neutral speech (in validation): 
	Precision: 0.5968, Recall: 0.7400, F1_score: 0.6607
Model performance on Sad speech (in validation): 
	Precision: 0.8113, Recall: 0.8600, F1_score: 0.8350
Epoch 16/100

Training Phase:
0.78it/s]
Training:  80%|████████  | 1286/1600 [01:11<00:15, 20.78it/s]
Training:  94%|█████████▎| 1497/1600 [01:11<00:04, 20.86it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 213/1600 [00:10<01:05, 21.28it/s]
Training:  27%|██▋       | 426/1600 [00:20<00:56, 20.76it/s]
Training:  40%|███▉      | 638/1600 [00:30<00:45, 20.94it/s]
Training:  53%|█████▎    | 850/1600 [00:40<00:35, 20.88it/s]
Training:  66%|██████▋   | 1063/1600 [00:50<00:25, 21.02it/s]
Training:  80%|███████▉  | 1279/1600 [01:00<00:15, 21.18it/s]
Training:  80%|███████▉  | 1279/1600 [01:11<00:15, 21.18it/s]
Training:  93%|█████████▎| 1492/1600 [01:11<00:05, 21.01it/s]
                                               Training loss: 288.3006, Training accuracy: 0.9437
Macro F1-score: 0.9436
Model performance on Angry speech (in training): 
	Precision: 0.9799, Recall: 0.9750, F1_score: 0.9774
Model performance on Happy speech (in training): 
	Precision: 0.9349, Recall: 0.8975, F1_score: 0.9158
Model performance on Neutral speech (in training): 
	Precision: 0.9284, Recall: 0.9400, F1_score: 0.9342
Model performance on Sad speech (in training): 
	Precision: 0.9322, Recall: 0.9625, F1_score: 0.9471

Eval Phase: 
Validation loss: 205.6369, Validation accuracy: 0.7000
Macro F1-score: 0.6982
Model performance on Angry speech (in validation): 
	Precision: 0.8140, Recall: 0.7000, F1_score: 0.7527
Model performance on Happy speech (in validation): 
	Precision: 0.6429, Recall: 0.7200, F1_score: 0.6792
Model performance on Neutral speech (in validation): 
	Precision: 0.6829, Recall: 0.5600, F1_score: 0.6154
Model performance on Sad speech (in validation): 
	Precision: 0.6833, Recall: 0.8200, F1_score: 0.7455
Epoch 17/100

Training Phase:
Training loss: 272.3414, Training accuracy: 0.9394
Macro F1-score: 0.9392
Model performance on Angry speech (in training): 
	Precision: 0.9601, Recall: 0.9625, F1_score: 0.9613
Model performance on Happy speech (in training): 
	Precision: 0.9280, Recall: 0.9025, F1_score: 0.9151
Model performance on Neutral speech (in training): 
	Precision: 0.9342, Recall: 0.9225, F1_score: 0.9283
Model performance on Sad speech (in training): 
	Precision: 0.9349, Recall: 0.9700, F1_score: 0.9521

Eval Phase: 
Validation loss: 245.3340, Validation accuracy: 0.6850
Macro F1-score: 0.6890
Model performance on Angry speech (in validation): 
	Precision: 0.9375, Recall: 0.6000, F1_score: 0.7317
Model performance on Happy speech (in validation): 
	Precision: 0.7045, Recall: 0.6200, F1_score: 0.6596
Model performance on Neutral speech (in validation): 
	Precision: 0.5294, Recall: 0.7200, F1_score: 0.6102
Model performance on Sad speech (in validation): 
	Precision: 0.7143, Recall: 0.8000, F1_score: 0.7547
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7300

Test Phase: 
              

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 213/1600 [00:10<01:05, 21.25it/s]
Training:  27%|██▋       | 426/1600 [00:20<00:55, 21.01it/s]
Training:  40%|███▉      | 635/1600 [00:30<00:46, 20.60it/s]
Training:  53%|█████▎    | 847/1600 [00:40<00:36, 20.82it/s]
Training:  66%|██████▌   | 1059/1600 [00:50<00:25, 20.92it/s]
Training:  80%|███████▉  | 1273/1600 [01:00<00:15, 21.08it/s]
Training:  93%|█████████▎| 1487/1600 [01:11<00:05, 20.95it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   1%|          | 2/200 [00:00<00:19, 10.35it/s]
Testing:   2%|▎         | 5/200 [00:00<00:12, 16.09it/s]
Testing:   4%|▎         | 7/200 [00:00<00:11, 16.27it/s]
Testing:   5%|▌         | 10/200 [00:00<00:10, 17.85it/s]
Testing:   6%|▋         | 13/200 [00:00<00:09, 19.36it/s]
Testing:   8%|▊         | 16/200 [00:00<00:08, 21.20it/s]
Testing:  10%|▉         | 19/200 [00:01<00:09, 18.95it/s]
Testing:  12%|█▏        | 23/200 [00:01<00:07, 22.51it/s]
Testing:  14%|█▎        | 27/200 [00:01<00:06, 25.53it/s]
Testing:  16%|█▌        | 31/200 [00:01<00:05, 28.33it/s]
Testing:  17%|█▋        | 34/200 [00:01<00:06, 26.59it/s]
Testing:  19%|█▉        | 38/200 [00:01<00:06, 26.14it/s]
Testing:  21%|██        | 42/200 [00:01<00:05, 28.76it/s]
Testing:  22%|██▎       | 45/200 [00:01<00:05, 28.48it/s]
Testing:  24%|██▍       | 48/200 [00:02<00:05, 25.40it/s]
Testing:  26%|██▋       | 53/200 [00:02<00:04, 31.04it/s]
Testing:  30%|███       | 60/200 [00:02<00:03, 38.86it/s]
Testing:  32%|███▎      | 65/200 [00:02<00:03, 39.85it/s]
Testing:  35%|███▌      | 70/200 [00:02<00:03, 39.53it/s]
Testing:  38%|███▊      | 75/200 [00:02<00:03, 41.38it/s]
Testing:  40%|████      | 81/200 [00:02<00:02, 40.66it/s]
Testing:  43%|████▎     | 86/200 [00:02<00:02, 39.22it/s]
Testing:  46%|████▌     | 91/200 [00:03<00:02, 41.17it/s]
Testing:  48%|████▊     | 97/200 [00:03<00:02, 43.61it/s]
Testing:  51%|█████     | 102/200 [00:03<00:02, 45.06it/s]
Testing:  56%|█████▌    | 111/200 [00:03<00:01, 55.67it/s]
Testing:  60%|█████▉    | 119/200 [00:03<00:01, 61.05it/s]
Testing:  63%|██████▎   | 126/200 [00:03<00:01, 59.53it/s]
Testing:  66%|██████▋   | 133/200 [00:03<00:01, 60.44it/s]
Testing:  71%|███████   | 142/200 [00:03<00:00, 66.61it/s]
Testing:  74%|███████▍  | 149/200 [00:03<00:00, 56.79it/s]
Testing:  78%|███████▊  | 157/200 [00:04<00:00, 57.20it/s]
Testing:  82%|████████▎ | 165/200 [00:04<00:00, 62.52it/s]
TestiTest loss: 201.0696, Test accuracy: 0.6500
Macro F1-score: 0.6428
Model performance on Angry speech (in test): 
	Precision: 0.7288, Recall: 0.8600, F1_score: 0.7890
Model performance on Happy speech (in test): 
	Precision: 0.5714, Recall: 0.5600, F1_score: 0.5657
Model performance on Neutral speech (in test): 
	Precision: 0.5000, Recall: 0.4200, F1_score: 0.4565
Model performance on Sad speech (in test): 
	Precision: 0.7600, Recall: 0.7600, F1_score: 0.7600

======================= This is fold_1 on de =======================

Load dataset: 
Loading en train data: fold_1...
Preprocess en fold_1 data for de model
ng:  86%|████████▌ | 172/200 [00:04<00:00, 55.72it/s]
Testing:  90%|█████████ | 180/200 [00:04<00:00, 61.24it/s]
Testing:  94%|█████████▍| 188/200 [00:04<00:00, 64.37it/s]
Testing:  98%|█████████▊| 196/200 [00:04<00:00, 67.40it/s]
                                                          

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:   0%|          | 4/1600 [00:00<01:00, 26.17 examples/s]
Map:   1%|          | 18/1600 [00:00<00:21, 74.68 examples/s]
Map:   2%|▏         | 29/1600 [00:00<00:18, 87.26 examples/s]
Map:   3%|▎         | 45/1600 [00:00<00:17, 87.33 examples/s]
Map:   3%|▎         | 55/1600 [00:00<00:18, 84.29 examples/s]
Map:   4%|▍         | 65/1600 [00:00<00:18, 85.21 examples/s]
Map:   5%|▍         | 77/1600 [00:00<00:16, 92.15 examples/s]
Map:   6%|▌         | 88/1600 [00:01<00:16, 90.14 examples/s]
Map:   6%|▌         | 98/1600 [00:01<00:17, 87.23 examples/s]
Map:   7%|▋         | 109/1600 [00:01<00:16, 90.76 examples/s]
Map:   8%|▊         | 120/1600 [00:01<00:19, 76.59 examples/s]
Map:   8%|▊         | 133/1600 [00:01<00:17, 84.37 examples/s]
Map:   9%|▉         | 145/1600 [00:01<00:16, 89.55 examples/s]
Map:  10%|▉         | 155/1600 [00:01<00:15, 90.90 examples/s]
Map:  11%|█         | 169/1600 [00:01<00:14, 99.23 examples/s]
Map:  11%|█▏        | 183/1600 [00:02<00:13, 105.87 examples/s]
Map:  12%|█▏        | 199/1600 [00:02<00:11, 119.63 examples/s]
Map:  13%|█▎        | 214/1600 [00:02<00:11, 120.60 examples/s]
Map:  14%|█▍        | 228/1600 [00:02<00:16, 81.11 examples/s] 
Map:  15%|█▌        | 244/1600 [00:02<00:14, 95.15 examples/s]
Map:  16%|█▌        | 257/1600 [00:02<00:13, 102.10 examples/s]
Map:  17%|█▋        | 272/1600 [00:02<00:12, 108.15 examples/s]
Map:  18%|█▊        | 287/1600 [00:03<00:11, 115.34 examples/s]
Map:  19%|█▉        | 300/1600 [00:03<00:11, 114.31 examples/s]
Map:  20%|█▉        | 316/1600 [00:03<00:11, 110.41 examples/s]
Map:  21%|██        | 331/1600 [00:03<00:10, 115.77 examples/s]
Map:  22%|██▏       | 344/1600 [00:03<00:10, 118.16 examples/s]
Map:  22%|██▏       | 357/1600 [00:03<00:10, 119.95 examples/s]
Map:  24%|██▎       | 376/1600 [00:03<00:10, 118.84 examples/s]
Map:  25%|██▍       | 393/1600 [00:03<00:09, 125.57 examples/s]
Map:  26%|██▌       | 408/1600 [00:04<00:10, 110.99 examples/s]
Map:  27%|██▋       | 425/1600 [00:04<00:11, 103.93 examples/s]
Map:  27%|██▋       | 439/1600 [00:04<00:10, 108.69 examples/s]
Map:  28%|██▊       | 452/1600 [00:04<00:10, 108.36 examples/s]
Map:  29%|██▉       | 468/1600 [00:04<00:09, 118.52 examples/s]
Map:  30%|███       | 484/1600 [00:04<00:08, 127.30 examples/s]
Map:  31%|███       | 498/1600 [00:04<00:08, 124.43 examples/s]
Map:  32%|███▏      | 517/1600 [00:04<00:08, 120.70 examples/s]
Map:  33%|███▎      | 531/1600 [00:05<00:08, 120.84 examples/s]
Map:  34%|███▍      | 549/1600 [00:05<00:09, 115.60 examples/s]
Map:  35%|███▌      | 563/1600 [00:05<00:08, 117.13 examples/s]
Map:  36%|███▌      | 578/1600 [00:05<00:08, 123.27 examples/s]
Map:  37%|███▋      | 594/1600 [00:05<00:08, 113.21 examples/s]
Map:  38%|███▊      | 613/1600 [00:05<00:09, 100.90 examples/s]
Map:  39%|███▉      | 625/1600 [00:05<00:09, 101.51 examples/s]
Map:  40%|███▉      | 638/1600 [00:06<00:09, 103.90 examples/s]
Map:  41%|████      | 652/1600 [00:06<00:08, 110.82 examples/s]
Map:  42%|████▏     | 667/1600 [00:06<00:07, 118.80 examples/s]
Map:  43%|████▎     | 682/1600 [00:06<00:08, 109.37 examples/s]
Map:  43%|████▎     | 694/1600 [00:06<00:08, 108.51 examples/s]
Map:  44%|████▍     | 708/1600 [00:06<00:09, 96.20 examples/s] 
Map:  45%|████▌     | 720/1600 [00:06<00:08, 99.31 examples/s]
Map:  46%|████▌     | 734/1600 [00:07<00:08, 101.39 examples/s]
Map:  47%|████▋     | 748/1600 [00:07<00:08, 96.21 examples/s] 
Map:  48%|████▊     | 763/1600 [00:07<00:07, 107.44 examples/s]
Map:  49%|████▉     | 780/1600 [00:07<00:06, 118.45 examples/s]
Map:  50%|████▉     | 799/1600 [00:07<00:07, 102.61 examples/s]
Map:  51%|█████     | 816/1600 [00:07<00:08, 95.49 examples/s] 
Map:  52%|█████▏    | 828/1600 [00:07<00:07, 99.12 examples/s]
Map:  53%|█████▎    | 847/1600 [00:08<00:07, 101.18 examples/s]
Map:  54%|█████▍    | 861/1600 [00:08<00:07, 105.51 examples/s]
Map:  55%|█████▍    | 879/1600 [00:08<00:07, 99.92 examples/s] 
Map:  56%|█████▌    | 892/1600 [00:08<00:06, 105.57 examples/s]
Map:  57%|█████▋    | 908/1600 [00:08<00:07, 96.50 examples/s] 
Map:  58%|█████▊    | 925/1600 [00:08<00:06, 107.74 examples/s]
Map:  59%|█████▉    | 941/1600 [00:08<00:05, 117.85 examples/s]
Map:  60%|█████▉    | 954/1600 [00:09<00:05, 116.25 examples/s]
Map:  60%|██████    | 967/1600 [00:09<00:06, 102.22 examples/s]
Map:  62%|██████▏   | 984/1600 [00:09<00:06, 99.82 examples/s] 
Map:  62%|██████▏   | 996/1600 [00:09<00:07, 81.92 examples/s]
Map:  62%|██████▏   | 996/1600 [00:21<00:07, 81.92 examples/s]
Map:  62%|██████▎   | 1000/1600 [01:06<15:17,  1.53s/ examples]
Map:  64%|██████▍   | 1020/1600 [01:06<08:23,  1.15 examples/s]
Map:  65%|██████▍   | 1037/1600 [01:06<05:21,  1.75 examples/s]
Map:  66%|██████▌   | 1055/1600 [01:06<03:25,  2.65 examples/s]
Map:  67%|██████▋   | 1070/1600 [01:06<02:24,  3.68 examples/s]
Map:  68%|██████▊   | 1087/1600 [01:06<01:36,  5.31 examples/s]
Map:  69%|██████▊   | 1098/1600 [01:07<01:14,  6.78 examples/s]
Map:  69%|██████▉   | 1109/1600 [01:07<00:55,  8.78 examples/s]
Map:  70%|███████   | 1123/1600 [01:07<00:38, 12.34 examples/s]
Map:  71%|███████   | 1137/1600 [01:07<00:27, 16.58 examples/s]
Map:  72%|███████▏  | 1148/1600 [01:07<00:21, 21.11 examples/s]
Map:  72%|███████▏  | 1159/1600 [01:07<00:16, 26.89 examples/s]
Map:  74%|███████▎  | 1176/1600 [01:07<00:11, 37.25 examples/s]
Map:  74%|███████▍  | 1190/1600 [01:07<00:08, 47.32 examples/s]
Map:  75%|███████▌  | 1205/1600 [01:08<00:06, 59.27 examples/s]
Map:  76%|███████▌  | 1219/1600 [01:08<00:05, 65.00 examples/s]
Map:  77%|███████▋  | 1230/1600 [01:08<00:05, 71.05 examples/s]
Map:  78%|███████▊  | 1241/1600 [01:08<00:04, 73.70 examples/s]
Map:  78%|███████▊  | 1253/1600 [01:08<00:04, 71.14 examples/s]
Map:  79%|███████▉  | 1267/1600 [01:08<00:04, 76.84 examples/s]
Map:  80%|████████  | 1281/1600 [01:08<00:04, 79.16 examples/s]
Map:  81%|████████  | 1290/1600 [01:09<00:03, 79.24 examples/s]
Map:  82%|████████▏ | 1305/1600 [01:09<00:03, 82.92 examples/s]
Map:  82%|████████▏ | 1318/1600 [01:09<00:03, 91.67 examples/s]
Map:  84%|████████▎ | 1336/1600 [01:09<00:02, 108.95 examples/s]
Map:  85%|████████▍ | 1353/1600 [01:09<00:02, 121.63 examples/s]
Map:  86%|████████▌ | 1368/1600 [01:09<00:02, 105.61 examples/s]
Map:  87%|████████▋ | 1386/1600 [01:09<00:02, 105.65 examples/s]
Map:  88%|████████▊ | 1400/1600 [01:10<00:01, 112.19 examples/s]
Map:  88%|████████▊ | 1416/1600 [01:10<00:01, 121.55 examples/s]
Map:  90%|████████▉ | 1433/1600 [01:10<00:01, 132.90 examples/s]
Map:  91%|█████████ | 1449/1600 [01:10<00:01, 138.74 examples/s]
Map:  92%|█████████▏| 1465/1600 [01:10<00:00, 141.27 examples/s]
Map:  93%|█████████▎| 1486/1600 [01:10<00:00, 123.00 examples/s]
Map:  94%|█████████▍| 1502/1600 [01:10<00:00, 112.28 examples/s]
Map:  95%|█████████▍| 1516/1600 [01:10<00:00, 114.67 examples/s]
Map:  96%|█████████▌| 1529/1600 [01:11<00:00, 116.55 examples/s]
Map:  96%|█████████▋| 1544/1600 [01:11<00:00, 121.79 examples/s]
Map:  98%|█████████▊| 1561/1600 [01:11<00:00, 129.68 examples/s]
Map:  99%|█████████▉| 1583/1600 [01:11<00:00, 147.57 examples/s]
Map: 100%|█████████▉| 1593/1600 [01:22<00:00, 147.57 examples/s]
Map: 100%|██████████| 1600/1600 [01:44<00:00,  1.75 examples/s] 
Map: 100%|██████████| 1600/1600 [01:44<00:00, 15.34 examples/s]

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▎         | 5/200 [00:00<00:04, 45.48 examples/s]
Map:   7%|▋         | 14/200 [00:00<00:02, 67.01 examples/s]
Map:  14%|█▎        | 27/200 [00:00<00:01, 91.28 examples/s]
Map:  22%|██▏       | 44/200 [00:00<00:01, 114.13 examples/s]
Map:  28%|██▊       | 57/200 [00:00<00:01, 109.35 examples/s]
Map:  34%|███▍      | 68/200 [00:00<00:01, 102.89 examples/s]
Map:  40%|███▉      | 79/200 [00:00<00:01, 104.08 examples/s]
Map:  47%|████▋     | 94/200 [00:00<00:01, 98.29 examples/s] 
Map:  56%|█████▌    | 111/200 [00:01<00:00, 112.69 examples/s]
Map:  63%|██████▎   | 126/200 [00:01<00:00, 118.20 examples/s]
Map:  72%|███████▏  | 144/200 [00:01<00:00, 113.69 examples/s]
Map:  78%|███████▊  | 157/200 [00:01<00:00, 113.55 examples/s]
Map:  87%|████████▋ | 174/200 [00:01<00:00, 126.58 examples/s]
Map:  94%|█████████▍| 189/200 [00:01<00:00, 114.04 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 15.70 examples/s] 

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   4%|▍         | 8/200 [00:00<00:03, 63.91 examples/s]
Map:  13%|█▎        | 26/200 [00:00<00:01, 118.22 examples/s]
Map:  21%|██        | 42/200 [00:00<00:01, 122.57 examples/s]
Map:  28%|██▊       | 55/200 [00:00<00:01, 120.21 examples/s]
Map:  34%|███▍      | 68/200 [00:00<00:01, 118.34 examples/s]
Map:  43%|████▎     | 86/200 [00:00<00:00, 132.73 examples/s]
Map:  50%|█████     | 100/200 [00:00<00:00, 132.94 examples/s]
Map:  57%|█████▋    | 114/200 [00:00<00:00, 111.02 examples/s]
Map:  64%|██████▎   | 127/200 [00:01<00:00, 114.13 examples/s]
Map:  70%|██████▉   | 139/200 [00:01<00:00, 107.97 examples/s]
Map:  78%|███████▊  | 156/200 [00:01<00:00, 103.81 examples/s]
Map:  85%|████████▌ | 170/200 [00:01<00:00, 111.30 examples/s]
Map:  92%|█████████▏| 183/200 [00:01<00:00, 114.68 examples/s]
Map:  99%|█████████▉| 198/200 [00:01<00:00, 102.84 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 16.00 examples/s] 
Loading en eval data: fold_1...
Preprocess en fold_1 data for de model
Loading en test data: fold_1...
Preprocess en fold_1 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1598.4746, Training accuracy: 0.5769
Macro F1-score: 0.5692
Model performance on Angry speech (in training): 
	Precision: 0.6308, Recall: 0.6750, F1_score: 0.6522
Model performance on Happy speech (in training): 
	Precision: 0.5683, Recall: 0.3950, F1_score: 0.4661
Model performance on Neutral speech (in training): 
	Precision: 0.4826, Recall: 0.4850, F1_score: 0.4838
Model performance on Sad speech (in training): 
	Precision: 0.6118, Recall: 0.7525, F1_score: 0.6749

Eval Phase: 
Validation loss: 165.0773, Validation accuracy: 0.6300
Macro F1-score: 0.6112
Model performance on Angry speech (in validation): 
	Precision: 0.6885, Recall: 0.8400, F1_score: 0.7568
Model performance on Happy speech (in validation): 
	Precision: 0.4583, Recall: 0.4400, F1_score: 0.4490
Model performance on Neutral speech (in validation): 
	Precision: 0.6923, Recall: 0.3600, F1_score: 0.4737
Model performance on Sad speech (in validation): 
	Precision: 0.6769, Recall: 0.8800, F1_score: 0.7652
New best accuracy for layer 5 on epoch 1: 0.6300. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 205/1600 [00:10<01:08, 20.40it/s]
Training:  26%|██▌       | 409/1600 [00:20<00:59, 19.93it/s]
Training:  38%|███▊      | 606/1600 [00:30<00:51, 19.47it/s]
Training:  50%|█████     | 807/1600 [00:40<00:40, 19.68it/s]
Training:  63%|██████▎   | 1008/1600 [00:51<00:30, 19.57it/s]
Training:  75%|███████▌  | 1202/1600 [01:01<00:20, 19.15it/s]
Training:  88%|████████▊ | 1406/1600 [01:11<00:09, 19.53it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 209/1600 [00:10<01:06, 20.77it/s]
Training:  26%|██▌       | 418/1600 [00:20<00:56, 20.85it/s]
Training:  39%|███▉      | 627/1600 [00:30<00:47, 20.40it/s]
Training:  52%|█████▏    | 827/160Training loss: 1371.4142, Training accuracy: 0.6500
Macro F1-score: 0.6470
Model performance on Angry speech (in training): 
	Precision: 0.7383, Recall: 0.7475, F1_score: 0.7429
Model performance on Happy speech (in training): 
	Precision: 0.6450, Recall: 0.4950, F1_score: 0.5601
Model performance on Neutral speech (in training): 
	Precision: 0.5615, Recall: 0.6050, F1_score: 0.5824
Model performance on Sad speech (in training): 
	Precision: 0.6586, Recall: 0.7525, F1_score: 0.7025

Eval Phase: 
Validation loss: 215.0863, Validation accuracy: 0.5800
Macro F1-score: 0.5359
Model performance on Angry speech (in validation): 
	Precision: 0.4717, Recall: 1.0000, F1_score: 0.6410
Model performance on Happy speech (in validation): 
	Precision: 0.5238, Recall: 0.2200, F1_score: 0.3099
Model performance on Neutral speech (in validation): 
	Precision: 0.7778, Recall: 0.2800, F1_score: 0.4118
Model performance on Sad speech (in validation): 
	Precision: 0.7455, Recall: 0.8200, F1_score: 0.7810
Epoch 3/100

Training Phase:
0 [00:41<00:39, 19.61it/s]
Training:  65%|██████▍   | 1037/1600 [00:51<00:28, 20.06it/s]
Training:  78%|███████▊  | 1247/1600 [01:01<00:17, 20.11it/s]
Training:  91%|█████████ | 1450/1600 [01:12<00:07, 19.60it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 186/1600 [00:10<01:16, 18.52it/s]
Training:  24%|██▍       | 383/1600 [00:20<01:03, 19.14it/s]
Training:  36%|███▋      | 583/1600 [00:30<00:52, 19.51it/s]
Training:  49%|████▉     | 783/1600 [00:40<00:41, 19.62it/s]
Training:  61%|██████▏   | 982/1600 [00:50<00:31, 19.56it/s]
Training:  74%|███████▎  | 1177/1600 [01:00<00:21, 19.53it/s]
Training:  86%|████████▌ | 1377/1600 [01:10<00:11, 19.68it/s]
Training:  99%|████████Training loss: 1236.3066, Training accuracy: 0.6937
Macro F1-score: 0.6914
Model performance on Angry speech (in training): 
	Precision: 0.7965, Recall: 0.7925, F1_score: 0.7945
Model performance on Happy speech (in training): 
	Precision: 0.7036, Recall: 0.5400, F1_score: 0.6110
Model performance on Neutral speech (in training): 
	Precision: 0.6014, Recall: 0.6525, F1_score: 0.6259
Model performance on Sad speech (in training): 
	Precision: 0.6855, Recall: 0.7900, F1_score: 0.7340

Eval Phase: 
Validation loss: 198.0716, Validation accuracy: 0.5850
Macro F1-score: 0.5627
Model performance on Angry speech (in validation): 
	Precision: 0.5517, Recall: 0.9600, F1_score: 0.7007
Model performance on Happy speech (in validation): 
	Precision: 0.4231, Recall: 0.4400, F1_score: 0.4314
Model performance on Neutral speech (in validation): 
	Precision: 0.6500, Recall: 0.2600, F1_score: 0.3714
Model performance on Sad speech (in validation): 
	Precision: 0.8293, Recall: 0.6800, F1_score: 0.7473
Epoch 4/100

Training Phase:
Training loss: 1089.2529, Training accuracy: 0.7400
Macro F1-score: 0.7390
Model performance on Angry speech (in training): 
	Precision: 0.8391, Recall: 0.7950, F1_score: 0.8164
Model performance on Happy speech (in training): 
	Precision: 0.7432, Recall: 0.6150, F1_score: 0.6731
Model performance on Neutral speech (in training): 
	Precision: 0.6621, Recall: 0.7200, F1_score: 0.6898
Model performance on Sad speech (in training): 
	Precision: 0.7297, Recall: 0.8300, F1_score: 0.7766

Eval Phase: 
Validation loss: 266.7058, Validation accuracy: 0.5450
Macro F1-score: 0.5181
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 1.0000, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.4762, Recall: 0.2000, F1_score: 0.2817
Model performance on Neutral speech (in validation): 
	Precision: 0.4528, Recall: 0.4800, F1_score: 0.4660
Model performance on Sad speech (in validation): 
	Precision: 0.9615, Recall: 0.5000, F1_score: 0.6579
Epoch 5/100

Training Phase:
█▊| 1577/1600 [01:21<00:01, 19.32it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.57it/s]
Training:  25%|██▍       | 394/1600 [00:20<01:01, 19.65it/s]
Training:  37%|███▋      | 592/1600 [00:30<00:51, 19.67it/s]
Training:  50%|████▉     | 797/1600 [00:40<00:40, 19.99it/s]
Training:  63%|██████▎   | 1002/1600 [00:50<00:30, 19.87it/s]
Training:  75%|███████▌  | 1202/1600 [01:00<00:20, 19.89it/s]
Training:  88%|████████▊ | 1402/1600 [01:10<00:10, 19.73it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 205/16Training loss: 924.5745, Training accuracy: 0.7756
Macro F1-score: 0.7744
Model performance on Angry speech (in training): 
	Precision: 0.8596, Recall: 0.8575, F1_score: 0.8586
Model performance on Happy speech (in training): 
	Precision: 0.7982, Recall: 0.6525, F1_score: 0.7180
Model performance on Neutral speech (in training): 
	Precision: 0.7119, Recall: 0.7475, F1_score: 0.7293
Model performance on Sad speech (in training): 
	Precision: 0.7445, Recall: 0.8450, F1_score: 0.7916

Eval Phase: 
Validation loss: 287.3845, Validation accuracy: 0.5200
Macro F1-score: 0.4714
Model performance on Angry speech (in validation): 
	Precision: 0.4587, Recall: 1.0000, F1_score: 0.6289
Model performance on Happy speech (in validation): 
	Precision: 0.4000, Recall: 0.0800, F1_score: 0.1333
Model performance on Neutral speech (in validation): 
	Precision: 0.4694, Recall: 0.4600, F1_score: 0.4646
Model performance on Sad speech (in validation): 
	Precision: 0.8438, Recall: 0.5400, F1_score: 0.6585
Epoch 6/100

Training Phase:
00 [00:10<01:08, 20.48it/s]
Training:  26%|██▋       | 420/1600 [00:20<00:56, 21.04it/s]
Training:  40%|███▉      | 635/1600 [00:30<00:46, 20.74it/s]
Training:  52%|█████▏    | 839/1600 [00:40<00:36, 20.58it/s]
Training:  65%|██████▌   | 1043/1600 [00:50<00:27, 20.50it/s]
Training:  79%|███████▊  | 1259/1600 [01:00<00:16, 20.83it/s]
Training:  92%|█████████▏| 1474/1600 [01:10<00:06, 20.97it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 203/1600 [00:10<01:08, 20.29it/s]
Training:  25%|██▌       | 406/1600 [00:20<00:59, 20.18it/s]
Training:  38%|███▊      | 607/1600 [00:30<00:49, 19.88it/s]
Training:  50%|█████     | 803/1600 [00:40<00:40, 19.57it/s]
Training:  62%|██████▎   | 1000/1600 [00:50<00Training loss: 765.8783, Training accuracy: 0.8156
Macro F1-score: 0.8153
Model performance on Angry speech (in training): 
	Precision: 0.8990, Recall: 0.8900, F1_score: 0.8945
Model performance on Happy speech (in training): 
	Precision: 0.8187, Recall: 0.7450, F1_score: 0.7801
Model performance on Neutral speech (in training): 
	Precision: 0.7631, Recall: 0.7650, F1_score: 0.7640
Model performance on Sad speech (in training): 
	Precision: 0.7859, Recall: 0.8625, F1_score: 0.8224

Eval Phase: 
Validation loss: 230.9716, Validation accuracy: 0.5900
Macro F1-score: 0.5483
Model performance on Angry speech (in validation): 
	Precision: 0.5455, Recall: 0.9600, F1_score: 0.6957
Model performance on Happy speech (in validation): 
	Precision: 0.4474, Recall: 0.3400, F1_score: 0.3864
Model performance on Neutral speech (in validation): 
	Precision: 0.6875, Recall: 0.2200, F1_score: 0.3333
Model performance on Sad speech (in validation): 
	Precision: 0.7241, Recall: 0.8400, F1_score: 0.7778
Epoch 7/100

Training Phase:
:30, 19.62it/s]
Training:  75%|███████▌  | 1201/1600 [01:00<00:20, 19.76it/s]
Training:  88%|████████▊ | 1402/1600 [01:11<00:10, 19.66it/s]
Training: 100%|█████████▉| 1597/1600 [01:21<00:00, 19.59it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.93it/s]
Training:  25%|██▌       | 400/1600 [00:20<01:01, 19.36it/s]
Training:  37%|███▋      | 592/1600 [00:30<00:52, 19.28it/s]
Training:  49%|████▉     | 785/1600 [00:40<00:42, 19.26it/s]
Training:  62%|██████▏   | 987/1600 [00:50<00:31, 19.58it/s]
Training:  62%|██████▏   | 987/1600 [01:00<00:31, 19.58it/s]
Training:  74%|███████▍  | 1188/1600 [01:00<00:21, 19.53it/s]
Training:  86%|████████▋ | 1384Training loss: 640.1746, Training accuracy: 0.8450
Macro F1-score: 0.8449
Model performance on Angry speech (in training): 
	Precision: 0.9207, Recall: 0.9000, F1_score: 0.9102
Model performance on Happy speech (in training): 
	Precision: 0.8476, Recall: 0.7925, F1_score: 0.8191
Model performance on Neutral speech (in training): 
	Precision: 0.8056, Recall: 0.7975, F1_score: 0.8015
Model performance on Sad speech (in training): 
	Precision: 0.8109, Recall: 0.8900, F1_score: 0.8486

Eval Phase: 
Validation loss: 265.2460, Validation accuracy: 0.5950
Macro F1-score: 0.5686
Model performance on Angry speech (in validation): 
	Precision: 0.5161, Recall: 0.9600, F1_score: 0.6713
Model performance on Happy speech (in validation): 
	Precision: 0.5500, Recall: 0.2200, F1_score: 0.3143
Model performance on Neutral speech (in validation): 
	Precision: 0.5652, Recall: 0.5200, F1_score: 0.5417
Model performance on Sad speech (in validation): 
	Precision: 0.8293, Recall: 0.6800, F1_score: 0.7473
Epoch 8/100

Training Phase:
Training loss: 498.6431, Training accuracy: 0.8812
Macro F1-score: 0.8812
Model performance on Angry speech (in training): 
	Precision: 0.9391, Recall: 0.9250, F1_score: 0.9320
Model performance on Happy speech (in training): 
	Precision: 0.8796, Recall: 0.8400, F1_score: 0.8593
Model performance on Neutral speech (in training): 
	Precision: 0.8374, Recall: 0.8500, F1_score: 0.8437
Model performance on Sad speech (in training): 
	Precision: 0.8708, Recall: 0.9100, F1_score: 0.8900

Eval Phase: 
/1600 [01:11<00:11, 19.53it/s]
Training:  99%|█████████▉| 1584/1600 [01:21<00:00, 19.66it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 188/1600 [00:10<01:15, 18.79it/s]
Training:  24%|██▍       | 381/1600 [00:20<01:04, 19.04it/s]
Training:  36%|███▌      | 579/1600 [00:30<00:52, 19.35it/s]
Training:  49%|████▊     | 777/1600 [00:40<00:42, 19.42it/s]
Training:  61%|██████    | 974/1600 [00:50<00:32, 19.52it/s]
Training:  74%|███████▎  | 1176/1600 [01:00<00:21, 19.75it/s]
Training:  86%|████████▌ | 1378/1600 [01:10<00:11, 19.56it/s]
Training:  99%|█████████▊| 1578/1600 [01:20<00:01, 19.69it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?,Validation loss: 414.6418, Validation accuracy: 0.5000
Macro F1-score: 0.4691
Model performance on Angry speech (in validation): 
	Precision: 0.3769, Recall: 0.9800, F1_score: 0.5444
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.2000, F1_score: 0.3077
Model performance on Neutral speech (in validation): 
	Precision: 0.5238, Recall: 0.2200, F1_score: 0.3099
Model performance on Sad speech (in validation): 
	Precision: 0.8824, Recall: 0.6000, F1_score: 0.7143
Epoch 9/100

Training Phase:
Training loss: 378.4661, Training accuracy: 0.9144
Macro F1-score: 0.9143
Model performance on Angry speech (in training): 
	Precision: 0.9600, Recall: 0.9600, F1_score: 0.9600
Model performance on Happy speech (in training): 
	Precision: 0.9115, Recall: 0.8750, F1_score: 0.8929
Model performance on Neutral speech (in training): 
	Precision: 0.8919, Recall: 0.9075, F1_score: 0.8996
Model performance on Sad speech (in training): 
	Precision: 0.8949, Recall: 0.9150, F1_score: 0.9048

Eval Phase: 
Validation loss: 362.2640, Validation accuracy: 0.5600
Macro F1-score: 0.5181
Model performance on Angry speech (in validation): 
	Precision: 0.4660, Recall: 0.9600, F1_score: 0.6275
Model performance on Happy speech (in validation): 
	Precision: 0.5312, Recall: 0.3400, F1_score: 0.4146
Model performance on Neutral speech (in validation): 
	Precision: 0.5000, Recall: 0.1600, F1_score: 0.2424
Model performance on Sad speech (in validation): 
	Precision: 0.7959, Recall: 0.7800, F1_score: 0.7879
Epoch 10/100

Training Phase:
 ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 190/1600 [00:10<01:14, 18.99it/s]
Training:  24%|██▍       | 383/1600 [00:20<01:03, 19.16it/s]
Training:  36%|███▌      | 577/1600 [00:30<00:53, 19.22it/s]
Training:  49%|████▊     | 778/1600 [00:40<00:42, 19.56it/s]
Training:  61%|██████    | 979/1600 [00:50<00:31, 19.51it/s]
Training:  74%|███████▎  | 1176/1600 [01:00<00:21, 19.54it/s]
Training:  86%|████████▋ | 1383/1600 [01:10<00:10, 19.92it/s]
Training:  99%|█████████▉| 1590/1600 [01:21<00:00, 19.57it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.36it/s]
Training:  25%|██▍       | 394/1600 [Training loss: 327.4630, Training accuracy: 0.9169
Macro F1-score: 0.9168
Model performance on Angry speech (in training): 
	Precision: 0.9571, Recall: 0.9475, F1_score: 0.9523
Model performance on Happy speech (in training): 
	Precision: 0.9215, Recall: 0.8800, F1_score: 0.9003
Model performance on Neutral speech (in training): 
	Precision: 0.8958, Recall: 0.9025, F1_score: 0.8991
Model performance on Sad speech (in training): 
	Precision: 0.8950, Recall: 0.9375, F1_score: 0.9158

Eval Phase: 
Validation loss: 378.6734, Validation accuracy: 0.6100
Macro F1-score: 0.5727
Model performance on Angry speech (in validation): 
	Precision: 0.5217, Recall: 0.9600, F1_score: 0.6761
Model performance on Happy speech (in validation): 
	Precision: 0.4857, Recall: 0.3400, F1_score: 0.4000
Model performance on Neutral speech (in validation): 
	Precision: 0.8667, Recall: 0.2600, F1_score: 0.4000
Model performance on Sad speech (in validation): 
	Precision: 0.7586, Recall: 0.8800, F1_score: 0.8148
Epoch 11/100

Training Phase:
00:20<01:01, 19.70it/s]
Training:  37%|███▋      | 594/1600 [00:30<00:51, 19.52it/s]
Training:  49%|████▉     | 789/1600 [00:40<00:41, 19.51it/s]
Training:  62%|██████▏   | 986/1600 [00:50<00:31, 19.53it/s]
Training:  74%|███████▍  | 1182/1600 [01:00<00:21, 19.53it/s]
Training:  86%|████████▋ | 1380/1600 [01:10<00:11, 19.62it/s]
Training:  86%|████████▋ | 1380/1600 [01:20<00:11, 19.62it/s]
Training:  98%|█████████▊| 1575/1600 [01:20<00:01, 19.52it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.52it/s]
Training:  24%|██▍       | 392/1600 [00:20<01:02, 19.35it/s]
Training:  37%|███▋      | 591/1600 [00:30<00:51, 19.47it/s]
Training:  49%|████▉     | 788/1600Training loss: 271.0050, Training accuracy: 0.9369
Macro F1-score: 0.9370
Model performance on Angry speech (in training): 
	Precision: 0.9797, Recall: 0.9675, F1_score: 0.9736
Model performance on Happy speech (in training): 
	Precision: 0.9513, Recall: 0.9275, F1_score: 0.9392
Model performance on Neutral speech (in training): 
	Precision: 0.9057, Recall: 0.9125, F1_score: 0.9091
Model performance on Sad speech (in training): 
	Precision: 0.9126, Recall: 0.9400, F1_score: 0.9261

Eval Phase: 
Validation loss: 362.5165, Validation accuracy: 0.6150
Macro F1-score: 0.5899
Model performance on Angry speech (in validation): 
	Precision: 0.5053, Recall: 0.9600, F1_score: 0.6621
Model performance on Happy speech (in validation): 
	Precision: 0.6000, Recall: 0.3000, F1_score: 0.4000
Model performance on Neutral speech (in validation): 
	Precision: 0.6129, Recall: 0.3800, F1_score: 0.4691
Model performance on Sad speech (in validation): 
	Precision: 0.8367, Recall: 0.8200, F1_score: 0.8283
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6300

Test Phase: 
 [00:40<00:41, 19.52it/s]
Training:  62%|██████▏   | 987/1600 [00:50<00:31, 19.62it/s]
Training:  74%|███████▍  | 1186/1600 [01:00<00:21, 19.54it/s]
Training:  86%|████████▋ | 1383/1600 [01:10<00:11, 19.56it/s]
Training:  99%|█████████▊| 1579/1600 [01:21<00:01, 19.45it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   4%|▍         | 8/200 [00:00<00:02, 76.36it/s]
Testing:   8%|▊         | 17/200 [00:00<00:02, 79.08it/s]
Testing:  12%|█▎        | 25/200 [00:00<00:02, 78.53it/s]
Testing:  17%|█▋        | 34/200 [00:00<00:02, 79.79it/s]
Testing:  21%|██        | 42/200 [00:00<00:02, 77.82it/s]
Testing:  25%|██▌       | 50/200 [00:00<00:01, 77.30it/s]
Testing:  29%|██▉       | 58/200 [00:00<00:01, 75.46it/s]
Testing:  33%|███▎      | 66/200 [00:00<00:01, 76.07it/s]
Testing:  37%|███▋      | 74/200 [00:00<00:01, 76.95it/s]
Testing:  41%|████      | 82/200 [00:01<00:01, 77.35it/s]
Testing:  46%|████▌     | 91/200 [00:01<00:01, 78.37it/s]
Testing:  50%|████▉     | 99/200 [00:01<00:01, 78.45it/s]
Testing:  54%|█████▍    | 108/200 [00:01<00:01, 75.98it/s]
Testing:  58%|█████▊    | 116/200 [00:01<00:01, 74.83it/s]
Testing:  62%|██████▏   | 124/200 [00:01<00:01, 75.84it/s]
Testing:  66%|██████▋   | 133/200 [00:01<00:00, 77.79it/s]
Testing:  70%|███████   | 141/200 [00:01<00:00, 76.92it/s]
Testing:  75%|███████▌  | 150/200 [00:01<00:00, 77.73it/s]
Testing:  79%|███████▉  | 158/200 [00:02<00:00, 78.02it/s]
Testing:  83%|████████▎ | 166/200 [00:02<00:00, 76.82it/s]
Testing:  88%|████████▊ | 175/200 [00:02<00:00, 78.27it/s]
Testing:  92%|█████████▏| 183/200 [00:Test loss: 175.8465, Test accuracy: 0.6250
Macro F1-score: 0.6163
Model performance on Angry speech (in test): 
	Precision: 0.6364, Recall: 0.7000, F1_score: 0.6667
Model performance on Happy speech (in test): 
	Precision: 0.5532, Recall: 0.5200, F1_score: 0.5361
Model performance on Neutral speech (in test): 
	Precision: 0.5946, Recall: 0.4400, F1_score: 0.5057
Model performance on Sad speech (in test): 
	Precision: 0.6885, Recall: 0.8400, F1_score: 0.7568

======================= This is fold_2 on de =======================

Load dataset: 
Loading en train data: fold_2...
Preprocess en fold_2 data for de model
02<00:00, 77.60it/s]
Testing:  96%|█████████▌| 191/200 [00:02<00:00, 76.66it/s]
Testing: 100%|█████████▉| 199/200 [00:02<00:00, 75.63it/s]
                                                          

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:   0%|          | 4/1600 [00:00<00:42, 37.22 examples/s]
Map:   1%|▏         | 20/1600 [00:00<00:15, 99.07 examples/s]
Map:   2%|▏         | 31/1600 [00:00<00:17, 90.06 examples/s]
Map:   3%|▎         | 45/1600 [00:00<00:15, 99.06 examples/s]
Map:   3%|▎         | 55/1600 [00:00<00:16, 91.01 examples/s]
Map:   4%|▍         | 71/1600 [00:00<00:16, 90.51 examples/s]
Map:   5%|▌         | 82/1600 [00:00<00:16, 94.05 examples/s]
Map:   6%|▌         | 92/1600 [00:01<00:16, 89.44 examples/s]
Map:   6%|▋         | 103/1600 [00:01<00:16, 88.67 examples/s]
Map:   7%|▋         | 116/1600 [00:01<00:18, 81.62 examples/s]
Map:   8%|▊         | 129/1600 [00:01<00:16, 91.51 examples/s]
Map:   9%|▉         | 140/1600 [00:01<00:15, 94.14 examples/s]
Map:  10%|▉         | 156/1600 [00:01<00:14, 96.27 examples/s]
Map:  11%|█         | 169/1600 [00:01<00:14, 101.75 examples/s]
Map:  11%|█▏        | 183/1600 [00:01<00:13, 107.34 examples/s]
Map:  12%|█▏        | 199/1600 [00:02<00:11, 118.57 examples/s]
Map:  13%|█▎        | 212/1600 [00:02<00:13, 105.49 examples/s]
Map:  14%|█▍        | 226/1600 [00:02<00:13, 105.46 examples/s]
Map:  15%|█▍        | 239/1600 [00:02<00:12, 108.30 examples/s]
Map:  16%|█▌        | 252/1600 [00:02<00:12, 112.00 examples/s]
Map:  17%|█▋        | 268/1600 [00:02<00:11, 117.28 examples/s]
Map:  18%|█▊        | 282/1600 [00:02<00:10, 121.92 examples/s]
Map:  19%|█▊        | 297/1600 [00:02<00:10, 125.46 examples/s]
Map:  20%|█▉        | 312/1600 [00:03<00:11, 113.20 examples/s]
Map:  20%|██        | 328/1600 [00:03<00:10, 118.98 examples/s]
Map:  21%|██▏       | 342/1600 [00:03<00:10, 120.97 examples/s]
Map:  22%|██▏       | 357/1600 [00:03<00:09, 124.34 examples/s]
Map:  24%|██▎       | 376/1600 [00:03<00:10, 121.53 examples/s]
Map:  25%|██▍       | 393/1600 [00:03<00:09, 127.93 examples/s]
Map:  26%|██▌       | 408/1600 [00:03<00:11, 105.08 examples/s]
Map:  26%|██▋       | 424/1600 [00:04<00:11, 103.54 examples/s]
Map:  27%|██▋       | 438/1600 [00:04<00:10, 110.24 examples/s]
Map:  28%|██▊       | 453/1600 [00:04<00:09, 119.17 examples/s]
Map:  30%|██▉       | 472/1600 [00:04<00:08, 135.88 examples/s]
Map:  30%|███       | 487/1600 [00:04<00:08, 137.82 examples/s]
Map:  32%|███▏      | 506/1600 [00:04<00:13, 81.79 examples/s] 
Map:  32%|███▏      | 519/1600 [00:05<00:13, 79.90 examples/s]
Map:  33%|███▎      | 534/1600 [00:05<00:13, 79.97 examples/s]
Map:  34%|███▍      | 549/1600 [00:05<00:11, 91.52 examples/s]
Map:  35%|███▌      | 567/1600 [00:05<00:09, 107.44 examples/s]
Map:  36%|███▋      | 584/1600 [00:05<00:08, 119.23 examples/s]
Map:  38%|███▊      | 602/1600 [00:05<00:09, 103.30 examples/s]
Map:  39%|███▉      | 620/1600 [00:05<00:09, 106.46 examples/s]
Map:  40%|███▉      | 636/1600 [00:06<00:08, 114.73 examples/s]
Map:  41%|████      | 655/1600 [00:06<00:08, 115.35 examples/s]
Map:  42%|████▏     | 672/1600 [00:06<00:08, 110.53 examples/s]
Map:  43%|████▎     | 684/1600 [00:06<00:08, 106.51 examples/s]
Map:  44%|████▎     | 696/1600 [00:06<00:09, 92.77 examples/s] 
Map:  44%|████▍     | 708/1600 [00:06<00:09, 96.40 examples/s]
Map:  45%|████▌     | 727/1600 [00:06<00:07, 113.90 examples/s]
Map:  46%|████▋     | 741/1600 [00:07<00:07, 117.66 examples/s]
Map:  47%|████▋     | 758/1600 [00:07<00:07, 109.67 examples/s]
Map:  48%|████▊     | 773/1600 [00:07<00:08, 101.93 examples/s]
Map:  49%|████▉     | 785/1600 [00:07<00:08, 100.61 examples/s]
Map:  50%|████▉     | 798/1600 [00:07<00:09, 85.17 examples/s] 
Map:  51%|█████     | 812/1600 [00:07<00:09, 84.80 examples/s]
Map:  52%|█████▏    | 824/1600 [00:07<00:08, 88.97 examples/s]
Map:  52%|█████▏    | 834/1600 [00:08<00:08, 91.26 examples/s]
Map:  53%|█████▎    | 846/1600 [00:08<00:07, 97.81 examples/s]
Map:  54%|█████▎    | 859/1600 [00:08<00:07, 104.40 examples/s]
Map:  54%|█████▍    | 870/1600 [00:08<00:07, 99.38 examples/s] 
Map:  55%|█████▌    | 884/1600 [00:08<00:08, 87.78 examples/s]
Map:  56%|█████▌    | 897/1600 [00:08<00:07, 96.21 examples/s]
Map:  57%|█████▋    | 910/1600 [00:08<00:06, 102.89 examples/s]
Map:  58%|█████▊    | 925/1600 [00:08<00:06, 110.95 examples/s]
Map:  59%|█████▉    | 941/1600 [00:09<00:05, 122.04 examples/s]
Map:  60%|█████▉    | 957/1600 [00:09<00:05, 114.30 examples/s]
Map:  61%|██████    | 974/1600 [00:09<00:05, 106.23 examples/s]
Map:  62%|██████▏   | 991/1600 [00:09<00:07, 84.58 examples/s] 
Map:  62%|██████▏   | 996/1600 [00:25<00:07, 84.58 examples/s]
Map:  62%|██████▎   | 1000/1600 [01:04<12:03,  1.21s/ examples]
Map:  64%|██████▎   | 1019/1600 [01:04<07:22,  1.31 examples/s]
Map:  65%|██████▍   | 1037/1600 [01:05<04:46,  1.97 examples/s]
Map:  66%|██████▌   | 1051/1600 [01:05<03:24,  2.68 examples/s]
Map:  67%|██████▋   | 1065/1600 [01:05<02:25,  3.67 examples/s]
Map:  68%|██████▊   | 1081/1600 [01:05<01:38,  5.25 examples/s]
Map:  69%|██████▊   | 1098/1600 [01:05<01:06,  7.56 examples/s]
Map:  69%|██████▉   | 1109/1600 [01:05<00:51,  9.58 examples/s]
Map:  70%|███████   | 1123/1600 [01:05<00:36, 13.18 examples/s]
Map:  71%|███████   | 1137/1600 [01:06<00:26, 17.45 examples/s]
Map:  72%|███████▏  | 1148/1600 [01:06<00:20, 22.00 examples/s]
Map:  72%|███████▎  | 1160/1600 [01:06<00:15, 28.28 examples/s]
Map:  73%|███████▎  | 1174/1600 [01:06<00:11, 36.70 examples/s]
Map:  74%|███████▍  | 1190/1600 [01:06<00:08, 48.74 examples/s]
Map:  75%|███████▌  | 1204/1600 [01:06<00:06, 59.94 examples/s]
Map:  76%|███████▌  | 1219/1600 [01:06<00:05, 66.24 examples/s]
Map:  77%|███████▋  | 1230/1600 [01:06<00:05, 71.77 examples/s]
Map:  78%|███████▊  | 1241/1600 [01:07<00:04, 73.84 examples/s]
Map:  78%|███████▊  | 1253/1600 [01:07<00:04, 72.25 examples/s]
Map:  79%|███████▉  | 1267/1600 [01:07<00:04, 77.57 examples/s]
Map:  80%|████████  | 1281/1600 [01:07<00:04, 79.32 examples/s]
Map:  81%|████████  | 1290/1600 [01:07<00:03, 79.47 examples/s]
Map:  81%|████████▏ | 1303/1600 [01:07<00:03, 77.72 examples/s]
Map:  82%|████████▏ | 1318/1600 [01:07<00:03, 88.58 examples/s]
Map:  84%|████████▎ | 1336/1600 [01:08<00:02, 105.60 examples/s]
Map:  85%|████████▍ | 1353/1600 [01:08<00:02, 118.28 examples/s]
Map:  86%|████████▌ | 1368/1600 [01:08<00:02, 104.44 examples/s]
Map:  87%|████████▋ | 1385/1600 [01:08<00:02, 105.82 examples/s]
Map:  87%|████████▋ | 1398/1600 [01:08<00:01, 109.57 examples/s]
Map:  88%|████████▊ | 1416/1600 [01:08<00:01, 119.63 examples/s]
Map:  90%|████████▉ | 1433/1600 [01:08<00:01, 131.25 examples/s]
Map:  91%|█████████ | 1449/1600 [01:08<00:01, 137.61 examples/s]
Map:  92%|█████████▏| 1465/1600 [01:09<00:00, 140.82 examples/s]
Map:  93%|█████████▎| 1481/1600 [01:09<00:00, 121.50 examples/s]
Map:  93%|█████████▎| 1494/1600 [01:09<00:00, 120.92 examples/s]
Map:  94%|█████████▍| 1511/1600 [01:09<00:00, 114.76 examples/s]
Map:  95%|█████████▌| 1524/1600 [01:09<00:00, 116.88 examples/s]
Map:  96%|█████████▌| 1538/1600 [01:09<00:00, 121.49 examples/s]
Map:  97%|█████████▋| 1552/1600 [01:09<00:00, 123.54 examples/s]
Map:  98%|█████████▊| 1570/1600 [01:09<00:00, 138.11 examples/s]
Map:  99%|█████████▉| 1587/1600 [01:10<00:00, 129.66 examples/s]
Map: 100%|█████████▉| 1595/1600 [01:25<00:00, 129.66 examples/s]
Map: 100%|██████████| 1600/1600 [01:41<00:00,  1.57 examples/s] 
Map: 100%|██████████| 1600/1600 [01:41<00:00, 15.72 examples/s]

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▎         | 5/200 [00:00<00:05, 34.12 examples/s]
Map:   6%|▌         | 12/200 [00:00<00:03, 51.14 examples/s]
Map:  12%|█▎        | 25/200 [00:00<00:02, 81.98 examples/s]
Map:  19%|█▉        | 38/200 [00:00<00:01, 97.90 examples/s]
Map:  26%|██▋       | 53/200 [00:00<00:01, 107.65 examples/s]
Map:  34%|███▎      | 67/200 [00:00<00:01, 114.73 examples/s]
Map:  40%|███▉      | 79/200 [00:00<00:01, 113.42 examples/s]
Map:  46%|████▌     | 91/200 [00:00<00:00, 110.21 examples/s]
Map:  52%|█████▎    | 105/200 [00:01<00:00, 95.06 examples/s]
Map:  60%|██████    | 120/200 [00:01<00:00, 105.57 examples/s]
Map:  66%|██████▋   | 133/200 [00:01<00:00, 109.52 examples/s]
Map:  75%|███████▌  | 150/200 [00:01<00:00, 108.42 examples/s]
Map:  81%|████████  | 162/200 [00:01<00:00, 108.12 examples/s]
Map:  88%|████████▊ | 175/200 [00:01<00:00, 112.24 examples/s]
Map:  96%|█████████▌| 191/200 [00:01<00:00, 119.54 examples/s]
Map: 100%|██████████| 200/200 [00:11<00:00, 16.68 examples/s] 

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   3%|▎         | 6/200 [00:00<00:03, 51.78 examples/s]
Map:   8%|▊         | 15/200 [00:00<00:02, 69.00 examples/s]
Map:  16%|█▋        | 33/200 [00:00<00:01, 109.50 examples/s]
Map:  23%|██▎       | 46/200 [00:00<00:01, 109.61 examples/s]
Map:  31%|███       | 62/200 [00:00<00:01, 120.85 examples/s]
Map:  38%|███▊      | 77/200 [00:00<00:00, 126.26 examples/s]
Map:  46%|████▋     | 93/200 [00:00<00:00, 114.37 examples/s]
Map:  52%|█████▎    | 105/200 [00:00<00:00, 103.44 examples/s]
Map:  60%|█████▉    | 119/200 [00:01<00:00, 109.33 examples/s]
Map:  68%|██████▊   | 136/200 [00:01<00:00, 107.98 examples/s]
Map:  74%|███████▍  | 149/200 [00:01<00:00, 107.72 examples/s]
Map:  82%|████████▏ | 163/200 [00:01<00:00, 98.33 examples/s] 
Map:  88%|████████▊ | 177/200 [00:01<00:00, 106.52 examples/s]
Map:  98%|█████████▊| 196/200 [00:01<00:00, 112.01 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 16.39 examples/s] 
Loading en eval data: fold_2...
Preprocess en fold_2 data for de model
Loading en test data: fold_2...
Preprocess en fold_2 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1507.8451, Training accuracy: 0.5931
Macro F1-score: 0.5876
Model performance on Angry speech (in training): 
	Precision: 0.6613, Recall: 0.7125, F1_score: 0.6859
Model performance on Happy speech (in training): 
	Precision: 0.5202, Recall: 0.4500, F1_score: 0.4826
Model performance on Neutral speech (in training): 
	Precision: 0.5123, Recall: 0.4700, F1_score: 0.4902
Model performance on Sad speech (in training): 
	Precision: 0.6491, Recall: 0.7400, F1_score: 0.6916

Eval Phase: 
Validation loss: 191.0509, Validation accuracy: 0.5900
Macro F1-score: 0.5616
Model performance on Angry speech (in validation): 
	Precision: 0.6557, Recall: 0.8000, F1_score: 0.7207
Model performance on Happy speech (in validation): 
	Precision: 0.8000, Recall: 0.2400, F1_score: 0.3692
Model performance on Neutral speech (in validation): 
	Precision: 0.5106, Recall: 0.4800, F1_score: 0.4948
Model performance on Sad speech (in validation): 
	Precision: 0.5455, Recall: 0.8400, F1_score: 0.6614
New best accuracy for layer 5 on epoch 1: 0.5900. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 203/1600 [00:10<01:09, 20.23it/s]
Training:  25%|██▌       | 406/1600 [00:20<00:59, 20.19it/s]
Training:  38%|███▊      | 608/1600 [00:30<00:49, 20.15it/s]
Training:  51%|█████     | 812/1600 [00:40<00:38, 20.24it/s]
Training:  64%|██████▎   | 1016/1600 [00:50<00:29, 19.87it/s]
Training:  76%|███████▌  | 1209/1600 [01:00<00:19, 19.58it/s]
Training:  88%|████████▊ | 1406/1600 [01:10<00:09, 19.61it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.46it/s]
Training:  25%|██▍       | 394/1600 [00:20<01:01, 19.67it/s]
Training:  37%|███▋      | 595/1600 [00:30<00:50, 19.86it/s]
Training:  50%|█████     | 800/1600 Training loss: 1287.0509, Training accuracy: 0.6694
Macro F1-score: 0.6643
Model performance on Angry speech (in training): 
	Precision: 0.7314, Recall: 0.7625, F1_score: 0.7466
Model performance on Happy speech (in training): 
	Precision: 0.6319, Recall: 0.5150, F1_score: 0.5675
Model performance on Neutral speech (in training): 
	Precision: 0.6000, Recall: 0.5775, F1_score: 0.5885
Model performance on Sad speech (in training): 
	Precision: 0.6970, Recall: 0.8225, F1_score: 0.7546

Eval Phase: 
Validation loss: 212.8530, Validation accuracy: 0.5950
Macro F1-score: 0.5610
Model performance on Angry speech (in validation): 
	Precision: 0.7170, Recall: 0.7600, F1_score: 0.7379
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.2200, F1_score: 0.3607
Model performance on Neutral speech (in validation): 
	Precision: 0.5366, Recall: 0.4400, F1_score: 0.4835
Model performance on Sad speech (in validation): 
	Precision: 0.5053, Recall: 0.9600, F1_score: 0.6621
New best accuracy for layer 5 on epoch 2: 0.5950. Model saved.
Epoch 3/100

Training Phase:
[00:40<00:39, 20.08it/s]
Training:  63%|██████▎   | 1005/1600 [00:50<00:30, 19.77it/s]
Training:  75%|███████▌  | 1204/1600 [01:00<00:19, 19.81it/s]
Training:  88%|████████▊ | 1407/1600 [01:10<00:09, 19.94it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 204/1600 [00:10<01:08, 20.33it/s]
Training:  26%|██▌       | 414/1600 [00:20<00:57, 20.69it/s]
Training:  39%|███▉      | 624/1600 [00:31<00:49, 19.57it/s]
Training:  51%|█████▏    | 822/1600 [00:41<00:39, 19.63it/s]
Training:  64%|██████▍   | 1020/1600 [00:51<00:29, 19.44it/s]
Training:  76%|███████▌  | 1219/1600 [01:01<00:19, 19.58it/s]
Training:  89%|████████▉ | 1426/1600 [01:11<00:08, 19.93it/s]
                                      Training loss: 1131.2175, Training accuracy: 0.7156
Macro F1-score: 0.7123
Model performance on Angry speech (in training): 
	Precision: 0.7831, Recall: 0.8125, F1_score: 0.7975
Model performance on Happy speech (in training): 
	Precision: 0.6935, Recall: 0.5825, F1_score: 0.6332
Model performance on Neutral speech (in training): 
	Precision: 0.6496, Recall: 0.6350, F1_score: 0.6422
Model performance on Sad speech (in training): 
	Precision: 0.7271, Recall: 0.8325, F1_score: 0.7762

Eval Phase: 
Validation loss: 213.9106, Validation accuracy: 0.5700
Macro F1-score: 0.5354
Model performance on Angry speech (in validation): 
	Precision: 0.6852, Recall: 0.7400, F1_score: 0.7115
Model performance on Happy speech (in validation): 
	Precision: 0.9091, Recall: 0.2000, F1_score: 0.3279
Model performance on Neutral speech (in validation): 
	Precision: 0.5385, Recall: 0.4200, F1_score: 0.4719
Model performance on Sad speech (in validation): 
	Precision: 0.4792, Recall: 0.9200, F1_score: 0.6301
Epoch 4/100

Training Phase:
Training loss: 991.2573, Training accuracy: 0.7506
Macro F1-score: 0.7486
Model performance on Angry speech (in training): 
	Precision: 0.8295, Recall: 0.8150, F1_score: 0.8222
Model performance on Happy speech (in training): 
	Precision: 0.7450, Recall: 0.6500, F1_score: 0.6943
Model performance on Neutral speech (in training): 
	Precision: 0.6961, Recall: 0.6700, F1_score: 0.6828
Model performance on Sad speech (in training): 
	Precision: 0.7336, Recall: 0.8675, F1_score: 0.7950

Eval Phase: 
Validation loss: 204.5263, Validation accuracy: 0.6000
Macro F1-score: 0.5857
Model performance on Angry speech (in validation): 
	Precision: 0.6538, Recall: 0.6800, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.6786, Recall: 0.3800, F1_score: 0.4872
Model performance on Neutral speech (in validation): 
	Precision: 0.6875, Recall: 0.4400, F1_score: 0.5366
Model performance on Sad speech (in validation): 
	Precision: 0.5114, Recall: 0.9000, F1_score: 0.6522
New best accuracy for layer 5 on epoch 4: 0.6000. Model saved.
Epoch 5/100

Training Phase:
                       

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 192/1600 [00:10<01:13, 19.15it/s]
Training:  24%|██▍       | 389/1600 [00:20<01:02, 19.44it/s]
Training:  37%|███▋      | 587/1600 [00:30<00:51, 19.59it/s]
Training:  49%|████▉     | 785/1600 [00:40<00:41, 19.63it/s]
Training:  62%|██████▏   | 984/1600 [00:50<00:31, 19.70it/s]
Training:  74%|███████▍  | 1186/1600 [01:00<00:20, 19.84it/s]
Training:  87%|████████▋ | 1388/1600 [01:10<00:10, 19.92it/s]
Training:  99%|█████████▉| 1591/1600 [01:20<00:00, 20.00it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 194/16Training loss: 838.8157, Training accuracy: 0.7981
Macro F1-score: 0.7973
Model performance on Angry speech (in training): 
	Precision: 0.8522, Recall: 0.8650, F1_score: 0.8586
Model performance on Happy speech (in training): 
	Precision: 0.8056, Recall: 0.7250, F1_score: 0.7632
Model performance on Neutral speech (in training): 
	Precision: 0.7363, Recall: 0.7400, F1_score: 0.7382
Model performance on Sad speech (in training): 
	Precision: 0.7986, Recall: 0.8625, F1_score: 0.8293

Eval Phase: 
Validation loss: 222.9276, Validation accuracy: 0.6050
Macro F1-score: 0.5839
Model performance on Angry speech (in validation): 
	Precision: 0.6379, Recall: 0.7400, F1_score: 0.6852
Model performance on Happy speech (in validation): 
	Precision: 0.6296, Recall: 0.3400, F1_score: 0.4416
Model performance on Neutral speech (in validation): 
	Precision: 0.5789, Recall: 0.4400, F1_score: 0.5000
Model performance on Sad speech (in validation): 
	Precision: 0.5844, Recall: 0.9000, F1_score: 0.7087
New best accuracy for layer 5 on epoch 5: 0.6050. Model saved.
Epoch 6/100

Training Phase:
00 [00:10<01:12, 19.38it/s]
Training:  25%|██▍       | 393/1600 [00:20<01:01, 19.66it/s]
Training:  37%|███▋      | 592/1600 [00:30<00:51, 19.74it/s]
Training:  49%|████▉     | 791/1600 [00:40<00:40, 19.78it/s]
Training:  62%|██████▏   | 991/1600 [00:50<00:30, 19.83it/s]
Training:  74%|███████▍  | 1191/1600 [01:00<00:20, 19.79it/s]
Training:  87%|████████▋ | 1389/1600 [01:10<00:10, 19.72it/s]
Training: 100%|█████████▉| 1593/1600 [01:20<00:00, 19.92it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 198/1600 [00:10<01:11, 19.73it/s]
Training:  25%|██▍       | 396/1600 [00:20<01:01, 19.67it/s]
Training:  37%|███▋      | 594/1600 [00:30<00:51, 19.72it/s]
Training:  50%|████▉     | 794/1600 [00:40<0Training loss: 648.8212, Training accuracy: 0.8444
Macro F1-score: 0.8437
Model performance on Angry speech (in training): 
	Precision: 0.9030, Recall: 0.9075, F1_score: 0.9052
Model performance on Happy speech (in training): 
	Precision: 0.8470, Recall: 0.7750, F1_score: 0.8094
Model performance on Neutral speech (in training): 
	Precision: 0.8010, Recall: 0.7950, F1_score: 0.7980
Model performance on Sad speech (in training): 
	Precision: 0.8276, Recall: 0.9000, F1_score: 0.8623

Eval Phase: 
Validation loss: 213.5259, Validation accuracy: 0.6400
Macro F1-score: 0.6384
Model performance on Angry speech (in validation): 
	Precision: 0.7381, Recall: 0.6200, F1_score: 0.6739
Model performance on Happy speech (in validation): 
	Precision: 0.6250, Recall: 0.5000, F1_score: 0.5556
Model performance on Neutral speech (in validation): 
	Precision: 0.5556, Recall: 0.7000, F1_score: 0.6195
Model performance on Sad speech (in validation): 
	Precision: 0.6727, Recall: 0.7400, F1_score: 0.7048
New best accuracy for layer 5 on epoch 6: 0.6400. Model saved.
Epoch 7/100

Training Phase:
0:40, 19.80it/s]
Training:  62%|██████▏   | 998/1600 [00:50<00:30, 20.01it/s]
Training:  75%|███████▌  | 1202/1600 [01:00<00:19, 20.12it/s]
Training:  88%|████████▊ | 1406/1600 [01:10<00:09, 19.97it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.40it/s]
Training:  25%|██▍       | 394/1600 [00:20<01:01, 19.73it/s]
Training:  37%|███▋      | 594/1600 [00:30<00:50, 19.74it/s]
Training:  50%|████▉     | 793/1600 [00:40<00:40, 19.80it/s]
Training:  50%|████▉     | 793/1600 [00:50<00:40, 19.80it/s]
Training:  62%|██████▏   | 995/1600 [00:50<00:30, 19.93it/s]
Training:  75%|███████▍  | 1197/1600 [01:00<00:20, 19.99it/s]
Training:  87%|████████▋ | 1399/1600 [01:Training loss: 523.1310, Training accuracy: 0.8738
Macro F1-score: 0.8729
Model performance on Angry speech (in training): 
	Precision: 0.9293, Recall: 0.9525, F1_score: 0.9407
Model performance on Happy speech (in training): 
	Precision: 0.8672, Recall: 0.8000, F1_score: 0.8322
Model performance on Neutral speech (in training): 
	Precision: 0.8304, Recall: 0.8200, F1_score: 0.8252
Model performance on Sad speech (in training): 
	Precision: 0.8662, Recall: 0.9225, F1_score: 0.8935

Eval Phase: 
Validation loss: 235.2446, Validation accuracy: 0.6100
Macro F1-score: 0.6008
Model performance on Angry speech (in validation): 
	Precision: 0.6875, Recall: 0.6600, F1_score: 0.6735
Model performance on Happy speech (in validation): 
	Precision: 0.8571, Recall: 0.3600, F1_score: 0.5070
Model performance on Neutral speech (in validation): 
	Precision: 0.4754, Recall: 0.5800, F1_score: 0.5225
Model performance on Sad speech (in validation): 
	Precision: 0.6000, Recall: 0.8400, F1_score: 0.7000
Epoch 8/100

Training Phase:
Training loss: 410.0101, Training accuracy: 0.9094
Macro F1-score: 0.9094
Model performance on Angry speech (in training): 
	Precision: 0.9649, Recall: 0.9625, F1_score: 0.9637
Model performance on Happy speech (in training): 
	Precision: 0.9136, Recall: 0.8725, F1_score: 0.8926
Model performance on Neutral speech (in training): 
	Precision: 0.8847, Recall: 0.8825, F1_score: 0.8836
Model performance on Sad speech (in training): 
	Precision: 0.8762, Recall: 0.9200, F1_score: 0.8976

Eval Phase: 
10<00:10, 19.89it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 203/1600 [00:10<01:08, 20.26it/s]
Training:  25%|██▌       | 406/1600 [00:20<00:59, 20.14it/s]
Training:  25%|██▌       | 406/1600 [00:30<00:59, 20.14it/s]
Training:  38%|███▊      | 603/1600 [00:30<00:50, 19.79it/s]
Training:  50%|█████     | 807/1600 [00:40<00:39, 20.01it/s]
Training:  63%|██████▎   | 1011/1600 [00:50<00:29, 19.92it/s]
Training:  76%|███████▌  | 1209/1600 [01:00<00:19, 19.85it/s]
Training:  88%|████████▊ | 1407/1600 [01:10<00:09, 19.64it/s]
Training: 100%|██████████| 1600/1600 [01:21<00:00, 19.42it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
              Validation loss: 275.7425, Validation accuracy: 0.6350
Macro F1-score: 0.6270
Model performance on Angry speech (in validation): 
	Precision: 0.7778, Recall: 0.5600, F1_score: 0.6512
Model performance on Happy speech (in validation): 
	Precision: 0.5614, Recall: 0.6400, F1_score: 0.5981
Model performance on Neutral speech (in validation): 
	Precision: 0.7333, Recall: 0.4400, F1_score: 0.5500
Model performance on Sad speech (in validation): 
	Precision: 0.5844, Recall: 0.9000, F1_score: 0.7087
Epoch 9/100

Training Phase:
Training loss: 338.7506, Training accuracy: 0.9294
Macro F1-score: 0.9294
Model performance on Angry speech (in training): 
	Precision: 0.9673, Recall: 0.9600, F1_score: 0.9636
Model performance on Happy speech (in training): 
	Precision: 0.9186, Recall: 0.9025, F1_score: 0.9105
Model performance on Neutral speech (in training): 
	Precision: 0.9154, Recall: 0.9200, F1_score: 0.9177
Model performance on Sad speech (in training): 
	Precision: 0.9167, Recall: 0.9350, F1_score: 0.9257

Eval Phase: 
Validation loss: 291.5712, Validation accuracy: 0.6050
Macro F1-score: 0.5948
Model performance on Angry speech (in validation): 
	Precision: 0.6981, Recall: 0.7400, F1_score: 0.7184
Model performance on Happy speech (in validation): 
	Precision: 0.7895, Recall: 0.3000, F1_score: 0.4348
Model performance on Neutral speech (in validation): 
	Precision: 0.4474, Recall: 0.6800, F1_score: 0.5397
Model performance on Sad speech (in validation): 
	Precision: 0.6731, Recall: 0.7000, F1_score: 0.6863
Epoch 10/100

Training Phase:
                                     

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 196/1600 [00:10<01:12, 19.37it/s]
Training:  24%|██▍       | 390/1600 [00:20<01:02, 19.37it/s]
Training:  37%|███▋      | 585/1600 [00:30<00:52, 19.40it/s]
Training:  49%|████▉     | 784/1600 [00:40<00:41, 19.57it/s]
Training:  61%|██████▏   | 983/1600 [00:50<00:31, 19.40it/s]
Training:  74%|███████▎  | 1178/1600 [01:00<00:21, 19.41it/s]
Training:  86%|████████▌ | 1374/1600 [01:10<00:11, 19.45it/s]
Training:  98%|█████████▊| 1570/1600 [01:20<00:01, 19.45it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 193/1600 [00:10<01:13, 19.22it/s]
Training:  24%|██▍       | 389/1600 [00:20<01:02, 19.41itTraining loss: 263.9922, Training accuracy: 0.9394
Macro F1-score: 0.9393
Model performance on Angry speech (in training): 
	Precision: 0.9626, Recall: 0.9650, F1_score: 0.9638
Model performance on Happy speech (in training): 
	Precision: 0.9357, Recall: 0.9100, F1_score: 0.9227
Model performance on Neutral speech (in training): 
	Precision: 0.9202, Recall: 0.9225, F1_score: 0.9213
Model performance on Sad speech (in training): 
	Precision: 0.9389, Recall: 0.9600, F1_score: 0.9493

Eval Phase: 
Validation loss: 309.4777, Validation accuracy: 0.6200
Macro F1-score: 0.6110
Model performance on Angry speech (in validation): 
	Precision: 0.6863, Recall: 0.7000, F1_score: 0.6931
Model performance on Happy speech (in validation): 
	Precision: 0.8000, Recall: 0.4000, F1_score: 0.5333
Model performance on Neutral speech (in validation): 
	Precision: 0.5510, Recall: 0.5400, F1_score: 0.5455
Model performance on Sad speech (in validation): 
	Precision: 0.5600, Recall: 0.8400, F1_score: 0.6720
Epoch 11/100

Training Phase:
/s]
Training:  37%|███▋      | 587/1600 [00:30<00:51, 19.57it/s]
Training:  49%|████▉     | 788/1600 [00:40<00:41, 19.72it/s]
Training:  62%|██████▏   | 988/1600 [00:50<00:30, 19.74it/s]
Training:  74%|███████▍  | 1186/1600 [01:00<00:21, 19.39it/s]
Training:  86%|████████▌ | 1377/1600 [01:10<00:11, 19.27it/s]
Training:  98%|█████████▊| 1574/1600 [01:20<00:01, 19.38it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 189/1600 [00:10<01:14, 18.87it/s]
Training:  24%|██▍       | 380/1600 [00:20<01:04, 18.98it/s]
Training:  36%|███▌      | 575/1600 [00:30<00:53, 19.19it/s]
Training:  49%|████▊     | 778/1600 [00:40<00:41, 19.62it/s]
Training:  61%|██████▏   | 981/1600 [00:50<00:31, 19.71it/s]Training loss: 231.9642, Training accuracy: 0.9481
Macro F1-score: 0.9481
Model performance on Angry speech (in training): 
	Precision: 0.9825, Recall: 0.9800, F1_score: 0.9812
Model performance on Happy speech (in training): 
	Precision: 0.9466, Recall: 0.9300, F1_score: 0.9382
Model performance on Neutral speech (in training): 
	Precision: 0.9392, Recall: 0.9275, F1_score: 0.9333
Model performance on Sad speech (in training): 
	Precision: 0.9249, Recall: 0.9550, F1_score: 0.9397

Eval Phase: 
Validation loss: 348.6332, Validation accuracy: 0.5750
Macro F1-score: 0.5628
Model performance on Angry speech (in validation): 
	Precision: 0.7714, Recall: 0.5400, F1_score: 0.6353
Model performance on Happy speech (in validation): 
	Precision: 0.5926, Recall: 0.3200, F1_score: 0.4156
Model performance on Neutral speech (in validation): 
	Precision: 0.5472, Recall: 0.5800, F1_score: 0.5631
Model performance on Sad speech (in validation): 
	Precision: 0.5059, Recall: 0.8600, F1_score: 0.6370
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6400

Test Phase: 

Training:  74%|███████▍  | 1180/1600 [01:00<00:21, 19.59it/s]
Training:  86%|████████▌ | 1374/1600 [01:11<00:11, 19.24it/s]
Training:  98%|█████████▊| 1575/1600 [01:21<00:01, 19.50it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   4%|▍         | 8/200 [00:00<00:02, 75.01it/s]
Testing:   8%|▊         | 16/200 [00:00<00:02, 69.65it/s]
Testing:  12%|█▏        | 24/200 [00:00<00:02, 71.01it/s]
Testing:  16%|█▌        | 32/200 [00:00<00:02, 71.35it/s]
Testing:  20%|██        | 41/200 [00:00<00:02, 74.80it/s]
Testing:  25%|██▌       | 50/200 [00:00<00:01, 76.67it/s]
Testing:  29%|██▉       | 58/200 [00:00<00:01, 77.06it/s]
Testing:  33%|███▎      | 66/200 [00:00<00:01, 77.03it/s]
Testing:  37%|███▋      | 74/200 [00:00<00:01, 77.50it/s]
Testing:  41%|████      | 82/200 [00:01<00:01, 76.93it/s]
Testing:  46%|████▌     | 91/200 [00:01<00:01, 78.47it/s]
Testing:  50%|█████     | 100/200 [00:01<00:01, 78.31it/s]
Testing:  54%|█████▍    | 108/200 [00:01<00:01, 78.55it/s]
Testing:  58%|█████▊    | 116/200 [00:01<00:01, 78.64it/s]
Testing:  62%|██████▏   | 124/200 [00:01<00:00, 78.77it/s]
Testing:  66%|██████▌   | 132/200 [00:01<00:00, 77.19it/s]
Testing:  70%|███████   | 140/200 [00:01<00:00, 77.89it/s]
Testing:  74%|███████▍  | 148/200 [00:01<00:00, 77.64it/s]
Testing:  78%|███████▊  | 156/200 [00:02<00:00, 78.19it/s]
Testing:  82%|████████▎ | 165/200 [00:02<00:00, 78.95it/s]
Testing:  87%|████████▋ | 174/200 [00:02<00:00, 79.39it/s]
Testing:  91%|█████████ | 182/200 [00:02<00:00, 78.79it/s]
Testing:  95%|█████████▌| 190/200 [00:02<00:00, 79.13it/s]
TTest loss: 216.7325, Test accuracy: 0.6450
Macro F1-score: 0.6441
Model performance on Angry speech (in test): 
	Precision: 0.6809, Recall: 0.6400, F1_score: 0.6598
Model performance on Happy speech (in test): 
	Precision: 0.6585, Recall: 0.5400, F1_score: 0.5934
Model performance on Neutral speech (in test): 
	Precision: 0.5672, Recall: 0.7600, F1_score: 0.6496
Model performance on Sad speech (in test): 
	Precision: 0.7111, Recall: 0.6400, F1_score: 0.6737

======================= This is fold_3 on de =======================

Load dataset: 
Loading en train data: fold_3...
Preprocess en fold_3 data for de model
esting:  99%|█████████▉| 198/200 [00:02<00:00, 79.32it/s]
                                                          

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:   0%|          | 4/1600 [00:00<00:47, 33.71 examples/s]
Map:   1%|▏         | 20/1600 [00:00<00:16, 97.47 examples/s]
Map:   2%|▏         | 31/1600 [00:00<00:18, 83.76 examples/s]
Map:   3%|▎         | 45/1600 [00:00<00:16, 96.66 examples/s]
Map:   3%|▎         | 55/1600 [00:00<00:17, 89.71 examples/s]
Map:   4%|▍         | 65/1600 [00:00<00:17, 88.76 examples/s]
Map:   5%|▍         | 77/1600 [00:00<00:16, 94.74 examples/s]
Map:   6%|▌         | 88/1600 [00:00<00:16, 91.52 examples/s]
Map:   6%|▌         | 98/1600 [00:01<00:16, 88.65 examples/s]
Map:   7%|▋         | 110/1600 [00:01<00:15, 93.98 examples/s]
Map:   8%|▊         | 124/1600 [00:01<00:14, 104.67 examples/s]
Map:   9%|▉         | 140/1600 [00:01<00:14, 103.20 examples/s]
Map:   9%|▉         | 151/1600 [00:01<00:17, 81.61 examples/s] 
Map:  10%|█         | 166/1600 [00:01<00:15, 92.69 examples/s]
Map:  11%|█         | 178/1600 [00:01<00:14, 98.66 examples/s]
Map:  12%|█▏        | 194/1600 [00:02<00:12, 112.59 examples/s]
Map:  13%|█▎        | 210/1600 [00:02<00:11, 118.64 examples/s]
Map:  14%|█▍        | 227/1600 [00:02<00:11, 114.67 examples/s]
Map:  15%|█▌        | 240/1600 [00:02<00:11, 117.45 examples/s]
Map:  16%|█▌        | 256/1600 [00:02<00:10, 124.87 examples/s]
Map:  17%|█▋        | 272/1600 [00:02<00:12, 107.13 examples/s]
Map:  18%|█▊        | 287/1600 [00:02<00:11, 113.20 examples/s]
Map:  19%|█▉        | 300/1600 [00:02<00:11, 113.25 examples/s]
Map:  20%|█▉        | 318/1600 [00:03<00:10, 127.62 examples/s]
Map:  21%|██        | 337/1600 [00:03<00:10, 125.69 examples/s]
Map:  22%|██▏       | 351/1600 [00:03<00:09, 128.15 examples/s]
Map:  23%|██▎       | 368/1600 [00:03<00:11, 106.22 examples/s]
Map:  24%|██▍       | 385/1600 [00:03<00:10, 118.64 examples/s]
Map:  25%|██▌       | 401/1600 [00:03<00:10, 110.27 examples/s]
Map:  26%|██▌       | 414/1600 [00:03<00:10, 112.91 examples/s]
Map:  27%|██▋       | 431/1600 [00:04<00:10, 109.95 examples/s]
Map:  28%|██▊       | 447/1600 [00:04<00:09, 120.91 examples/s]
Map:  29%|██▉       | 465/1600 [00:04<00:08, 133.74 examples/s]
Map:  30%|███       | 488/1600 [00:04<00:09, 122.76 examples/s]
Map:  32%|███▏      | 505/1600 [00:04<00:08, 131.48 examples/s]
Map:  33%|███▎      | 522/1600 [00:04<00:09, 109.90 examples/s]
Map:  34%|███▎      | 536/1600 [00:04<00:10, 103.32 examples/s]
Map:  34%|███▍      | 551/1600 [00:05<00:09, 110.82 examples/s]
Map:  36%|███▌      | 568/1600 [00:05<00:08, 120.32 examples/s]
Map:  37%|███▋      | 586/1600 [00:05<00:08, 119.51 examples/s]
Map:  38%|███▊      | 600/1600 [00:05<00:08, 122.44 examples/s]
Map:  38%|███▊      | 614/1600 [00:05<00:07, 123.71 examples/s]
Map:  39%|███▉      | 629/1600 [00:05<00:07, 126.40 examples/s]
Map:  40%|████      | 642/1600 [00:05<00:07, 125.73 examples/s]
Map:  41%|████      | 655/1600 [00:05<00:07, 124.90 examples/s]
Map:  42%|████▏     | 671/1600 [00:06<00:15, 59.29 examples/s] 
Map:  43%|████▎     | 682/1600 [00:06<00:14, 65.45 examples/s]
Map:  43%|████▎     | 692/1600 [00:06<00:13, 69.35 examples/s]
Map:  44%|████▍     | 704/1600 [00:06<00:11, 76.77 examples/s]
Map:  45%|████▌     | 723/1600 [00:06<00:08, 99.89 examples/s]
Map:  46%|████▌     | 736/1600 [00:07<00:08, 105.83 examples/s]
Map:  47%|████▋     | 753/1600 [00:07<00:07, 106.18 examples/s]
Map:  48%|████▊     | 768/1600 [00:07<00:08, 92.71 examples/s] 
Map:  49%|████▉     | 781/1600 [00:07<00:09, 88.68 examples/s]
Map:  49%|████▉     | 791/1600 [00:07<00:09, 88.17 examples/s]
Map:  50%|█████     | 802/1600 [00:07<00:09, 88.42 examples/s]
Map:  51%|█████     | 817/1600 [00:07<00:07, 98.20 examples/s]
Map:  52%|█████▏    | 829/1600 [00:08<00:07, 101.89 examples/s]
Map:  53%|█████▎    | 842/1600 [00:08<00:07, 107.62 examples/s]
Map:  54%|█████▎    | 856/1600 [00:08<00:06, 115.31 examples/s]
Map:  55%|█████▍    | 874/1600 [00:08<00:06, 107.34 examples/s]
Map:  55%|█████▌    | 887/1600 [00:08<00:06, 112.21 examples/s]
Map:  56%|█████▋    | 902/1600 [00:08<00:05, 118.14 examples/s]
Map:  57%|█████▋    | 915/1600 [00:08<00:05, 116.49 examples/s]
Map:  58%|█████▊    | 930/1600 [00:08<00:05, 122.67 examples/s]
Map:  59%|█████▉    | 944/1600 [00:08<00:05, 122.04 examples/s]
Map:  60%|█████▉    | 959/1600 [00:09<00:05, 121.87 examples/s]
Map:  61%|██████    | 974/1600 [00:09<00:05, 104.62 examples/s]
Map:  62%|██████▏   | 985/1600 [00:09<00:05, 103.78 examples/s]
Map:  62%|██████▏   | 998/1600 [00:09<00:05, 108.92 examples/s]
Map:  62%|██████▏   | 998/1600 [00:26<00:05, 108.92 examples/s]
Map:  62%|██████▎   | 1000/1600 [01:03<16:14,  1.62s/ examples]
Map:  63%|██████▎   | 1013/1600 [01:03<10:13,  1.04s/ examples]
Map:  64%|██████▍   | 1029/1600 [01:03<06:07,  1.56 examples/s]
Map:  65%|██████▌   | 1045/1600 [01:03<03:51,  2.40 examples/s]
Map:  66%|██████▋   | 1062/1600 [01:03<02:27,  3.65 examples/s]
Map:  67%|██████▋   | 1079/1600 [01:03<01:36,  5.38 examples/s]
Map:  68%|██████▊   | 1092/1600 [01:03<01:10,  7.23 examples/s]
Map:  69%|██████▉   | 1109/1600 [01:04<00:46, 10.49 examples/s]
Map:  70%|███████   | 1122/1600 [01:04<00:34, 13.82 examples/s]
Map:  71%|███████   | 1135/1600 [01:04<00:26, 17.88 examples/s]
Map:  72%|███████▏  | 1152/1600 [01:04<00:18, 24.22 examples/s]
Map:  73%|███████▎  | 1167/1600 [01:04<00:13, 32.18 examples/s]
Map:  74%|███████▎  | 1179/1600 [01:04<00:10, 39.43 examples/s]
Map:  75%|███████▍  | 1194/1600 [01:04<00:07, 50.82 examples/s]
Map:  76%|███████▌  | 1208/1600 [01:04<00:06, 61.08 examples/s]
Map:  76%|███████▋  | 1223/1600 [01:05<00:05, 68.00 examples/s]
Map:  77%|███████▋  | 1238/1600 [01:05<00:04, 74.42 examples/s]
Map:  78%|███████▊  | 1253/1600 [01:05<00:04, 72.80 examples/s]
Map:  79%|███████▉  | 1267/1600 [01:05<00:04, 77.32 examples/s]
Map:  80%|████████  | 1281/1600 [01:05<00:04, 79.49 examples/s]
Map:  81%|████████  | 1293/1600 [01:05<00:03, 77.79 examples/s]
Map:  82%|████████▏ | 1305/1600 [01:06<00:03, 84.21 examples/s]
Map:  82%|████████▏ | 1318/1600 [01:06<00:03, 92.41 examples/s]
Map:  84%|████████▎ | 1336/1600 [01:06<00:02, 109.09 examples/s]
Map:  85%|████████▍ | 1353/1600 [01:06<00:02, 120.98 examples/s]
Map:  86%|████████▌ | 1368/1600 [01:06<00:02, 106.25 examples/s]
Map:  87%|████████▋ | 1386/1600 [01:06<00:02, 105.58 examples/s]
Map:  88%|████████▊ | 1400/1600 [01:06<00:01, 111.77 examples/s]
Map:  88%|████████▊ | 1416/1600 [01:07<00:01, 121.38 examples/s]
Map:  90%|████████▉ | 1433/1600 [01:07<00:01, 132.89 examples/s]
Map:  91%|█████████ | 1449/1600 [01:07<00:01, 138.88 examples/s]
Map:  92%|█████████▏| 1465/1600 [01:07<00:00, 141.63 examples/s]
Map:  93%|█████████▎| 1481/1600 [01:07<00:00, 121.78 examples/s]
Map:  93%|█████████▎| 1494/1600 [01:07<00:00, 120.97 examples/s]
Map:  94%|█████████▍| 1511/1600 [01:07<00:00, 115.30 examples/s]
Map:  95%|█████████▌| 1524/1600 [01:07<00:00, 117.44 examples/s]
Map:  96%|█████████▌| 1538/1600 [01:07<00:00, 121.98 examples/s]
Map:  97%|█████████▋| 1553/1600 [01:08<00:00, 123.99 examples/s]
Map:  98%|█████████▊| 1574/1600 [01:08<00:00, 142.61 examples/s]
Map: 100%|█████████▉| 1596/1600 [01:08<00:00, 134.00 examples/s]
Map: 100%|█████████▉| 1596/1600 [01:19<00:00, 134.00 examples/s]
Map: 100%|██████████| 1600/1600 [01:39<00:00,  1.47 examples/s] 
Map: 100%|██████████| 1600/1600 [01:39<00:00, 16.04 examples/s]

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   3%|▎         | 6/200 [00:00<00:04, 48.19 examples/s]
Map:   8%|▊         | 16/200 [00:00<00:03, 57.10 examples/s]
Map:  12%|█▏        | 24/200 [00:00<00:02, 63.80 examples/s]
Map:  20%|██        | 40/200 [00:00<00:01, 92.46 examples/s]
Map:  28%|██▊       | 55/200 [00:00<00:01, 104.03 examples/s]
Map:  34%|███▍      | 69/200 [00:00<00:01, 109.48 examples/s]
Map:  42%|████▏     | 83/200 [00:00<00:01, 112.46 examples/s]
Map:  50%|█████     | 101/200 [00:01<00:01, 95.26 examples/s]
Map:  56%|█████▋    | 113/200 [00:01<00:00, 90.44 examples/s]
Map:  64%|██████▎   | 127/200 [00:01<00:00, 99.80 examples/s]
Map:  70%|██████▉   | 139/200 [00:01<00:00, 101.79 examples/s]
Map:  76%|███████▋  | 153/200 [00:01<00:00, 94.85 examples/s] 
Map:  83%|████████▎ | 166/200 [00:01<00:00, 101.01 examples/s]
Map:  92%|█████████▏| 183/200 [00:01<00:00, 99.46 examples/s] 
Map:  98%|█████████▊| 197/200 [00:02<00:00, 104.22 examples/s]
Map: 100%|██████████| 200/200 [00:13<00:00, 14.45 examples/s] 

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▏         | 3/200 [00:00<00:07, 26.02 examples/s]
Map:   4%|▎         | 7/200 [00:00<00:07, 24.24 examples/s]
Map:  10%|▉         | 19/200 [00:00<00:03, 58.38 examples/s]
Map:  16%|█▌        | 32/200 [00:00<00:02, 76.83 examples/s]
Map:  22%|██▎       | 45/200 [00:00<00:01, 90.16 examples/s]
Map:  28%|██▊       | 57/200 [00:00<00:01, 98.67 examples/s]
Map:  36%|███▋      | 73/200 [00:00<00:01, 115.81 examples/s]
Map:  46%|████▌     | 91/200 [00:01<00:00, 110.29 examples/s]
Map:  52%|█████▎    | 105/200 [00:01<00:01, 94.95 examples/s]
Map:  60%|██████    | 121/200 [00:01<00:00, 107.83 examples/s]
Map:  68%|██████▊   | 135/200 [00:01<00:00, 101.74 examples/s]
Map:  74%|███████▍  | 149/200 [00:01<00:00, 102.90 examples/s]
Map:  81%|████████  | 162/200 [00:01<00:00, 106.15 examples/s]
Map:  88%|████████▊ | 177/200 [00:01<00:00, 112.89 examples/s]
Map:  95%|█████████▌| 190/200 [00:01<00:00, 116.71 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 15.53 examples/s] 
Loading en eval data: fold_3...
Preprocess en fold_3 data for de model
Loading en test data: fold_3...
Preprocess en fold_3 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1519.0929, Training accuracy: 0.6094
Macro F1-score: 0.6041
Model performance on Angry speech (in training): 
	Precision: 0.6690, Recall: 0.7225, F1_score: 0.6947
Model performance on Happy speech (in training): 
	Precision: 0.5282, Recall: 0.4450, F1_score: 0.4830
Model performance on Neutral speech (in training): 
	Precision: 0.5351, Recall: 0.5150, F1_score: 0.5248
Model performance on Sad speech (in training): 
	Precision: 0.6771, Recall: 0.7550, F1_score: 0.7139

Eval Phase: 
Validation loss: 165.2569, Validation accuracy: 0.6700
Macro F1-score: 0.6574
Model performance on Angry speech (in validation): 
	Precision: 0.8222, Recall: 0.7400, F1_score: 0.7789
Model performance on Happy speech (in validation): 
	Precision: 0.6905, Recall: 0.5800, F1_score: 0.6304
Model performance on Neutral speech (in validation): 
	Precision: 0.6061, Recall: 0.4000, F1_score: 0.4819
Model performance on Sad speech (in validation): 
	Precision: 0.6000, Recall: 0.9600, F1_score: 0.7385
New best accuracy for layer 5 on epoch 1: 0.6700. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 221/1600 [00:10<01:02, 21.96it/s]
Training:  28%|██▊       | 441/1600 [00:20<00:54, 21.22it/s]
Training:  41%|████      | 650/1600 [00:30<00:45, 21.07it/s]
Training:  54%|█████▎    | 859/1600 [00:40<00:35, 20.80it/s]
Training:  67%|██████▋   | 1071/1600 [00:50<00:25, 20.92it/s]
Training:  80%|████████  | 1285/1600 [01:01<00:14, 21.05it/s]
Training:  94%|█████████▍| 1502/1600 [01:11<00:04, 21.21it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▎        | 218/1600 [00:10<01:03, 21.71it/s]
Training:  27%|██▋       | 436/1600 [00:20<00:54, 21.35it/s]
Training:  40%|████      | 648/1600 [00:30<00:44, 21.21it/s]
Training:  54%|█████▍    | 862Training loss: 1298.6206, Training accuracy: 0.6700
Macro F1-score: 0.6667
Model performance on Angry speech (in training): 
	Precision: 0.7240, Recall: 0.7475, F1_score: 0.7355
Model performance on Happy speech (in training): 
	Precision: 0.6302, Recall: 0.5325, F1_score: 0.5772
Model performance on Neutral speech (in training): 
	Precision: 0.5960, Recall: 0.5975, F1_score: 0.5968
Model performance on Sad speech (in training): 
	Precision: 0.7165, Recall: 0.8025, F1_score: 0.7571

Eval Phase: 
Validation loss: 180.5126, Validation accuracy: 0.6050
Macro F1-score: 0.5820
Model performance on Angry speech (in validation): 
	Precision: 0.6981, Recall: 0.7400, F1_score: 0.7184
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.2800, F1_score: 0.3944
Model performance on Neutral speech (in validation): 
	Precision: 0.4902, Recall: 0.5000, F1_score: 0.4950
Model performance on Sad speech (in validation): 
	Precision: 0.6000, Recall: 0.9000, F1_score: 0.7200
Epoch 3/100

Training Phase:
/1600 [00:40<00:34, 21.27it/s]
Training:  54%|█████▍    | 862/1600 [00:51<00:34, 21.27it/s]
Training:  67%|██████▋   | 1074/1600 [00:51<00:25, 20.70it/s]
Training:  80%|███████▉  | 1272/1600 [01:01<00:16, 20.15it/s]
Training:  92%|█████████▏| 1468/1600 [01:11<00:06, 19.95it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 203/1600 [00:10<01:09, 20.18it/s]
Training:  13%|█▎        | 203/1600 [00:20<01:09, 20.18it/s]
Training:  25%|██▌       | 403/1600 [00:20<00:59, 19.99it/s]
Training:  38%|███▊      | 602/1600 [00:30<00:50, 19.68it/s]
Training:  50%|█████     | 800/1600 [00:40<00:40, 19.70it/s]
Training:  62%|██████▏   | 998/1600 [00:50<00:30, 19.46it/s]
Training:  75%|███████▍  | 1199/1600 Training loss: 1147.2674, Training accuracy: 0.7144
Macro F1-score: 0.7125
Model performance on Angry speech (in training): 
	Precision: 0.7781, Recall: 0.7800, F1_score: 0.7790
Model performance on Happy speech (in training): 
	Precision: 0.7020, Recall: 0.6125, F1_score: 0.6542
Model performance on Neutral speech (in training): 
	Precision: 0.6482, Recall: 0.6450, F1_score: 0.6466
Model performance on Sad speech (in training): 
	Precision: 0.7257, Recall: 0.8200, F1_score: 0.7700

Eval Phase: 
Validation loss: 155.5504, Validation accuracy: 0.6750
Macro F1-score: 0.6704
Model performance on Angry speech (in validation): 
	Precision: 0.8409, Recall: 0.7400, F1_score: 0.7872
Model performance on Happy speech (in validation): 
	Precision: 0.7429, Recall: 0.5200, F1_score: 0.6118
Model performance on Neutral speech (in validation): 
	Precision: 0.5625, Recall: 0.5400, F1_score: 0.5510
Model performance on Sad speech (in validation): 
	Precision: 0.6164, Recall: 0.9000, F1_score: 0.7317
New best accuracy for layer 5 on epoch 3: 0.6750. Model saved.
Epoch 4/100

Training Phase:
[01:00<00:20, 19.65it/s]
Training:  88%|████████▊ | 1400/1600 [01:11<00:10, 19.65it/s]
Training: 100%|█████████▉| 1597/1600 [01:21<00:00, 19.54it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 196/1600 [00:10<01:11, 19.56it/s]
Training:  25%|██▍       | 393/1600 [00:20<01:01, 19.56it/s]
Training:  37%|███▋      | 595/1600 [00:30<00:50, 19.82it/s]
Training:  50%|████▉     | 798/1600 [00:40<00:40, 19.96it/s]
Training:  62%|██████▎   | 1000/1600 [00:50<00:30, 19.86it/s]
Training:  75%|███████▍  | 1197/1600 [01:00<00:20, 19.63it/s]
Training:  87%|████████▋ | 1390/1600 [01:10<00:10, 19.44it/s]
Training:  99%|█████████▉| 1590/1600 [01:20<00:00, 19.60it/s]
                              Training loss: 943.1843, Training accuracy: 0.7719
Macro F1-score: 0.7703
Model performance on Angry speech (in training): 
	Precision: 0.8272, Recall: 0.8375, F1_score: 0.8323
Model performance on Happy speech (in training): 
	Precision: 0.7593, Recall: 0.6625, F1_score: 0.7076
Model performance on Neutral speech (in training): 
	Precision: 0.7252, Recall: 0.7325, F1_score: 0.7289
Model performance on Sad speech (in training): 
	Precision: 0.7738, Recall: 0.8550, F1_score: 0.8124

Eval Phase: 
Validation loss: 180.0777, Validation accuracy: 0.6450
Macro F1-score: 0.6359
Model performance on Angry speech (in validation): 
	Precision: 0.6301, Recall: 0.9200, F1_score: 0.7480
Model performance on Happy speech (in validation): 
	Precision: 0.6765, Recall: 0.4600, F1_score: 0.5476
Model performance on Neutral speech (in validation): 
	Precision: 0.5686, Recall: 0.5800, F1_score: 0.5743
Model performance on Sad speech (in validation): 
	Precision: 0.7381, Recall: 0.6200, F1_score: 0.6739
Epoch 5/100

Training Phase:
Training loss: 760.1600, Training accuracy: 0.8213
Macro F1-score: 0.8202
Model performance on Angry speech (in training): 
	Precision: 0.8759, Recall: 0.8825, F1_score: 0.8792
Model performance on Happy speech (in training): 
	Precision: 0.8305, Recall: 0.7225, F1_score: 0.7727
Model performance on Neutral speech (in training): 
	Precision: 0.7857, Recall: 0.7975, F1_score: 0.7916
Model performance on Sad speech (in training): 
	Precision: 0.7968, Recall: 0.8825, F1_score: 0.8375

Eval Phase: 
Validation loss: 191.7949, Validation accuracy: 0.6200
Macro F1-score: 0.5988
Model performance on Angry speech (in validation): 
	Precision: 0.6615, Recall: 0.8600, F1_score: 0.7478
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.3000, F1_score: 0.4286
Model performance on Neutral speech (in validation): 
	Precision: 0.5294, Recall: 0.5400, F1_score: 0.5347
Model performance on Sad speech (in validation): 
	Precision: 0.6094, Recall: 0.7800, F1_score: 0.6842
Epoch 6/100

Training Phase:
                               

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.91it/s]
Training:  25%|██▌       | 400/1600 [00:20<01:01, 19.61it/s]
Training:  38%|███▊      | 600/1600 [00:30<00:50, 19.76it/s]
Training:  50%|█████     | 800/1600 [00:40<00:40, 19.72it/s]
Training:  62%|██████▏   | 999/1600 [00:50<00:30, 19.77it/s]
Training:  75%|███████▍  | 1198/1600 [01:00<00:20, 19.72it/s]
Training:  87%|████████▋ | 1399/1600 [01:10<00:10, 19.82it/s]
Training: 100%|██████████| 1600/1600 [01:21<00:00, 19.51it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        Training loss: 576.0606, Training accuracy: 0.8612
Macro F1-score: 0.8609
Model performance on Angry speech (in training): 
	Precision: 0.9209, Recall: 0.9025, F1_score: 0.9116
Model performance on Happy speech (in training): 
	Precision: 0.8650, Recall: 0.7850, F1_score: 0.8231
Model performance on Neutral speech (in training): 
	Precision: 0.8301, Recall: 0.8550, F1_score: 0.8424
Model performance on Sad speech (in training): 
	Precision: 0.8337, Recall: 0.9025, F1_score: 0.8667

Eval Phase: 
Validation loss: 164.2170, Validation accuracy: 0.7150
Macro F1-score: 0.7067
Model performance on Angry speech (in validation): 
	Precision: 0.8200, Recall: 0.8200, F1_score: 0.8200
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.5400, F1_score: 0.6279
Model performance on Neutral speech (in validation): 
	Precision: 0.6364, Recall: 0.5600, F1_score: 0.5957
Model performance on Sad speech (in validation): 
	Precision: 0.6714, Recall: 0.9400, F1_score: 0.7833
New best accuracy for layer 5 on epoch 6: 0.7150. Model saved.
Epoch 7/100

Training Phase:
| 204/1600 [00:10<01:08, 20.39it/s]
Training:  26%|██▌       | 408/1600 [00:20<01:00, 19.72it/s]
Training:  38%|███▊      | 608/1600 [00:30<00:50, 19.81it/s]
Training:  50%|█████     | 808/1600 [00:41<00:40, 19.48it/s]
Training:  63%|██████▎   | 1005/1600 [00:51<00:30, 19.54it/s]
Training:  76%|███████▌  | 1210/1600 [01:01<00:19, 19.84it/s]
Training:  88%|████████▊ | 1415/1600 [01:11<00:09, 19.63it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▎        | 200/1600 [00:10<01:10, 19.88it/s]
Training:  25%|██▍       | 399/1600 [00:20<01:01, 19.40it/s]
Training:  38%|███▊      | 600/1600 [00:30<00:50, 19.69it/s]
Training:  50%|█████     | 802/1600 [00:40<00:40, 19.86it/s]
Training:  63%|██████▎   | 1004/1600 [00:5Training loss: 474.1083, Training accuracy: 0.8906
Macro F1-score: 0.8906
Model performance on Angry speech (in training): 
	Precision: 0.9323, Recall: 0.9300, F1_score: 0.9312
Model performance on Happy speech (in training): 
	Precision: 0.8834, Recall: 0.8525, F1_score: 0.8677
Model performance on Neutral speech (in training): 
	Precision: 0.8906, Recall: 0.8750, F1_score: 0.8827
Model performance on Sad speech (in training): 
	Precision: 0.8578, Recall: 0.9050, F1_score: 0.8808

Eval Phase: 
Validation loss: 208.4017, Validation accuracy: 0.6350
Macro F1-score: 0.6274
Model performance on Angry speech (in validation): 
	Precision: 0.8537, Recall: 0.7000, F1_score: 0.7692
Model performance on Happy speech (in validation): 
	Precision: 0.7097, Recall: 0.4400, F1_score: 0.5432
Model performance on Neutral speech (in validation): 
	Precision: 0.5455, Recall: 0.4800, F1_score: 0.5106
Model performance on Sad speech (in validation): 
	Precision: 0.5476, Recall: 0.9200, F1_score: 0.6866
Epoch 8/100

Training Phase:
0<00:30, 19.81it/s]
Training:  75%|███████▌  | 1206/1600 [01:00<00:19, 19.91it/s]
Training:  88%|████████▊ | 1407/1600 [01:11<00:09, 19.64it/s]
Training: 100%|██████████| 1600/1600 [01:21<00:00, 19.53it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 192/1600 [00:10<01:13, 19.16it/s]
Training:  24%|██▍       | 391/1600 [00:20<01:01, 19.59it/s]
Training:  37%|███▋      | 592/1600 [00:30<00:50, 19.79it/s]
Training:  50%|████▉     | 793/1600 [00:40<00:40, 19.75it/s]
Training:  62%|██████▏   | 990/1600 [00:50<00:30, 19.73it/s]
Training:  74%|███████▍  | 1187/1600 [01:00<00:21, 19.59it/s]
Training:  87%|████████▋ | 1386/1600 [01:10<00:10, 19.69it/s]
Training:  99%|████████�Training loss: 394.9296, Training accuracy: 0.9175
Macro F1-score: 0.9174
Model performance on Angry speech (in training): 
	Precision: 0.9571, Recall: 0.9475, F1_score: 0.9523
Model performance on Happy speech (in training): 
	Precision: 0.9206, Recall: 0.8700, F1_score: 0.8946
Model performance on Neutral speech (in training): 
	Precision: 0.8988, Recall: 0.9100, F1_score: 0.9043
Model performance on Sad speech (in training): 
	Precision: 0.8955, Recall: 0.9425, F1_score: 0.9184

Eval Phase: 
Validation loss: 214.1582, Validation accuracy: 0.6650
Macro F1-score: 0.6535
Model performance on Angry speech (in validation): 
	Precision: 0.7917, Recall: 0.7600, F1_score: 0.7755
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.4800, F1_score: 0.5581
Model performance on Neutral speech (in validation): 
	Precision: 0.5581, Recall: 0.4800, F1_score: 0.5161
Model performance on Sad speech (in validation): 
	Precision: 0.6438, Recall: 0.9400, F1_score: 0.7642
Epoch 9/100

Training Phase:
Training loss: 269.2247, Training accuracy: 0.9344
Macro F1-score: 0.9344
Model performance on Angry speech (in training): 
	Precision: 0.9476, Recall: 0.9500, F1_score: 0.9488
Model performance on Happy speech (in training): 
	Precision: 0.9514, Recall: 0.9300, F1_score: 0.9406
Model performance on Neutral speech (in training): 
	Precision: 0.9293, Recall: 0.9200, F1_score: 0.9246
Model performance on Sad speech (in training): 
	Precision: 0.9102, Recall: 0.9375, F1_score: 0.9236

Eval Phase: 
Validation loss: 241.8809, Validation accuracy: 0.6450
Macro F1-score: 0.6382
Model performance on Angry speech (in validation): 
	Precision: 0.6833, Recall: 0.8200, F1_score: 0.7455
Model performance on Happy speech (in validation): 
	Precision: 0.5581, Recall: 0.4800, F1_score: 0.5161
Model performance on Neutral speech (in validation): 
	Precision: 0.5556, Recall: 0.5000, F1_score: 0.5263
Model performance on Sad speech (in validation): 
	Precision: 0.7500, Recall: 0.7800, F1_score: 0.7647
Epoch 10/100

Training Phase:
��▉| 1585/1600 [01:20<00:00, 19.62it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 199/1600 [00:10<01:10, 19.81it/s]
Training:  25%|██▍       | 398/1600 [00:20<01:01, 19.59it/s]
Training:  37%|███▋      | 593/1600 [00:30<00:51, 19.51it/s]
Training:  49%|████▉     | 788/1600 [00:40<00:41, 19.42it/s]
Training:  61%|██████▏   | 981/1600 [00:50<00:31, 19.37it/s]
Training:  74%|███████▍  | 1180/1600 [01:00<00:21, 19.54it/s]
Training:  86%|████████▋ | 1382/1600 [01:10<00:11, 19.73it/s]
Training:  99%|█████████▉| 1584/1600 [01:20<00:00, 19.87it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

TrainingTraining loss: 224.9912, Training accuracy: 0.9544
Macro F1-score: 0.9544
Model performance on Angry speech (in training): 
	Precision: 0.9726, Recall: 0.9750, F1_score: 0.9738
Model performance on Happy speech (in training): 
	Precision: 0.9688, Recall: 0.9300, F1_score: 0.9490
Model performance on Neutral speech (in training): 
	Precision: 0.9457, Recall: 0.9575, F1_score: 0.9516
Model performance on Sad speech (in training): 
	Precision: 0.9317, Recall: 0.9550, F1_score: 0.9432

Eval Phase: 
Validation loss: 288.3554, Validation accuracy: 0.6200
Macro F1-score: 0.6044
Model performance on Angry speech (in validation): 
	Precision: 0.7609, Recall: 0.7000, F1_score: 0.7292
Model performance on Happy speech (in validation): 
	Precision: 0.5882, Recall: 0.4000, F1_score: 0.4762
Model performance on Neutral speech (in validation): 
	Precision: 0.5366, Recall: 0.4400, F1_score: 0.4835
Model performance on Sad speech (in validation): 
	Precision: 0.5949, Recall: 0.9400, F1_score: 0.7287
Epoch 11/100

Training Phase:
:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.42it/s]
Training:  25%|██▍       | 395/1600 [00:20<01:01, 19.73it/s]
Training:  37%|███▋      | 597/1600 [00:30<00:50, 19.90it/s]
Training:  50%|████▉     | 799/1600 [00:40<00:40, 19.69it/s]
Training:  62%|██████▏   | 999/1600 [00:50<00:30, 19.77it/s]
Training:  62%|██████▏   | 999/1600 [01:00<00:30, 19.77it/s]
Training:  75%|███████▍  | 1199/1600 [01:00<00:20, 19.59it/s]
Training:  87%|████████▋ | 1392/1600 [01:11<00:10, 19.40it/s]
Training:  99%|█████████▉| 1583/1600 [01:21<00:00, 19.28it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 195/1600 [00:10<01:12, 19.45it/s]
Training:  24%|██▍       | 39Training loss: 228.3630, Training accuracy: 0.9487
Macro F1-score: 0.9487
Model performance on Angry speech (in training): 
	Precision: 0.9727, Recall: 0.9800, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9464, Recall: 0.9275, F1_score: 0.9369
Model performance on Neutral speech (in training): 
	Precision: 0.9446, Recall: 0.9375, F1_score: 0.9410
Model performance on Sad speech (in training): 
	Precision: 0.9314, Recall: 0.9500, F1_score: 0.9406

Eval Phase: 
Validation loss: 236.2719, Validation accuracy: 0.6250
Macro F1-score: 0.6285
Model performance on Angry speech (in validation): 
	Precision: 0.8158, Recall: 0.6200, F1_score: 0.7045
Model performance on Happy speech (in validation): 
	Precision: 0.5600, Recall: 0.5600, F1_score: 0.5600
Model performance on Neutral speech (in validation): 
	Precision: 0.5085, Recall: 0.6000, F1_score: 0.5505
Model performance on Sad speech (in validation): 
	Precision: 0.6792, Recall: 0.7200, F1_score: 0.6990
Epoch 12/100

Training Phase:
0/1600 [00:21<01:05, 18.36it/s]
Training:  35%|███▌      | 567/1600 [00:31<00:57, 17.99it/s]
Training:  48%|████▊     | 767/1600 [00:41<00:44, 18.73it/s]
Training:  60%|██████    | 967/1600 [00:51<00:33, 19.09it/s]
Training:  73%|███████▎  | 1165/1600 [01:01<00:22, 19.14it/s]
Training:  85%|████████▌ | 1367/1600 [01:11<00:11, 19.47it/s]
Training:  99%|█████████▊| 1579/1600 [01:21<00:01, 19.99it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 208/1600 [00:10<01:07, 20.75it/s]
Training:  26%|██▌       | 417/1600 [00:20<00:56, 20.80it/s]
Training:  39%|███▉      | 626/1600 [00:30<00:46, 20.72it/s]
Training:  52%|█████▏    | 833/1600 [00:40<00:37, 20.47it/s]
Training:  65%|██████▌   | 1041/Training loss: 173.8230, Training accuracy: 0.9637
Macro F1-score: 0.9637
Model performance on Angry speech (in training): 
	Precision: 0.9726, Recall: 0.9750, F1_score: 0.9738
Model performance on Happy speech (in training): 
	Precision: 0.9672, Recall: 0.9575, F1_score: 0.9623
Model performance on Neutral speech (in training): 
	Precision: 0.9550, Recall: 0.9550, F1_score: 0.9550
Model performance on Sad speech (in training): 
	Precision: 0.9603, Recall: 0.9675, F1_score: 0.9639

Eval Phase: 
Validation loss: 281.1417, Validation accuracy: 0.6650
Macro F1-score: 0.6676
Model performance on Angry speech (in validation): 
	Precision: 0.8974, Recall: 0.7000, F1_score: 0.7865
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.4400, F1_score: 0.5301
Model performance on Neutral speech (in validation): 
	Precision: 0.4872, Recall: 0.7600, F1_score: 0.5938
Model performance on Sad speech (in validation): 
	Precision: 0.7600, Recall: 0.7600, F1_score: 0.7600
Epoch 13/100

Training Phase:
Training loss: 177.9962, Training accuracy: 0.9644
Macro F1-score: 0.9644
Model performance on Angry speech (in training): 
	Precision: 0.9750, Recall: 0.9750, F1_score: 0.9750
Model performance on Happy speech (in training): 
	Precision: 0.9672, Recall: 0.9575, F1_score: 0.9623
Model performance on Neutral speech (in training): 
	Precision: 0.9527, Recall: 0.9575, F1_score: 0.9551
Model performance on Sad speech (in training): 
	Precision: 0.9627, Recall: 0.9675, F1_score: 0.9651

Eval Phase: 
1600 [00:50<00:27, 20.58it/s]
Training:  78%|███████▊  | 1249/1600 [01:00<00:17, 20.64it/s]
Training:  91%|█████████▏| 1462/1600 [01:10<00:06, 20.82it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 202/1600 [00:10<01:09, 20.12it/s]
Training:  26%|██▌       | 415/1600 [00:20<00:57, 20.75it/s]
Training:  39%|███▉      | 628/1600 [00:30<00:46, 20.98it/s]
Training:  53%|█████▎    | 842/1600 [00:40<00:35, 21.13it/s]
Training:  66%|██████▌   | 1056/1600 [00:50<00:25, 21.13it/s]
Training:  79%|███████▉  | 1268/1600 [01:00<00:15, 21.03it/s]
Training:  92%|█████████▏| 1477/1600 [01:10<00:05, 20.78it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<Validation loss: 321.8986, Validation accuracy: 0.6350
Macro F1-score: 0.6169
Model performance on Angry speech (in validation): 
	Precision: 0.6364, Recall: 0.8400, F1_score: 0.7241
Model performance on Happy speech (in validation): 
	Precision: 0.6154, Recall: 0.3200, F1_score: 0.4211
Model performance on Neutral speech (in validation): 
	Precision: 0.5385, Recall: 0.5600, F1_score: 0.5490
Model performance on Sad speech (in validation): 
	Precision: 0.7321, Recall: 0.8200, F1_score: 0.7736
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7150

Test Phase: 
?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   4%|▍         | 9/200 [00:00<00:02, 85.31it/s]
Testing:   9%|▉         | 18/200 [00:00<00:02, 86.62it/s]
Testing:  14%|█▍        | 28/200 [00:00<00:01, 91.40it/s]
Testing:  19%|█▉        | 38/200 [00:00<00:01, 91.73it/s]
Testing:  24%|██▍       | 48/200 [00:00<00:01, 88.90it/s]
Testing:  28%|██▊       | 57/200 [00:00<00:01, 87.43it/s]
Testing:  33%|███▎      | 66/200 [00:00<00:01, 84.23it/s]
Testing:  38%|███▊      | 75/200 [00:00<00:01, 85.62it/s]
Testing:  42%|████▎     | 85/200 [00:00<00:01, 87.50it/s]
Testing:  47%|████▋     | 94/200 [00:01<00:01, 88.05it/s]
Testing:  52%|█████▏    | 103/200 [00:01<00:01, 87.73it/s]
Testing:  56%|█████▋    | 113/200 [00:01<00:00, 90.25it/s]
Testing:  62%|██████▏   | 123/200 [00:01<00:00, 80.34it/s]
Testing:  66%|██████▋   | 133/200 [00:01<00:Test loss: 185.7729, Test accuracy: 0.6750
Macro F1-score: 0.6690
Model performance on Angry speech (in test): 
	Precision: 0.6949, Recall: 0.8200, F1_score: 0.7523
Model performance on Happy speech (in test): 
	Precision: 0.6977, Recall: 0.6000, F1_score: 0.6452
Model performance on Neutral speech (in test): 
	Precision: 0.6098, Recall: 0.5000, F1_score: 0.5495
Model performance on Sad speech (in test): 
	Precision: 0.6842, Recall: 0.7800, F1_score: 0.7290

======================= This is fold_4 on de =======================

Load dataset: 
Loading en train data: fold_4...
Preprocess en fold_4 data for de model
00, 83.77it/s]
Testing:  72%|███████▏  | 143/200 [00:01<00:00, 86.20it/s]
Testing:  76%|███████▋  | 153/200 [00:01<00:00, 87.79it/s]
Testing:  82%|████████▏ | 163/200 [00:01<00:00, 89.29it/s]
Testing:  86%|████████▋ | 173/200 [00:01<00:00, 89.66it/s]
Testing:  92%|█████████▏| 183/200 [00:02<00:00, 90.32it/s]
Testing:  96%|█████████▋| 193/200 [00:02<00:00, 90.46it/s]
                                                          

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:   0%|          | 4/1600 [00:00<00:51, 31.10 examples/s]
Map:   1%|          | 15/1600 [00:00<00:22, 70.33 examples/s]
Map:   2%|▏         | 29/1600 [00:00<00:16, 95.43 examples/s]
Map:   3%|▎         | 41/1600 [00:00<00:18, 83.78 examples/s]
Map:   3%|▎         | 52/1600 [00:00<00:17, 86.50 examples/s]
Map:   4%|▍         | 61/1600 [00:00<00:18, 82.68 examples/s]
Map:   5%|▍         | 73/1600 [00:00<00:16, 90.19 examples/s]
Map:   5%|▌         | 86/1600 [00:00<00:15, 96.93 examples/s]
Map:   6%|▌         | 99/1600 [00:01<00:17, 87.47 examples/s]
Map:   7%|▋         | 112/1600 [00:01<00:15, 95.35 examples/s]
Map:   8%|▊         | 122/1600 [00:01<00:17, 82.37 examples/s]
Map:   8%|▊         | 134/1600 [00:01<00:16, 88.61 examples/s]
Map:   9%|▉         | 146/1600 [00:01<00:15, 91.59 examples/s]
Map:  10%|▉         | 159/1600 [00:01<00:14, 96.91 examples/s]
Map:  11%|█         | 172/1600 [00:01<00:13, 104.80 examples/s]
Map:  12%|█▏        | 185/1600 [00:01<00:12, 110.64 examples/s]
Map:  13%|█▎        | 201/1600 [00:02<00:11, 122.70 examples/s]
Map:  14%|█▎        | 216/1600 [00:02<00:11, 123.40 examples/s]
Map:  15%|█▍        | 235/1600 [00:02<00:11, 119.92 examples/s]
Map:  16%|█▌        | 250/1600 [00:02<00:14, 90.29 examples/s] 
Map:  16%|█▋        | 263/1600 [00:02<00:13, 96.86 examples/s]
Map:  17%|█▋        | 279/1600 [00:02<00:12, 107.31 examples/s]
Map:  18%|█▊        | 294/1600 [00:02<00:11, 112.12 examples/s]
Map:  19%|█▉        | 307/1600 [00:03<00:11, 115.56 examples/s]
Map:  20%|██        | 324/1600 [00:03<00:10, 126.85 examples/s]
Map:  22%|██▏       | 345/1600 [00:03<00:10, 125.01 examples/s]
Map:  23%|██▎       | 361/1600 [00:03<00:11, 110.98 examples/s]
Map:  23%|██▎       | 373/1600 [00:03<00:10, 112.16 examples/s]
Map:  24%|██▍       | 392/1600 [00:03<00:09, 126.93 examples/s]
Map:  26%|██▌       | 408/1600 [00:03<00:10, 114.75 examples/s]
Map:  27%|██▋       | 425/1600 [00:04<00:10, 111.85 examples/s]
Map:  28%|██▊       | 440/1600 [00:04<00:09, 118.18 examples/s]
Map:  29%|██▊       | 458/1600 [00:04<00:08, 128.09 examples/s]
Map:  30%|██▉       | 478/1600 [00:04<00:09, 120.08 examples/s]
Map:  31%|███       | 496/1600 [00:04<00:08, 132.42 examples/s]
Map:  32%|███▏      | 511/1600 [00:04<00:09, 119.29 examples/s]
Map:  33%|███▎      | 527/1600 [00:04<00:09, 112.24 examples/s]
Map:  34%|███▍      | 543/1600 [00:05<00:09, 108.14 examples/s]
Map:  35%|███▍      | 558/1600 [00:05<00:09, 107.85 examples/s]
Map:  36%|███▌      | 574/1600 [00:05<00:08, 118.20 examples/s]
Map:  37%|███▋      | 590/1600 [00:05<00:08, 125.18 examples/s]
Map:  38%|███▊      | 606/1600 [00:05<00:07, 133.46 examples/s]
Map:  39%|███▉      | 620/1600 [00:05<00:07, 130.18 examples/s]
Map:  40%|███▉      | 637/1600 [00:05<00:07, 133.02 examples/s]
Map:  41%|████      | 651/1600 [00:05<00:07, 130.15 examples/s]
Map:  42%|████▏     | 665/1600 [00:06<00:08, 105.84 examples/s]
Map:  42%|████▏     | 677/1600 [00:06<00:08, 104.74 examples/s]
Map:  43%|████▎     | 689/1600 [00:06<00:08, 102.65 examples/s]
Map:  44%|████▍     | 700/1600 [00:06<00:08, 101.10 examples/s]
Map:  45%|████▌     | 720/1600 [00:06<00:07, 123.70 examples/s]
Map:  46%|████▌     | 734/1600 [00:06<00:06, 125.38 examples/s]
Map:  47%|████▋     | 748/1600 [00:06<00:06, 125.10 examples/s]
Map:  48%|████▊     | 766/1600 [00:07<00:08, 101.29 examples/s]
Map:  49%|████▉     | 780/1600 [00:07<00:08, 93.66 examples/s] 
Map:  50%|████▉     | 794/1600 [00:07<00:08, 91.74 examples/s]
Map:  50%|█████     | 808/1600 [00:07<00:08, 97.30 examples/s]
Map:  51%|█████     | 819/1600 [00:07<00:07, 98.47 examples/s]
Map:  52%|█████▏    | 833/1600 [00:07<00:07, 106.91 examples/s]
Map:  53%|█████▎    | 846/1600 [00:07<00:07, 96.73 examples/s] 
Map:  54%|█████▍    | 862/1600 [00:07<00:06, 108.57 examples/s]
Map:  55%|█████▍    | 878/1600 [00:08<00:06, 119.77 examples/s]
Map:  56%|█████▌    | 892/1600 [00:08<00:05, 120.73 examples/s]
Map:  57%|█████▋    | 905/1600 [00:08<00:05, 122.34 examples/s]
Map:  57%|█████▋    | 919/1600 [00:08<00:05, 123.32 examples/s]
Map:  58%|█████▊    | 935/1600 [00:08<00:05, 129.09 examples/s]
Map:  59%|█████▉    | 951/1600 [00:08<00:05, 115.29 examples/s]
Map:  60%|██████    | 963/1600 [00:08<00:05, 114.90 examples/s]
Map:  61%|██████▏   | 980/1600 [00:08<00:05, 111.77 examples/s]
Map:  62%|██████▏   | 992/1600 [00:09<00:05, 107.60 examples/s]
Map:  62%|██████▏   | 992/1600 [00:20<00:05, 107.60 examples/s]
Map:  62%|██████▎   | 1000/1600 [01:02<13:15,  1.33s/ examples]
Map:  63%|██████▎   | 1014/1600 [01:03<08:42,  1.12 examples/s]
Map:  64%|██████▍   | 1030/1600 [01:03<05:32,  1.72 examples/s]
Map:  65%|██████▌   | 1046/1600 [01:03<03:36,  2.56 examples/s]
Map:  67%|██████▋   | 1065/1600 [01:03<02:15,  3.95 examples/s]
Map:  67%|██████▋   | 1078/1600 [01:03<01:38,  5.27 examples/s]
Map:  68%|██████▊   | 1092/1600 [01:03<01:09,  7.26 examples/s]
Map:  69%|██████▉   | 1109/1600 [01:03<00:46, 10.52 examples/s]
Map:  70%|███████   | 1122/1600 [01:03<00:34, 13.87 examples/s]
Map:  71%|███████   | 1135/1600 [01:04<00:25, 17.91 examples/s]
Map:  72%|███████▏  | 1152/1600 [01:04<00:18, 24.28 examples/s]
Map:  73%|███████▎  | 1167/1600 [01:04<00:13, 32.29 examples/s]
Map:  74%|███████▎  | 1179/1600 [01:04<00:10, 39.57 examples/s]
Map:  75%|███████▍  | 1194/1600 [01:04<00:07, 51.07 examples/s]
Map:  76%|███████▌  | 1209/1600 [01:04<00:06, 62.97 examples/s]
Map:  77%|███████▋  | 1226/1600 [01:04<00:05, 71.10 examples/s]
Map:  78%|███████▊  | 1242/1600 [01:05<00:04, 76.74 examples/s]
Map:  78%|███████▊  | 1254/1600 [01:05<00:04, 74.95 examples/s]
Map:  79%|███████▉  | 1268/1600 [01:05<00:03, 84.39 examples/s]
Map:  80%|████████  | 1281/1600 [01:05<00:03, 92.26 examples/s]
Map:  81%|████████  | 1294/1600 [01:05<00:03, 97.58 examples/s]
Map:  82%|████████▏ | 1306/1600 [01:05<00:02, 100.08 examples/s]
Map:  82%|████████▏ | 1319/1600 [01:05<00:02, 103.43 examples/s]
Map:  83%|████████▎ | 1335/1600 [01:05<00:02, 111.76 examples/s]
Map:  85%|████████▍ | 1353/1600 [01:06<00:01, 124.35 examples/s]
Map:  85%|████████▌ | 1367/1600 [01:06<00:02, 84.88 examples/s] 
Map:  86%|████████▌ | 1379/1600 [01:06<00:02, 88.42 examples/s]
Map:  87%|████████▋ | 1390/1600 [01:06<00:02, 90.77 examples/s]
Map:  88%|████████▊ | 1408/1600 [01:06<00:01, 97.25 examples/s]
Map:  89%|████████▉ | 1422/1600 [01:06<00:01, 105.29 examples/s]
Map:  90%|█████████ | 1440/1600 [01:06<00:01, 121.43 examples/s]
Map:  91%|█████████ | 1455/1600 [01:07<00:01, 122.03 examples/s]
Map:  92%|█████████▏| 1469/1600 [01:07<00:01, 109.90 examples/s]
Map:  93%|█████████▎| 1485/1600 [01:07<00:01, 85.68 examples/s] 
Map:  94%|█████████▎| 1499/1600 [01:07<00:01, 90.35 examples/s]
Map:  94%|█████████▍| 1511/1600 [01:07<00:00, 94.99 examples/s]
Map:  95%|█████████▌| 1524/1600 [01:07<00:00, 101.12 examples/s]
Map:  96%|█████████▌| 1536/1600 [01:07<00:00, 104.41 examples/s]
Map:  97%|█████████▋| 1548/1600 [01:08<00:00, 107.29 examples/s]
Map:  98%|█████████▊| 1565/1600 [01:08<00:00, 102.50 examples/s]
Map:  99%|█████████▉| 1580/1600 [01:08<00:00, 110.59 examples/s]
Map: 100%|█████████▉| 1594/1600 [01:08<00:00, 96.34 examples/s] 
Map: 100%|█████████▉| 1594/1600 [01:21<00:00, 96.34 examples/s]
Map: 100%|██████████| 1600/1600 [01:42<00:00,  1.15 examples/s]
Map: 100%|██████████| 1600/1600 [01:42<00:00, 15.61 examples/s]

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▏         | 3/200 [00:00<00:08, 24.58 examples/s]
Map:   4%|▎         | 7/200 [00:00<00:06, 27.96 examples/s]
Map:  10%|█         | 20/200 [00:00<00:02, 64.14 examples/s]
Map:  14%|█▍        | 29/200 [00:00<00:02, 71.94 examples/s]
Map:  20%|██        | 41/200 [00:00<00:01, 82.48 examples/s]
Map:  25%|██▌       | 50/200 [00:00<00:01, 78.19 examples/s]
Map:  32%|███▎      | 65/200 [00:00<00:01, 97.78 examples/s]
Map:  40%|███▉      | 79/200 [00:00<00:01, 105.60 examples/s]
Map:  45%|████▌     | 90/200 [00:01<00:01, 105.71 examples/s]
Map:  52%|█████▎    | 105/200 [00:01<00:00, 98.05 examples/s]
Map:  62%|██████▏   | 123/200 [00:01<00:00, 113.77 examples/s]
Map:  70%|███████   | 140/200 [00:01<00:00, 126.19 examples/s]
Map:  78%|███████▊  | 156/200 [00:01<00:00, 114.27 examples/s]
Map:  86%|████████▌ | 172/200 [00:01<00:00, 124.76 examples/s]
Map:  96%|█████████▌| 191/200 [00:01<00:00, 136.82 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 16.00 examples/s] 

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▏         | 3/200 [00:00<00:07, 26.21 examples/s]
Map:   4%|▍         | 8/200 [00:00<00:05, 32.41 examples/s]
Map:  10%|█         | 20/200 [00:00<00:02, 64.12 examples/s]
Map:  16%|█▌        | 32/200 [00:00<00:02, 80.33 examples/s]
Map:  22%|██▎       | 45/200 [00:00<00:01, 93.66 examples/s]
Map:  29%|██▉       | 58/200 [00:00<00:01, 97.64 examples/s]
Map:  38%|███▊      | 75/200 [00:00<00:01, 115.14 examples/s]
Map:  45%|████▌     | 90/200 [00:00<00:00, 120.79 examples/s]
Map:  52%|█████▎    | 105/200 [00:01<00:00, 107.74 examples/s]
Map:  62%|██████▏   | 124/200 [00:01<00:00, 124.22 examples/s]
Map:  69%|██████▉   | 138/200 [00:01<00:00, 121.85 examples/s]
Map:  76%|███████▌  | 152/200 [00:01<00:00, 125.22 examples/s]
Map:  84%|████████▍ | 168/200 [00:01<00:00, 129.67 examples/s]
Map:  92%|█████████▎| 185/200 [00:01<00:00, 137.28 examples/s]
Map: 100%|██████████| 200/200 [00:11<00:00,  5.11 examples/s] 
Map: 100%|██████████| 200/200 [00:11<00:00, 17.43 examples/s]
Loading en eval data: fold_4...
Preprocess en fold_4 data for de model
Loading en test data: fold_4...
Preprocess en fold_4 data for de model
Use de model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.5.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.5.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.5.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1532.5599, Training accuracy: 0.6144
Macro F1-score: 0.6102
Model performance on Angry speech (in training): 
	Precision: 0.6832, Recall: 0.6900, F1_score: 0.6866
Model performance on Happy speech (in training): 
	Precision: 0.5595, Recall: 0.4700, F1_score: 0.5109
Model performance on Neutral speech (in training): 
	Precision: 0.5225, Recall: 0.5225, F1_score: 0.5225
Model performance on Sad speech (in training): 
	Precision: 0.6739, Recall: 0.7750, F1_score: 0.7209

Eval Phase: 
Validation loss: 190.6850, Validation accuracy: 0.5850
Macro F1-score: 0.5546
Model performance on Angry speech (in validation): 
	Precision: 0.5422, Recall: 0.9000, F1_score: 0.6767
Model performance on Happy speech (in validation): 
	Precision: 0.7059, Recall: 0.2400, F1_score: 0.3582
Model performance on Neutral speech (in validation): 
	Precision: 0.5429, Recall: 0.7600, F1_score: 0.6333
Model performance on Sad speech (in validation): 
	Precision: 0.7333, Recall: 0.4400, F1_score: 0.5500
New best accuracy for layer 5 on epoch 1: 0.5850. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 188/1600 [00:10<01:15, 18.72it/s]
Training:  24%|██▍       | 384/1600 [00:20<01:03, 19.19it/s]
Training:  36%|███▋      | 583/1600 [00:30<00:52, 19.51it/s]
Training:  49%|████▉     | 782/1600 [00:40<00:42, 19.37it/s]
Training:  61%|██████    | 974/1600 [00:50<00:32, 19.22it/s]
Training:  73%|███████▎  | 1168/1600 [01:00<00:22, 19.26it/s]
Training:  85%|████████▌ | 1363/1600 [01:10<00:12, 19.33it/s]
Training:  97%|█████████▋| 1559/1600 [01:20<00:02, 19.41it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 186/1600 [00:10<01:16, 18.56it/s]
Training:  23%|██▎       | 372/1600 [00:20<01:07, 18.21it/s]
Training:  36%|███▌      | 5Training loss: 1244.4237, Training accuracy: 0.6750
Macro F1-score: 0.6714
Model performance on Angry speech (in training): 
	Precision: 0.7271, Recall: 0.7725, F1_score: 0.7491
Model performance on Happy speech (in training): 
	Precision: 0.6576, Recall: 0.5425, F1_score: 0.5945
Model performance on Neutral speech (in training): 
	Precision: 0.5840, Recall: 0.5825, F1_score: 0.5832
Model performance on Sad speech (in training): 
	Precision: 0.7197, Recall: 0.8025, F1_score: 0.7589

Eval Phase: 
Validation loss: 174.7602, Validation accuracy: 0.6400
Macro F1-score: 0.6311
Model performance on Angry speech (in validation): 
	Precision: 0.7857, Recall: 0.6600, F1_score: 0.7174
Model performance on Happy speech (in validation): 
	Precision: 0.7826, Recall: 0.3600, F1_score: 0.4932
Model performance on Neutral speech (in validation): 
	Precision: 0.5263, Recall: 0.8000, F1_score: 0.6349
Model performance on Sad speech (in validation): 
	Precision: 0.6271, Recall: 0.7400, F1_score: 0.6789
New best accuracy for layer 5 on epoch 2: 0.6400. Model saved.
Epoch 3/100

Training Phase:
69/1600 [00:30<00:54, 18.85it/s]
Training:  48%|████▊     | 771/1600 [00:40<00:42, 19.36it/s]
Training:  61%|██████    | 973/1600 [00:50<00:32, 19.46it/s]
Training:  73%|███████▎  | 1170/1600 [01:01<00:22, 19.27it/s]
Training:  85%|████████▌ | 1363/1600 [01:11<00:12, 19.28it/s]
Training:  97%|█████████▋| 1556/1600 [01:21<00:02, 19.14it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 204/1600 [00:10<01:08, 20.31it/s]
Training:  26%|██▌       | 415/1600 [00:20<00:57, 20.76it/s]
Training:  39%|███▉      | 626/1600 [00:30<00:47, 20.72it/s]
Training:  52%|█████▏    | 833/1600 [00:40<00:37, 20.59it/s]
Training:  65%|██████▍   | 1037/1600 [00:50<00:27, 20.51it/s]
Training:  78%|███████�Training loss: 1087.9127, Training accuracy: 0.7212
Macro F1-score: 0.7186
Model performance on Angry speech (in training): 
	Precision: 0.7740, Recall: 0.8050, F1_score: 0.7892
Model performance on Happy speech (in training): 
	Precision: 0.6980, Recall: 0.6125, F1_score: 0.6525
Model performance on Neutral speech (in training): 
	Precision: 0.6763, Recall: 0.6425, F1_score: 0.6590
Model performance on Sad speech (in training): 
	Precision: 0.7285, Recall: 0.8250, F1_score: 0.7737

Eval Phase: 
Validation loss: 192.1945, Validation accuracy: 0.6100
Macro F1-score: 0.5740
Model performance on Angry speech (in validation): 
	Precision: 0.5972, Recall: 0.8600, F1_score: 0.7049
Model performance on Happy speech (in validation): 
	Precision: 0.7143, Recall: 0.2000, F1_score: 0.3125
Model performance on Neutral speech (in validation): 
	Precision: 0.5600, Recall: 0.8400, F1_score: 0.6720
Model performance on Sad speech (in validation): 
	Precision: 0.6923, Recall: 0.5400, F1_score: 0.6067
Epoch 4/100

Training Phase:
Training loss: 909.1095, Training accuracy: 0.7825
Macro F1-score: 0.7813
Model performance on Angry speech (in training): 
	Precision: 0.8363, Recall: 0.8300, F1_score: 0.8331
Model performance on Happy speech (in training): 
	Precision: 0.7841, Recall: 0.6900, F1_score: 0.7340
Model performance on Neutral speech (in training): 
	Precision: 0.7291, Recall: 0.7400, F1_score: 0.7345
Model performance on Sad speech (in training): 
	Precision: 0.7820, Recall: 0.8700, F1_score: 0.8237

Eval Phase: 
�  | 1241/1600 [01:00<00:17, 20.21it/s]
Training:  90%|████████▉ | 1439/1600 [01:10<00:08, 20.07it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 194/1600 [00:10<01:12, 19.34it/s]
Training:  24%|██▍       | 390/1600 [00:20<01:02, 19.48it/s]
Training:  37%|███▋      | 586/1600 [00:30<00:52, 19.38it/s]
Training:  49%|████▉     | 782/1600 [00:40<00:42, 19.45it/s]
Training:  61%|██████▏   | 982/1600 [00:50<00:31, 19.63it/s]
Training:  74%|███████▍  | 1182/1600 [01:00<00:21, 19.33it/s]
Training:  86%|████████▌ | 1375/1600 [01:10<00:11, 19.32it/s]
Training:  98%|█████████▊| 1571/1600 [01:20<00:01, 19.39it/s]
                                                             

Evaluating:   0%|          | 0/200 Validation loss: 182.6078, Validation accuracy: 0.6500
Macro F1-score: 0.6466
Model performance on Angry speech (in validation): 
	Precision: 0.8222, Recall: 0.7400, F1_score: 0.7789
Model performance on Happy speech (in validation): 
	Precision: 0.6286, Recall: 0.4400, F1_score: 0.5176
Model performance on Neutral speech (in validation): 
	Precision: 0.5571, Recall: 0.7800, F1_score: 0.6500
Model performance on Sad speech (in validation): 
	Precision: 0.6400, Recall: 0.6400, F1_score: 0.6400
New best accuracy for layer 5 on epoch 4: 0.6500. Model saved.
Epoch 5/100

Training Phase:
Training loss: 722.6030, Training accuracy: 0.8250
Macro F1-score: 0.8241
Model performance on Angry speech (in training): 
	Precision: 0.8725, Recall: 0.8900, F1_score: 0.8812
Model performance on Happy speech (in training): 
	Precision: 0.8414, Recall: 0.7425, F1_score: 0.7888
Model performance on Neutral speech (in training): 
	Precision: 0.7839, Recall: 0.7800, F1_score: 0.7820
Model performance on Sad speech (in training): 
	Precision: 0.8050, Recall: 0.8875, F1_score: 0.8442

Eval Phase: 
Validation loss: 210.2806, Validation accuracy: 0.6500
Macro F1-score: 0.6458
Model performance on Angry speech (in validation): 
	Precision: 0.7368, Recall: 0.8400, F1_score: 0.7850
Model performance on Happy speech (in validation): 
	Precision: 0.5682, Recall: 0.5000, F1_score: 0.5319
Model performance on Neutral speech (in validation): 
	Precision: 0.6400, Recall: 0.6400, F1_score: 0.6400
Model performance on Sad speech (in validation): 
	Precision: 0.6327, Recall: 0.6200, F1_score: 0.6263
Epoch 6/100

Training Phase:
[00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 198/1600 [00:10<01:11, 19.74it/s]
Training:  25%|██▌       | 406/1600 [00:20<00:58, 20.29it/s]
Training:  38%|███▊      | 613/1600 [00:30<00:49, 20.03it/s]
Training:  51%|█████     | 811/1600 [00:40<00:40, 19.67it/s]
Training:  63%|██████▎   | 1012/1600 [00:50<00:29, 19.78it/s]
Training:  76%|███████▌  | 1212/1600 [01:01<00:19, 19.57it/s]
Training:  88%|████████▊ | 1412/1600 [01:11<00:09, 19.70it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 192/1600 [00:10<01:13, 19.14it/s]
Training:  24%|██▍       | 391/1600 [00:20<01:01, 19.54it/s]
Training:  37%|███▋      | 590/1600 [0Training loss: 560.4633, Training accuracy: 0.8675
Macro F1-score: 0.8672
Model performance on Angry speech (in training): 
	Precision: 0.9241, Recall: 0.9125, F1_score: 0.9182
Model performance on Happy speech (in training): 
	Precision: 0.8855, Recall: 0.7925, F1_score: 0.8364
Model performance on Neutral speech (in training): 
	Precision: 0.8197, Recall: 0.8525, F1_score: 0.8358
Model performance on Sad speech (in training): 
	Precision: 0.8469, Recall: 0.9125, F1_score: 0.8785

Eval Phase: 
Validation loss: 211.6158, Validation accuracy: 0.6500
Macro F1-score: 0.6173
Model performance on Angry speech (in validation): 
	Precision: 0.6897, Recall: 0.8000, F1_score: 0.7407
Model performance on Happy speech (in validation): 
	Precision: 0.8462, Recall: 0.2200, F1_score: 0.3492
Model performance on Neutral speech (in validation): 
	Precision: 0.5915, Recall: 0.8400, F1_score: 0.6942
Model performance on Sad speech (in validation): 
	Precision: 0.6379, Recall: 0.7400, F1_score: 0.6852
Epoch 7/100

Training Phase:
0:30<00:51, 19.52it/s]
Training:  37%|███▋      | 590/1600 [00:40<00:51, 19.52it/s]
Training:  49%|████▉     | 787/1600 [00:40<00:41, 19.56it/s]
Training:  62%|██████▏   | 984/1600 [00:50<00:31, 19.43it/s]
Training:  74%|███████▍  | 1183/1600 [01:00<00:21, 19.57it/s]
Training:  86%|████████▋ | 1382/1600 [01:10<00:11, 19.46it/s]
Training:  98%|█████████▊| 1575/1600 [01:21<00:01, 19.31it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 188/1600 [00:10<01:15, 18.74it/s]
Training:  24%|██▍       | 387/1600 [00:20<01:02, 19.39it/s]
Training:  37%|███▋      | 586/1600 [00:30<00:52, 19.39it/s]
Training:  50%|████▉     | 795/1600 [00:40<00:40, 19.96it/s]
Training:  63%|██████▎   | 1004/1600 [00:Training loss: 470.4725, Training accuracy: 0.8888
Macro F1-score: 0.8885
Model performance on Angry speech (in training): 
	Precision: 0.9248, Recall: 0.9225, F1_score: 0.9237
Model performance on Happy speech (in training): 
	Precision: 0.8976, Recall: 0.8325, F1_score: 0.8638
Model performance on Neutral speech (in training): 
	Precision: 0.8614, Recall: 0.8700, F1_score: 0.8657
Model performance on Sad speech (in training): 
	Precision: 0.8732, Recall: 0.9300, F1_score: 0.9007

Eval Phase: 
Validation loss: 224.5199, Validation accuracy: 0.6400
Macro F1-score: 0.6382
Model performance on Angry speech (in validation): 
	Precision: 0.8537, Recall: 0.7000, F1_score: 0.7692
Model performance on Happy speech (in validation): 
	Precision: 0.5897, Recall: 0.4600, F1_score: 0.5169
Model performance on Neutral speech (in validation): 
	Precision: 0.5714, Recall: 0.8000, F1_score: 0.6667
Model performance on Sad speech (in validation): 
	Precision: 0.6000, Recall: 0.6000, F1_score: 0.6000
Epoch 8/100

Training Phase:
Training loss: 365.6459, Training accuracy: 0.9231
Macro F1-score: 0.9232
Model performance on Angry speech (in training): 
	Precision: 0.9574, Recall: 0.9550, F1_score: 0.9562
Model performance on Happy speech (in training): 
	Precision: 0.9375, Recall: 0.9000, F1_score: 0.9184
Model performance on Neutral speech (in training): 
	Precision: 0.8958, Recall: 0.9025, F1_score: 0.8991
Model performance on Sad speech (in training): 
	Precision: 0.9034, Recall: 0.9350, F1_score: 0.9189

Eval Phase: 
50<00:29, 20.28it/s]
Training:  76%|███████▌  | 1213/1600 [01:00<00:18, 20.38it/s]
Training:  89%|████████▊ | 1419/1600 [01:10<00:08, 20.39it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  11%|█▏        | 182/1600 [00:10<01:17, 18.19it/s]
Training:  24%|██▍       | 384/1600 [00:20<01:02, 19.37it/s]
Training:  37%|███▋      | 595/1600 [00:30<00:49, 20.15it/s]
Training:  50%|█████     | 806/1600 [00:40<00:38, 20.43it/s]
Training:  64%|██████▎   | 1016/1600 [00:50<00:28, 20.61it/s]
Training:  77%|███████▋  | 1226/1600 [01:01<00:18, 20.11it/s]
Training:  90%|████████▉ | 1432/1600 [01:11<00:08, 20.24it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
     Validation loss: 273.6005, Validation accuracy: 0.6450
Macro F1-score: 0.6296
Model performance on Angry speech (in validation): 
	Precision: 0.8000, Recall: 0.8000, F1_score: 0.8000
Model performance on Happy speech (in validation): 
	Precision: 0.6000, Recall: 0.4800, F1_score: 0.5333
Model performance on Neutral speech (in validation): 
	Precision: 0.5542, Recall: 0.9200, F1_score: 0.6917
Model performance on Sad speech (in validation): 
	Precision: 0.7037, Recall: 0.3800, F1_score: 0.4935
Epoch 9/100

Training Phase:
Training loss: 275.6012, Training accuracy: 0.9431
Macro F1-score: 0.9432
Model performance on Angry speech (in training): 
	Precision: 0.9747, Recall: 0.9625, F1_score: 0.9686
Model performance on Happy speech (in training): 
	Precision: 0.9593, Recall: 0.9425, F1_score: 0.9508
Model performance on Neutral speech (in training): 
	Precision: 0.9086, Recall: 0.9200, F1_score: 0.9143
Model performance on Sad speech (in training): 
	Precision: 0.9312, Recall: 0.9475, F1_score: 0.9393

Eval Phase: 
Validation loss: 281.9965, Validation accuracy: 0.6050
Macro F1-score: 0.5804
Model performance on Angry speech (in validation): 
	Precision: 0.6389, Recall: 0.9200, F1_score: 0.7541
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.2600, F1_score: 0.3421
Model performance on Neutral speech (in validation): 
	Precision: 0.6296, Recall: 0.6800, F1_score: 0.6538
Model performance on Sad speech (in validation): 
	Precision: 0.5833, Recall: 0.5600, F1_score: 0.5714
Epoch 10/100

Training Phase:
                                              

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 209/1600 [00:10<01:06, 20.84it/s]
Training:  26%|██▌       | 418/1600 [00:20<00:57, 20.71it/s]
Training:  26%|██▌       | 418/1600 [00:30<00:57, 20.71it/s]
Training:  38%|███▊      | 603/1600 [00:30<00:51, 19.49it/s]
Training:  49%|████▉     | 784/1600 [00:40<00:43, 18.89it/s]
Training:  61%|██████    | 971/1600 [00:50<00:33, 18.81it/s]
Training:  72%|███████▏  | 1158/1600 [01:00<00:23, 18.66it/s]
Training:  84%|████████▍ | 1343/1600 [01:10<00:13, 18.60it/s]
Training:  96%|█████████▌| 1532/1600 [01:20<00:03, 18.67it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  12%|█▏        | 184/1600 [00:10<01:17, Training loss: 204.6398, Training accuracy: 0.9544
Macro F1-score: 0.9543
Model performance on Angry speech (in training): 
	Precision: 0.9701, Recall: 0.9750, F1_score: 0.9726
Model performance on Happy speech (in training): 
	Precision: 0.9545, Recall: 0.9450, F1_score: 0.9497
Model performance on Neutral speech (in training): 
	Precision: 0.9491, Recall: 0.9325, F1_score: 0.9407
Model performance on Sad speech (in training): 
	Precision: 0.9438, Recall: 0.9650, F1_score: 0.9543

Eval Phase: 
Validation loss: 269.0896, Validation accuracy: 0.6550
Macro F1-score: 0.6523
Model performance on Angry speech (in validation): 
	Precision: 0.8085, Recall: 0.7600, F1_score: 0.7835
Model performance on Happy speech (in validation): 
	Precision: 0.6000, Recall: 0.4800, F1_score: 0.5333
Model performance on Neutral speech (in validation): 
	Precision: 0.6230, Recall: 0.7600, F1_score: 0.6847
Model performance on Sad speech (in validation): 
	Precision: 0.5962, Recall: 0.6200, F1_score: 0.6078
New best accuracy for layer 5 on epoch 10: 0.6550. Model saved.
Epoch 11/100

Training Phase:
18.37it/s]
Training:  23%|██▎       | 374/1600 [00:20<01:05, 18.71it/s]
Training:  35%|███▌      | 564/1600 [00:30<00:55, 18.76it/s]
Training:  47%|████▋     | 753/1600 [00:40<00:45, 18.81it/s]
Training:  60%|██████    | 962/1600 [00:50<00:32, 19.54it/s]
Training:  73%|███████▎  | 1171/1600 [01:00<00:21, 19.90it/s]
Training:  86%|████████▌ | 1378/1600 [01:10<00:11, 20.13it/s]
Training:  99%|█████████▉| 1585/1600 [01:20<00:00, 20.27it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▎        | 217/1600 [00:10<01:03, 21.66it/s]
Training:  27%|██▋       | 434/1600 [00:20<00:55, 20.97it/s]
Training:  40%|████      | 640/1600 [00:30<00:46, 20.66it/s]
Training:  53%|█████▎    | 845/1600 [00:40<00:36, 20.57it/s]
Training loss: 219.2890, Training accuracy: 0.9481
Macro F1-score: 0.9482
Model performance on Angry speech (in training): 
	Precision: 0.9774, Recall: 0.9725, F1_score: 0.9749
Model performance on Happy speech (in training): 
	Precision: 0.9594, Recall: 0.9450, F1_score: 0.9521
Model performance on Neutral speech (in training): 
	Precision: 0.9236, Recall: 0.9375, F1_score: 0.9305
Model performance on Sad speech (in training): 
	Precision: 0.9328, Recall: 0.9375, F1_score: 0.9352

Eval Phase: 
Validation loss: 304.2121, Validation accuracy: 0.6200
Macro F1-score: 0.6162
Model performance on Angry speech (in validation): 
	Precision: 0.7451, Recall: 0.7600, F1_score: 0.7525
Model performance on Happy speech (in validation): 
	Precision: 0.6111, Recall: 0.4400, F1_score: 0.5116
Model performance on Neutral speech (in validation): 
	Precision: 0.5763, Recall: 0.6800, F1_score: 0.6239
Model performance on Sad speech (in validation): 
	Precision: 0.5556, Recall: 0.6000, F1_score: 0.5769
Epoch 12/100

Training Phase:
Training:  66%|██████▌   | 1055/1600 [00:50<00:26, 20.70it/s]
Training:  79%|███████▉  | 1265/1600 [01:01<00:16, 20.63it/s]
Training:  92%|█████████▏| 1471/1600 [01:11<00:06, 20.50it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 205/1600 [00:10<01:08, 20.45it/s]
Training:  13%|█▎        | 205/1600 [00:20<01:08, 20.45it/s]
Training:  26%|██▌       | 408/1600 [00:20<00:58, 20.26it/s]
Training:  39%|███▊      | 619/1600 [00:30<00:47, 20.63it/s]
Training:  52%|█████▏    | 836/1600 [00:40<00:36, 21.03it/s]
Training:  66%|██████▌   | 1053/1600 [00:50<00:26, 20.87it/s]
Training:  79%|███████▉  | 1266/1600 [01:00<00:15, 21.01it/s]
Training:  92%|█████████▏| 1479/1600 [01:11<00:05, 20.81Training loss: 150.9216, Training accuracy: 0.9656
Macro F1-score: 0.9656
Model performance on Angry speech (in training): 
	Precision: 0.9751, Recall: 0.9775, F1_score: 0.9763
Model performance on Happy speech (in training): 
	Precision: 0.9673, Recall: 0.9625, F1_score: 0.9649
Model performance on Neutral speech (in training): 
	Precision: 0.9673, Recall: 0.9600, F1_score: 0.9636
Model performance on Sad speech (in training): 
	Precision: 0.9530, Recall: 0.9625, F1_score: 0.9577

Eval Phase: 
Validation loss: 330.0136, Validation accuracy: 0.5950
Macro F1-score: 0.5854
Model performance on Angry speech (in validation): 
	Precision: 0.6833, Recall: 0.8200, F1_score: 0.7455
Model performance on Happy speech (in validation): 
	Precision: 0.5250, Recall: 0.4200, F1_score: 0.4667
Model performance on Neutral speech (in validation): 
	Precision: 0.5897, Recall: 0.4600, F1_score: 0.5169
Model performance on Sad speech (in validation): 
	Precision: 0.5574, Recall: 0.6800, F1_score: 0.6126
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6550

Test Phase: 
it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   4%|▍         | 8/200 [00:00<00:02, 76.75it/s]
Testing:   8%|▊         | 16/200 [00:00<00:02, 77.98it/s]
Testing:  13%|█▎        | 26/200 [00:00<00:02, 85.65it/s]
Testing:  18%|█▊        | 35/200 [00:00<00:02, 79.01it/s]
Testing:  22%|██▎       | 45/200 [00:00<00:01, 83.30it/s]
Testing:  27%|██▋       | 54/200 [00:00<00:01, 82.04it/s]
Testing:  32%|███▏      | 64/200 [00:00<00:01, 86.47it/s]
Testing:  37%|███▋      | 74/200 [00:00<00:01, 88.55it/s]
Testing:  42%|████▏     | 84/200 [00:00<00:01, 89.49it/s]
Testing:  47%|████▋     | 94/200 [00:01<00:01, 90.63it/s]
Testing:  52%|█████▏    | 104/200 [00:01<00:01, 92.33it/s]
Testing:  57%|█████▋    | 114/200 [00:01<00:00, 90.58it/s]
Testing:  62%|██Test loss: 226.2996, Test accuracy: 0.7050
Macro F1-score: 0.7006
Model performance on Angry speech (in test): 
	Precision: 0.7069, Recall: 0.8200, F1_score: 0.7593
Model performance on Happy speech (in test): 
	Precision: 0.7429, Recall: 0.5200, F1_score: 0.6118
Model performance on Neutral speech (in test): 
	Precision: 0.6349, Recall: 0.8000, F1_score: 0.7080
Model performance on Sad speech (in test): 
	Precision: 0.7727, Recall: 0.6800, F1_score: 0.7234

de, all folds layer accuracy: ['0.6500', '0.6250', '0.6450', '0.6750', '0.7050']
de, all emo precision: {'Angry': ['0.7288', '0.6364', '0.6809', '0.6949', '0.7069'], 'Happy': ['0.5714', '0.5532', '0.6585', '0.6977', '0.7429'], 'Neutral': ['0.5000', '0.5946', '0.5672', '0.6098', '0.6349'], 'Sad': ['0.7600', '0.6885', '0.7111', '0.6842', '0.7727']}
de, all emo recall: {'Angry': ['0.8600', '0.7000', '0.6400', '0.8200', '0.8200'], 'Happy': ['0.5600', '0.5200', '0.5400', '0.6000', '0.5200'], 'Neutral': ['0.4200', '0.4400', '0.7600', '0.5000', '0.8000'], 'Sad': ['0.7600', '0.8400', '0.6400', '0.7800', '0.6800']}
de, all emo f1score: {'Angry': ['0.7890', '0.6667', '0.6598', '0.7523', '0.7593'], 'Happy': ['0.5657', '0.5361', '0.5934', '0.6452', '0.6118'], 'Neutral': ['0.4565', '0.5057', '0.6496', '0.5495', '0.7080'], 'Sad': ['0.7600', '0.7568', '0.6737', '0.7290', '0.7234']}
████▏   | 124/200 [00:01<00:00, 91.44it/s]
Testing:  67%|██████▋   | 134/200 [00:01<00:00, 87.00it/s]
Testing:  72%|███████▏  | 143/200 [00:01<00:00, 82.75it/s]
Testing:  76%|███████▌  | 152/200 [00:01<00:00, 84.44it/s]
Testing:  81%|████████  | 162/200 [00:01<00:00, 84.15it/s]
Testing:  86%|████████▌ | 171/200 [00:01<00:00, 84.29it/s]
Testing:  90%|█████████ | 181/200 [00:02<00:00, 88.01it/s]
Testing:  95%|█████████▌| 190/200 [00:02<00:00, 88.41it/s]
Testing: 100%|█████████▉| 199/200 [00:02<00:00, 87.38it/s]
                                                          
------------------NEXT SCRIPT: RUNNER_CN, TL on EN----------------------
/work/tc062/tc062/zhan7721/.venv/hgf/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Matplotlib created a temporary cache directory at /dev/shm/zhan7721_5911930/matplotlib-w0x87p0p because the default path (/home/tc062/tc062/zhan7721/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.

======================= This is fold_0 on cn =======================

Load dataset: 
Loading en train data: fold_0...
Preprocess en fold_0 data for cn model
Loading en eval data: fold_0...
Preprocess en fold_0 data for cn model
Loading en test data: fold_0...
Preprocess en fold_0 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1978.1616, Training accuracy: 0.4213
Macro F1-score: 0.3630
Model performance on Angry speech (in training): 
	Precision: 0.6852, Recall: 0.0925, F1_score: 0.1630
Model performance on Happy speech (in training): 
	Precision: 0.3304, Recall: 0.7425, F1_score: 0.4573
Model performance on Neutral speech (in training): 
	Precision: 0.4211, Recall: 0.1400, F1_score: 0.2101
Model performance on Sad speech (in training): 
	Precision: 0.5525, Recall: 0.7100, F1_score: 0.6214

Eval Phase: 
Validation loss: 215.6936, Validation accuracy: 0.6000
Macro F1-score: 0.5848
Model performance on Angry speech (in validation): 
	Precision: 0.9459, Recall: 0.7000, F1_score: 0.8046
Model performance on Happy speech (in validation): 
	Precision: 0.7391, Recall: 0.3400, F1_score: 0.4658
Model performance on Neutral speech (in validation): 
	Precision: 0.5143, Recall: 0.3600, F1_score: 0.4235
Model performance on Sad speech (in validation): 
	Precision: 0.4762, Recall: 1.0000, F1_score: 0.6452
New best accuracy for layer 4 on epoch 1: 0.6000. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:   0%|          | 1/1600 [00:14<6:38:14, 14.94s/it]
Training:   9%|▊         | 138/1600 [00:24<03:37,  6.73it/s]
Training:  19%|█▉        | 302/1600 [00:34<02:01, 10.70it/s]
Training:  29%|██▉       | 470/1600 [00:45<01:27, 12.92it/s]
Training:  42%|████▏     | 674/1600 [00:55<00:59, 15.47it/s]
Training:  55%|█████▍    | 877/1600 [01:05<00:43, 16.72it/s]
Training:  67%|██████▋   | 1076/1600 [01:15<00:29, 17.72it/s]
Training:  81%|████████  | 1296/1600 [01:25<00:15, 19.05it/s]
Training:  95%|█████████▍| 1516/1600 [01:36<00:04, 19.76it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 239/1600 [00:10<00:56, 23.89it/s]
Training:  30%|██▉       | 478/1600 [00:20<00:Training loss: 1569.4206, Training accuracy: 0.6100
Macro F1-score: 0.6004
Model performance on Angry speech (in training): 
	Precision: 0.6721, Recall: 0.7275, F1_score: 0.6987
Model performance on Happy speech (in training): 
	Precision: 0.5730, Recall: 0.3925, F1_score: 0.4659
Model performance on Neutral speech (in training): 
	Precision: 0.5161, Recall: 0.5200, F1_score: 0.5181
Model performance on Sad speech (in training): 
	Precision: 0.6531, Recall: 0.8000, F1_score: 0.7191

Eval Phase: 
Validation loss: 160.0355, Validation accuracy: 0.6700
Macro F1-score: 0.6748
Model performance on Angry speech (in validation): 
	Precision: 0.8974, Recall: 0.7000, F1_score: 0.7865
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.4400, F1_score: 0.5301
Model performance on Neutral speech (in validation): 
	Precision: 0.4831, Recall: 0.8600, F1_score: 0.6187
Model performance on Sad speech (in validation): 
	Precision: 0.8718, Recall: 0.6800, F1_score: 0.7640
New best accuracy for layer 4 on epoch 2: 0.6700. Model saved.
Epoch 3/100

Training Phase:
48, 23.13it/s]
Training:  45%|████▍     | 715/1600 [00:30<00:37, 23.37it/s]
Training:  60%|█████▉    | 952/1600 [00:40<00:27, 23.27it/s]
Training:  75%|███████▍  | 1195/1600 [00:50<00:17, 23.62it/s]
Training:  90%|████████▉ | 1438/1600 [01:01<00:06, 23.35it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 228/1600 [00:10<01:00, 22.78it/s]
Training:  29%|██▉       | 463/1600 [00:20<00:49, 23.20it/s]
Training:  44%|████▍     | 703/1600 [00:30<00:38, 23.56it/s]
Training:  59%|█████▉    | 943/1600 [00:40<00:28, 23.33it/s]
Training:  73%|███████▎  | 1173/1600 [00:50<00:18, 23.19it/s]
Training:  88%|████████▊ | 1413/1600 [01:00<00:07, 23.43it/s]
                                                      Training loss: 1391.4157, Training accuracy: 0.6613
Macro F1-score: 0.6570
Model performance on Angry speech (in training): 
	Precision: 0.7513, Recall: 0.7325, F1_score: 0.7418
Model performance on Happy speech (in training): 
	Precision: 0.6643, Recall: 0.4650, F1_score: 0.5471
Model performance on Neutral speech (in training): 
	Precision: 0.5610, Recall: 0.6550, F1_score: 0.6044
Model performance on Sad speech (in training): 
	Precision: 0.6847, Recall: 0.7925, F1_score: 0.7346

Eval Phase: 
Validation loss: 150.1401, Validation accuracy: 0.7350
Macro F1-score: 0.7359
Model performance on Angry speech (in validation): 
	Precision: 0.9412, Recall: 0.6400, F1_score: 0.7619
Model performance on Happy speech (in validation): 
	Precision: 0.6731, Recall: 0.7000, F1_score: 0.6863
Model performance on Neutral speech (in validation): 
	Precision: 0.6491, Recall: 0.7400, F1_score: 0.6916
Model performance on Sad speech (in validation): 
	Precision: 0.7544, Recall: 0.8600, F1_score: 0.8037
New best accuracy for layer 4 on epoch 3: 0.7350. Model saved.
Epoch 4/100

Training Phase:
Training loss: 1295.5706, Training accuracy: 0.6869
Macro F1-score: 0.6829
Model performance on Angry speech (in training): 
	Precision: 0.7723, Recall: 0.7800, F1_score: 0.7761
Model performance on Happy speech (in training): 
	Precision: 0.6689, Recall: 0.5100, F1_score: 0.5787
Model performance on Neutral speech (in training): 
	Precision: 0.6084, Recall: 0.6525, F1_score: 0.6297
Model performance on Sad speech (in training): 
	Precision: 0.6970, Recall: 0.8050, F1_score: 0.7471

Eval Phase: 
Validation loss: 138.2886, Validation accuracy: 0.7600
Macro F1-score: 0.7623
Model performance on Angry speech (in validation): 
	Precision: 0.9250, Recall: 0.7400, F1_score: 0.8222
Model performance on Happy speech (in validation): 
	Precision: 0.7292, Recall: 0.7000, F1_score: 0.7143
Model performance on Neutral speech (in validation): 
	Precision: 0.6441, Recall: 0.7600, F1_score: 0.6972
Model performance on Sad speech (in validation): 
	Precision: 0.7925, Recall: 0.8400, F1_score: 0.8155
New best accuracy for layer 4 on epoch 4: 0.7600. Model saved.
Epoch 5/100

Training Phase:
       

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 236/1600 [00:10<00:57, 23.58it/s]
Training:  30%|██▉       | 477/1600 [00:20<00:47, 23.86it/s]
Training:  45%|████▍     | 718/1600 [00:30<00:36, 23.87it/s]
Training:  60%|█████▉    | 957/1600 [00:40<00:27, 23.52it/s]
Training:  75%|███████▍  | 1197/1600 [00:50<00:17, 23.68it/s]
Training:  90%|████████▉ | 1437/1600 [01:01<00:06, 23.33it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 236/1600 [00:10<00:57, 23.59it/s]
Training:  30%|██▉       | 472/1600 [00:20<00:48, 23.48it/s]
Training:  44%|████▍     | 707/1600 [00:30<00:38, 23.36it/s]
TraTraining loss: 1176.6103, Training accuracy: 0.7175
Macro F1-score: 0.7147
Model performance on Angry speech (in training): 
	Precision: 0.7915, Recall: 0.7875, F1_score: 0.7895
Model performance on Happy speech (in training): 
	Precision: 0.6925, Recall: 0.5575, F1_score: 0.6177
Model performance on Neutral speech (in training): 
	Precision: 0.6437, Recall: 0.7000, F1_score: 0.6707
Model performance on Sad speech (in training): 
	Precision: 0.7416, Recall: 0.8250, F1_score: 0.7811

Eval Phase: 
Validation loss: 135.9015, Validation accuracy: 0.7800
Macro F1-score: 0.7785
Model performance on Angry speech (in validation): 
	Precision: 0.9474, Recall: 0.7200, F1_score: 0.8182
Model performance on Happy speech (in validation): 
	Precision: 0.7167, Recall: 0.8600, F1_score: 0.7818
Model performance on Neutral speech (in validation): 
	Precision: 0.7442, Recall: 0.6400, F1_score: 0.6882
Model performance on Sad speech (in validation): 
	Precision: 0.7627, Recall: 0.9000, F1_score: 0.8257
New best accuracy for layer 4 on epoch 5: 0.7800. Model saved.
Epoch 6/100

Training Phase:
Training loss: 1053.3116, Training accuracy: 0.7512
Macro F1-score: 0.7502
Model performance on Angry speech (in training): 
	Precision: 0.8417, Recall: 0.7975, F1_score: 0.8190
Model performance on Happy speech (in training): 
	Precision: 0.7396, Recall: 0.6250, F1_score: 0.6775
Model performance on Neutral speech (in training): 
	Precision: 0.6689, Recall: 0.7375, F1_score: 0.7015
Model performance on Sad speech (in training): 
	Precision: 0.7647, Recall: 0.8450, F1_score: 0.8029

Eval Phase: 
ining:  59%|█████▉    | 946/1600 [00:40<00:27, 23.56it/s]
Training:  74%|███████▍  | 1185/1600 [00:50<00:17, 23.39it/s]
Training:  89%|████████▉ | 1423/1600 [01:00<00:07, 23.53it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 224/1600 [00:10<01:01, 22.36it/s]
Training:  28%|██▊       | 454/1600 [00:20<00:50, 22.70it/s]
Training:  44%|████▍     | 702/1600 [00:30<00:38, 23.62it/s]
Training:  59%|█████▉    | 950/1600 [00:40<00:27, 23.56it/s]
Training:  74%|███████▍  | 1185/1600 [00:50<00:17, 23.37it/s]
Training:  89%|████████▊ | 1417/1600 [01:00<00:07, 23.30it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                            Validation loss: 138.8527, Validation accuracy: 0.7650
Macro F1-score: 0.7653
Model performance on Angry speech (in validation): 
	Precision: 0.8696, Recall: 0.8000, F1_score: 0.8333
Model performance on Happy speech (in validation): 
	Precision: 0.6885, Recall: 0.8400, F1_score: 0.7568
Model performance on Neutral speech (in validation): 
	Precision: 0.7174, Recall: 0.6600, F1_score: 0.6875
Model performance on Sad speech (in validation): 
	Precision: 0.8085, Recall: 0.7600, F1_score: 0.7835
Epoch 7/100

Training Phase:
Training loss: 1030.1636, Training accuracy: 0.7575
Macro F1-score: 0.7559
Model performance on Angry speech (in training): 
	Precision: 0.8136, Recall: 0.8075, F1_score: 0.8105
Model performance on Happy speech (in training): 
	Precision: 0.7642, Recall: 0.6400, F1_score: 0.6966
Model performance on Neutral speech (in training): 
	Precision: 0.7026, Recall: 0.7325, F1_score: 0.7173
Model performance on Sad speech (in training): 
	Precision: 0.7539, Recall: 0.8500, F1_score: 0.7991

Eval Phase: 
Validation loss: 150.6036, Validation accuracy: 0.7650
Macro F1-score: 0.7641
Model performance on Angry speech (in validation): 
	Precision: 0.9048, Recall: 0.7600, F1_score: 0.8261
Model performance on Happy speech (in validation): 
	Precision: 0.7955, Recall: 0.7000, F1_score: 0.7447
Model performance on Neutral speech (in validation): 
	Precision: 0.7500, Recall: 0.6600, F1_score: 0.7021
Model performance on Sad speech (in validation): 
	Precision: 0.6714, Recall: 0.9400, F1_score: 0.7833
Epoch 8/100

Training Phase:
                       

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 232/1600 [00:10<00:59, 23.18it/s]
Training:  30%|██▉       | 473/1600 [00:20<00:47, 23.69it/s]
Training:  45%|████▍     | 714/1600 [00:30<00:37, 23.38it/s]
Training:  59%|█████▉    | 945/1600 [00:40<00:28, 23.25it/s]
Training:  74%|███████▎  | 1178/1600 [00:50<00:18, 23.23it/s]
Training:  88%|████████▊ | 1410/1600 [01:00<00:08, 23.07it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 234/1600 [00:10<00:58, 23.37it/s]
Training:  15%|█▍        | 234/1600 [00:20<00:58, 23.37it/s]
Training:  29%|██▉       | 468/1600 [00:20<00:48, 23.24it/s]
Training:  44%|████▍     | 700/1600 [00:30<00:38, 23.12it/s]
Training:  59%|████Training loss: 919.4520, Training accuracy: 0.7913
Macro F1-score: 0.7894
Model performance on Angry speech (in training): 
	Precision: 0.8413, Recall: 0.8350, F1_score: 0.8381
Model performance on Happy speech (in training): 
	Precision: 0.7904, Recall: 0.6600, F1_score: 0.7193
Model performance on Neutral speech (in training): 
	Precision: 0.7336, Recall: 0.7850, F1_score: 0.7585
Model performance on Sad speech (in training): 
	Precision: 0.8027, Recall: 0.8850, F1_score: 0.8419

Eval Phase: 
Validation loss: 147.3519, Validation accuracy: 0.7350
Macro F1-score: 0.7383
Model performance on Angry speech (in validation): 
	Precision: 0.9189, Recall: 0.6800, F1_score: 0.7816
Model performance on Happy speech (in validation): 
	Precision: 0.6552, Recall: 0.7600, F1_score: 0.7037
Model performance on Neutral speech (in validation): 
	Precision: 0.6441, Recall: 0.7600, F1_score: 0.6972
Model performance on Sad speech (in validation): 
	Precision: 0.8043, Recall: 0.7400, F1_score: 0.7708
Epoch 9/100

Training Phase:
Training loss: 835.1307, Training accuracy: 0.8087
Macro F1-score: 0.8073
Model performance on Angry speech (in training): 
	Precision: 0.8675, Recall: 0.8675, F1_score: 0.8675
Model performance on Happy speech (in training): 
	Precision: 0.8119, Recall: 0.6800, F1_score: 0.7401
Model performance on Neutral speech (in training): 
	Precision: 0.7361, Recall: 0.7950, F1_score: 0.7644
Model performance on Sad speech (in training): 
	Precision: 0.8245, Recall: 0.8925, F1_score: 0.8571

Eval Phase: 
█▊    | 938/1600 [00:40<00:28, 23.39it/s]
Training:  74%|███████▎  | 1176/1600 [00:50<00:18, 23.15it/s]
Training:  88%|████████▊ | 1404/1600 [01:00<00:08, 23.02it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 237/1600 [00:10<00:57, 23.65it/s]
Training:  30%|██▉       | 474/1600 [00:20<00:47, 23.67it/s]
Training:  44%|████▍     | 711/1600 [00:30<00:38, 23.38it/s]
Training:  59%|█████▉    | 942/1600 [00:40<00:28, 23.22it/s]
Training:  74%|███████▎  | 1178/1600 [00:50<00:18, 23.33it/s]
Training:  74%|███████▎  | 1178/1600 [01:00<00:18, 23.33it/s]
Training:  88%|████████▊ | 1414/1600 [01:00<00:08, 23.10it/s]
                                                             

Evaluating:   0%|         Validation loss: 155.0758, Validation accuracy: 0.7100
Macro F1-score: 0.7098
Model performance on Angry speech (in validation): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Happy speech (in validation): 
	Precision: 0.5616, Recall: 0.8200, F1_score: 0.6667
Model performance on Neutral speech (in validation): 
	Precision: 0.7073, Recall: 0.5800, F1_score: 0.6374
Model performance on Sad speech (in validation): 
	Precision: 0.7818, Recall: 0.8600, F1_score: 0.8190
Epoch 10/100

Training Phase:
Training loss: 770.4071, Training accuracy: 0.8181
Macro F1-score: 0.8168
Model performance on Angry speech (in training): 
	Precision: 0.8872, Recall: 0.8850, F1_score: 0.8861
Model performance on Happy speech (in training): 
	Precision: 0.8187, Recall: 0.7000, F1_score: 0.7547
Model performance on Neutral speech (in training): 
	Precision: 0.7560, Recall: 0.7900, F1_score: 0.7726
Model performance on Sad speech (in training): 
	Precision: 0.8141, Recall: 0.8975, F1_score: 0.8537

Eval Phase: 
Validation loss: 148.6506, Validation accuracy: 0.7500
Macro F1-score: 0.7536
Model performance on Angry speech (in validation): 
	Precision: 0.9487, Recall: 0.7400, F1_score: 0.8315
Model performance on Happy speech (in validation): 
	Precision: 0.7170, Recall: 0.7600, F1_score: 0.7379
Model performance on Neutral speech (in validation): 
	Precision: 0.6316, Recall: 0.7200, F1_score: 0.6729
Model performance on Sad speech (in validation): 
	Precision: 0.7647, Recall: 0.7800, F1_score: 0.7723
Epoch 11/100

Training Phase:
 | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 233/1600 [00:10<00:58, 23.26it/s]
Training:  29%|██▉       | 466/1600 [00:20<00:49, 22.89it/s]
Training:  43%|████▎     | 694/1600 [00:30<00:39, 22.82it/s]
Training:  58%|█████▊    | 930/1600 [00:40<00:28, 23.11it/s]
Training:  73%|███████▎  | 1166/1600 [00:50<00:18, 23.02it/s]
Training:  88%|████████▊ | 1414/1600 [01:00<00:07, 23.59it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 231/1600 [00:10<00:59, 23.06it/s]
Training:  29%|██▉       | 462/1600 [00:20<00:50, 22.68it/s]
Training:  43%|████▎     | 694/1600 [00:30<00:39, 22.88it/s]
Training:  58%|█████▊    |Training loss: 693.8000, Training accuracy: 0.8331
Macro F1-score: 0.8325
Model performance on Angry speech (in training): 
	Precision: 0.8942, Recall: 0.8875, F1_score: 0.8908
Model performance on Happy speech (in training): 
	Precision: 0.8199, Recall: 0.7400, F1_score: 0.7779
Model performance on Neutral speech (in training): 
	Precision: 0.7786, Recall: 0.8175, F1_score: 0.7976
Model performance on Sad speech (in training): 
	Precision: 0.8412, Recall: 0.8875, F1_score: 0.8637

Eval Phase: 
Validation loss: 198.1901, Validation accuracy: 0.6950
Macro F1-score: 0.6873
Model performance on Angry speech (in validation): 
	Precision: 1.0000, Recall: 0.4400, F1_score: 0.6111
Model performance on Happy speech (in validation): 
	Precision: 0.6119, Recall: 0.8200, F1_score: 0.7009
Model performance on Neutral speech (in validation): 
	Precision: 0.6226, Recall: 0.6600, F1_score: 0.6408
Model performance on Sad speech (in validation): 
	Precision: 0.7414, Recall: 0.8600, F1_score: 0.7963
Epoch 12/100

Training Phase:
Training loss: 660.3704, Training accuracy: 0.8419
Macro F1-score: 0.8410
Model performance on Angry speech (in training): 
	Precision: 0.8815, Recall: 0.8925, F1_score: 0.8870
Model performance on Happy speech (in training): 
	Precision: 0.8260, Recall: 0.7475, F1_score: 0.7848
Model performance on Neutral speech (in training): 
	Precision: 0.8078, Recall: 0.8300, F1_score: 0.8187
Model performance on Sad speech (in training): 
	Precision: 0.8507, Recall: 0.8975, F1_score: 0.8735

Eval Phase: 
Validation loss: 203.2934, Validation accuracy: 0.6950
Macro F1-score: 0.6960
Model performance on Angry speech (in validation): 
	Precision: 0.9189, Recall: 0.6800, F1_score: 0.7816
Model performance on Happy speech (in validation): 
	Precision: 0.5570, Recall: 0.8800, F1_score: 0.6822
Model performance on Neutral speech (in validation): 
	Precision: 0.6842, Recall: 0.5200, F1_score: 0.5909
Model performance on Sad speech (in validation): 
	Precision: 0.7609, Recall: 0.7000, F1_score: 0.7292
Epoch 13/100

Training Phase:
 926/1600 [00:40<00:29, 22.72it/s]
Training:  73%|███████▎  | 1164/1600 [00:50<00:18, 23.10it/s]
Training:  88%|████████▊ | 1402/1600 [01:00<00:08, 23.17it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 230/1600 [00:10<00:59, 22.97it/s]
Training:  29%|██▉       | 464/1600 [00:20<00:48, 23.19it/s]
Training:  44%|████▎     | 698/1600 [00:30<00:39, 22.93it/s]
Training:  58%|█████▊    | 930/1600 [00:40<00:29, 23.00it/s]
Training:  73%|███████▎  | 1166/1600 [00:50<00:18, 23.19it/s]
Training:  88%|████████▊ | 1414/1600 [01:00<00:07, 23.71it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training: Training loss: 574.3151, Training accuracy: 0.8638
Macro F1-score: 0.8637
Model performance on Angry speech (in training): 
	Precision: 0.9231, Recall: 0.9000, F1_score: 0.9114
Model performance on Happy speech (in training): 
	Precision: 0.8625, Recall: 0.8000, F1_score: 0.8301
Model performance on Neutral speech (in training): 
	Precision: 0.8156, Recall: 0.8625, F1_score: 0.8384
Model performance on Sad speech (in training): 
	Precision: 0.8582, Recall: 0.8925, F1_score: 0.8750

Eval Phase: 
Validation loss: 206.9221, Validation accuracy: 0.7150
Macro F1-score: 0.7126
Model performance on Angry speech (in validation): 
	Precision: 0.9355, Recall: 0.5800, F1_score: 0.7160
Model performance on Happy speech (in validation): 
	Precision: 0.6897, Recall: 0.8000, F1_score: 0.7407
Model performance on Neutral speech (in validation): 
	Precision: 0.6596, Recall: 0.6200, F1_score: 0.6392
Model performance on Sad speech (in validation): 
	Precision: 0.6719, Recall: 0.8600, F1_score: 0.7544
Epoch 14/100

Training Phase:
  0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 235/1600 [00:10<00:58, 23.46it/s]
Training:  29%|██▉       | 470/1600 [00:20<00:48, 23.40it/s]
Training:  44%|████▍     | 704/1600 [00:30<00:38, 23.15it/s]
Training:  59%|█████▉    | 940/1600 [00:40<00:28, 23.32it/s]
Training:  74%|███████▎  | 1178/1600 [00:50<00:17, 23.49it/s]
Training:  88%|████████▊ | 1416/1600 [01:00<00:07, 23.25it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 228/1600 [00:10<01:00, 22.80it/s]
Training:  28%|██▊       | 456/1600 [00:20<00:50, 22.79it/s]
Training:  43%|████▎     | 688/1600 [00:30<00:39, 22.93it/s]
Training:  58%|█████▊    | 927/1600 [00:40<00:28, 23.28it/s]
Training:  73%|███████▎  | 1166/1600 [Training loss: 532.9561, Training accuracy: 0.8694
Macro F1-score: 0.8691
Model performance on Angry speech (in training): 
	Precision: 0.9264, Recall: 0.9125, F1_score: 0.9194
Model performance on Happy speech (in training): 
	Precision: 0.8625, Recall: 0.8000, F1_score: 0.8301
Model performance on Neutral speech (in training): 
	Precision: 0.8257, Recall: 0.8525, F1_score: 0.8389
Model performance on Sad speech (in training): 
	Precision: 0.8649, Recall: 0.9125, F1_score: 0.8881

Eval Phase: 
Validation loss: 211.1961, Validation accuracy: 0.6850
Macro F1-score: 0.6818
Model performance on Angry speech (in validation): 
	Precision: 0.8889, Recall: 0.6400, F1_score: 0.7442
Model performance on Happy speech (in validation): 
	Precision: 0.5942, Recall: 0.8200, F1_score: 0.6891
Model performance on Neutral speech (in validation): 
	Precision: 0.6154, Recall: 0.4800, F1_score: 0.5393
Model performance on Sad speech (in validation): 
	Precision: 0.7143, Recall: 0.8000, F1_score: 0.7547
Epoch 15/100

Training Phase:
Training loss: 474.9396, Training accuracy: 0.8988
Macro F1-score: 0.8985
Model performance on Angry speech (in training): 
	Precision: 0.9246, Recall: 0.9200, F1_score: 0.9223
Model performance on Happy speech (in training): 
	Precision: 0.9016, Recall: 0.8475, F1_score: 0.8737
Model performance on Neutral speech (in training): 
	Precision: 0.8759, Recall: 0.8825, F1_score: 0.8792
Model performance on Sad speech (in training): 
	Precision: 0.8936, Recall: 0.9450, F1_score: 0.9186

Eval Phase: 
Validation loss: 210.0471, Validation accuracy: 0.6750
Macro F1-score: 0.6763
Model performance on Angry speech (in validation): 
	Precision: 0.8889, Recall: 0.6400, F1_score: 0.7442
Model performance on Happy speech (in validation): 
	Precision: 0.6102, Recall: 0.7200, F1_score: 0.6606
Model performance on Neutral speech (in validation): 
	Precision: 0.5833, Recall: 0.5600, F1_score: 0.5714
Model performance on Sad speech (in validation): 
	Precision: 0.6842, Recall: 0.7800, F1_score: 0.7290
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7800

Test Phase: 
00:50<00:18, 23.04it/s]
Training:  88%|████████▊ | 1410/1600 [01:00<00:08, 23.47it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 238/1600 [00:10<00:57, 23.78it/s]
Training:  30%|██▉       | 476/1600 [00:20<00:48, 23.40it/s]
Training:  44%|████▍     | 708/1600 [00:30<00:38, 23.10it/s]
Training:  59%|█████▉    | 946/1600 [00:40<00:28, 23.34it/s]
Training:  74%|███████▍  | 1187/1600 [00:50<00:17, 23.60it/s]
Training:  89%|████████▉ | 1428/1600 [01:01<00:07, 23.24it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   2%|▏         | 3/200 [00:00<00:11, 16.48it/s]
Testing:   2%|▎         | 5/200 [00:00<00:12, 16.21it/s]
Testing:   4%|▎         | 7/200 [00:00<00:11, 16.31it/s]
Testing:   5%|▌         | 10/200 [00:00<00:10, 18.33it/s]
Testing:   6%|▋         | 13/200 [00:00<00:09, 20.44it/s]
Testing:   8%|▊         | 16/200 [00:00<00:08, 21.01it/s]
Testing:  10%|▉         | 19/200 [00:00<00:07, 22.97it/s]
Testing:  11%|█         | 22/200 [00:01<00:07, 24.25it/s]
Testing:  13%|█▎        | 26/200 [00:01<00:06, 25.60it/s]
Testing:  14%|█▍        | 29/200 [00:01<00:06, 24.63it/s]
Testing:  16%|█▌        | 32/200 [00:01<00:07, 23.82it/s]
Testing:  18%|█▊        | 36/200 [00:01<00:06, 26.01it/s]
Testing:  20%|█▉        | 39/200 [00:01<00:06, 26.57it/s]
Testing:  22%|██▏       | 44/200 [00:01<00:05, 28.25it/s]
Testing:  24%|██▍       | 49/200 [00:01<00:04, 31.68it/s]
Testing:  26%|██▋       | 53/200 [00:02<00:04, 32.75it/s]
Testing:  30%|██▉       | 59/200 [00:02<00:03, 35.88it/s]
Testing:  32%|███▏      | 64/200 [00:02<00:03, 37.13it/s]
Testing:  34%|███▍      | 68/200 [00:02<00:03, 36.21it/s]
Testing:  38%|███▊      | 76/200 [00:02<00:02, 46.26it/s]
Testing:  42%|████▏     | 83/200 [00:02<00:02, 51.51it/s]
Testing:  45%|████▌     | 90/200 [00:02<00:02, 49.04it/s]
Testing:  48%|████▊     | 97/200 [00:02<00:01, 53.41it/s]
Testing:  52%|█████▏    | 104/200 [00:03<00:01, 54.63it/s]
Testing:  55%|█████▌    | 110/200 [00:03<00:01, 53.02it/s]
Testing:  60%|█████▉    | 119/200 [00:03<00:01, 61.97it/s]
Testing:  64%|██████▎   | 127/200 [00:03<00:01, 62.77it/s]
Testing:  67%|██████▋   | 134/200 [00:03<00:01, 53.16it/s]
Testing:  70%|███████   | 140/200 [00:03<00:01, 52.75it/s]
Testing:  73%|███████▎  | 146/200 [00:03<00:01, 52.47it/s]
Testing:  76%|███████▌  | 152/200 [00:03<00:00, 51.56it/s]
Testing:  80%|███████▉  | 159/200 [00:04<00:00, 54.79itTest loss: 167.2068, Test accuracy: 0.6450
Macro F1-score: 0.6378
Model performance on Angry speech (in test): 
	Precision: 0.8571, Recall: 0.7200, F1_score: 0.7826
Model performance on Happy speech (in test): 
	Precision: 0.5179, Recall: 0.5800, F1_score: 0.5472
Model performance on Neutral speech (in test): 
	Precision: 0.6667, Recall: 0.4000, F1_score: 0.5000
Model performance on Sad speech (in test): 
	Precision: 0.6111, Recall: 0.8800, F1_score: 0.7213

======================= This is fold_1 on cn =======================

Load dataset: 
Loading en train data: fold_1...
Preprocess en fold_1 data for cn model
/s]
Testing:  82%|████████▎ | 165/200 [00:04<00:00, 55.06it/s]
Testing:  86%|████████▋ | 173/200 [00:04<00:00, 61.00it/s]
Testing:  90%|█████████ | 180/200 [00:04<00:00, 54.68it/s]
Testing:  94%|█████████▍| 188/200 [00:04<00:00, 59.89it/s]
Testing:  99%|█████████▉| 198/200 [00:04<00:00, 66.92it/s]
                                                          

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:   0%|          | 4/1600 [00:00<01:00, 26.28 examples/s]
Map:   1%|          | 14/1600 [00:00<00:28, 54.84 examples/s]
Map:   2%|▏         | 28/1600 [00:00<00:19, 81.48 examples/s]
Map:   2%|▏         | 38/1600 [00:00<00:20, 77.24 examples/s]
Map:   3%|▎         | 51/1600 [00:00<00:17, 89.42 examples/s]
Map:   4%|▍         | 65/1600 [00:00<00:17, 85.71 examples/s]
Map:   5%|▍         | 78/1600 [00:00<00:16, 92.98 examples/s]
Map:   6%|▌         | 89/1600 [00:01<00:17, 87.89 examples/s]
Map:   6%|▌         | 99/1600 [00:01<00:17, 88.14 examples/s]
Map:   7%|▋         | 113/1600 [00:01<00:15, 95.93 examples/s]
Map:   8%|▊         | 128/1600 [00:01<00:16, 88.92 examples/s]
Map:   9%|▉         | 140/1600 [00:01<00:16, 90.69 examples/s]
Map:   9%|▉         | 151/1600 [00:01<00:15, 90.58 examples/s]
Map:  10%|█         | 167/1600 [00:01<00:14, 101.48 examples/s]
Map:  11%|█▏        | 180/1600 [00:01<00:13, 107.56 examples/s]
Map:  12%|█▏        | 196/1600 [00:02<00:11, 120.09 examples/s]
Map:  13%|█▎        | 211/1600 [00:02<00:11, 123.06 examples/s]
Map:  14%|█▍        | 230/1600 [00:02<00:13, 103.56 examples/s]
Map:  15%|█▌        | 247/1600 [00:02<00:11, 114.36 examples/s]
Map:  16%|█▋        | 260/1600 [00:02<00:11, 115.05 examples/s]
Map:  17%|█▋        | 276/1600 [00:02<00:10, 123.21 examples/s]
Map:  18%|█▊        | 290/1600 [00:02<00:10, 123.32 examples/s]
Map:  19%|█▉        | 304/1600 [00:02<00:10, 124.93 examples/s]
Map:  20%|██        | 321/1600 [00:03<00:10, 117.04 examples/s]
Map:  21%|██        | 335/1600 [00:03<00:10, 117.72 examples/s]
Map:  22%|██▏       | 350/1600 [00:03<00:10, 123.39 examples/s]
Map:  23%|██▎       | 363/1600 [00:03<00:10, 121.95 examples/s]
Map:  24%|██▎       | 376/1600 [00:03<00:09, 123.44 examples/s]
Map:  25%|██▍       | 393/1600 [00:03<00:09, 130.69 examples/s]
Map:  26%|██▌       | 408/1600 [00:03<00:10, 114.08 examples/s]
Map:  26%|██▋       | 424/1600 [00:04<00:11, 104.95 examples/s]
Map:  27%|██▋       | 439/1600 [00:04<00:10, 110.90 examples/s]
Map:  28%|██▊       | 452/1600 [00:04<00:10, 110.43 examples/s]
Map:  29%|██▉       | 468/1600 [00:04<00:09, 121.30 examples/s]
Map:  30%|███       | 485/1600 [00:04<00:08, 128.31 examples/s]
Map:  32%|███▏      | 506/1600 [00:04<00:09, 118.21 examples/s]
Map:  33%|███▎      | 523/1600 [00:04<00:08, 128.42 examples/s]
Map:  34%|███▍      | 542/1600 [00:04<00:08, 123.63 examples/s]
Map:  35%|███▍      | 555/1600 [00:05<00:08, 123.72 examples/s]
Map:  36%|███▌      | 576/1600 [00:05<00:08, 124.74 examples/s]
Map:  37%|███▋      | 594/1600 [00:05<00:08, 117.28 examples/s]
Map:  38%|███▊      | 612/1600 [00:05<00:09, 103.48 examples/s]
Map:  39%|███▉      | 625/1600 [00:05<00:09, 105.36 examples/s]
Map:  40%|███▉      | 638/1600 [00:05<00:08, 107.17 examples/s]
Map:  41%|████      | 652/1600 [00:05<00:08, 113.78 examples/s]
Map:  42%|████▏     | 667/1600 [00:06<00:07, 121.58 examples/s]
Map:  43%|████▎     | 682/1600 [00:06<00:08, 111.86 examples/s]
Map:  43%|████▎     | 694/1600 [00:06<00:08, 110.65 examples/s]
Map:  44%|████▍     | 708/1600 [00:06<00:09, 98.44 examples/s] 
Map:  45%|████▌     | 720/1600 [00:06<00:08, 101.60 examples/s]
Map:  46%|████▌     | 734/1600 [00:06<00:08, 103.82 examples/s]
Map:  47%|████▋     | 749/1600 [00:06<00:08, 98.31 examples/s] 
Map:  48%|████▊     | 765/1600 [00:07<00:07, 110.55 examples/s]
Map:  49%|████▉     | 782/1600 [00:07<00:06, 122.80 examples/s]
Map:  50%|████▉     | 797/1600 [00:07<00:07, 103.53 examples/s]
Map:  51%|█████     | 812/1600 [00:07<00:07, 99.90 examples/s] 
Map:  51%|█████▏    | 823/1600 [00:07<00:07, 101.02 examples/s]
Map:  52%|█████▏    | 834/1600 [00:07<00:07, 99.64 examples/s] 
Map:  53%|█████▎    | 847/1600 [00:07<00:07, 104.75 examples/s]
Map:  54%|█████▍    | 861/1600 [00:07<00:06, 109.85 examples/s]
Map:  55%|█████▍    | 879/1600 [00:08<00:07, 102.80 examples/s]
Map:  56%|█████▌    | 892/1600 [00:08<00:07, 93.44 examples/s] 
Map:  57%|█████▋    | 907/1600 [00:08<00:06, 102.35 examples/s]
Map:  58%|█████▊    | 925/1600 [00:08<00:05, 114.63 examples/s]
Map:  59%|█████▉    | 941/1600 [00:08<00:05, 124.33 examples/s]
Map:  60%|█████▉    | 954/1600 [00:08<00:05, 121.89 examples/s]
Map:  60%|██████    | 967/1600 [00:08<00:05, 106.39 examples/s]
Map:  62%|██████▏   | 984/1600 [00:09<00:05, 103.56 examples/s]
Map:  62%|██████▏   | 993/1600 [00:20<00:05, 103.56 examples/s]
Map:  62%|██████▎   | 1000/1600 [01:06<11:15,  1.13s/ examples]
Map:  64%|██████▍   | 1021/1600 [01:06<06:52,  1.40 examples/s]
Map:  65%|██████▌   | 1040/1600 [01:06<04:31,  2.07 examples/s]
Map:  66%|██████▌   | 1054/1600 [01:06<03:18,  2.75 examples/s]
Map:  67%|██████▋   | 1068/1600 [01:07<02:23,  3.71 examples/s]
Map:  68%|██████▊   | 1083/1600 [01:07<01:40,  5.13 examples/s]
Map:  69%|██████▉   | 1101/1600 [01:07<01:06,  7.50 examples/s]
Map:  70%|██████▉   | 1113/1600 [01:07<00:50,  9.66 examples/s]
Map:  71%|███████   | 1130/1600 [01:07<00:33, 13.91 examples/s]
Map:  72%|███████▏  | 1148/1600 [01:07<00:23, 19.01 examples/s]
Map:  72%|███████▎  | 1160/1600 [01:07<00:18, 23.68 examples/s]
Map:  73%|███████▎  | 1174/1600 [01:08<00:14, 30.36 examples/s]
Map:  74%|███████▍  | 1190/1600 [01:08<00:10, 40.37 examples/s]
Map:  75%|███████▌  | 1205/1600 [01:08<00:07, 51.00 examples/s]
Map:  76%|███████▌  | 1219/1600 [01:08<00:06, 57.83 examples/s]
Map:  77%|███████▋  | 1236/1600 [01:08<00:05, 67.00 examples/s]
Map:  78%|███████▊  | 1248/1600 [01:08<00:05, 65.43 examples/s]
Map:  79%|███████▉  | 1260/1600 [01:08<00:04, 72.23 examples/s]
Map:  80%|███████▉  | 1273/1600 [01:09<00:04, 70.72 examples/s]
Map:  80%|████████  | 1287/1600 [01:09<00:03, 80.11 examples/s]
Map:  82%|████████▏ | 1304/1600 [01:09<00:03, 83.42 examples/s]
Map:  82%|████████▏ | 1318/1600 [01:09<00:03, 92.20 examples/s]
Map:  84%|████████▎ | 1336/1600 [01:09<00:02, 108.76 examples/s]
Map:  85%|████████▍ | 1353/1600 [01:09<00:02, 121.76 examples/s]
Map:  86%|████████▌ | 1368/1600 [01:10<00:02, 77.74 examples/s] 
Map:  86%|████████▌ | 1379/1600 [01:10<00:02, 82.13 examples/s]
Map:  87%|████████▋ | 1395/1600 [01:10<00:02, 95.47 examples/s]
Map:  88%|████████▊ | 1413/1600 [01:10<00:01, 111.88 examples/s]
Map:  90%|████████▉ | 1436/1600 [01:10<00:01, 122.54 examples/s]
Map:  91%|█████████ | 1452/1600 [01:10<00:01, 128.51 examples/s]
Map:  92%|█████████▏| 1470/1600 [01:10<00:00, 140.16 examples/s]
Map:  93%|█████████▎| 1486/1600 [01:11<00:01, 57.18 examples/s] 
Map:  94%|█████████▍| 1502/1600 [01:11<00:01, 64.64 examples/s]
Map:  95%|█████████▍| 1516/1600 [01:11<00:01, 74.19 examples/s]
Map:  96%|█████████▌| 1529/1600 [01:11<00:00, 83.02 examples/s]
Map:  96%|█████████▋| 1544/1600 [01:12<00:00, 94.64 examples/s]
Map:  97%|█████████▋| 1558/1600 [01:12<00:00, 95.15 examples/s]
Map:  99%|█████████▉| 1580/1600 [01:12<00:00, 121.20 examples/s]
Map: 100%|█████████▉| 1599/1600 [01:12<00:00, 134.89 examples/s]
Map: 100%|█████████▉| 1599/1600 [01:30<00:00, 134.89 examples/s]
Map: 100%|██████████| 1600/1600 [01:44<00:00,  1.31 examples/s] 
Map: 100%|██████████| 1600/1600 [01:44<00:00, 15.36 examples/s]

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   3%|▎         | 6/200 [00:00<00:04, 47.70 examples/s]
Map:   8%|▊         | 17/200 [00:00<00:02, 66.71 examples/s]
Map:  14%|█▍        | 29/200 [00:00<00:02, 82.13 examples/s]
Map:  24%|██▍       | 49/200 [00:00<00:01, 118.53 examples/s]
Map:  34%|███▍      | 68/200 [00:00<00:01, 107.45 examples/s]
Map:  40%|████      | 80/200 [00:00<00:01, 109.19 examples/s]
Map:  50%|████▉     | 99/200 [00:00<00:00, 109.09 examples/s]
Map:  56%|█████▋    | 113/200 [00:01<00:00, 115.21 examples/s]
Map:  64%|██████▎   | 127/200 [00:01<00:00, 120.75 examples/s]
Map:  72%|███████▎  | 145/200 [00:01<00:00, 115.40 examples/s]
Map:  80%|███████▉  | 159/200 [00:01<00:00, 120.78 examples/s]
Map:  88%|████████▊ | 177/200 [00:01<00:00, 128.48 examples/s]
Map:  97%|█████████▋| 194/200 [00:01<00:00, 116.49 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 15.97 examples/s] 

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   4%|▍         | 8/200 [00:00<00:03, 60.37 examples/s]
Map:  13%|█▎        | 26/200 [00:00<00:01, 118.36 examples/s]
Map:  22%|██▏       | 43/200 [00:00<00:01, 126.42 examples/s]
Map:  28%|██▊       | 56/200 [00:00<00:01, 124.80 examples/s]
Map:  36%|███▌      | 71/200 [00:00<00:01, 123.30 examples/s]
Map:  45%|████▌     | 90/200 [00:00<00:00, 135.00 examples/s]
Map:  55%|█████▍    | 109/200 [00:00<00:00, 124.41 examples/s]
Map:  64%|██████▍   | 128/200 [00:01<00:00, 120.81 examples/s]
Map:  72%|███████▎  | 145/200 [00:01<00:00, 115.97 examples/s]
Map:  82%|████████▏ | 164/200 [00:01<00:00, 114.41 examples/s]
Map:  90%|████████▉ | 179/200 [00:01<00:00, 118.77 examples/s]
Map:  97%|█████████▋| 194/200 [00:01<00:00, 109.34 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 16.01 examples/s] 
Loading en eval data: fold_1...
Preprocess en fold_1 data for cn model
Loading en test data: fold_1...
Preprocess en fold_1 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1472.6250, Training accuracy: 0.6225
Macro F1-score: 0.6186
Model performance on Angry speech (in training): 
	Precision: 0.7007, Recall: 0.7375, F1_score: 0.7186
Model performance on Happy speech (in training): 
	Precision: 0.5919, Recall: 0.4750, F1_score: 0.5270
Model performance on Neutral speech (in training): 
	Precision: 0.5394, Recall: 0.5475, F1_score: 0.5434
Model performance on Sad speech (in training): 
	Precision: 0.6460, Recall: 0.7300, F1_score: 0.6854

Eval Phase: 
Validation loss: 185.4227, Validation accuracy: 0.6300
Macro F1-score: 0.6073
Model performance on Angry speech (in validation): 
	Precision: 0.5217, Recall: 0.9600, F1_score: 0.6761
Model performance on Happy speech (in validation): 
	Precision: 0.6190, Recall: 0.2600, F1_score: 0.3662
Model performance on Neutral speech (in validation): 
	Precision: 0.6429, Recall: 0.5400, F1_score: 0.5870
Model performance on Sad speech (in validation): 
	Precision: 0.8444, Recall: 0.7600, F1_score: 0.8000
New best accuracy for layer 4 on epoch 1: 0.6300. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 222/1600 [00:10<01:02, 22.14it/s]
Training:  28%|██▊       | 447/1600 [00:20<00:51, 22.32it/s]
Training:  42%|████▎     | 680/1600 [00:30<00:40, 22.74it/s]
Training:  57%|█████▋    | 913/1600 [00:40<00:29, 22.92it/s]
Training:  72%|███████▏  | 1145/1600 [00:50<00:19, 23.01it/s]
Training:  86%|████████▌ | 1377/1600 [01:00<00:09, 22.67it/s]
Training: 100%|█████████▉| 1598/1600 [01:10<00:00, 22.38it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 233/1600 [00:10<00:58, 23.22it/s]
Training:  29%|██▉       | 466/1600 [00:20<00:49, 22.85it/s]
Training:  43%|████▎     | 692/1600 [00:30<00:40, 22.52it/s]
Training:  57%|█████▋ Training loss: 1255.5854, Training accuracy: 0.6831
Macro F1-score: 0.6811
Model performance on Angry speech (in training): 
	Precision: 0.7679, Recall: 0.7525, F1_score: 0.7601
Model performance on Happy speech (in training): 
	Precision: 0.6616, Recall: 0.5425, F1_score: 0.5962
Model performance on Neutral speech (in training): 
	Precision: 0.6037, Recall: 0.6550, F1_score: 0.6283
Model performance on Sad speech (in training): 
	Precision: 0.7018, Recall: 0.7825, F1_score: 0.7400

Eval Phase: 
Validation loss: 206.6357, Validation accuracy: 0.6200
Macro F1-score: 0.5791
Model performance on Angry speech (in validation): 
	Precision: 0.5106, Recall: 0.9600, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.5714, Recall: 0.1600, F1_score: 0.2500
Model performance on Neutral speech (in validation): 
	Precision: 0.6667, Recall: 0.5200, F1_score: 0.5843
Model performance on Sad speech (in validation): 
	Precision: 0.7925, Recall: 0.8400, F1_score: 0.8155
Epoch 3/100

Training Phase:
   | 915/1600 [00:40<00:30, 22.41it/s]
Training:  71%|███████   | 1138/1600 [00:50<00:20, 22.23it/s]
Training:  85%|████████▍ | 1358/1600 [01:01<00:11, 21.77it/s]
Training:  98%|█████████▊| 1572/1600 [01:11<00:01, 21.64it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▎        | 219/1600 [00:10<01:03, 21.88it/s]
Training:  28%|██▊       | 442/1600 [00:20<00:52, 22.06it/s]
Training:  42%|████▏     | 668/1600 [00:30<00:41, 22.29it/s]
Training:  56%|█████▌    | 894/1600 [00:40<00:32, 21.84it/s]
Training:  70%|██████▉   | 1118/1600 [00:50<00:21, 21.99it/s]
Training:  85%|████████▍ | 1356/1600 [01:00<00:10, 22.56it/s]
Training:  85%|████████▍ | 1356/1600 [01:11<00:10, 22.56it/s]
Training: 100%|�Training loss: 1153.7022, Training accuracy: 0.7212
Macro F1-score: 0.7195
Model performance on Angry speech (in training): 
	Precision: 0.8077, Recall: 0.7875, F1_score: 0.7975
Model performance on Happy speech (in training): 
	Precision: 0.7287, Recall: 0.5775, F1_score: 0.6444
Model performance on Neutral speech (in training): 
	Precision: 0.6464, Recall: 0.7175, F1_score: 0.6801
Model performance on Sad speech (in training): 
	Precision: 0.7149, Recall: 0.8025, F1_score: 0.7562

Eval Phase: 
Validation loss: 202.5732, Validation accuracy: 0.6700
Macro F1-score: 0.6569
Model performance on Angry speech (in validation): 
	Precision: 0.5326, Recall: 0.9800, F1_score: 0.6901
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.3600, F1_score: 0.4675
Model performance on Neutral speech (in validation): 
	Precision: 0.7500, Recall: 0.5400, F1_score: 0.6279
Model performance on Sad speech (in validation): 
	Precision: 0.8889, Recall: 0.8000, F1_score: 0.8421
New best accuracy for layer 4 on epoch 3: 0.6700. Model saved.
Epoch 4/100

Training Phase:
Training loss: 1051.8510, Training accuracy: 0.7419
Macro F1-score: 0.7408
Model performance on Angry speech (in training): 
	Precision: 0.8407, Recall: 0.8050, F1_score: 0.8225
Model performance on Happy speech (in training): 
	Precision: 0.7431, Recall: 0.6075, F1_score: 0.6685
Model performance on Neutral speech (in training): 
	Precision: 0.6629, Recall: 0.7375, F1_score: 0.6982
Model performance on Sad speech (in training): 
	Precision: 0.7348, Recall: 0.8175, F1_score: 0.7740

Eval Phase: 
Validation loss: 253.8118, Validation accuracy: 0.5900
Macro F1-score: 0.5520
Model performance on Angry speech (in validation): 
	Precision: 0.4900, Recall: 0.9800, F1_score: 0.6533
Model performance on Happy speech (in validation): 
	Precision: 0.6154, Recall: 0.1600, F1_score: 0.2540
Model performance on Neutral speech (in validation): 
	Precision: 0.5714, Recall: 0.4800, F1_score: 0.5217
Model performance on Sad speech (in validation): 
	Precision: 0.8222, Recall: 0.7400, F1_score: 0.7789
Epoch 5/100

Training Phase:
��████████▉| 1592/1600 [01:11<00:00, 22.45it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 236/1600 [00:10<00:57, 23.53it/s]
Training:  30%|██▉       | 474/1600 [00:20<00:47, 23.65it/s]
Training:  45%|████▍     | 718/1600 [00:30<00:36, 23.98it/s]
Training:  60%|██████    | 962/1600 [00:40<00:26, 23.92it/s]
Training:  75%|███████▌  | 1201/1600 [00:50<00:16, 23.55it/s]
Training:  89%|████████▉ | 1431/1600 [01:00<00:07, 23.32it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 233/1600 [00:10<00:58, 23.21it/s]
Training:  29%|██Training loss: 951.7325, Training accuracy: 0.7788
Macro F1-score: 0.7777
Model performance on Angry speech (in training): 
	Precision: 0.8546, Recall: 0.8525, F1_score: 0.8536
Model performance on Happy speech (in training): 
	Precision: 0.7988, Recall: 0.6650, F1_score: 0.7258
Model performance on Neutral speech (in training): 
	Precision: 0.7085, Recall: 0.7475, F1_score: 0.7275
Model performance on Sad speech (in training): 
	Precision: 0.7623, Recall: 0.8500, F1_score: 0.8038

Eval Phase: 
Validation loss: 214.0048, Validation accuracy: 0.6250
Macro F1-score: 0.5961
Model performance on Angry speech (in validation): 
	Precision: 0.5714, Recall: 0.9600, F1_score: 0.7164
Model performance on Happy speech (in validation): 
	Precision: 0.5143, Recall: 0.3600, F1_score: 0.4235
Model performance on Neutral speech (in validation): 
	Precision: 0.7083, Recall: 0.3400, F1_score: 0.4595
Model performance on Sad speech (in validation): 
	Precision: 0.7368, Recall: 0.8400, F1_score: 0.7850
Epoch 6/100

Training Phase:
▉       | 466/1600 [00:20<00:49, 22.96it/s]
Training:  43%|████▎     | 694/1600 [00:30<00:39, 22.78it/s]
Training:  58%|█████▊    | 932/1600 [00:40<00:28, 23.12it/s]
Training:  73%|███████▎  | 1169/1600 [00:50<00:18, 23.15it/s]
Training:  88%|████████▊ | 1408/1600 [01:00<00:08, 23.39it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▎        | 217/1600 [00:10<01:03, 21.63it/s]
Training:  27%|██▋       | 437/1600 [00:20<00:53, 21.81it/s]
Training:  42%|████▏     | 665/1600 [00:30<00:42, 22.25it/s]
Training:  56%|█████▌    | 893/1600 [00:40<00:32, 22.06it/s]
Training:  70%|██████▉   | 1113/1600 [00:50<00:22, 22.02it/s]
Training:  83%|████████▎ | 1333/1600 [01:00<00:12, 21.74it/s]
Training:  97%|███�Training loss: 884.1335, Training accuracy: 0.7831
Macro F1-score: 0.7819
Model performance on Angry speech (in training): 
	Precision: 0.8483, Recall: 0.8525, F1_score: 0.8504
Model performance on Happy speech (in training): 
	Precision: 0.8097, Recall: 0.6700, F1_score: 0.7332
Model performance on Neutral speech (in training): 
	Precision: 0.7062, Recall: 0.7450, F1_score: 0.7251
Model performance on Sad speech (in training): 
	Precision: 0.7775, Recall: 0.8650, F1_score: 0.8189

Eval Phase: 
Validation loss: 244.4672, Validation accuracy: 0.6050
Macro F1-score: 0.5701
Model performance on Angry speech (in validation): 
	Precision: 0.5000, Recall: 1.0000, F1_score: 0.6667
Model performance on Happy speech (in validation): 
	Precision: 0.6000, Recall: 0.1800, F1_score: 0.2769
Model performance on Neutral speech (in validation): 
	Precision: 0.6098, Recall: 0.5000, F1_score: 0.5495
Model performance on Sad speech (in validation): 
	Precision: 0.8409, Recall: 0.7400, F1_score: 0.7872
Epoch 7/100

Training Phase:
Training loss: 779.7795, Training accuracy: 0.8113
Macro F1-score: 0.8112
Model performance on Angry speech (in training): 
	Precision: 0.8869, Recall: 0.8625, F1_score: 0.8745
Model performance on Happy speech (in training): 
	Precision: 0.8277, Recall: 0.7325, F1_score: 0.7772
Model performance on Neutral speech (in training): 
	Precision: 0.7471, Recall: 0.7975, F1_score: 0.7715
Model performance on Sad speech (in training): 
	Precision: 0.7930, Recall: 0.8525, F1_score: 0.8217

Eval Phase: 
��█████▋| 1559/1600 [01:10<00:01, 21.99it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 234/1600 [00:10<00:58, 23.38it/s]
Training:  29%|██▉       | 470/1600 [00:20<00:48, 23.45it/s]
Training:  44%|████▍     | 706/1600 [00:30<00:38, 23.03it/s]
Training:  59%|█████▉    | 950/1600 [00:40<00:27, 23.55it/s]
Training:  59%|█████▉    | 950/1600 [00:50<00:27, 23.55it/s]
Training:  73%|███████▎  | 1167/1600 [00:50<00:18, 22.83it/s]
Training:  73%|███████▎  | 1167/1600 [01:00<00:18, 22.83it/s]
Training:  86%|████████▋ | 1381/1600 [01:00<00:09, 22.31it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                Validation loss: 313.3462, Validation accuracy: 0.6050
Macro F1-score: 0.5647
Model performance on Angry speech (in validation): 
	Precision: 0.4949, Recall: 0.9800, F1_score: 0.6577
Model performance on Happy speech (in validation): 
	Precision: 0.5263, Recall: 0.2000, F1_score: 0.2899
Model performance on Neutral speech (in validation): 
	Precision: 0.7308, Recall: 0.3800, F1_score: 0.5000
Model performance on Sad speech (in validation): 
	Precision: 0.7679, Recall: 0.8600, F1_score: 0.8113
Epoch 8/100

Training Phase:
Training loss: 709.9205, Training accuracy: 0.8319
Macro F1-score: 0.8318
Model performance on Angry speech (in training): 
	Precision: 0.9082, Recall: 0.8900, F1_score: 0.8990
Model performance on Happy speech (in training): 
	Precision: 0.8531, Recall: 0.7550, F1_score: 0.8011
Model performance on Neutral speech (in training): 
	Precision: 0.7589, Recall: 0.8025, F1_score: 0.7801
Model performance on Sad speech (in training): 
	Precision: 0.8167, Recall: 0.8800, F1_score: 0.8472

Eval Phase: 
Validation loss: 283.3197, Validation accuracy: 0.5800
Macro F1-score: 0.5508
Model performance on Angry speech (in validation): 
	Precision: 0.4949, Recall: 0.9800, F1_score: 0.6577
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.2400, F1_score: 0.3243
Model performance on Neutral speech (in validation): 
	Precision: 0.5938, Recall: 0.3800, F1_score: 0.4634
Model performance on Sad speech (in validation): 
	Precision: 0.8000, Recall: 0.7200, F1_score: 0.7579
Epoch 9/100

Training Phase:
   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 227/1600 [00:10<01:00, 22.69it/s]
Training:  28%|██▊       | 454/1600 [00:20<00:52, 21.80it/s]
Training:  42%|████▏     | 671/1600 [00:30<00:42, 21.74it/s]
Training:  56%|█████▌    | 888/1600 [00:40<00:33, 21.56it/s]
Training:  69%|██████▉   | 1105/1600 [00:50<00:22, 21.60it/s]
Training:  83%|████████▎ | 1322/1600 [01:01<00:12, 21.45it/s]
Training:  96%|█████████▋| 1540/1600 [01:11<00:02, 21.54it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 215/1600 [00:10<01:04, 21.47it/s]
Training:  27%|██▋       | 439/1600 [00:20<00:52, 21.98it/s]
Training:  41%|████▏     | 663/1600 [00:30<00:43, 21.68it/s]
Training:  55%|█████�Training loss: 614.4405, Training accuracy: 0.8594
Macro F1-score: 0.8591
Model performance on Angry speech (in training): 
	Precision: 0.9082, Recall: 0.9150, F1_score: 0.9116
Model performance on Happy speech (in training): 
	Precision: 0.8852, Recall: 0.7900, F1_score: 0.8349
Model performance on Neutral speech (in training): 
	Precision: 0.7990, Recall: 0.8350, F1_score: 0.8166
Model performance on Sad speech (in training): 
	Precision: 0.8507, Recall: 0.8975, F1_score: 0.8735

Eval Phase: 
Validation loss: 264.6582, Validation accuracy: 0.6350
Macro F1-score: 0.6152
Model performance on Angry speech (in validation): 
	Precision: 0.5765, Recall: 0.9800, F1_score: 0.7259
Model performance on Happy speech (in validation): 
	Precision: 0.5714, Recall: 0.3200, F1_score: 0.4103
Model performance on Neutral speech (in validation): 
	Precision: 0.5581, Recall: 0.4800, F1_score: 0.5161
Model performance on Sad speech (in validation): 
	Precision: 0.8636, Recall: 0.7600, F1_score: 0.8085
Epoch 10/100

Training Phase:
�    | 886/1600 [00:40<00:32, 21.91it/s]
Training:  70%|███████   | 1121/1600 [00:50<00:21, 22.47it/s]
Training:  85%|████████▍ | 1356/1600 [01:00<00:10, 22.80it/s]
Training:  99%|█████████▉| 1591/1600 [01:10<00:00, 22.79it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 226/1600 [00:10<01:01, 22.51it/s]
Training:  28%|██▊       | 452/1600 [00:20<00:50, 22.51it/s]
Training:  42%|████▎     | 680/1600 [00:30<00:40, 22.63it/s]
Training:  57%|█████▋    | 908/1600 [00:40<00:30, 22.33it/s]
Training:  71%|███████   | 1134/1600 [00:50<00:20, 22.42it/s]
Training:  85%|████████▌ | 1360/1600 [01:00<00:10, 22.47it/s]
Training:  99%|█████████▉| 1586/1600 [01:10<00:00, 22.46it/s]
            Training loss: 552.8746, Training accuracy: 0.8700
Macro F1-score: 0.8703
Model performance on Angry speech (in training): 
	Precision: 0.9313, Recall: 0.9150, F1_score: 0.9231
Model performance on Happy speech (in training): 
	Precision: 0.8970, Recall: 0.8275, F1_score: 0.8609
Model performance on Neutral speech (in training): 
	Precision: 0.8019, Recall: 0.8400, F1_score: 0.8205
Model performance on Sad speech (in training): 
	Precision: 0.8568, Recall: 0.8975, F1_score: 0.8767

Eval Phase: 
Validation loss: 332.7243, Validation accuracy: 0.6000
Macro F1-score: 0.5674
Model performance on Angry speech (in validation): 
	Precision: 0.5402, Recall: 0.9400, F1_score: 0.6861
Model performance on Happy speech (in validation): 
	Precision: 0.5000, Recall: 0.2600, F1_score: 0.3421
Model performance on Neutral speech (in validation): 
	Precision: 0.6333, Recall: 0.3800, F1_score: 0.4750
Model performance on Sad speech (in validation): 
	Precision: 0.7193, Recall: 0.8200, F1_score: 0.7664
Epoch 11/100

Training Phase:
Training loss: 515.5850, Training accuracy: 0.8750
Macro F1-score: 0.8750
Model performance on Angry speech (in training): 
	Precision: 0.9242, Recall: 0.9150, F1_score: 0.9196
Model performance on Happy speech (in training): 
	Precision: 0.8859, Recall: 0.8350, F1_score: 0.8597
Model performance on Neutral speech (in training): 
	Precision: 0.8366, Recall: 0.8450, F1_score: 0.8408
Model performance on Sad speech (in training): 
	Precision: 0.8558, Recall: 0.9050, F1_score: 0.8797

Eval Phase: 
Validation loss: 456.3712, Validation accuracy: 0.5400
Macro F1-score: 0.5047
Model performance on Angry speech (in validation): 
	Precision: 0.4587, Recall: 1.0000, F1_score: 0.6289
Model performance on Happy speech (in validation): 
	Precision: 0.5333, Recall: 0.1600, F1_score: 0.2462
Model performance on Neutral speech (in validation): 
	Precision: 0.5122, Recall: 0.4200, F1_score: 0.4615
Model performance on Sad speech (in validation): 
	Precision: 0.8286, Recall: 0.5800, F1_score: 0.6824
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6700

Test Phase: 
                                                 

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 221/1600 [00:10<01:02, 22.03it/s]
Training:  28%|██▊       | 444/1600 [00:20<00:52, 22.16it/s]
Training:  42%|████▏     | 667/1600 [00:30<00:43, 21.67it/s]
Training:  55%|█████▌    | 887/1600 [00:40<00:32, 21.78it/s]
Training:  69%|██████▉   | 1109/1600 [00:50<00:22, 21.92it/s]
Training:  83%|████████▎ | 1331/1600 [01:00<00:12, 21.84it/s]
Training:  97%|█████████▋| 1548/1600 [01:11<00:02, 21.49it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   6%|▌         | 11/200 [00:00<00:01, 105.08it/s]
Testing:  11%|█         | 22/200 [00:00<00:01, 95.02it/s] 
Testing:  16%|█▌        | 32/200 [00:00<00:01, 91.33it/s]
Testing:  21%|██        | 42/200 [00:00<00:01, 92.83it/s]
Testing:  26%|██▌       | 52/200 [00:00<00:01, 93.49it/s]
Testing:  31%|███       | 62/200 [00:00<00:01, 94.55it/s]
Testing:  36%|███▌      | 72/200 [00:00<00:01, 95.45it/s]
Testing:  41%|████      | 82/200 [00:00<00:01, 89.63it/s]
Testing:  46%|████▌     | 92/200 [00:00<00:01, 91.46it/s]
Testing:  51%|█████     | 102/200 [00:01<00:01, 90.71it/s]
Testing:  56%|█████▌    | 112/200 [00:01<00:00, 91.96it/s]
Testing:  62%|██████▏   | 123/200 [00:01<00:00, 94.81it/s]
Testing:  67%|██████▋   | 134/200 [00:01<00:00, 97.15it/s]
Testing:  72%|███████▏  | 144/200 [00:01<00:00, 95.79it/s]
Testing:  77%|███████▋  | 154/200 [00:01<00:00, 92.85it/s]
Testing:  82%|████████▏ | 164/200 [00:01<00:00, 91.07it/s]
Testing:  87%|███Test loss: 208.6962, Test accuracy: 0.5950
Macro F1-score: 0.5711
Model performance on Angry speech (in test): 
	Precision: 0.5222, Recall: 0.9400, F1_score: 0.6714
Model performance on Happy speech (in test): 
	Precision: 0.5714, Recall: 0.2400, F1_score: 0.3380
Model performance on Neutral speech (in test): 
	Precision: 0.5918, Recall: 0.5800, F1_score: 0.5859
Model performance on Sad speech (in test): 
	Precision: 0.7750, Recall: 0.6200, F1_score: 0.6889

======================= This is fold_2 on cn =======================

Load dataset: 
Loading en train data: fold_2...
Preprocess en fold_2 data for cn model
█████▋ | 174/200 [00:01<00:00, 89.22it/s]
Testing:  92%|█████████▎| 185/200 [00:01<00:00, 92.77it/s]
Testing:  98%|█████████▊| 196/200 [00:02<00:00, 96.17it/s]
                                                          

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:   0%|          | 4/1600 [00:00<00:51, 30.97 examples/s]
Map:   1%|          | 16/1600 [00:00<00:21, 72.27 examples/s]
Map:   2%|▏         | 30/1600 [00:00<00:16, 94.82 examples/s]
Map:   3%|▎         | 46/1600 [00:00<00:16, 97.03 examples/s]
Map:   4%|▍         | 60/1600 [00:00<00:17, 87.90 examples/s]
Map:   4%|▍         | 71/1600 [00:00<00:16, 90.67 examples/s]
Map:   5%|▌         | 84/1600 [00:00<00:16, 94.35 examples/s]
Map:   6%|▌         | 95/1600 [00:01<00:19, 78.73 examples/s]
Map:   7%|▋         | 105/1600 [00:01<00:18, 80.11 examples/s]
Map:   8%|▊         | 120/1600 [00:01<00:15, 95.15 examples/s]
Map:   8%|▊         | 133/1600 [00:01<00:14, 98.45 examples/s]
Map:   9%|▉         | 145/1600 [00:01<00:14, 100.88 examples/s]
Map:  10%|▉         | 156/1600 [00:01<00:14, 101.23 examples/s]
Map:  11%|█         | 170/1600 [00:01<00:13, 107.82 examples/s]
Map:  11%|█▏        | 183/1600 [00:01<00:12, 110.78 examples/s]
Map:  12%|█▏        | 195/1600 [00:02<00:13, 100.91 examples/s]
Map:  13%|█▎        | 210/1600 [00:02<00:12, 109.34 examples/s]
Map:  14%|█▍        | 227/1600 [00:02<00:12, 108.54 examples/s]
Map:  15%|█▌        | 243/1600 [00:02<00:11, 116.76 examples/s]
Map:  16%|█▌        | 257/1600 [00:02<00:11, 120.10 examples/s]
Map:  17%|█▋        | 272/1600 [00:02<00:10, 122.06 examples/s]
Map:  18%|█▊        | 290/1600 [00:02<00:11, 109.80 examples/s]
Map:  19%|█▉        | 306/1600 [00:03<00:11, 116.70 examples/s]
Map:  20%|██        | 324/1600 [00:03<00:10, 127.12 examples/s]
Map:  22%|██▏       | 344/1600 [00:03<00:09, 127.69 examples/s]
Map:  22%|██▏       | 358/1600 [00:03<00:09, 127.78 examples/s]
Map:  23%|██▎       | 371/1600 [00:03<00:10, 121.22 examples/s]
Map:  24%|██▍       | 390/1600 [00:03<00:10, 119.32 examples/s]
Map:  26%|██▌       | 408/1600 [00:03<00:10, 112.22 examples/s]
Map:  26%|██▋       | 424/1600 [00:04<00:10, 109.20 examples/s]
Map:  27%|██▋       | 438/1600 [00:04<00:10, 115.12 examples/s]
Map:  28%|██▊       | 455/1600 [00:04<00:08, 127.31 examples/s]
Map:  30%|██▉       | 473/1600 [00:04<00:08, 140.26 examples/s]
Map:  31%|███       | 495/1600 [00:04<00:08, 127.02 examples/s]
Map:  32%|███▏      | 511/1600 [00:04<00:09, 117.44 examples/s]
Map:  33%|███▎      | 528/1600 [00:04<00:09, 111.55 examples/s]
Map:  34%|███▍      | 545/1600 [00:05<00:09, 108.52 examples/s]
Map:  35%|███▌      | 564/1600 [00:05<00:08, 123.00 examples/s]
Map:  36%|███▋      | 581/1600 [00:05<00:08, 116.82 examples/s]
Map:  37%|███▋      | 599/1600 [00:05<00:07, 126.22 examples/s]
Map:  38%|███▊      | 613/1600 [00:05<00:07, 126.96 examples/s]
Map:  39%|███▉      | 629/1600 [00:05<00:07, 131.64 examples/s]
Map:  40%|████      | 644/1600 [00:05<00:07, 129.02 examples/s]
Map:  41%|████      | 659/1600 [00:05<00:07, 130.27 examples/s]
Map:  42%|████▏     | 674/1600 [00:06<00:08, 104.34 examples/s]
Map:  43%|████▎     | 689/1600 [00:06<00:09, 100.83 examples/s]
Map:  44%|████▍     | 700/1600 [00:06<00:08, 100.17 examples/s]
Map:  45%|████▌     | 720/1600 [00:06<00:07, 122.08 examples/s]
Map:  46%|████▋     | 741/1600 [00:06<00:06, 125.52 examples/s]
Map:  47%|████▋     | 758/1600 [00:06<00:07, 116.66 examples/s]
Map:  48%|████▊     | 773/1600 [00:07<00:15, 54.29 examples/s] 
Map:  49%|████▉     | 785/1600 [00:07<00:13, 60.89 examples/s]
Map:  50%|█████     | 800/1600 [00:07<00:11, 66.87 examples/s]
Map:  51%|█████     | 810/1600 [00:07<00:11, 70.42 examples/s]
Map:  51%|█████▏    | 823/1600 [00:07<00:09, 78.41 examples/s]
Map:  52%|█████▏    | 834/1600 [00:08<00:09, 83.43 examples/s]
Map:  53%|█████▎    | 847/1600 [00:08<00:08, 91.95 examples/s]
Map:  54%|█████▍    | 861/1600 [00:08<00:07, 100.13 examples/s]
Map:  55%|█████▍    | 874/1600 [00:08<00:08, 82.00 examples/s] 
Map:  55%|█████▌    | 886/1600 [00:08<00:07, 89.54 examples/s]
Map:  56%|█████▋    | 900/1600 [00:08<00:07, 99.44 examples/s]
Map:  57%|█████▋    | 918/1600 [00:08<00:06, 111.22 examples/s]
Map:  58%|█████▊    | 934/1600 [00:08<00:05, 122.18 examples/s]
Map:  59%|█████▉    | 947/1600 [00:09<00:05, 121.86 examples/s]
Map:  60%|██████    | 960/1600 [00:09<00:06, 99.48 examples/s] 
Map:  61%|██████    | 971/1600 [00:09<00:06, 101.08 examples/s]
Map:  62%|██████▏   | 984/1600 [00:09<00:06, 94.88 examples/s] 
Map:  62%|██████▏   | 997/1600 [00:09<00:06, 89.53 examples/s]
Map:  62%|██████▏   | 997/1600 [00:26<00:06, 89.53 examples/s]
Map:  62%|██████▎   | 1000/1600 [01:07<17:23,  1.74s/ examples]
Map:  64%|██████▍   | 1021/1600 [01:07<08:52,  1.09 examples/s]
Map:  65%|██████▌   | 1040/1600 [01:07<05:20,  1.75 examples/s]
Map:  66%|██████▌   | 1054/1600 [01:07<03:44,  2.43 examples/s]
Map:  67%|██████▋   | 1072/1600 [01:08<02:24,  3.65 examples/s]
Map:  68%|██████▊   | 1088/1600 [01:08<01:39,  5.16 examples/s]
Map:  69%|██████▉   | 1101/1600 [01:08<01:12,  6.86 examples/s]
Map:  70%|██████▉   | 1113/1600 [01:08<00:53,  9.03 examples/s]
Map:  71%|███████   | 1130/1600 [01:08<00:35, 13.29 examples/s]
Map:  72%|███████▏  | 1148/1600 [01:08<00:24, 18.41 examples/s]
Map:  72%|███████▎  | 1160/1600 [01:08<00:19, 23.13 examples/s]
Map:  73%|███████▎  | 1174/1600 [01:09<00:14, 29.91 examples/s]
Map:  74%|███████▍  | 1190/1600 [01:09<00:10, 39.99 examples/s]
Map:  75%|███████▌  | 1205/1600 [01:09<00:07, 50.77 examples/s]
Map:  76%|███████▌  | 1219/1600 [01:09<00:06, 57.70 examples/s]
Map:  77%|███████▋  | 1231/1600 [01:09<00:05, 64.26 examples/s]
Map:  78%|███████▊  | 1248/1600 [01:09<00:05, 66.74 examples/s]
Map:  79%|███████▉  | 1260/1600 [01:09<00:04, 73.25 examples/s]
Map:  80%|███████▉  | 1273/1600 [01:10<00:04, 71.97 examples/s]
Map:  80%|████████  | 1285/1600 [01:10<00:03, 80.56 examples/s]
Map:  81%|████████  | 1297/1600 [01:10<00:03, 78.11 examples/s]
Map:  82%|████████▏ | 1313/1600 [01:10<00:03, 91.87 examples/s]
Map:  83%|████████▎ | 1330/1600 [01:10<00:02, 106.46 examples/s]
Map:  84%|████████▍ | 1347/1600 [01:10<00:02, 119.58 examples/s]
Map:  85%|████████▌ | 1362/1600 [01:10<00:02, 105.90 examples/s]
Map:  86%|████████▌ | 1379/1600 [01:11<00:02, 105.23 examples/s]
Map:  87%|████████▋ | 1395/1600 [01:11<00:01, 114.44 examples/s]
Map:  88%|████████▊ | 1409/1600 [01:11<00:01, 119.18 examples/s]
Map:  89%|████████▉ | 1424/1600 [01:11<00:01, 121.73 examples/s]
Map:  90%|█████████ | 1444/1600 [01:11<00:01, 135.41 examples/s]
Map:  91%|█████████▏| 1462/1600 [01:11<00:00, 141.90 examples/s]
Map:  93%|█████████▎| 1482/1600 [01:11<00:00, 125.06 examples/s]
Map:  94%|█████████▍| 1502/1600 [01:12<00:00, 118.22 examples/s]
Map:  95%|█████████▍| 1516/1600 [01:12<00:00, 120.11 examples/s]
Map:  96%|█████████▌| 1529/1600 [01:12<00:00, 121.42 examples/s]
Map:  96%|█████████▋| 1544/1600 [01:12<00:00, 126.83 examples/s]
Map:  98%|█████████▊| 1561/1600 [01:12<00:00, 135.13 examples/s]
Map:  99%|█████████▉| 1583/1600 [01:12<00:00, 154.60 examples/s]
Map: 100%|█████████▉| 1596/1600 [01:26<00:00, 154.60 examples/s]
Map: 100%|██████████| 1600/1600 [01:45<00:00,  1.78 examples/s] 
Map: 100%|██████████| 1600/1600 [01:45<00:00, 15.19 examples/s]

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▏         | 4/200 [00:00<00:07, 24.96 examples/s]
Map:   6%|▌         | 11/200 [00:00<00:04, 44.29 examples/s]
Map:  12%|█▎        | 25/200 [00:00<00:02, 78.14 examples/s]
Map:  19%|█▉        | 38/200 [00:00<00:01, 94.70 examples/s]
Map:  26%|██▋       | 53/200 [00:00<00:01, 106.96 examples/s]
Map:  34%|███▍      | 68/200 [00:00<00:01, 115.54 examples/s]
Map:  40%|████      | 80/200 [00:00<00:01, 115.67 examples/s]
Map:  46%|████▌     | 92/200 [00:00<00:00, 113.80 examples/s]
Map:  52%|█████▎    | 105/200 [00:01<00:00, 95.70 examples/s]
Map:  60%|██████    | 120/200 [00:01<00:00, 107.02 examples/s]
Map:  66%|██████▋   | 133/200 [00:01<00:00, 111.57 examples/s]
Map:  75%|███████▌  | 150/200 [00:01<00:00, 110.84 examples/s]
Map:  81%|████████  | 162/200 [00:01<00:00, 110.64 examples/s]
Map:  88%|████████▊ | 176/200 [00:01<00:00, 116.55 examples/s]
Map:  96%|█████████▌| 192/200 [00:01<00:00, 123.27 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 16.19 examples/s] 

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   4%|▎         | 7/200 [00:00<00:03, 60.04 examples/s]
Map:  10%|▉         | 19/200 [00:00<00:02, 88.63 examples/s]
Map:  17%|█▋        | 34/200 [00:00<00:01, 106.03 examples/s]
Map:  24%|██▍       | 49/200 [00:00<00:01, 120.10 examples/s]
Map:  32%|███▎      | 65/200 [00:00<00:01, 129.68 examples/s]
Map:  40%|████      | 80/200 [00:00<00:00, 129.45 examples/s]
Map:  50%|█████     | 101/200 [00:00<00:00, 126.04 examples/s]
Map:  60%|█████▉    | 119/200 [00:01<00:00, 109.43 examples/s]
Map:  68%|██████▊   | 136/200 [00:01<00:00, 108.43 examples/s]
Map:  74%|███████▍  | 149/200 [00:01<00:00, 108.80 examples/s]
Map:  82%|████████▏ | 164/200 [00:01<00:00, 100.80 examples/s]
Map:  89%|████████▉ | 178/200 [00:01<00:00, 106.45 examples/s]
Map:  96%|█████████▌| 192/200 [00:01<00:00, 111.44 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 16.08 examples/s] 
Loading en eval data: fold_2...
Preprocess en fold_2 data for cn model
Loading en test data: fold_2...
Preprocess en fold_2 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1324.9468, Training accuracy: 0.6550
Macro F1-score: 0.6519
Model performance on Angry speech (in training): 
	Precision: 0.7153, Recall: 0.7475, F1_score: 0.7311
Model performance on Happy speech (in training): 
	Precision: 0.5910, Recall: 0.5275, F1_score: 0.5575
Model performance on Neutral speech (in training): 
	Precision: 0.5948, Recall: 0.5725, F1_score: 0.5834
Model performance on Sad speech (in training): 
	Precision: 0.7023, Recall: 0.7725, F1_score: 0.7357

Eval Phase: 
Validation loss: 226.3812, Validation accuracy: 0.6050
Macro F1-score: 0.5756
Model performance on Angry speech (in validation): 
	Precision: 0.6852, Recall: 0.7400, F1_score: 0.7115
Model performance on Happy speech (in validation): 
	Precision: 1.0000, Recall: 0.2200, F1_score: 0.3607
Model performance on Neutral speech (in validation): 
	Precision: 0.6842, Recall: 0.5200, F1_score: 0.5909
Model performance on Sad speech (in validation): 
	Precision: 0.4845, Recall: 0.9400, F1_score: 0.6395
New best accuracy for layer 4 on epoch 1: 0.6050. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 205/1600 [00:10<01:08, 20.45it/s]
Training:  26%|██▌       | 410/1600 [00:20<00:58, 20.31it/s]
Training:  39%|███▉      | 623/1600 [00:30<00:47, 20.71it/s]
Training:  52%|█████▎    | 840/1600 [00:40<00:36, 21.09it/s]
Training:  66%|██████▌   | 1057/1600 [00:50<00:26, 20.81it/s]
Training:  79%|███████▉  | 1261/1600 [01:01<00:16, 20.56it/s]
Training:  92%|█████████▏| 1474/1600 [01:11<00:06, 20.79it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 213/1600 [00:10<01:05, 21.29it/s]
Training:  27%|██▋       | 426/1600 [00:20<00:56, 20.84it/s]
Training:  40%|███▉      | 632/1600 [00:30<00:47, 20.52it/s]
Training:  52%|█████▏    | 839Training loss: 1157.6506, Training accuracy: 0.7050
Macro F1-score: 0.7034
Model performance on Angry speech (in training): 
	Precision: 0.7910, Recall: 0.7950, F1_score: 0.7930
Model performance on Happy speech (in training): 
	Precision: 0.6676, Recall: 0.5875, F1_score: 0.6250
Model performance on Neutral speech (in training): 
	Precision: 0.6174, Recall: 0.6375, F1_score: 0.6273
Model performance on Sad speech (in training): 
	Precision: 0.7390, Recall: 0.8000, F1_score: 0.7683

Eval Phase: 
Validation loss: 183.7148, Validation accuracy: 0.6800
Macro F1-score: 0.6657
Model performance on Angry speech (in validation): 
	Precision: 0.7843, Recall: 0.8000, F1_score: 0.7921
Model performance on Happy speech (in validation): 
	Precision: 0.9000, Recall: 0.3600, F1_score: 0.5143
Model performance on Neutral speech (in validation): 
	Precision: 0.6471, Recall: 0.6600, F1_score: 0.6535
Model performance on Sad speech (in validation): 
	Precision: 0.5769, Recall: 0.9000, F1_score: 0.7031
New best accuracy for layer 4 on epoch 2: 0.6800. Model saved.
Epoch 3/100

Training Phase:
/1600 [00:40<00:36, 20.59it/s]
Training:  66%|██████▌   | 1056/1600 [00:50<00:25, 20.97it/s]
Training:  80%|███████▉  | 1277/1600 [01:00<00:15, 21.32it/s]
Training:  94%|█████████▎| 1498/1600 [01:11<00:04, 20.91it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 213/1600 [00:10<01:05, 21.27it/s]
Training:  27%|██▋       | 428/1600 [00:20<00:54, 21.40it/s]
Training:  41%|████      | 653/1600 [00:30<00:43, 21.89it/s]
Training:  55%|█████▍    | 878/1600 [00:40<00:33, 21.59it/s]
Training:  69%|██████▉   | 1102/1600 [00:50<00:22, 21.87it/s]
Training:  83%|████████▎ | 1326/1600 [01:00<00:12, 21.91it/s]
Training:  97%|█████████▋| 1551/1600 [01:10<00:02, 22.09it/s]
                          Training loss: 1066.0762, Training accuracy: 0.7294
Macro F1-score: 0.7268
Model performance on Angry speech (in training): 
	Precision: 0.8005, Recall: 0.8225, F1_score: 0.8113
Model performance on Happy speech (in training): 
	Precision: 0.6936, Recall: 0.6000, F1_score: 0.6434
Model performance on Neutral speech (in training): 
	Precision: 0.6617, Recall: 0.6600, F1_score: 0.6608
Model performance on Sad speech (in training): 
	Precision: 0.7523, Recall: 0.8350, F1_score: 0.7915

Eval Phase: 
Validation loss: 235.1671, Validation accuracy: 0.6100
Macro F1-score: 0.5925
Model performance on Angry speech (in validation): 
	Precision: 0.7551, Recall: 0.7400, F1_score: 0.7475
Model performance on Happy speech (in validation): 
	Precision: 0.8421, Recall: 0.3200, F1_score: 0.4638
Model performance on Neutral speech (in validation): 
	Precision: 0.6471, Recall: 0.4400, F1_score: 0.5238
Model performance on Sad speech (in validation): 
	Precision: 0.4796, Recall: 0.9400, F1_score: 0.6351
Epoch 4/100

Training Phase:
Training loss: 957.4348, Training accuracy: 0.7675
Macro F1-score: 0.7658
Model performance on Angry speech (in training): 
	Precision: 0.8391, Recall: 0.8475, F1_score: 0.8433
Model performance on Happy speech (in training): 
	Precision: 0.7529, Recall: 0.6400, F1_score: 0.6919
Model performance on Neutral speech (in training): 
	Precision: 0.6929, Recall: 0.7275, F1_score: 0.7098
Model performance on Sad speech (in training): 
	Precision: 0.7844, Recall: 0.8550, F1_score: 0.8182

Eval Phase: 
Validation loss: 184.0831, Validation accuracy: 0.6550
Macro F1-score: 0.6517
Model performance on Angry speech (in validation): 
	Precision: 0.8043, Recall: 0.7400, F1_score: 0.7708
Model performance on Happy speech (in validation): 
	Precision: 0.7097, Recall: 0.4400, F1_score: 0.5432
Model performance on Neutral speech (in validation): 
	Precision: 0.5469, Recall: 0.7000, F1_score: 0.6140
Model performance on Sad speech (in validation): 
	Precision: 0.6271, Recall: 0.7400, F1_score: 0.6789
Epoch 5/100

Training Phase:
                                   

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▌        | 242/1600 [00:10<00:56, 24.17it/s]
Training:  30%|███       | 484/1600 [00:20<00:48, 23.20it/s]
Training:  44%|████▍     | 712/1600 [00:30<00:38, 22.95it/s]
Training:  60%|█████▉    | 952/1600 [00:40<00:27, 23.34it/s]
Training:  74%|███████▍  | 1192/1600 [00:51<00:17, 23.27it/s]
Training:  89%|████████▉ | 1429/1600 [01:01<00:07, 23.36it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 230/1600 [00:10<00:59, 22.94it/s]
Training:  29%|██▉       | 460/1600 [00:20<00:50, 22.53it/s]
Training:  29%|██▉       | 460/1600 [00:Training loss: 869.7152, Training accuracy: 0.7881
Macro F1-score: 0.7868
Model performance on Angry speech (in training): 
	Precision: 0.8475, Recall: 0.8475, F1_score: 0.8475
Model performance on Happy speech (in training): 
	Precision: 0.7882, Recall: 0.6700, F1_score: 0.7243
Model performance on Neutral speech (in training): 
	Precision: 0.7160, Recall: 0.7625, F1_score: 0.7385
Model performance on Sad speech (in training): 
	Precision: 0.8041, Recall: 0.8725, F1_score: 0.8369

Eval Phase: 
Validation loss: 185.4336, Validation accuracy: 0.6600
Macro F1-score: 0.6571
Model performance on Angry speech (in validation): 
	Precision: 0.7222, Recall: 0.7800, F1_score: 0.7500
Model performance on Happy speech (in validation): 
	Precision: 0.7353, Recall: 0.5000, F1_score: 0.5952
Model performance on Neutral speech (in validation): 
	Precision: 0.5714, Recall: 0.6400, F1_score: 0.6038
Model performance on Sad speech (in validation): 
	Precision: 0.6429, Recall: 0.7200, F1_score: 0.6792
Epoch 6/100

Training Phase:
31<00:50, 22.53it/s]
Training:  39%|███▉      | 628/1600 [00:31<00:50, 19.20it/s]
Training:  54%|█████▍    | 860/1600 [00:41<00:35, 20.72it/s]
Training:  68%|██████▊   | 1092/1600 [00:51<00:23, 21.24it/s]
Training:  83%|████████▎ | 1323/1600 [01:01<00:12, 21.85it/s]
Training:  97%|█████████▋| 1554/1600 [01:11<00:02, 22.16it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 224/1600 [00:10<01:01, 22.33it/s]
Training:  28%|██▊       | 448/1600 [00:20<00:52, 21.98it/s]
Training:  43%|████▎     | 689/1600 [00:30<00:39, 22.92it/s]
Training:  58%|█████▊    | 930/1600 [00:41<00:29, 22.80it/s]
Training:  72%|███████▎  | 1160/1600 [00:51<00:19, 22.86it/s]
Training:  87%|████████▋ | 1391/Training loss: 750.5452, Training accuracy: 0.8187
Macro F1-score: 0.8182
Model performance on Angry speech (in training): 
	Precision: 0.8838, Recall: 0.8750, F1_score: 0.8794
Model performance on Happy speech (in training): 
	Precision: 0.8172, Recall: 0.7375, F1_score: 0.7753
Model performance on Neutral speech (in training): 
	Precision: 0.7464, Recall: 0.7725, F1_score: 0.7592
Model performance on Sad speech (in training): 
	Precision: 0.8298, Recall: 0.8900, F1_score: 0.8589

Eval Phase: 
Validation loss: 233.1216, Validation accuracy: 0.6450
Macro F1-score: 0.6349
Model performance on Angry speech (in validation): 
	Precision: 0.7826, Recall: 0.7200, F1_score: 0.7500
Model performance on Happy speech (in validation): 
	Precision: 0.9000, Recall: 0.3600, F1_score: 0.5143
Model performance on Neutral speech (in validation): 
	Precision: 0.5614, Recall: 0.6400, F1_score: 0.5981
Model performance on Sad speech (in validation): 
	Precision: 0.5584, Recall: 0.8600, F1_score: 0.6772
Epoch 7/100

Training Phase:
Training loss: 666.3395, Training accuracy: 0.8425
Macro F1-score: 0.8419
Model performance on Angry speech (in training): 
	Precision: 0.8936, Recall: 0.9025, F1_score: 0.8980
Model performance on Happy speech (in training): 
	Precision: 0.8356, Recall: 0.7750, F1_score: 0.8042
Model performance on Neutral speech (in training): 
	Precision: 0.7950, Recall: 0.7950, F1_score: 0.7950
Model performance on Sad speech (in training): 
	Precision: 0.8447, Recall: 0.8975, F1_score: 0.8703

Eval Phase: 
Validation loss: 239.1773, Validation accuracy: 0.6250
Macro F1-score: 0.6143
Model performance on Angry speech (in validation): 
	Precision: 0.8182, Recall: 0.7200, F1_score: 0.7660
Model performance on Happy speech (in validation): 
	Precision: 0.8824, Recall: 0.3000, F1_score: 0.4478
Model performance on Neutral speech (in validation): 
	Precision: 0.4805, Recall: 0.7400, F1_score: 0.5827
Model performance on Sad speech (in validation): 
	Precision: 0.5968, Recall: 0.7400, F1_score: 0.6607
Epoch 8/100

Training Phase:
1600 [01:01<00:09, 22.93it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 233/1600 [00:10<00:59, 23.15it/s]
Training:  29%|██▉       | 468/1600 [00:20<00:48, 23.34it/s]
Training:  44%|████▍     | 703/1600 [00:30<00:38, 23.14it/s]
Training:  59%|█████▉    | 944/1600 [00:40<00:27, 23.51it/s]
Training:  74%|███████▍  | 1185/1600 [00:50<00:17, 23.60it/s]
Training:  89%|████████▉ | 1423/1600 [01:00<00:07, 23.54it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▌        | 241/1600 [00:10<00:56, 24.08it/s]
Training:  30%|███       | 482/1600 [00:20<00:47, 2Training loss: 606.5162, Training accuracy: 0.8575
Macro F1-score: 0.8571
Model performance on Angry speech (in training): 
	Precision: 0.8936, Recall: 0.9025, F1_score: 0.8980
Model performance on Happy speech (in training): 
	Precision: 0.8676, Recall: 0.8025, F1_score: 0.8338
Model performance on Neutral speech (in training): 
	Precision: 0.8144, Recall: 0.8225, F1_score: 0.8184
Model performance on Sad speech (in training): 
	Precision: 0.8555, Recall: 0.9025, F1_score: 0.8783

Eval Phase: 
Validation loss: 228.8298, Validation accuracy: 0.6300
Macro F1-score: 0.6299
Model performance on Angry speech (in validation): 
	Precision: 0.8085, Recall: 0.7600, F1_score: 0.7835
Model performance on Happy speech (in validation): 
	Precision: 0.8077, Recall: 0.4200, F1_score: 0.5526
Model performance on Neutral speech (in validation): 
	Precision: 0.4776, Recall: 0.6400, F1_score: 0.5470
Model performance on Sad speech (in validation): 
	Precision: 0.5833, Recall: 0.7000, F1_score: 0.6364
Epoch 9/100

Training Phase:
3.38it/s]
Training:  45%|████▍     | 717/1600 [00:30<00:37, 23.41it/s]
Training:  60%|█████▉    | 953/1600 [00:40<00:27, 23.48it/s]
Training:  74%|███████▍  | 1189/1600 [00:50<00:17, 23.40it/s]
Training:  89%|████████▉ | 1422/1600 [01:00<00:07, 23.28it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▌        | 242/1600 [00:10<00:56, 24.20it/s]
Training:  30%|███       | 484/1600 [00:20<00:46, 24.10it/s]
Training:  45%|████▌     | 725/1600 [00:30<00:36, 23.93it/s]
Training:  60%|██████    | 963/1600 [00:40<00:27, 23.47it/s]
Training:  74%|███████▍  | 1191/1600 [00:50<00:17, 23.18it/s]
Training:  89%|████████▉ | 1426/1600 [01:00<00:07, 23.28it/s]
                                                           Training loss: 535.3992, Training accuracy: 0.8694
Macro F1-score: 0.8692
Model performance on Angry speech (in training): 
	Precision: 0.9248, Recall: 0.9225, F1_score: 0.9237
Model performance on Happy speech (in training): 
	Precision: 0.8530, Recall: 0.8125, F1_score: 0.8323
Model performance on Neutral speech (in training): 
	Precision: 0.8235, Recall: 0.8400, F1_score: 0.8317
Model performance on Sad speech (in training): 
	Precision: 0.8762, Recall: 0.9025, F1_score: 0.8892

Eval Phase: 
Validation loss: 276.6629, Validation accuracy: 0.6300
Macro F1-score: 0.6136
Model performance on Angry speech (in validation): 
	Precision: 0.7600, Recall: 0.7600, F1_score: 0.7600
Model performance on Happy speech (in validation): 
	Precision: 0.9375, Recall: 0.3000, F1_score: 0.4545
Model performance on Neutral speech (in validation): 
	Precision: 0.5769, Recall: 0.6000, F1_score: 0.5882
Model performance on Sad speech (in validation): 
	Precision: 0.5244, Recall: 0.8600, F1_score: 0.6515
Epoch 10/100

Training Phase:
Training loss: 450.1700, Training accuracy: 0.8988
Macro F1-score: 0.8985
Model performance on Angry speech (in training): 
	Precision: 0.9225, Recall: 0.9225, F1_score: 0.9225
Model performance on Happy speech (in training): 
	Precision: 0.8966, Recall: 0.8450, F1_score: 0.8700
Model performance on Neutral speech (in training): 
	Precision: 0.8824, Recall: 0.9000, F1_score: 0.8911
Model performance on Sad speech (in training): 
	Precision: 0.8940, Recall: 0.9275, F1_score: 0.9104

Eval Phase: 
Validation loss: 294.9282, Validation accuracy: 0.6100
Macro F1-score: 0.6007
Model performance on Angry speech (in validation): 
	Precision: 0.7000, Recall: 0.7000, F1_score: 0.7000
Model performance on Happy speech (in validation): 
	Precision: 0.7000, Recall: 0.4200, F1_score: 0.5250
Model performance on Neutral speech (in validation): 
	Precision: 0.5714, Recall: 0.4800, F1_score: 0.5217
Model performance on Sad speech (in validation): 
	Precision: 0.5385, Recall: 0.8400, F1_score: 0.6562
Epoch 11/100

Training Phase:
  

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 230/1600 [00:10<00:59, 22.97it/s]
Training:  29%|██▉       | 463/1600 [00:20<00:49, 23.16it/s]
Training:  44%|████▍     | 705/1600 [00:30<00:37, 23.62it/s]
Training:  59%|█████▉    | 947/1600 [00:40<00:27, 23.54it/s]
Training:  74%|███████▍  | 1186/1600 [00:50<00:17, 23.66it/s]
Training:  89%|████████▉ | 1425/1600 [01:00<00:07, 23.67it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 224/1600 [00:10<01:01, 22.38it/s]
Training:  29%|██▉       | 464/1600 [00:20<00:48, 23.30it/s]
Training:  44%|████▍     | 704/1600 [00:30<00:38, 23.46it/s]
TrainingTraining loss: 417.1238, Training accuracy: 0.8962
Macro F1-score: 0.8962
Model performance on Angry speech (in training): 
	Precision: 0.9545, Recall: 0.9450, F1_score: 0.9497
Model performance on Happy speech (in training): 
	Precision: 0.8857, Recall: 0.8525, F1_score: 0.8688
Model performance on Neutral speech (in training): 
	Precision: 0.8428, Recall: 0.8575, F1_score: 0.8501
Model performance on Sad speech (in training): 
	Precision: 0.9029, Recall: 0.9300, F1_score: 0.9163

Eval Phase: 
Validation loss: 324.9186, Validation accuracy: 0.5750
Macro F1-score: 0.5617
Model performance on Angry speech (in validation): 
	Precision: 0.6154, Recall: 0.8000, F1_score: 0.6957
Model performance on Happy speech (in validation): 
	Precision: 0.6538, Recall: 0.3400, F1_score: 0.4474
Model performance on Neutral speech (in validation): 
	Precision: 0.4800, Recall: 0.4800, F1_score: 0.4800
Model performance on Sad speech (in validation): 
	Precision: 0.5763, Recall: 0.6800, F1_score: 0.6239
Epoch 12/100

Training Phase:
Training loss: 390.3621, Training accuracy: 0.9087
Macro F1-score: 0.9087
Model performance on Angry speech (in training): 
	Precision: 0.9526, Recall: 0.9550, F1_score: 0.9538
Model performance on Happy speech (in training): 
	Precision: 0.9132, Recall: 0.8675, F1_score: 0.8897
Model performance on Neutral speech (in training): 
	Precision: 0.8686, Recall: 0.8925, F1_score: 0.8804
Model performance on Sad speech (in training): 
	Precision: 0.9020, Recall: 0.9200, F1_score: 0.9109

Eval Phase: 
:  59%|█████▉    | 941/1600 [00:40<00:28, 23.18it/s]
Training:  73%|███████▎  | 1170/1600 [00:50<00:18, 23.04it/s]
Training:  88%|████████▊ | 1415/1600 [01:00<00:07, 23.52it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 208/1600 [00:10<01:06, 20.79it/s]
Training:  28%|██▊       | 450/1600 [00:20<00:50, 22.80it/s]
Training:  43%|████▎     | 692/1600 [00:30<00:38, 23.32it/s]
Training:  58%|█████▊    | 932/1600 [00:41<00:29, 22.72it/s]
Training:  72%|███████▏  | 1151/1600 [00:51<00:20, 22.34it/s]
Training:  86%|████████▌ | 1378/1600 [01:01<00:09, 22.44it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                 Validation loss: 292.4700, Validation accuracy: 0.6350
Macro F1-score: 0.6269
Model performance on Angry speech (in validation): 
	Precision: 0.7451, Recall: 0.7600, F1_score: 0.7525
Model performance on Happy speech (in validation): 
	Precision: 0.8077, Recall: 0.4200, F1_score: 0.5526
Model performance on Neutral speech (in validation): 
	Precision: 0.5417, Recall: 0.5200, F1_score: 0.5306
Model performance on Sad speech (in validation): 
	Precision: 0.5600, Recall: 0.8400, F1_score: 0.6720
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.6800

Test Phase: 
                  

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   5%|▌         | 10/200 [00:00<00:01, 97.75it/s]
Testing:  10%|█         | 20/200 [00:00<00:01, 93.49it/s]
Testing:  16%|█▌        | 32/200 [00:00<00:01, 101.86it/s]
Testing:  22%|██▏       | 43/200 [00:00<00:01, 101.14it/s]
Testing:  27%|██▋       | 54/200 [00:00<00:01, 100.85it/s]
Testing:  32%|███▎      | 65/200 [00:00<00:01, 97.59it/s] 
Testing:  38%|███▊      | 75/200 [00:00<00:01, 98.01it/s]
Testing:  43%|████▎     | 86/200 [00:00<00:01, 99.80it/s]
Testing:  48%|████▊     | 96/200 [00:00<00:01, 98.46it/s]
Testing:  53%|█████▎    | 106/200 [00:01<00:00, 97.26it/s]
Testing:  58%|█████▊    | 117/200 [00:01<00:00, 100.96it/s]
Testing:  64%|██████▍   | 128/200 [00:01<00:00, 101.27it/s]
Testing:  70%|██████▉   | 139/200 [00:01<00:00, 102.72it/s]
Testing:  75%|███████▌  | 150/200 [00:01<00:00, 101.86it/s]
TestinTest loss: 177.8081, Test accuracy: 0.6350
Macro F1-score: 0.6190
Model performance on Angry speech (in test): 
	Precision: 0.6610, Recall: 0.7800, F1_score: 0.7156
Model performance on Happy speech (in test): 
	Precision: 0.7826, Recall: 0.3600, F1_score: 0.4932
Model performance on Neutral speech (in test): 
	Precision: 0.5870, Recall: 0.5400, F1_score: 0.5625
Model performance on Sad speech (in test): 
	Precision: 0.5972, Recall: 0.8600, F1_score: 0.7049

======================= This is fold_3 on cn =======================

Load dataset: 
Loading en train data: fold_3...
Preprocess en fold_3 data for cn model
g:  80%|████████  | 161/200 [00:01<00:00, 99.84it/s] 
Testing:  86%|████████▌ | 172/200 [00:01<00:00, 95.27it/s]
Testing:  92%|█████████▏| 183/200 [00:01<00:00, 99.08it/s]
Testing:  96%|█████████▋| 193/200 [00:01<00:00, 99.12it/s]
                                                          

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:   0%|          | 4/1600 [00:00<01:03, 25.20 examples/s]
Map:   1%|          | 15/1600 [00:00<00:25, 61.54 examples/s]
Map:   2%|▏         | 29/1600 [00:00<00:17, 90.66 examples/s]
Map:   3%|▎         | 46/1600 [00:00<00:16, 96.14 examples/s]
Map:   4%|▍         | 60/1600 [00:00<00:17, 87.80 examples/s]
Map:   4%|▍         | 71/1600 [00:00<00:16, 90.80 examples/s]
Map:   5%|▌         | 84/1600 [00:00<00:15, 94.93 examples/s]
Map:   6%|▌         | 95/1600 [00:01<00:16, 92.55 examples/s]
Map:   7%|▋         | 105/1600 [00:01<00:16, 90.87 examples/s]
Map:   8%|▊         | 121/1600 [00:01<00:17, 83.91 examples/s]
Map:   8%|▊         | 134/1600 [00:01<00:16, 91.49 examples/s]
Map:   9%|▉         | 146/1600 [00:01<00:15, 93.84 examples/s]
Map:  10%|▉         | 159/1600 [00:01<00:14, 98.62 examples/s]
Map:  11%|█         | 173/1600 [00:01<00:13, 107.06 examples/s]
Map:  12%|█▏        | 187/1600 [00:01<00:12, 115.09 examples/s]
Map:  13%|█▎        | 204/1600 [00:02<00:10, 128.73 examples/s]
Map:  14%|█▎        | 218/1600 [00:02<00:11, 120.63 examples/s]
Map:  14%|█▍        | 232/1600 [00:02<00:11, 123.07 examples/s]
Map:  16%|█▌        | 249/1600 [00:02<00:14, 90.53 examples/s] 
Map:  16%|█▋        | 263/1600 [00:02<00:13, 98.87 examples/s]
Map:  17%|█▋        | 279/1600 [00:02<00:12, 109.63 examples/s]
Map:  18%|█▊        | 294/1600 [00:02<00:11, 114.79 examples/s]
Map:  19%|█▉        | 311/1600 [00:03<00:10, 126.64 examples/s]
Map:  20%|██        | 327/1600 [00:03<00:09, 131.64 examples/s]
Map:  21%|██▏       | 341/1600 [00:03<00:09, 129.49 examples/s]
Map:  22%|██▏       | 356/1600 [00:03<00:10, 115.81 examples/s]
Map:  24%|██▎       | 376/1600 [00:03<00:10, 116.98 examples/s]
Map:  25%|██▍       | 393/1600 [00:03<00:09, 126.07 examples/s]
Map:  26%|██▌       | 409/1600 [00:03<00:10, 116.66 examples/s]
Map:  26%|██▋       | 422/1600 [00:03<00:10, 117.00 examples/s]
Map:  28%|██▊       | 442/1600 [00:04<00:09, 116.33 examples/s]
Map:  29%|██▊       | 458/1600 [00:04<00:09, 122.71 examples/s]
Map:  29%|██▉       | 471/1600 [00:04<00:09, 115.26 examples/s]
Map:  30%|███       | 487/1600 [00:04<00:08, 125.41 examples/s]
Map:  32%|███▏      | 504/1600 [00:04<00:08, 136.38 examples/s]
Map:  33%|███▎      | 523/1600 [00:04<00:09, 114.48 examples/s]
Map:  34%|███▎      | 538/1600 [00:05<00:10, 104.99 examples/s]
Map:  35%|███▍      | 555/1600 [00:05<00:08, 118.33 examples/s]
Map:  36%|███▌      | 575/1600 [00:05<00:08, 116.14 examples/s]
Map:  37%|███▋      | 590/1600 [00:05<00:08, 123.00 examples/s]
Map:  38%|███▊      | 607/1600 [00:05<00:07, 132.06 examples/s]
Map:  39%|███▉      | 629/1600 [00:05<00:07, 132.54 examples/s]
Map:  40%|████      | 644/1600 [00:05<00:07, 130.44 examples/s]
Map:  41%|████▏     | 660/1600 [00:05<00:08, 112.85 examples/s]
Map:  42%|████▏     | 672/1600 [00:06<00:08, 113.70 examples/s]
Map:  43%|████▎     | 684/1600 [00:06<00:08, 109.26 examples/s]
Map:  44%|████▎     | 696/1600 [00:06<00:08, 107.70 examples/s]
Map:  44%|████▍     | 709/1600 [00:06<00:08, 111.12 examples/s]
Map:  45%|████▌     | 727/1600 [00:06<00:06, 125.73 examples/s]
Map:  46%|████▋     | 741/1600 [00:06<00:06, 128.75 examples/s]
Map:  47%|████▋     | 759/1600 [00:06<00:08, 96.31 examples/s] 
Map:  48%|████▊     | 774/1600 [00:07<00:08, 92.14 examples/s]
Map:  49%|████▉     | 785/1600 [00:07<00:08, 94.06 examples/s]
Map:  50%|█████     | 800/1600 [00:07<00:08, 91.56 examples/s]
Map:  51%|█████     | 814/1600 [00:07<00:07, 101.02 examples/s]
Map:  52%|█████▏    | 826/1600 [00:07<00:07, 104.45 examples/s]
Map:  52%|█████▎    | 840/1600 [00:07<00:06, 111.23 examples/s]
Map:  54%|█████▎    | 856/1600 [00:07<00:07, 100.66 examples/s]
Map:  55%|█████▍    | 873/1600 [00:07<00:06, 115.56 examples/s]
Map:  55%|█████▌    | 887/1600 [00:08<00:06, 118.03 examples/s]
Map:  56%|█████▋    | 902/1600 [00:08<00:05, 123.68 examples/s]
Map:  57%|█████▋    | 915/1600 [00:08<00:05, 122.04 examples/s]
Map:  58%|█████▊    | 931/1600 [00:08<00:05, 129.64 examples/s]
Map:  60%|█████▉    | 952/1600 [00:08<00:05, 114.65 examples/s]
Map:  60%|██████    | 966/1600 [00:08<00:05, 113.27 examples/s]
Map:  61%|██████▏   | 980/1600 [00:08<00:05, 115.19 examples/s]
Map:  62%|██████▏   | 992/1600 [00:08<00:05, 110.78 examples/s]
Map:  62%|██████▏   | 992/1600 [00:25<00:05, 110.78 examples/s]
Map:  62%|██████▎   | 1000/1600 [01:04<13:23,  1.34s/ examples]
Map:  63%|██████▎   | 1014/1600 [01:04<08:50,  1.10 examples/s]
Map:  64%|██████▍   | 1028/1600 [01:04<05:55,  1.61 examples/s]
Map:  65%|██████▌   | 1046/1600 [01:04<03:38,  2.54 examples/s]
Map:  66%|██████▋   | 1064/1600 [01:04<02:20,  3.82 examples/s]
Map:  68%|██████▊   | 1083/1600 [01:04<01:30,  5.71 examples/s]
Map:  69%|██████▊   | 1097/1600 [01:04<01:05,  7.64 examples/s]
Map:  69%|██████▉   | 1110/1600 [01:05<00:48, 10.08 examples/s]
Map:  70%|███████   | 1122/1600 [01:05<00:36, 13.14 examples/s]
Map:  71%|███████   | 1136/1600 [01:05<00:26, 17.53 examples/s]
Map:  72%|███████▏  | 1153/1600 [01:05<00:18, 23.91 examples/s]
Map:  73%|███████▎  | 1170/1600 [01:05<00:13, 32.83 examples/s]
Map:  74%|███████▍  | 1184/1600 [01:05<00:10, 41.05 examples/s]
Map:  75%|███████▌  | 1201/1600 [01:05<00:07, 53.83 examples/s]
Map:  76%|███████▌  | 1217/1600 [01:06<00:06, 61.68 examples/s]
Map:  77%|███████▋  | 1229/1600 [01:06<00:05, 69.04 examples/s]
Map:  78%|███████▊  | 1242/1600 [01:06<00:04, 72.26 examples/s]
Map:  78%|███████▊  | 1253/1600 [01:06<00:05, 69.09 examples/s]
Map:  79%|███████▉  | 1267/1600 [01:06<00:04, 75.04 examples/s]
Map:  80%|████████  | 1282/1600 [01:06<00:04, 76.23 examples/s]
Map:  81%|████████  | 1292/1600 [01:06<00:03, 77.15 examples/s]
Map:  82%|████████▏ | 1304/1600 [01:07<00:03, 82.51 examples/s]
Map:  82%|████████▏ | 1318/1600 [01:07<00:03, 92.58 examples/s]
Map:  84%|████████▎ | 1336/1600 [01:07<00:02, 110.24 examples/s]
Map:  85%|████████▍ | 1353/1600 [01:07<00:02, 123.32 examples/s]
Map:  86%|████████▌ | 1368/1600 [01:07<00:02, 103.84 examples/s]
Map:  87%|████████▋ | 1386/1600 [01:07<00:02, 105.07 examples/s]
Map:  88%|████████▊ | 1401/1600 [01:07<00:01, 113.82 examples/s]
Map:  89%|████████▊ | 1417/1600 [01:08<00:01, 122.39 examples/s]
Map:  90%|████████▉ | 1436/1600 [01:08<00:01, 137.09 examples/s]
Map:  91%|█████████ | 1453/1600 [01:08<00:01, 140.40 examples/s]
Map:  92%|█████████▏| 1474/1600 [01:08<00:00, 151.22 examples/s]
Map:  93%|█████████▎| 1495/1600 [01:08<00:00, 118.54 examples/s]
Map:  94%|█████████▍| 1509/1600 [01:08<00:00, 117.85 examples/s]
Map:  95%|█████████▌| 1523/1600 [01:08<00:00, 121.17 examples/s]
Map:  96%|█████████▌| 1538/1600 [01:08<00:00, 126.74 examples/s]
Map:  97%|█████████▋| 1553/1600 [01:09<00:00, 128.22 examples/s]
Map:  98%|█████████▊| 1574/1600 [01:09<00:00, 147.25 examples/s]
Map: 100%|█████████▉| 1596/1600 [01:09<00:00, 137.87 examples/s]
Map: 100%|█████████▉| 1596/1600 [01:26<00:00, 137.87 examples/s]
Map: 100%|██████████| 1600/1600 [01:41<00:00,  1.48 examples/s] 
Map: 100%|██████████| 1600/1600 [01:41<00:00, 15.79 examples/s]

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   3%|▎         | 6/200 [00:00<00:04, 39.88 examples/s]
Map:   8%|▊         | 16/200 [00:00<00:03, 59.87 examples/s]
Map:  12%|█▎        | 25/200 [00:00<00:02, 69.50 examples/s]
Map:  20%|██        | 41/200 [00:00<00:01, 100.19 examples/s]
Map:  28%|██▊       | 55/200 [00:00<00:01, 108.04 examples/s]
Map:  35%|███▌      | 70/200 [00:00<00:01, 110.45 examples/s]
Map:  42%|████▎     | 85/200 [00:00<00:00, 117.78 examples/s]
Map:  49%|████▉     | 98/200 [00:01<00:01, 95.42 examples/s] 
Map:  56%|█████▋    | 113/200 [00:01<00:00, 92.06 examples/s]
Map:  64%|██████▎   | 127/200 [00:01<00:00, 101.58 examples/s]
Map:  70%|██████▉   | 139/200 [00:01<00:00, 103.51 examples/s]
Map:  76%|███████▋  | 153/200 [00:01<00:00, 96.25 examples/s] 
Map:  83%|████████▎ | 166/200 [00:01<00:00, 102.80 examples/s]
Map:  88%|████████▊ | 177/200 [00:01<00:00, 100.01 examples/s]
Map:  95%|█████████▌| 190/200 [00:01<00:00, 103.25 examples/s]
Map: 100%|██████████| 200/200 [00:13<00:00, 14.33 examples/s] 

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▏         | 4/200 [00:00<00:05, 34.21 examples/s]
Map:   7%|▋         | 14/200 [00:00<00:02, 66.03 examples/s]
Map:  13%|█▎        | 26/200 [00:00<00:01, 88.21 examples/s]
Map:  21%|██        | 42/200 [00:00<00:01, 108.60 examples/s]
Map:  26%|██▋       | 53/200 [00:00<00:01, 104.16 examples/s]
Map:  36%|███▌      | 71/200 [00:00<00:01, 125.54 examples/s]
Map:  42%|████▏     | 84/200 [00:00<00:00, 118.14 examples/s]
Map:  50%|█████     | 101/200 [00:00<00:00, 113.75 examples/s]
Map:  57%|█████▋    | 114/200 [00:01<00:00, 106.07 examples/s]
Map:  64%|██████▍   | 128/200 [00:01<00:00, 109.71 examples/s]
Map:  73%|███████▎  | 146/200 [00:01<00:00, 108.11 examples/s]
Map:  78%|███████▊  | 157/200 [00:01<00:00, 106.93 examples/s]
Map:  85%|████████▌ | 170/200 [00:01<00:00, 109.62 examples/s]
Map:  93%|█████████▎| 186/200 [00:01<00:00, 120.20 examples/s]
Map: 100%|██████████| 200/200 [00:01<00:00, 122.36 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 15.63 examples/s] 
Loading en eval data: fold_3...
Preprocess en fold_3 data for cn model
Loading en test data: fold_3...
Preprocess en fold_3 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1285.0222, Training accuracy: 0.6731
Macro F1-score: 0.6696
Model performance on Angry speech (in training): 
	Precision: 0.7325, Recall: 0.7600, F1_score: 0.7460
Model performance on Happy speech (in training): 
	Precision: 0.6161, Recall: 0.5175, F1_score: 0.5625
Model performance on Neutral speech (in training): 
	Precision: 0.6123, Recall: 0.6200, F1_score: 0.6161
Model performance on Sad speech (in training): 
	Precision: 0.7162, Recall: 0.7950, F1_score: 0.7536

Eval Phase: 
Validation loss: 139.2008, Validation accuracy: 0.7200
Macro F1-score: 0.7113
Model performance on Angry speech (in validation): 
	Precision: 0.8400, Recall: 0.8400, F1_score: 0.8400
Model performance on Happy speech (in validation): 
	Precision: 0.7111, Recall: 0.6400, F1_score: 0.6737
Model performance on Neutral speech (in validation): 
	Precision: 0.6486, Recall: 0.4800, F1_score: 0.5517
Model performance on Sad speech (in validation): 
	Precision: 0.6765, Recall: 0.9200, F1_score: 0.7797
New best accuracy for layer 4 on epoch 1: 0.7200. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▌        | 243/1600 [00:10<00:55, 24.27it/s]
Training:  30%|███       | 486/1600 [00:20<00:48, 23.09it/s]
Training:  45%|████▍     | 717/1600 [00:30<00:38, 23.09it/s]
Training:  60%|█████▉    | 955/1600 [00:40<00:27, 23.35it/s]
Training:  75%|███████▍  | 1193/1600 [00:51<00:17, 23.31it/s]
Training:  89%|████████▉ | 1430/1600 [01:01<00:07, 23.41it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 235/1600 [00:10<00:58, 23.41it/s]
Training:  29%|██▉       | 471/1600 [00:20<00:48, 23.51it/s]
Training:  44%|████▍     | 707/1600 [00:30<00:38, 23.45it/s]
Training:  59%|█████▉    | 947/1600 [00:40<00:27, 23.65it/s]
Training:  74%|███████▍  | Training loss: 1170.3799, Training accuracy: 0.7006
Macro F1-score: 0.6981
Model performance on Angry speech (in training): 
	Precision: 0.7877, Recall: 0.7975, F1_score: 0.7925
Model performance on Happy speech (in training): 
	Precision: 0.6677, Recall: 0.5525, F1_score: 0.6047
Model performance on Neutral speech (in training): 
	Precision: 0.6325, Recall: 0.6625, F1_score: 0.6471
Model performance on Sad speech (in training): 
	Precision: 0.7101, Recall: 0.7900, F1_score: 0.7479

Eval Phase: 
Validation loss: 134.2123, Validation accuracy: 0.7200
Macro F1-score: 0.7110
Model performance on Angry speech (in validation): 
	Precision: 0.8800, Recall: 0.8800, F1_score: 0.8800
Model performance on Happy speech (in validation): 
	Precision: 0.7500, Recall: 0.6600, F1_score: 0.7021
Model performance on Neutral speech (in validation): 
	Precision: 0.6667, Recall: 0.4400, F1_score: 0.5301
Model performance on Sad speech (in validation): 
	Precision: 0.6164, Recall: 0.9000, F1_score: 0.7317
Epoch 3/100

Training Phase:
Training loss: 1060.1529, Training accuracy: 0.7350
Macro F1-score: 0.7327
Model performance on Angry speech (in training): 
	Precision: 0.7833, Recall: 0.7950, F1_score: 0.7891
Model performance on Happy speech (in training): 
	Precision: 0.7249, Recall: 0.6125, F1_score: 0.6640
Model performance on Neutral speech (in training): 
	Precision: 0.6716, Recall: 0.6850, F1_score: 0.6782
Model performance on Sad speech (in training): 
	Precision: 0.7567, Recall: 0.8475, F1_score: 0.7995

Eval Phase: 
Validation loss: 193.3060, Validation accuracy: 0.6350
Macro F1-score: 0.6030
Model performance on Angry speech (in validation): 
	Precision: 0.8696, Recall: 0.8000, F1_score: 0.8333
Model performance on Happy speech (in validation): 
	Precision: 0.7568, Recall: 0.5600, F1_score: 0.6437
Model performance on Neutral speech (in validation): 
	Precision: 0.6923, Recall: 0.1800, F1_score: 0.2857
Model performance on Sad speech (in validation): 
	Precision: 0.4808, Recall: 1.0000, F1_score: 0.6494
Epoch 4/100

Training Phase:
1187/1600 [00:50<00:17, 23.53it/s]
Training:  89%|████████▉ | 1421/1600 [01:00<00:07, 23.28it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 232/1600 [00:10<00:58, 23.19it/s]
Training:  29%|██▉       | 470/1600 [00:20<00:48, 23.48it/s]
Training:  44%|████▍     | 707/1600 [00:30<00:37, 23.52it/s]
Training:  59%|█████▉    | 945/1600 [00:40<00:27, 23.60it/s]
Training:  74%|███████▍  | 1183/1600 [00:50<00:17, 23.34it/s]
Training:  89%|████████▉ | 1427/1600 [01:00<00:07, 23.65it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 237/16Training loss: 996.3810, Training accuracy: 0.7500
Macro F1-score: 0.7487
Model performance on Angry speech (in training): 
	Precision: 0.8193, Recall: 0.8050, F1_score: 0.8121
Model performance on Happy speech (in training): 
	Precision: 0.7324, Recall: 0.6500, F1_score: 0.6887
Model performance on Neutral speech (in training): 
	Precision: 0.6914, Recall: 0.7000, F1_score: 0.6957
Model performance on Sad speech (in training): 
	Precision: 0.7562, Recall: 0.8450, F1_score: 0.7981

Eval Phase: 
Validation loss: 138.5137, Validation accuracy: 0.7150
Macro F1-score: 0.7025
Model performance on Angry speech (in validation): 
	Precision: 0.8491, Recall: 0.9000, F1_score: 0.8738
Model performance on Happy speech (in validation): 
	Precision: 0.6957, Recall: 0.6400, F1_score: 0.6667
Model performance on Neutral speech (in validation): 
	Precision: 0.7241, Recall: 0.4200, F1_score: 0.5316
Model performance on Sad speech (in validation): 
	Precision: 0.6250, Recall: 0.9000, F1_score: 0.7377
Epoch 5/100

Training Phase:
00 [00:10<00:57, 23.66it/s]
Training:  30%|██▉       | 474/1600 [00:20<00:47, 23.60it/s]
Training:  44%|████▍     | 710/1600 [00:30<00:38, 23.26it/s]
Training:  59%|█████▉    | 940/1600 [00:40<00:28, 23.16it/s]
Training:  74%|███████▎  | 1178/1600 [00:50<00:18, 23.37it/s]
Training:  88%|████████▊ | 1416/1600 [01:01<00:07, 23.02it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 238/1600 [00:10<00:57, 23.77it/s]
Training:  30%|██▉       | 476/1600 [00:20<00:48, 23.00it/s]
Training:  45%|████▍     | 715/1600 [00:30<00:37, 23.38it/s]
Training:  60%|█████▉    | 954/1600 [00:40<00:27, 23.47it/s]
Training:  74%|███████▍  | 1192/1600 [00:50<00:17, 23.56it/s]
Training:  89%|████████▉ | 1430/1600Training loss: 886.9624, Training accuracy: 0.7856
Macro F1-score: 0.7843
Model performance on Angry speech (in training): 
	Precision: 0.8416, Recall: 0.8500, F1_score: 0.8458
Model performance on Happy speech (in training): 
	Precision: 0.7861, Recall: 0.6800, F1_score: 0.7292
Model performance on Neutral speech (in training): 
	Precision: 0.7233, Recall: 0.7450, F1_score: 0.7340
Model performance on Sad speech (in training): 
	Precision: 0.7922, Recall: 0.8675, F1_score: 0.8282

Eval Phase: 
Validation loss: 144.6112, Validation accuracy: 0.7350
Macro F1-score: 0.7211
Model performance on Angry speech (in validation): 
	Precision: 0.7778, Recall: 0.9800, F1_score: 0.8673
Model performance on Happy speech (in validation): 
	Precision: 0.8235, Recall: 0.5600, F1_score: 0.6667
Model performance on Neutral speech (in validation): 
	Precision: 0.6757, Recall: 0.5000, F1_score: 0.5747
Model performance on Sad speech (in validation): 
	Precision: 0.6818, Recall: 0.9000, F1_score: 0.7759
New best accuracy for layer 4 on epoch 5: 0.7350. Model saved.
Epoch 6/100

Training Phase:
Training loss: 803.6686, Training accuracy: 0.7975
Macro F1-score: 0.7960
Model performance on Angry speech (in training): 
	Precision: 0.8494, Recall: 0.8600, F1_score: 0.8547
Model performance on Happy speech (in training): 
	Precision: 0.7937, Recall: 0.6925, F1_score: 0.7397
Model performance on Neutral speech (in training): 
	Precision: 0.7512, Recall: 0.7550, F1_score: 0.7531
Model performance on Sad speech (in training): 
	Precision: 0.7950, Recall: 0.8825, F1_score: 0.8365

Eval Phase: 
Validation loss: 139.2438, Validation accuracy: 0.7350
Macro F1-score: 0.7286
Model performance on Angry speech (in validation): 
	Precision: 0.8679, Recall: 0.9200, F1_score: 0.8932
Model performance on Happy speech (in validation): 
	Precision: 0.7045, Recall: 0.6200, F1_score: 0.6596
Model performance on Neutral speech (in validation): 
	Precision: 0.6429, Recall: 0.5400, F1_score: 0.5870
Model performance on Sad speech (in validation): 
	Precision: 0.7049, Recall: 0.8600, F1_score: 0.7748
Epoch 7/100

Training Phase:
 [01:01<00:07, 23.18it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▌        | 242/1600 [00:10<00:56, 24.18it/s]
Training:  30%|███       | 484/1600 [00:20<00:46, 23.82it/s]
Training:  45%|████▌     | 726/1600 [00:30<00:36, 23.97it/s]
Training:  60%|██████    | 968/1600 [00:41<00:27, 23.38it/s]
Training:  75%|███████▍  | 1196/1600 [00:51<00:17, 23.16it/s]
Training:  90%|████████▉ | 1435/1600 [01:01<00:07, 23.39it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 231/1600 [00:10<00:59, 23.09it/s]
Training:  30%|██▉       | 473/1600 [00:20<00:47, 23.72Training loss: 701.5586, Training accuracy: 0.8331
Macro F1-score: 0.8326
Model performance on Angry speech (in training): 
	Precision: 0.8858, Recall: 0.8725, F1_score: 0.8791
Model performance on Happy speech (in training): 
	Precision: 0.8453, Recall: 0.7650, F1_score: 0.8031
Model performance on Neutral speech (in training): 
	Precision: 0.7770, Recall: 0.7925, F1_score: 0.7847
Model performance on Sad speech (in training): 
	Precision: 0.8280, Recall: 0.9025, F1_score: 0.8636

Eval Phase: 
Validation loss: 150.0519, Validation accuracy: 0.7250
Macro F1-score: 0.7242
Model performance on Angry speech (in validation): 
	Precision: 0.8958, Recall: 0.8600, F1_score: 0.8776
Model performance on Happy speech (in validation): 
	Precision: 0.6957, Recall: 0.6400, F1_score: 0.6667
Model performance on Neutral speech (in validation): 
	Precision: 0.6383, Recall: 0.6000, F1_score: 0.6186
Model performance on Sad speech (in validation): 
	Precision: 0.6780, Recall: 0.8000, F1_score: 0.7339
Epoch 8/100

Training Phase:
Training loss: 609.7208, Training accuracy: 0.8556
Macro F1-score: 0.8547
Model performance on Angry speech (in training): 
	Precision: 0.8916, Recall: 0.9050, F1_score: 0.8983
Model performance on Happy speech (in training): 
	Precision: 0.8754, Recall: 0.7725, F1_score: 0.8207
Model performance on Neutral speech (in training): 
	Precision: 0.8030, Recall: 0.8150, F1_score: 0.8089
Model performance on Sad speech (in training): 
	Precision: 0.8552, Recall: 0.9300, F1_score: 0.8910

Eval Phase: 
it/s]
Training:  45%|████▍     | 715/1600 [00:30<00:37, 23.64it/s]
Training:  60%|█████▉    | 954/1600 [00:40<00:27, 23.70it/s]
Training:  75%|███████▍  | 1193/1600 [00:50<00:17, 23.60it/s]
Training:  89%|████████▉ | 1428/1600 [01:00<00:07, 23.24it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 231/1600 [00:10<00:59, 23.09it/s]
Training:  29%|██▉       | 463/1600 [00:20<00:49, 23.12it/s]
Training:  44%|████▎     | 696/1600 [00:30<00:39, 23.17it/s]
Training:  58%|█████▊    | 935/1600 [00:40<00:28, 23.46it/s]
Training:  73%|███████▎  | 1174/1600 [00:50<00:18, 23.59it/s]
Training:  88%|████████▊ | 1413/1600 [01:00<00:07, 23.40it/s]
                                                             

Validation loss: 183.2857, Validation accuracy: 0.7050
Macro F1-score: 0.6996
Model performance on Angry speech (in validation): 
	Precision: 0.9070, Recall: 0.7800, F1_score: 0.8387
Model performance on Happy speech (in validation): 
	Precision: 0.7931, Recall: 0.4600, F1_score: 0.5823
Model performance on Neutral speech (in validation): 
	Precision: 0.6346, Recall: 0.6600, F1_score: 0.6471
Model performance on Sad speech (in validation): 
	Precision: 0.6053, Recall: 0.9200, F1_score: 0.7302
Epoch 9/100

Training Phase:
Training loss: 514.3276, Training accuracy: 0.8794
Macro F1-score: 0.8794
Model performance on Angry speech (in training): 
	Precision: 0.9152, Recall: 0.8900, F1_score: 0.9024
Model performance on Happy speech (in training): 
	Precision: 0.8802, Recall: 0.8450, F1_score: 0.8622
Model performance on Neutral speech (in training): 
	Precision: 0.8467, Recall: 0.8700, F1_score: 0.8582
Model performance on Sad speech (in training): 
	Precision: 0.8774, Recall: 0.9125, F1_score: 0.8946

Eval Phase: 
Validation loss: 210.5685, Validation accuracy: 0.7000
Macro F1-score: 0.6854
Model performance on Angry speech (in validation): 
	Precision: 0.8214, Recall: 0.9200, F1_score: 0.8679
Model performance on Happy speech (in validation): 
	Precision: 0.8333, Recall: 0.4000, F1_score: 0.5405
Model performance on Neutral speech (in validation): 
	Precision: 0.6122, Recall: 0.6000, F1_score: 0.6061
Model performance on Sad speech (in validation): 
	Precision: 0.6197, Recall: 0.8800, F1_score: 0.7273
Epoch 10/100

Training Phase:
Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 237/1600 [00:10<00:57, 23.63it/s]
Training:  30%|██▉       | 477/1600 [00:20<00:47, 23.83it/s]
Training:  45%|████▍     | 717/1600 [00:30<00:37, 23.33it/s]
Training:  60%|█████▉    | 952/1600 [00:40<00:27, 23.37it/s]
Training:  74%|███████▍  | 1192/1600 [00:50<00:17, 23.59it/s]
Training:  74%|███████▍  | 1192/1600 [01:00<00:17, 23.59it/s]
Training:  89%|████████▉ | 1423/1600 [01:00<00:07, 23.17it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 229/1600 [00:10<01:00, 22.83it/s]
Training:  29%|██▊       | 459/1600 [00:20<00:49, 22.93it/s]
TrainTraining loss: 477.8363, Training accuracy: 0.8881
Macro F1-score: 0.8878
Model performance on Angry speech (in training): 
	Precision: 0.9296, Recall: 0.9250, F1_score: 0.9273
Model performance on Happy speech (in training): 
	Precision: 0.8695, Recall: 0.8325, F1_score: 0.8506
Model performance on Neutral speech (in training): 
	Precision: 0.8525, Recall: 0.8525, F1_score: 0.8525
Model performance on Sad speech (in training): 
	Precision: 0.8998, Recall: 0.9425, F1_score: 0.9206

Eval Phase: 
Validation loss: 196.6883, Validation accuracy: 0.7300
Macro F1-score: 0.7257
Model performance on Angry speech (in validation): 
	Precision: 0.8936, Recall: 0.8400, F1_score: 0.8660
Model performance on Happy speech (in validation): 
	Precision: 0.6800, Recall: 0.6800, F1_score: 0.6800
Model performance on Neutral speech (in validation): 
	Precision: 0.7222, Recall: 0.5200, F1_score: 0.6047
Model performance on Sad speech (in validation): 
	Precision: 0.6567, Recall: 0.8800, F1_score: 0.7521
Epoch 11/100

Training Phase:
ing:  29%|██▊       | 459/1600 [00:30<00:49, 22.93it/s]
Training:  43%|████▎     | 688/1600 [00:30<00:40, 22.68it/s]
Training:  57%|█████▊    | 920/1600 [00:40<00:29, 22.88it/s]
Training:  72%|███████▏  | 1159/1600 [00:50<00:18, 23.24it/s]
Training:  87%|████████▋ | 1398/1600 [01:01<00:08, 22.74it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 238/1600 [00:10<00:57, 23.73it/s]
Training:  30%|██▉       | 476/1600 [00:21<00:50, 22.30it/s]
Training:  44%|████▍     | 703/1600 [00:31<00:39, 22.43it/s]
Training:  59%|█████▉    | 946/1600 [00:41<00:28, 23.13it/s]
Training:  74%|███████▍  | 1189/1600 [00:52<00:18, 22.81it/s]
Training:  89%|████████▊ | 1417/1600 [01:02<00:08, 22.76it/s]
       Training loss: 430.5179, Training accuracy: 0.8950
Macro F1-score: 0.8948
Model performance on Angry speech (in training): 
	Precision: 0.9236, Recall: 0.9375, F1_score: 0.9305
Model performance on Happy speech (in training): 
	Precision: 0.9032, Recall: 0.8400, F1_score: 0.8705
Model performance on Neutral speech (in training): 
	Precision: 0.8637, Recall: 0.8875, F1_score: 0.8755
Model performance on Sad speech (in training): 
	Precision: 0.8905, Recall: 0.9150, F1_score: 0.9026

Eval Phase: 
Validation loss: 208.7937, Validation accuracy: 0.6750
Macro F1-score: 0.6706
Model performance on Angry speech (in validation): 
	Precision: 0.9091, Recall: 0.8000, F1_score: 0.8511
Model performance on Happy speech (in validation): 
	Precision: 0.5385, Recall: 0.7000, F1_score: 0.6087
Model performance on Neutral speech (in validation): 
	Precision: 0.6061, Recall: 0.4000, F1_score: 0.4819
Model performance on Sad speech (in validation): 
	Precision: 0.6897, Recall: 0.8000, F1_score: 0.7407
Epoch 12/100

Training Phase:
Training loss: 376.8413, Training accuracy: 0.9125
Macro F1-score: 0.9124
Model performance on Angry speech (in training): 
	Precision: 0.9328, Recall: 0.9375, F1_score: 0.9352
Model performance on Happy speech (in training): 
	Precision: 0.9063, Recall: 0.8950, F1_score: 0.9006
Model performance on Neutral speech (in training): 
	Precision: 0.8889, Recall: 0.8800, F1_score: 0.8844
Model performance on Sad speech (in training): 
	Precision: 0.9214, Recall: 0.9375, F1_score: 0.9294

Eval Phase: 
Validation loss: 219.0769, Validation accuracy: 0.7200
Macro F1-score: 0.7194
Model performance on Angry speech (in validation): 
	Precision: 0.9111, Recall: 0.8200, F1_score: 0.8632
Model performance on Happy speech (in validation): 
	Precision: 0.6207, Recall: 0.7200, F1_score: 0.6667
Model performance on Neutral speech (in validation): 
	Precision: 0.6750, Recall: 0.5400, F1_score: 0.6000
Model performance on Sad speech (in validation): 
	Precision: 0.7018, Recall: 0.8000, F1_score: 0.7477
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7350

Test Phase: 
                                                      

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 236/1600 [00:10<00:57, 23.54it/s]
Training:  30%|██▉       | 478/1600 [00:20<00:47, 23.85it/s]
Training:  45%|████▍     | 719/1600 [00:30<00:36, 23.84it/s]
Training:  60%|█████▉    | 958/1600 [00:40<00:27, 23.76it/s]
Training:  75%|███████▍  | 1195/1600 [00:50<00:17, 23.40it/s]
Training:  89%|████████▉ | 1429/1600 [01:00<00:07, 23.38it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   2%|▏         | 3/200 [00:00<00:08, 23.67it/s]
Testing:   3%|▎         | 6/200 [00:00<00:08, 22.04it/s]
Testing:   4%|▍         | 9/200 [00:00<00:08, 22.69it/s]
Testing:   6%|▌         | 12/200 [00:00<00:09, 20.53it/s]
Testing:   8%|▊         | 15/200 [00:00<00:08, 21.90it/s]
Testing:  10%|▉         | 19/200 [00:00<00:07, 23.78it/s]
Testing:  12%|█▏        | 24/200 [00:00<00:06, 27.83it/s]
Testing:  14%|█▎        | 27/200 [00:01<00:06, 27.80it/s]
Testing:  16%|█▌        | 31/200 [00:01<00:05, 29.47it/s]
Testing:  18%|█▊        | 36/200 [00:01<00:04, 34.05it/s]
Testing:  22%|██▏       | 43/200 [00:01<00:03, 41.32it/s]
Testing:  24%|██▍       | 49/200 [00:01<00:03, 45.32it/s]
Testing:  27%|██▋       | 54/200 [00:01<00:03, 45.46it/s]
Testing:  30%|███       | 60/200 [00:01<00:03, 44.45it/s]
Testing:  33%|███▎      | 66/200 [00:01<00:02, 48.48it/s]
Testing:  37%|███▋      | 74/200 [00:01<00:02, 56.93it/s]
Testing:  40%|████      | 81/200 [00:02<00:02, 58.03it/s]
Testing:  44%|████▎     | 87/200 [00:02<00:01, 56.61it/s]
Testing:  47%|████▋     | 94/200 [00:02<00:01, 5Test loss: 126.3040, Test accuracy: 0.7550
Macro F1-score: 0.7475
Model performance on Angry speech (in test): 
	Precision: 0.6923, Recall: 0.9000, F1_score: 0.7826
Model performance on Happy speech (in test): 
	Precision: 0.8182, Recall: 0.7200, F1_score: 0.7660
Model performance on Neutral speech (in test): 
	Precision: 0.8125, Recall: 0.5200, F1_score: 0.6341
Model performance on Sad speech (in test): 
	Precision: 0.7458, Recall: 0.8800, F1_score: 0.8073

======================= This is fold_4 on cn =======================

Load dataset: 
Loading en train data: fold_4...
Preprocess en fold_4 data for cn model
8.80it/s]
Testing:  51%|█████     | 102/200 [00:02<00:01, 61.66it/s]
Testing:  55%|█████▌    | 110/200 [00:02<00:01, 66.04it/s]
Testing:  60%|██████    | 120/200 [00:02<00:01, 73.68it/s]
Testing:  64%|██████▍   | 128/200 [00:02<00:00, 73.22it/s]
Testing:  68%|██████▊   | 137/200 [00:02<00:00, 74.79it/s]
Testing:  72%|███████▎  | 145/200 [00:02<00:00, 75.48it/s]
Testing:  76%|███████▋  | 153/200 [00:03<00:00, 74.88it/s]
Testing:  81%|████████  | 162/200 [00:03<00:00, 78.24it/s]
Testing:  86%|████████▌ | 171/200 [00:03<00:00, 80.70it/s]
Testing:  90%|█████████ | 180/200 [00:03<00:00, 74.96it/s]
Testing:  96%|█████████▌| 191/200 [00:03<00:00, 82.35it/s]
                                                          

Map:   0%|          | 0/1600 [00:00<?, ? examples/s]
Map:   0%|          | 4/1600 [00:00<00:58, 27.31 examples/s]
Map:   1%|          | 14/1600 [00:00<00:25, 61.64 examples/s]
Map:   2%|▏         | 29/1600 [00:00<00:16, 94.18 examples/s]
Map:   3%|▎         | 47/1600 [00:00<00:15, 101.54 examples/s]
Map:   4%|▍         | 60/1600 [00:00<00:16, 90.71 examples/s] 
Map:   4%|▍         | 71/1600 [00:00<00:16, 92.44 examples/s]
Map:   5%|▌         | 83/1600 [00:00<00:15, 97.13 examples/s]
Map:   6%|▌         | 93/1600 [00:01<00:16, 92.75 examples/s]
Map:   7%|▋         | 109/1600 [00:01<00:18, 78.66 examples/s]
Map:   8%|▊         | 122/1600 [00:01<00:16, 88.79 examples/s]
Map:   8%|▊         | 134/1600 [00:01<00:15, 93.63 examples/s]
Map:   9%|▉         | 146/1600 [00:01<00:15, 95.39 examples/s]
Map:  10%|▉         | 159/1600 [00:01<00:14, 99.86 examples/s]
Map:  11%|█         | 173/1600 [00:01<00:13, 108.43 examples/s]
Map:  12%|█▏        | 187/1600 [00:01<00:12, 115.60 examples/s]
Map:  13%|█▎        | 204/1600 [00:02<00:10, 128.89 examples/s]
Map:  14%|█▍        | 221/1600 [00:02<00:13, 103.50 examples/s]
Map:  15%|█▍        | 235/1600 [00:02<00:12, 108.88 examples/s]
Map:  16%|█▌        | 249/1600 [00:02<00:11, 115.65 examples/s]
Map:  16%|█▋        | 263/1600 [00:02<00:11, 118.75 examples/s]
Map:  17%|█▋        | 279/1600 [00:02<00:10, 124.87 examples/s]
Map:  18%|█▊        | 294/1600 [00:02<00:10, 125.89 examples/s]
Map:  19%|█▉        | 310/1600 [00:02<00:09, 132.21 examples/s]
Map:  20%|██        | 325/1600 [00:03<00:09, 136.78 examples/s]
Map:  21%|██        | 339/1600 [00:03<00:11, 111.51 examples/s]
Map:  22%|██▏       | 355/1600 [00:03<00:10, 120.05 examples/s]
Map:  23%|██▎       | 373/1600 [00:03<00:10, 116.79 examples/s]
Map:  24%|██▍       | 392/1600 [00:03<00:09, 130.80 examples/s]
Map:  26%|██▌       | 408/1600 [00:03<00:10, 117.99 examples/s]
Map:  27%|██▋       | 425/1600 [00:03<00:10, 114.39 examples/s]
Map:  28%|██▊       | 440/1600 [00:04<00:09, 120.85 examples/s]
Map:  29%|██▊       | 458/1600 [00:04<00:10, 111.33 examples/s]
Map:  30%|██▉       | 476/1600 [00:04<00:09, 123.73 examples/s]
Map:  31%|███       | 495/1600 [00:04<00:08, 136.02 examples/s]
Map:  32%|███▏      | 512/1600 [00:04<00:08, 124.49 examples/s]
Map:  33%|███▎      | 528/1600 [00:04<00:09, 115.47 examples/s]
Map:  34%|███▍      | 541/1600 [00:04<00:10, 97.13 examples/s] 
Map:  35%|███▌      | 560/1600 [00:05<00:09, 113.35 examples/s]
Map:  36%|███▌      | 578/1600 [00:05<00:08, 125.19 examples/s]
Map:  37%|███▋      | 595/1600 [00:05<00:07, 131.62 examples/s]
Map:  38%|███▊      | 612/1600 [00:05<00:07, 137.07 examples/s]
Map:  39%|███▉      | 627/1600 [00:05<00:06, 139.51 examples/s]
Map:  40%|████      | 645/1600 [00:05<00:08, 117.99 examples/s]
Map:  41%|████      | 659/1600 [00:05<00:07, 122.88 examples/s]
Map:  42%|████▏     | 675/1600 [00:06<00:08, 113.00 examples/s]
Map:  43%|████▎     | 692/1600 [00:06<00:08, 108.24 examples/s]
Map:  44%|████▍     | 704/1600 [00:06<00:08, 108.36 examples/s]
Map:  45%|████▌     | 724/1600 [00:06<00:06, 126.90 examples/s]
Map:  46%|████▋     | 742/1600 [00:06<00:07, 113.33 examples/s]
Map:  47%|████▋     | 758/1600 [00:06<00:07, 108.67 examples/s]
Map:  48%|████▊     | 773/1600 [00:06<00:08, 102.22 examples/s]
Map:  49%|████▉     | 785/1600 [00:07<00:08, 101.54 examples/s]
Map:  50%|█████     | 800/1600 [00:07<00:08, 96.25 examples/s] 
Map:  51%|█████     | 814/1600 [00:07<00:07, 104.84 examples/s]
Map:  52%|█████▏    | 826/1600 [00:07<00:07, 107.50 examples/s]
Map:  52%|█████▏    | 838/1600 [00:07<00:07, 96.71 examples/s] 
Map:  53%|█████▎    | 854/1600 [00:07<00:06, 108.38 examples/s]
Map:  55%|█████▍    | 872/1600 [00:07<00:05, 123.03 examples/s]
Map:  55%|█████▌    | 885/1600 [00:07<00:05, 123.52 examples/s]
Map:  56%|█████▋    | 901/1600 [00:08<00:05, 131.25 examples/s]
Map:  57%|█████▊    | 920/1600 [00:08<00:05, 126.42 examples/s]
Map:  58%|█████▊    | 935/1600 [00:08<00:12, 52.69 examples/s] 
Map:  59%|█████▉    | 951/1600 [00:09<00:10, 64.69 examples/s]
Map:  60%|██████    | 963/1600 [00:09<00:08, 72.59 examples/s]
Map:  61%|██████▏   | 980/1600 [00:09<00:07, 81.60 examples/s]
Map:  62%|██████▏   | 992/1600 [00:09<00:07, 85.57 examples/s]
Map:  62%|██████▏   | 992/1600 [00:29<00:07, 85.57 examples/s]
Map:  62%|██████▎   | 1000/1600 [01:03<13:11,  1.32s/ examples]
Map:  64%|██████▎   | 1016/1600 [01:03<08:18,  1.17 examples/s]
Map:  65%|██████▍   | 1034/1600 [01:04<05:09,  1.83 examples/s]
Map:  66%|██████▌   | 1048/1600 [01:04<03:36,  2.56 examples/s]
Map:  66%|██████▋   | 1062/1600 [01:04<02:30,  3.58 examples/s]
Map:  68%|██████▊   | 1081/1600 [01:04<01:34,  5.50 examples/s]
Map:  68%|██████▊   | 1095/1600 [01:04<01:07,  7.47 examples/s]
Map:  70%|██████▉   | 1112/1600 [01:04<00:45, 10.68 examples/s]
Map:  70%|███████   | 1126/1600 [01:04<00:33, 14.25 examples/s]
Map:  72%|███████▏  | 1144/1600 [01:05<00:23, 19.21 examples/s]
Map:  73%|███████▎  | 1162/1600 [01:05<00:16, 26.88 examples/s]
Map:  74%|███████▎  | 1178/1600 [01:05<00:12, 34.99 examples/s]
Map:  74%|███████▍  | 1192/1600 [01:05<00:09, 43.52 examples/s]
Map:  76%|███████▌  | 1208/1600 [01:05<00:07, 55.53 examples/s]
Map:  76%|███████▋  | 1224/1600 [01:05<00:05, 63.38 examples/s]
Map:  78%|███████▊  | 1242/1600 [01:05<00:04, 72.18 examples/s]
Map:  78%|███████▊  | 1255/1600 [01:06<00:05, 63.61 examples/s]
Map:  80%|███████▉  | 1272/1600 [01:06<00:04, 75.51 examples/s]
Map:  80%|████████  | 1285/1600 [01:06<00:03, 81.59 examples/s]
Map:  81%|████████  | 1298/1600 [01:06<00:03, 87.83 examples/s]
Map:  82%|████████▏ | 1313/1600 [01:06<00:02, 98.52 examples/s]
Map:  83%|████████▎ | 1329/1600 [01:06<00:02, 109.39 examples/s]
Map:  84%|████████▍ | 1346/1600 [01:06<00:02, 123.20 examples/s]
Map:  85%|████████▌ | 1360/1600 [01:07<00:01, 123.53 examples/s]
Map:  86%|████████▌ | 1379/1600 [01:07<00:02, 89.71 examples/s] 
Map:  87%|████████▋ | 1391/1600 [01:07<00:02, 93.56 examples/s]
Map:  88%|████████▊ | 1412/1600 [01:07<00:01, 103.38 examples/s]
Map:  89%|████████▉ | 1425/1600 [01:07<00:01, 108.70 examples/s]
Map:  90%|█████████ | 1443/1600 [01:07<00:01, 124.12 examples/s]
Map:  91%|█████████▏| 1463/1600 [01:07<00:01, 124.07 examples/s]
Map:  92%|█████████▎| 1480/1600 [01:08<00:01, 91.27 examples/s] 
Map:  93%|█████████▎| 1492/1600 [01:08<00:01, 95.13 examples/s]
Map:  94%|█████████▍| 1506/1600 [01:08<00:00, 99.62 examples/s]
Map:  95%|█████████▍| 1519/1600 [01:08<00:00, 104.01 examples/s]
Map:  96%|█████████▌| 1532/1600 [01:08<00:00, 108.51 examples/s]
Map:  97%|█████████▋| 1545/1600 [01:08<00:00, 111.11 examples/s]
Map:  97%|█████████▋| 1558/1600 [01:08<00:00, 107.74 examples/s]
Map:  98%|█████████▊| 1572/1600 [01:09<00:00, 112.09 examples/s]
Map:  99%|█████████▉| 1587/1600 [01:09<00:00, 98.41 examples/s] 
Map: 100%|█████████▉| 1594/1600 [01:20<00:00, 98.41 examples/s]
Map: 100%|██████████| 1600/1600 [01:43<00:00,  1.31 examples/s]
Map: 100%|██████████| 1600/1600 [01:43<00:00, 15.40 examples/s]

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▏         | 3/200 [00:00<00:07, 25.73 examples/s]
Map:   4%|▎         | 7/200 [00:00<00:06, 30.70 examples/s]
Map:  10%|█         | 20/200 [00:00<00:02, 68.94 examples/s]
Map:  15%|█▌        | 30/200 [00:00<00:02, 77.01 examples/s]
Map:  20%|██        | 41/200 [00:00<00:01, 84.41 examples/s]
Map:  25%|██▌       | 50/200 [00:00<00:01, 79.35 examples/s]
Map:  33%|███▎      | 66/200 [00:00<00:01, 101.03 examples/s]
Map:  40%|███▉      | 79/200 [00:00<00:01, 107.02 examples/s]
Map:  46%|████▌     | 92/200 [00:01<00:00, 108.74 examples/s]
Map:  52%|█████▎    | 105/200 [00:01<00:00, 97.73 examples/s]
Map:  62%|██████▏   | 124/200 [00:01<00:00, 115.93 examples/s]
Map:  70%|███████   | 141/200 [00:01<00:00, 128.74 examples/s]
Map:  78%|███████▊  | 156/200 [00:01<00:00, 116.61 examples/s]
Map:  86%|████████▋ | 173/200 [00:01<00:00, 126.36 examples/s]
Map:  96%|█████████▌| 192/200 [00:01<00:00, 139.36 examples/s]
Map: 100%|██████████| 200/200 [00:12<00:00, 15.82 examples/s] 

Map:   0%|          | 0/200 [00:00<?, ? examples/s]
Map:   2%|▏         | 3/200 [00:00<00:07, 27.67 examples/s]
Map:   4%|▍         | 8/200 [00:00<00:05, 32.33 examples/s]
Map:  10%|█         | 20/200 [00:00<00:02, 65.12 examples/s]
Map:  16%|█▋        | 33/200 [00:00<00:01, 83.67 examples/s]
Map:  23%|██▎       | 46/200 [00:00<00:01, 93.86 examples/s]
Map:  29%|██▉       | 58/200 [00:00<00:01, 99.38 examples/s]
Map:  38%|███▊      | 75/200 [00:00<00:01, 117.95 examples/s]
Map:  45%|████▌     | 90/200 [00:00<00:00, 123.66 examples/s]
Map:  52%|█████▎    | 105/200 [00:01<00:00, 108.51 examples/s]
Map:  62%|██████▏   | 124/200 [00:01<00:00, 125.56 examples/s]
Map:  69%|██████▉   | 138/200 [00:01<00:00, 123.62 examples/s]
Map:  76%|███████▋  | 153/200 [00:01<00:00, 129.69 examples/s]
Map:  84%|████████▍ | 168/200 [00:01<00:00, 132.34 examples/s]
Map:  93%|█████████▎| 186/200 [00:01<00:00, 139.83 examples/s]
Map: 100%|██████████| 200/200 [00:11<00:00, 17.19 examples/s] 
Loading en eval data: fold_4...
Preprocess en fold_4 data for cn model
Loading en test data: fold_4...
Preprocess en fold_4 data for cn model
Use cn model to add lora
================== SET ALL PARAMS =====================
modified_wav2vec2.base_model.model.masked_spec_embed: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.1.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.2.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.3.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.4.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.5.conv.weight: False
modified_wav2vec2.base_model.model.feature_extractor.conv_layers.6.conv.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.weight: False
modified_wav2vec2.base_model.model.feature_projection.layer_norm.bias: False
modified_wav2vec2.base_model.model.feature_projection.projection.weight: False
modified_wav2vec2.base_model.model.feature_projection.projection.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.bias: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_g: False
modified_wav2vec2.base_model.model.encoder.pos_conv_embed.conv.weight_v: False
modified_wav2vec2.base_model.model.encoder.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.0.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.0.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.0.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.1.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.1.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.1.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.2.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.2.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.2.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.3.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.3.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.3.bottleneck_adaptor.up.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.k_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.v_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.q_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.attention.out_proj.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.base_layer.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_A.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.feed_forward.output_dense.lora_B.default.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.weight: False
modified_wav2vec2.base_model.model.encoder.layers.4.final_layer_norm.bias: False
modified_wav2vec2.base_model.model.encoder.layers.4.weighted_gate.gate: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.down.bias: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.weight: True
modified_wav2vec2.base_model.model.encoder.layers.4.bottleneck_adaptor.up.bias: True
normal_classifier.dense1.weight: True
normal_classifier.dense1.bias: True
normal_classifier.dense.weight: True
normal_classifier.dense.bias: True
normal_classifier.out.weight: True
normal_classifier.out.bias: True
Set optimizer and criterion
Epoch 1/100

Training Phase:
Training loss: 1242.1862, Training accuracy: 0.6987
Macro F1-score: 0.6978
Model performance on Angry speech (in training): 
	Precision: 0.7744, Recall: 0.7550, F1_score: 0.7646
Model performance on Happy speech (in training): 
	Precision: 0.6604, Recall: 0.6125, F1_score: 0.6355
Model performance on Neutral speech (in training): 
	Precision: 0.6262, Recall: 0.6325, F1_score: 0.6294
Model performance on Sad speech (in training): 
	Precision: 0.7310, Recall: 0.7950, F1_score: 0.7617

Eval Phase: 
Validation loss: 146.6288, Validation accuracy: 0.7050
Macro F1-score: 0.7002
Model performance on Angry speech (in validation): 
	Precision: 0.8148, Recall: 0.8800, F1_score: 0.8462
Model performance on Happy speech (in validation): 
	Precision: 0.6744, Recall: 0.5800, F1_score: 0.6237
Model performance on Neutral speech (in validation): 
	Precision: 0.7179, Recall: 0.5600, F1_score: 0.6292
Model performance on Sad speech (in validation): 
	Precision: 0.6250, Recall: 0.8000, F1_score: 0.7018
New best accuracy for layer 4 on epoch 1: 0.7050. Model saved.
Epoch 2/100

Training Phase:

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 234/1600 [00:10<00:58, 23.28it/s]
Training:  29%|██▉       | 467/1600 [00:20<00:49, 22.93it/s]
Training:  44%|████▍     | 700/1600 [00:30<00:39, 23.06it/s]
Training:  58%|█████▊    | 933/1600 [00:40<00:28, 23.02it/s]
Training:  73%|███████▎  | 1166/1600 [00:50<00:18, 23.09it/s]
Training:  87%|████████▋ | 1398/1600 [01:00<00:08, 23.01it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 237/1600 [00:10<00:57, 23.64it/s]
Training:  30%|██▉       | 474/1600 [00:20<00:47, 23.52it/s]
Training:  44%|████▍     | 709/1600 [00:30<00:38, 23.20it/s]
Training:  59%|█████▊    | 938/1600 [00:40<00:28, 23.07it/s]
Training:  73%|███████▎  | Training loss: 1124.9994, Training accuracy: 0.7156
Macro F1-score: 0.7140
Model performance on Angry speech (in training): 
	Precision: 0.7920, Recall: 0.7900, F1_score: 0.7910
Model performance on Happy speech (in training): 
	Precision: 0.7037, Recall: 0.6175, F1_score: 0.6578
Model performance on Neutral speech (in training): 
	Precision: 0.6368, Recall: 0.6400, F1_score: 0.6384
Model performance on Sad speech (in training): 
	Precision: 0.7277, Recall: 0.8150, F1_score: 0.7689

Eval Phase: 
Validation loss: 145.3508, Validation accuracy: 0.7150
Macro F1-score: 0.7082
Model performance on Angry speech (in validation): 
	Precision: 0.8750, Recall: 0.8400, F1_score: 0.8571
Model performance on Happy speech (in validation): 
	Precision: 0.7857, Recall: 0.4400, F1_score: 0.5641
Model performance on Neutral speech (in validation): 
	Precision: 0.6094, Recall: 0.7800, F1_score: 0.6842
Model performance on Sad speech (in validation): 
	Precision: 0.6667, Recall: 0.8000, F1_score: 0.7273
New best accuracy for layer 4 on epoch 2: 0.7150. Model saved.
Epoch 3/100

Training Phase:
Training loss: 1027.9006, Training accuracy: 0.7412
Macro F1-score: 0.7401
Model performance on Angry speech (in training): 
	Precision: 0.7951, Recall: 0.8050, F1_score: 0.8000
Model performance on Happy speech (in training): 
	Precision: 0.7573, Recall: 0.6475, F1_score: 0.6981
Model performance on Neutral speech (in training): 
	Precision: 0.6643, Recall: 0.6875, F1_score: 0.6757
Model performance on Sad speech (in training): 
	Precision: 0.7517, Recall: 0.8250, F1_score: 0.7867

Eval Phase: 
Validation loss: 151.9111, Validation accuracy: 0.6800
Macro F1-score: 0.6737
Model performance on Angry speech (in validation): 
	Precision: 0.8571, Recall: 0.8400, F1_score: 0.8485
Model performance on Happy speech (in validation): 
	Precision: 0.6923, Recall: 0.5400, F1_score: 0.6067
Model performance on Neutral speech (in validation): 
	Precision: 0.6486, Recall: 0.4800, F1_score: 0.5517
Model performance on Sad speech (in validation): 
	Precision: 0.5733, Recall: 0.8600, F1_score: 0.6880
Epoch 4/100

Training Phase:
1168/1600 [00:50<00:18, 23.04it/s]
Training:  87%|████████▋ | 1398/1600 [01:00<00:08, 22.95it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 229/1600 [00:10<01:00, 22.78it/s]
Training:  29%|██▉       | 460/1600 [00:20<00:49, 22.92it/s]
Training:  43%|████▎     | 691/1600 [00:30<00:39, 23.00it/s]
Training:  58%|█████▊    | 922/1600 [00:40<00:29, 22.65it/s]
Training:  72%|███████▏  | 1155/1600 [00:50<00:19, 22.86it/s]
Training:  87%|████████▋ | 1398/1600 [01:00<00:08, 23.34it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 230/16Training loss: 917.4580, Training accuracy: 0.7731
Macro F1-score: 0.7717
Model performance on Angry speech (in training): 
	Precision: 0.8317, Recall: 0.8275, F1_score: 0.8296
Model performance on Happy speech (in training): 
	Precision: 0.7781, Recall: 0.6750, F1_score: 0.7229
Model performance on Neutral speech (in training): 
	Precision: 0.7207, Recall: 0.7225, F1_score: 0.7216
Model performance on Sad speech (in training): 
	Precision: 0.7643, Recall: 0.8675, F1_score: 0.8126

Eval Phase: 
Validation loss: 163.6547, Validation accuracy: 0.6900
Macro F1-score: 0.6821
Model performance on Angry speech (in validation): 
	Precision: 0.8148, Recall: 0.8800, F1_score: 0.8462
Model performance on Happy speech (in validation): 
	Precision: 0.6471, Recall: 0.4400, F1_score: 0.5238
Model performance on Neutral speech (in validation): 
	Precision: 0.6271, Recall: 0.7400, F1_score: 0.6789
Model performance on Sad speech (in validation): 
	Precision: 0.6604, Recall: 0.7000, F1_score: 0.6796
Epoch 5/100

Training Phase:
00 [00:10<00:59, 22.94it/s]
Training:  29%|██▉       | 463/1600 [00:20<00:49, 23.12it/s]
Training:  44%|████▎     | 699/1600 [00:30<00:38, 23.30it/s]
Training:  58%|█████▊    | 935/1600 [00:40<00:28, 23.10it/s]
Training:  73%|███████▎  | 1164/1600 [00:50<00:18, 23.02it/s]
Training:  87%|████████▋ | 1397/1600 [01:00<00:08, 23.11it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 229/1600 [00:10<01:00, 22.84it/s]
Training:  29%|██▊       | 458/1600 [00:20<00:49, 22.87it/s]
Training:  43%|████▎     | 689/1600 [00:30<00:39, 22.97it/s]
Training:  57%|█████▊    | 920/1600 [00:40<00:30, 22.53it/s]
Training:  71%|███████   | 1139/1600 [00:50<00:20, 22.27it/s]
Training:  85%|████████▌ | 1360/1600 [Training loss: 806.6459, Training accuracy: 0.8094
Macro F1-score: 0.8088
Model performance on Angry speech (in training): 
	Precision: 0.8557, Recall: 0.8450, F1_score: 0.8503
Model performance on Happy speech (in training): 
	Precision: 0.8182, Recall: 0.7425, F1_score: 0.7785
Model performance on Neutral speech (in training): 
	Precision: 0.7687, Recall: 0.7725, F1_score: 0.7706
Model performance on Sad speech (in training): 
	Precision: 0.7977, Recall: 0.8775, F1_score: 0.8357

Eval Phase: 
Validation loss: 172.2027, Validation accuracy: 0.7050
Macro F1-score: 0.6981
Model performance on Angry speech (in validation): 
	Precision: 0.7231, Recall: 0.9400, F1_score: 0.8174
Model performance on Happy speech (in validation): 
	Precision: 0.6818, Recall: 0.6000, F1_score: 0.6383
Model performance on Neutral speech (in validation): 
	Precision: 0.7778, Recall: 0.5600, F1_score: 0.6512
Model performance on Sad speech (in validation): 
	Precision: 0.6545, Recall: 0.7200, F1_score: 0.6857
Epoch 6/100

Training Phase:
01:00<00:10, 22.21it/s]
Training:  99%|█████████▉| 1581/1600 [01:10<00:00, 22.16it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 206/1600 [00:10<01:08, 20.48it/s]
Training:  26%|██▌       | 417/1600 [00:20<00:56, 20.84it/s]
Training:  39%|███▉      | 631/1600 [00:30<00:45, 21.08it/s]
Training:  53%|█████▎    | 853/1600 [00:40<00:34, 21.51it/s]
Training:  53%|█████▎    | 853/1600 [00:50<00:34, 21.51it/s]
Training:  67%|██████▋   | 1073/1600 [00:50<00:24, 21.54it/s]
Training:  67%|██████▋   | 1073/1600 [01:00<00:24, 21.54it/s]
Training:  80%|███████▉  | 1272/1600 [01:00<00:15, 20.90it/s]
Training:  93%|█████████▎| 1495/1600 [01:10<00:04, 21.34it/s]
                                        Training loss: 712.6082, Training accuracy: 0.8300
Macro F1-score: 0.8293
Model performance on Angry speech (in training): 
	Precision: 0.8533, Recall: 0.8725, F1_score: 0.8628
Model performance on Happy speech (in training): 
	Precision: 0.8347, Recall: 0.7700, F1_score: 0.8010
Model performance on Neutral speech (in training): 
	Precision: 0.7870, Recall: 0.7850, F1_score: 0.7860
Model performance on Sad speech (in training): 
	Precision: 0.8440, Recall: 0.8925, F1_score: 0.8676

Eval Phase: 
Validation loss: 196.1733, Validation accuracy: 0.6800
Macro F1-score: 0.6783
Model performance on Angry speech (in validation): 
	Precision: 0.8409, Recall: 0.7400, F1_score: 0.7872
Model performance on Happy speech (in validation): 
	Precision: 0.6757, Recall: 0.5000, F1_score: 0.5747
Model performance on Neutral speech (in validation): 
	Precision: 0.6230, Recall: 0.7600, F1_score: 0.6847
Model performance on Sad speech (in validation): 
	Precision: 0.6207, Recall: 0.7200, F1_score: 0.6667
Epoch 7/100

Training Phase:
Training loss: 645.0739, Training accuracy: 0.8481
Macro F1-score: 0.8477
Model performance on Angry speech (in training): 
	Precision: 0.8886, Recall: 0.8975, F1_score: 0.8930
Model performance on Happy speech (in training): 
	Precision: 0.8548, Recall: 0.7950, F1_score: 0.8238
Model performance on Neutral speech (in training): 
	Precision: 0.8045, Recall: 0.8025, F1_score: 0.8035
Model performance on Sad speech (in training): 
	Precision: 0.8447, Recall: 0.8975, F1_score: 0.8703

Eval Phase: 
Validation loss: 196.6206, Validation accuracy: 0.7150
Macro F1-score: 0.7147
Model performance on Angry speech (in validation): 
	Precision: 0.8600, Recall: 0.8600, F1_score: 0.8600
Model performance on Happy speech (in validation): 
	Precision: 0.6154, Recall: 0.6400, F1_score: 0.6275
Model performance on Neutral speech (in validation): 
	Precision: 0.6727, Recall: 0.7400, F1_score: 0.7048
Model performance on Sad speech (in validation): 
	Precision: 0.7209, Recall: 0.6200, F1_score: 0.6667
Epoch 8/100

Training Phase:
                     

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  15%|█▍        | 236/1600 [00:10<00:57, 23.57it/s]
Training:  30%|██▉       | 472/1600 [00:20<00:48, 23.12it/s]
Training:  44%|████▍     | 701/1600 [00:30<00:39, 22.51it/s]
Training:  59%|█████▉    | 940/1600 [00:40<00:28, 23.05it/s]
Training:  74%|███████▎  | 1179/1600 [00:51<00:18, 23.19it/s]
Training:  88%|████████▊ | 1414/1600 [01:01<00:07, 23.25it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 223/1600 [00:10<01:01, 22.28it/s]
Training:  28%|██▊       | 448/1600 [00:20<00:51, 22.41it/s]
Training:  42%|████▏     | 676/1600 [00:30<00:40, Training loss: 535.2816, Training accuracy: 0.8712
Macro F1-score: 0.8710
Model performance on Angry speech (in training): 
	Precision: 0.9032, Recall: 0.9100, F1_score: 0.9066
Model performance on Happy speech (in training): 
	Precision: 0.8799, Recall: 0.8425, F1_score: 0.8608
Model performance on Neutral speech (in training): 
	Precision: 0.8283, Recall: 0.8200, F1_score: 0.8241
Model performance on Sad speech (in training): 
	Precision: 0.8732, Recall: 0.9125, F1_score: 0.8924

Eval Phase: 
Validation loss: 235.4062, Validation accuracy: 0.6550
Macro F1-score: 0.6433
Model performance on Angry speech (in validation): 
	Precision: 0.6575, Recall: 0.9600, F1_score: 0.7805
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.4800, F1_score: 0.5581
Model performance on Neutral speech (in validation): 
	Precision: 0.7429, Recall: 0.5200, F1_score: 0.6118
Model performance on Sad speech (in validation): 
	Precision: 0.5893, Recall: 0.6600, F1_score: 0.6226
Epoch 9/100

Training Phase:
22.56it/s]
Training:  57%|█████▋    | 911/1600 [00:40<00:30, 22.92it/s]
Training:  72%|███████▏  | 1146/1600 [00:50<00:20, 22.61it/s]
Training:  85%|████████▌ | 1367/1600 [01:00<00:10, 22.41it/s]
Training: 100%|█████████▉| 1592/1600 [01:10<00:00, 22.44it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 228/1600 [00:10<01:00, 22.65it/s]
Training:  28%|██▊       | 455/1600 [00:20<00:51, 22.42it/s]
Training:  42%|████▏     | 678/1600 [00:31<00:43, 21.34it/s]
Training:  55%|█████▌    | 885/1600 [00:41<00:33, 21.09it/s]
Training:  69%|██████▊   | 1098/1600 [00:51<00:23, 21.13it/s]
Training:  82%|████████▏ | 1311/1600 [01:01<00:13, 20.95it/s]
Training:  95%|█████████▍| 15Training loss: 503.3209, Training accuracy: 0.8744
Macro F1-score: 0.8739
Model performance on Angry speech (in training): 
	Precision: 0.8978, Recall: 0.9225, F1_score: 0.9100
Model performance on Happy speech (in training): 
	Precision: 0.8806, Recall: 0.8300, F1_score: 0.8546
Model performance on Neutral speech (in training): 
	Precision: 0.8452, Recall: 0.8325, F1_score: 0.8388
Model performance on Sad speech (in training): 
	Precision: 0.8732, Recall: 0.9125, F1_score: 0.8924

Eval Phase: 
Validation loss: 246.8009, Validation accuracy: 0.6400
Macro F1-score: 0.6333
Model performance on Angry speech (in validation): 
	Precision: 0.7500, Recall: 0.7800, F1_score: 0.7647
Model performance on Happy speech (in validation): 
	Precision: 0.6667, Recall: 0.4000, F1_score: 0.5000
Model performance on Neutral speech (in validation): 
	Precision: 0.5362, Recall: 0.7400, F1_score: 0.6218
Model performance on Sad speech (in validation): 
	Precision: 0.6531, Recall: 0.6400, F1_score: 0.6465
Epoch 10/100

Training Phase:
Training loss: 426.6749, Training accuracy: 0.9050
Macro F1-score: 0.9050
Model performance on Angry speech (in training): 
	Precision: 0.9347, Recall: 0.9300, F1_score: 0.9323
Model performance on Happy speech (in training): 
	Precision: 0.9077, Recall: 0.8850, F1_score: 0.8962
Model performance on Neutral speech (in training): 
	Precision: 0.8866, Recall: 0.8800, F1_score: 0.8833
Model performance on Sad speech (in training): 
	Precision: 0.8916, Recall: 0.9250, F1_score: 0.9080

Eval Phase: 
19/1600 [01:11<00:03, 20.88it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 205/1600 [00:10<01:08, 20.40it/s]
Training:  26%|██▌       | 415/1600 [00:20<00:57, 20.71it/s]
Training:  39%|███▉      | 627/1600 [00:30<00:46, 20.93it/s]
Training:  39%|███▉      | 627/1600 [00:40<00:46, 20.93it/s]
Training:  52%|█████▏    | 835/1600 [00:40<00:36, 20.69it/s]
Training:  65%|██████▌   | 1047/1600 [00:50<00:26, 20.85it/s]
Training:  65%|██████▌   | 1047/1600 [01:00<00:26, 20.85it/s]
Training:  78%|███████▊  | 1253/1600 [01:00<00:16, 20.74it/s]
Training:  92%|█████████▏| 1467/1600 [01:10<00:06, 20.93it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
  Validation loss: 237.2868, Validation accuracy: 0.6750
Macro F1-score: 0.6742
Model performance on Angry speech (in validation): 
	Precision: 0.8235, Recall: 0.8400, F1_score: 0.8317
Model performance on Happy speech (in validation): 
	Precision: 0.6170, Recall: 0.5800, F1_score: 0.5979
Model performance on Neutral speech (in validation): 
	Precision: 0.6327, Recall: 0.6200, F1_score: 0.6263
Model performance on Sad speech (in validation): 
	Precision: 0.6226, Recall: 0.6600, F1_score: 0.6408
Epoch 11/100

Training Phase:
Training loss: 385.2997, Training accuracy: 0.9087
Macro F1-score: 0.9088
Model performance on Angry speech (in training): 
	Precision: 0.9244, Recall: 0.9175, F1_score: 0.9210
Model performance on Happy speech (in training): 
	Precision: 0.9093, Recall: 0.9025, F1_score: 0.9059
Model performance on Neutral speech (in training): 
	Precision: 0.8747, Recall: 0.8900, F1_score: 0.8823
Model performance on Sad speech (in training): 
	Precision: 0.9273, Recall: 0.9250, F1_score: 0.9262

Eval Phase: 
Validation loss: 271.0489, Validation accuracy: 0.6550
Macro F1-score: 0.6541
Model performance on Angry speech (in validation): 
	Precision: 0.8235, Recall: 0.8400, F1_score: 0.8317
Model performance on Happy speech (in validation): 
	Precision: 0.5439, Recall: 0.6200, F1_score: 0.5794
Model performance on Neutral speech (in validation): 
	Precision: 0.6341, Recall: 0.5200, F1_score: 0.5714
Model performance on Sad speech (in validation): 
	Precision: 0.6275, Recall: 0.6400, F1_score: 0.6337
Epoch 12/100

Training Phase:
                                                 

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  13%|█▎        | 207/1600 [00:10<01:07, 20.63it/s]
Training:  26%|██▌       | 414/1600 [00:20<00:57, 20.48it/s]
Training:  39%|███▊      | 618/1600 [00:30<00:48, 20.14it/s]
Training:  52%|█████▏    | 834/1600 [00:40<00:37, 20.68it/s]
Training:  66%|██████▌   | 1050/1600 [00:50<00:26, 20.78it/s]
Training:  79%|███████▉  | 1262/1600 [01:00<00:16, 20.88it/s]
Training:  93%|█████████▎| 1485/1600 [01:10<00:05, 21.31it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Training:   0%|          | 0/1600 [00:00<?, ?it/s]
Training:  14%|█▍        | 222/1600 [00:10<01:02, 22.13it/s]
Training:  28%|██▊       | 444/1600 [00:20<00:52, 21.84it/s]
Training:  42%|████▏     | 677/1600 [00:30<00:41, 2Training loss: 324.7720, Training accuracy: 0.9275
Macro F1-score: 0.9274
Model performance on Angry speech (in training): 
	Precision: 0.9521, Recall: 0.9450, F1_score: 0.9486
Model performance on Happy speech (in training): 
	Precision: 0.9300, Recall: 0.9300, F1_score: 0.9300
Model performance on Neutral speech (in training): 
	Precision: 0.9082, Recall: 0.8900, F1_score: 0.8990
Model performance on Sad speech (in training): 
	Precision: 0.9197, Recall: 0.9450, F1_score: 0.9322

Eval Phase: 
Validation loss: 296.1290, Validation accuracy: 0.6350
Macro F1-score: 0.6276
Model performance on Angry speech (in validation): 
	Precision: 0.7800, Recall: 0.7800, F1_score: 0.7800
Model performance on Happy speech (in validation): 
	Precision: 0.5938, Recall: 0.3800, F1_score: 0.4634
Model performance on Neutral speech (in validation): 
	Precision: 0.5441, Recall: 0.7400, F1_score: 0.6271
Model performance on Sad speech (in validation): 
	Precision: 0.6400, Recall: 0.6400, F1_score: 0.6400
Validation loss does not decrease for 10 epochs. End training.
Model best accuracy on validation set: 0.7150

Test Phase: 
2.44it/s]
Training:  57%|█████▋    | 910/1600 [00:40<00:30, 22.74it/s]
Training:  71%|███████▏  | 1143/1600 [00:50<00:20, 22.78it/s]
Training:  86%|████████▋ | 1383/1600 [01:00<00:09, 23.15it/s]
                                                             

Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]
                                                   

Testing:   0%|          | 0/200 [00:00<?, ?it/s]
Testing:   0%|          | 1/200 [00:00<00:23,  8.51it/s]
Testing:   2%|▏         | 4/200 [00:00<00:10, 18.98it/s]
Testing:   3%|▎         | 6/200 [00:00<00:11, 17.42it/s]
Testing:   4%|▍         | 9/200 [00:00<00:10, 19.02it/s]
Testing:   6%|▌         | 12/200 [00:00<00:08, 22.01it/s]
Testing:   8%|▊         | 17/200 [00:00<00:06, 30.01it/s]
Testing:  10%|█         | 21/200 [00:00<00:05, 31.23it/s]
Testing:  13%|█▎        | 26/200 [00:00<00:05, 32.69it/s]
Testing:  16%|█▌        | 31/200 [00:01<00:05, 33.61it/s]
Testing:  18%|█▊        | 36/200 [00:01<00:04, 34.47it/s]
Testing:  20%|██        | 41/200 [00:01<00:04, 36.21it/s]
Testing:  23%|██▎       | 46/200 [00:01<00:03, 38.69it/s]
Testing:  26%|██▌       | 51/200 [00:01<00:03, 40.99it/s]
Testing:  28%|██▊       | 57/200 [00:01<00:03, 45.39it/s]
Testing:  33%|███▎      | 66/200 [00:01<00:02, 53.71it/s]
Testing:  36%|███▌      | 72/200 [00:01<00:02, 54.41it/s]
Testing:  40%|███▉      | 79/200 [00:02<00:02, 57.06it/s]
Testing:  43%|████▎     | 86/200 [00:02<00:01, 58.79it/s]
Testing:  47%|████▋     | 94/200 [00:02<00:01, 63.21it/s]
Testing:  50%|█████     | 101/200 [00:02<00:01, 62.24it/s]
Testing:  54%|█████▍    | 108/200 [00:02<00:01, 53.86it/s]
Testing:  57%|█████▋    | 114/200 [00:02<00:01, 53.09it/s]
Testing:  62%|██████▏   | 124/200 [00:02<00:01, 64.60it/s]
Testing:  66%|██████▋   | 133/200 [00:02<00:00, 69.92it/s]
Testing:  70%|███████   | 141/2Test loss: 125.0405, Test accuracy: 0.7550
Macro F1-score: 0.7508
Model performance on Angry speech (in test): 
	Precision: 0.7925, Recall: 0.8400, F1_score: 0.8155
Model performance on Happy speech (in test): 
	Precision: 0.8750, Recall: 0.5600, F1_score: 0.6829
Model performance on Neutral speech (in test): 
	Precision: 0.6727, Recall: 0.7400, F1_score: 0.7048
Model performance on Sad speech (in test): 
	Precision: 0.7333, Recall: 0.8800, F1_score: 0.8000

cn, all folds layer accuracy: ['0.6450', '0.5950', '0.6350', '0.7550', '0.7550']
cn, all emo precision: {'Angry': ['0.8571', '0.5222', '0.6610', '0.6923', '0.7925'], 'Happy': ['0.5179', '0.5714', '0.7826', '0.8182', '0.8750'], 'Neutral': ['0.6667', '0.5918', '0.5870', '0.8125', '0.6727'], 'Sad': ['0.6111', '0.7750', '0.5972', '0.7458', '0.7333']}
cn, all emo recall: {'Angry': ['0.7200', '0.9400', '0.7800', '0.9000', '0.8400'], 'Happy': ['0.5800', '0.2400', '0.3600', '0.7200', '0.5600'], 'Neutral': ['0.4000', '0.5800', '0.5400', '0.5200', '0.7400'], 'Sad': ['0.8800', '0.6200', '0.8600', '0.8800', '0.8800']}
cn, all emo f1score: {'Angry': ['0.7826', '0.6714', '0.7156', '0.7826', '0.8155'], 'Happy': ['0.5472', '0.3380', '0.4932', '0.7660', '0.6829'], 'Neutral': ['0.5000', '0.5859', '0.5625', '0.6341', '0.7048'], 'Sad': ['0.7213', '0.6889', '0.7049', '0.8073', '0.8000']}
00 [00:03<00:00, 59.13it/s]
Testing:  76%|███████▌  | 151/200 [00:03<00:00, 67.21it/s]
Testing:  80%|████████  | 161/200 [00:03<00:00, 73.86it/s]
Testing:  85%|████████▌ | 170/200 [00:03<00:00, 77.34it/s]
Testing:  90%|████████▉ | 179/200 [00:03<00:00, 78.03it/s]
Testing:  94%|█████████▍| 188/200 [00:03<00:00, 68.56it/s]
Testing:  98%|█████████▊| 196/200 [00:03<00:00, 70.51it/s]
                                                          
